<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>对话模型与非对话模型详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在大型语言模型 (LLM) 的领域中，”对话模型” (Chat Models) 和 “非对话模型” (或称为 “文本模型” Text Models) 是两种基本但又有所区别的模型范式，它们在设计、训练数据、输入&#x2F;输出格式以及最佳应用场景上存在差异。理解这两种模型的区别是有效利用 LLM 进行开发的关键。  核心思想：对话模型优化用于多轮、上下文感知的交互，通过消息列表进行输入输出；非对">
<meta property="og:type" content="article">
<meta property="og:title" content="对话模型与非对话模型详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/9a400d225757/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="在大型语言模型 (LLM) 的领域中，”对话模型” (Chat Models) 和 “非对话模型” (或称为 “文本模型” Text Models) 是两种基本但又有所区别的模型范式，它们在设计、训练数据、输入&#x2F;输出格式以及最佳应用场景上存在差异。理解这两种模型的区别是有效利用 LLM 进行开发的关键。  核心思想：对话模型优化用于多轮、上下文感知的交互，通过消息列表进行输入输出；非对">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg">
<meta property="article:published_time" content="2025-04-25T22:24:00.000Z">
<meta property="article:modified_time" content="2026-01-05T10:07:09.943Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "对话模型与非对话模型详解",
  "url": "https://blog.tbf1211.xx.kg/9a400d225757/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg",
  "datePublished": "2025-04-25T22:24:00.000Z",
  "dateModified": "2026-01-05T10:07:09.943Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/9a400d225757/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '对话模型与非对话模型详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">466</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">227</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">81</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-04.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">对话模型与非对话模型详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">对话模型与非对话模型详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-04-25T22:24:00.000Z" title="发表于 2025-04-26 06:24:00">2025-04-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>6分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>在大型语言模型 (LLM) 的领域中，”对话模型” (Chat Models) 和 “非对话模型” (或称为 “文本模型” Text Models) 是两种基本但又有所区别的模型范式，它们在设计、训练数据、输入&#x2F;输出格式以及最佳应用场景上存在差异。理解这两种模型的区别是有效利用 LLM 进行开发的关键。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>对话模型优化用于多轮、上下文感知的交互，通过消息列表进行输入输出；非对话模型则擅长单次、直接的文本指令处理，通过字符串进行输入输出。</strong></p>
</div>

<hr>
<h2 id="一、非对话模型-Text-Models-LLMs"><a href="#一、非对话模型-Text-Models-LLMs" class="headerlink" title="一、非对话模型 (Text Models &#x2F; LLMs)"></a>一、非对话模型 (Text Models &#x2F; LLMs)</h2><p>非对话模型是早期和传统的大型语言模型形式，它们通常设计为接收一个单一的字符串作为输入（通常称为 “prompt”），并生成一个单一的字符串作为输出。虽然这些模型也能在一定程度上处理对话，但通常需要通过在单次 Prompt 中手动构建对话历史来模拟。</p>
<h3 id="1-1-特点"><a href="#1-1-特点" class="headerlink" title="1.1 特点"></a>1.1 特点</h3><ol>
<li><strong>字符串输入&#x2F;输出</strong>：输入是一个字符串，输出也是一个字符串。<ul>
<li><strong>输入示例</strong>：<code>&quot;把以下文本总结一下：[文本内容]&quot;</code></li>
<li><strong>输出示例</strong>：<code>&quot;这是一段总结后的文本。&quot;</code></li>
</ul>
</li>
<li><strong>无内置上下文管理</strong>：模型本身不维护对话历史或上下文。如果需要模拟对话，用户必须通过将此前的对话轮次拼接成一个更长的 Prompt 来实现。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Prompt = &quot;用户：你好！\nAI：你好！有什么可以帮助你的吗？\n用户：我想问一个关于AI的问题。\nAI：&quot;</span><br></pre></td></tr></table></figure></li>
<li><strong>单次请求</strong>：每个请求通常被视为一个独立的任务，模型生成响应后即完成。</li>
<li><strong>通常更早出现</strong>：许多经典的基础模型（如最初的 GPT 系列、<code>text-davinci-003</code>）都属于此类。</li>
<li><strong>Token 成本</strong>: 由于每次都需要将完整的对话历史作为 Prompt 输入，对于长对话，Token 消耗会迅速增加。</li>
</ol>
<h3 id="1-2-优点"><a href="#1-2-优点" class="headerlink" title="1.2 优点"></a>1.2 优点</h3><ul>
<li><strong>简单直观</strong>：对于单次任务或简单的问答，用法非常直接。</li>
<li><strong>灵活性</strong>：Prompt 的结构相对自由，可以通过各种技巧（如 Few-Shot Prompting）在单个 Prompt 中实现复杂指令。</li>
<li><strong>广谱性</strong>：适用于各种文本生成任务，如摘要、翻译、代码生成、创意写作等。</li>
</ul>
<h3 id="1-3-缺点"><a href="#1-3-缺点" class="headerlink" title="1.3 缺点"></a>1.3 缺点</h3><ul>
<li><strong>复杂的对话管理</strong>：处理多轮对话时，需要额外的逻辑代码来管理和拼接对话历史，容易出错。</li>
<li><strong>上下文限制</strong>：随着对话轮次的增加，Prompt 会变得很长，容易超出模型的最大 Token 限制。</li>
<li><strong>性能下降</strong>：长 Prompt 会增加推理时间。</li>
</ul>
<h3 id="1-4-适用场景"><a href="#1-4-适用场景" class="headerlink" title="1.4 适用场景"></a>1.4 适用场景</h3><ul>
<li><strong>文本总结与提取</strong>：对给定文章进行摘要或提取关键信息。</li>
<li><strong>翻译</strong>：从一种语言翻译到另一种语言。</li>
<li><strong>文本补全</strong>：根据前缀推断并生成后续文本。</li>
<li><strong>代码生成</strong>：根据描述生成代码片段。</li>
<li><strong>内容创作</strong>：撰写文章、邮件、营销文案等。</li>
<li><strong>一次性问答</strong>：不涉及上下文的独立问题回复。</li>
</ul>
<h3 id="1-5-示例-使用-LangChain-的-LLM-接口"><a href="#1-5-示例-使用-LangChain-的-LLM-接口" class="headerlink" title="1.5 示例 (使用 LangChain 的 LLM 接口)"></a>1.5 示例 (使用 LangChain 的 <code>LLM</code> 接口)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 OPENAI_API_KEY 已设置在环境变量中</span></span><br><span class="line">llm = OpenAI(model_name=<span class="string">&quot;gpt-3.5-turbo-instruct&quot;</span>, temperature=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 单次调用</span></span><br><span class="line">prompt_text = <span class="string">&quot;写一个关于太空旅行的短诗。&quot;</span></span><br><span class="line">response = llm.invoke(prompt_text)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;非对话模型响应:\n&quot;</span>, response)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟对话 (需要手动拼接上下文，可能效果不佳)</span></span><br><span class="line">dialog_history = <span class="string">&quot;用户：你好！\nAI：你好！请问有什么可以帮助你的吗？\n用户：写一个关于机器学习的简要定义。\nAI：&quot;</span></span><br><span class="line">simulated_response = llm.invoke(dialog_history)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;非对话模型模拟对话响应:\n&quot;</span>, simulated_response)</span><br></pre></td></tr></table></figure>

<h2 id="二、对话模型-Chat-Models"><a href="#二、对话模型-Chat-Models" class="headerlink" title="二、对话模型 (Chat Models)"></a>二、对话模型 (Chat Models)</h2><p>对话模型是专门为处理多轮、上下文感知的对话而设计和优化的语言模型。它们不简单地接收一个字符串，而是接收一个<strong>消息列表</strong>（List of Messages），并且返回一个消息对象。这些消息通常带有明确的角色（如 <code>System</code>、<code>Human</code>、<code>AI</code>）。</p>
<h3 id="2-1-特点"><a href="#2-1-特点" class="headerlink" title="2.1 特点"></a>2.1 特点</h3><ol>
<li><strong>消息列表输入&#x2F;输出</strong>：输入是消息对象列表，输出是单个消息对象。<ul>
<li><strong>消息类型</strong>：<code>SystemMessage</code> (系统指令)、<code>HumanMessage</code> (用户输入)、<code>AIMessage</code> (AI 回复)、<code>FunctionMessage</code> (<code>ToolMessage</code>) 等。</li>
<li><strong>输入示例</strong>：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;你是一个乐于助人的助手。&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;你好！&quot;</span>),</span><br><span class="line">    AIMessage(content=<span class="string">&quot;你好！有什么可以帮助你的吗？&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;我想了解一下量子物理。&quot;</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li>
<li><strong>输出示例</strong>：<code>AIMessage(content=&quot;量子物理是...&quot;, role=&#39;assistant&#39;)</code></li>
</ul>
</li>
<li><strong>内置上下文管理优化</strong>：模型底层通常有更好的机制来理解和维护对话上下文，即便在长对话中也能保持一致性。它们在训练过程中被明确地指示如何处理交替的消息序列。</li>
<li><strong>角色感知</strong>：通过明确的消息角色，模型能够更好地理解其在对话中的定位以及用户和系统指令的意图。</li>
<li><strong>更高的 Token 效率</strong>：由于模型被优化来理解消息边界和角色，有时在处理对话时可以比文本模型更有效地利用 Token。</li>
<li><strong>通常更现代</strong>：OpenAI 的 <code>gpt-3.5-turbo</code>, <code>gpt-4</code> 和 Google 的 <code>Gemini</code> 系列等更先进的模型多为对话模型。</li>
</ol>
<h3 id="2-2-优点"><a href="#2-2-优点" class="headerlink" title="2.2 优点"></a>2.2 优点</h3><ul>
<li><strong>优化的对话体验</strong>：更自然、更流畅地处理多轮对话，更好地理解指代和上下文。</li>
<li><strong>简化对话管理</strong>：开发者无需手动拼接整个对话历史，只需传递消息列表。</li>
<li><strong>更强大的指令遵循能力</strong>：通过 <code>SystemMessage</code> 能更好地控制模型的行为和人格。</li>
<li><strong>支持函数调用 (Function Calling&#x2F;Tool Use)</strong>：许多对话模型扩展了功能，可以直接理解并调用外部工具，进一步增强其能力。</li>
</ul>
<h3 id="2-3-缺点"><a href="#2-3-缺点" class="headerlink" title="2.3 缺点"></a>2.3 缺点</h3><ul>
<li><strong>输入格式更严格</strong>：必须以消息列表的形式提供输入，不能直接传递纯字符串。</li>
<li><strong>对于简单任务可能过于复杂</strong>：如果只是想进行一次性的摘要或转换，使用对话模型可能需要额外的格式转换。</li>
</ul>
<h3 id="2-4-适用场景"><a href="#2-4-适用场景" class="headerlink" title="2.4 适用场景"></a>2.4 适用场景</h3><ul>
<li><strong>聊天机器人</strong>：构建客服机器人、个人助理或其他交互式对话系统。</li>
<li><strong>多轮问答系统</strong>：需要记忆和理解之前问题的上下文。</li>
<li><strong>智能助手</strong>：需要模仿人类对话、情感理解和复杂指令处理的场景。</li>
<li><strong>Agent (代理)</strong>：利用函数调用进行复杂任务分解和执行的系统。</li>
</ul>
<h3 id="2-5-示例-使用-LangChain-的-ChatModel-接口"><a href="#2-5-示例-使用-LangChain-的-ChatModel-接口" class="headerlink" title="2.5 示例 (使用 LangChain 的 ChatModel 接口)"></a>2.5 示例 (使用 LangChain 的 <code>ChatModel</code> 接口)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, SystemMessage, AIMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 OPENAI_API_KEY 已设置在环境变量中</span></span><br><span class="line">chat_model = ChatOpenAI(model_name=<span class="string">&quot;gpt-3.5-turbo&quot;</span>, temperature=<span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建消息列表进行对话</span></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;你是一个专业的烹饪助手，提供简洁美味的建议。&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;我想做一道简单的晚餐，有什么推荐？&quot;</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response_chat_1 = chat_model.invoke(messages)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;对话模型第一次响应:\n&quot;</span>, response_chat_1.content)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在上次对话的基础上继续 (添加新的 HumanMessage 和上一次的 AIMessage)</span></span><br><span class="line">messages.append(AIMessage(content=response_chat_1.content)) <span class="comment"># 将AI的回复加入历史</span></span><br><span class="line">messages.append(HumanMessage(content=<span class="string">&quot;听起来不错！请问需要哪些主要食材？&quot;</span>))</span><br><span class="line"></span><br><span class="line">response_chat_2 = chat_model.invoke(messages)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;对话模型第二次响应:\n&quot;</span>, response_chat_2.content)</span><br></pre></td></tr></table></figure>

<h2 id="三、比较总结"><a href="#三、比较总结" class="headerlink" title="三、比较总结"></a>三、比较总结</h2><table>
<thead>
<tr>
<th align="left">特征</th>
<th align="left">非对话模型 (Text Models &#x2F; LLMs)</th>
<th align="left">对话模型 (Chat Models)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>输入格式</strong></td>
<td align="left">单一字符串 (Prompt)</td>
<td align="left">消息对象列表 (<code>SystemMessage</code>, <code>HumanMessage</code>, <code>AIMessage</code> 等)</td>
</tr>
<tr>
<td align="left"><strong>输出格式</strong></td>
<td align="left">单一字符串</td>
<td align="left">单一消息对象 (<code>AIMessage</code>)</td>
</tr>
<tr>
<td align="left"><strong>上下文管理</strong></td>
<td align="left">需用户手动拼接 Prompt 历史</td>
<td align="left">模型内部优化处理，通过消息列表自然表达</td>
</tr>
<tr>
<td align="left"><strong>设计用途</strong></td>
<td align="left">通用文本生成、补全、摘要、翻译等单次任务</td>
<td align="left">多轮对话、上下文理解、角色扮演</td>
</tr>
<tr>
<td align="left"><strong>指令遵循</strong></td>
<td align="left">通过 Prompt 工程实现</td>
<td align="left">通过 <code>SystemMessage</code> 和消息角色更强地引导模型行为</td>
</tr>
<tr>
<td align="left"><strong>编程接口</strong></td>
<td align="left"><code>llm.invoke(&quot;prompt_string&quot;)</code></td>
<td align="left">&#96;chat_model.invoke</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/9a400d225757/">https://blog.tbf1211.xx.kg/9a400d225757/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-04.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-15.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a><a class="pagination-related" href="/bd9bce873bad/" title="Node.js 本地静态服务详解：http-server 与 live-server"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Node.js 本地静态服务详解：http-server 与 live-server</div></div><div class="info-2"><div class="info-item-1"> 在前端开发中，我们经常需要一个简单的本地 Web 服务器来预览静态文件，或者在开发 SPA (单页应用) 时提供必要的 HTTP 服务。Node.js 生态系统提供了许多这样的命令行工具，其中最常用和便捷的当属 http-server 和 live-server。本文将详细介绍这两个工具的安装、使用、特性以及它们之间的区别，帮助开发者根据需求选择合适的本地服务器。  核心思想：利用 Node.js 提供的便捷命令行工具，快速搭建本地静态文件服务器，其中 live-server 更进一步提供了实时重载功能以优化开发体验。   一、http-server 详解http-server 是一个简单、零配置的命令行 HTTP 服务器。它适用于快速提供本地文件服务，非常适合静态网站的预览、API 模拟等场景。 1.1 安装http-server 是一个 Node.js 包，通过 npm (Node Package Manager) 全局安装即可。 1npm install -g http-server  1.2 基本使用安装完成后，在任何包含静态文件的目录下运行 http-server 命...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/8be247b5e9ae/" title="微调大模型 (Finetuning LLMs) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-20</div><div class="info-item-2">微调大模型 (Finetuning LLMs) 详解</div></div><div class="info-2"><div class="info-item-1"> 微调 (Finetuning) 是人工智能领域，特别是大语言模型 (LLM) 领域中的一项关键技术。它指的是在预训练好的大型模型基础上，使用特定任务或领域的数据集进一步训练模型的过程。通过微调，我们可以将一个通用的、在海量数据上学习到基础语言理解和生成能力的大模型，高效地适配到具体的场景需求，从而显著提升模型在该特定任务上的性能、准确性和可靠性。  核心思想：微调的核心在于利用通用大模型强大的“基础能力”，并通过小规模、高质量的领域数据进行“二次开发”，使其专业化。对于LLM而言，参数高效微调 (PEFT) 极大降低了微调的资源门槛，使其在实践中变得可行且高效。   一、为什么需要微调大模型？通用大语言模型（如 GPT-系列、Llama、Mistral 等）在预训练阶段学习了海量的文本数据，拥有强大的泛化能力、语言理解能力和常识。然而，它们在直接应用于特定任务或领域时仍存在局限性：  知识截止日期 (Knowledge Cut-off)：预训练数据通常有截止日期，模型无法获取最新信息。 幻觉 (Hallucination)：模型可能会生成看似合理但实际上错误或捏造的信息。 领域...</div></div></div></a><a class="pagination-related" href="/82498674876b/" title="RAG（检索增强生成）技术详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-23.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-01</div><div class="info-item-2">RAG（检索增强生成）技术详解</div></div><div class="info-2"><div class="info-item-1"> RAG (Retrieval Augmented Generation)，即检索增强生成，是一种结合了检索系统与大型语言模型 (LLM) 的人工智能技术。它旨在提高 LLM 在回答问题、生成文本时的准确性、及时性和事实可靠性，尤其是在处理特定领域知识、最新信息或内部数据时。RAG 通过在生成答案之前，从外部知识库中检索相关信息，并将这些信息作为上下文提供给 LLM，从而“增强”其生成能力。  核心思想：克服大语言模型在知识时效性、幻觉和领域特异性方面的局限性。它通过动态地从权威数据源检索相关、准确的事实依据，并以此为基础指导 LLM 进行生成，使得 LLM 的输出更加准确、可追溯且富含最新信息。    一、为什么需要 RAG？大语言模型的局限性大语言模型（LLMs）在处理自然语言任务方面展现出惊人的能力，但它们也存在一些固有的局限性，RAG 正是为了解决这些问题而生：  知识时效性与更新难题 (Knowledge Staleness)  LLM 的知识来源于其训练数据，这些数据在模型发布后就成为了静态的。它们无法获取最新的事件、实时数据或新形成的知识。 每次需要更新知识时，都可...</div></div></div></a><a class="pagination-related" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-11</div><div class="info-item-2">智能体 (Agent) 详解：深入 LangChain 开发实践</div></div><div class="info-2"><div class="info-item-1"> 智能体 (Agent) 是人工智能领域中的一个核心概念，特指能够感知环境、进行决策并采取行动以实现特定目标或利益的实体。在当前的技术浪潮中，特别是随着大语言模型 (LLM) 的突破，智能体这一概念被赋予了新的活力和强大的实现路径。基于 LLM 的智能体能够理解复杂的指令、规划任务、执行外部工具并进行自我反思，从而展现出接近自主解决问题的能力。  核心思想：智能体是一个自主运行的系统，它通过感知 (Perception)、思考 (Thought&#x2F;Planning)、行动 (Action) 和反馈 (Feedback&#x2F;Memory) 的闭环循环，在动态环境中追求并实现预设目标。Python 中的 LangChain 库提供了一套强大的工具和框架，用于快速构建和部署基于 LLM 的智能体，使其能够与各种外部资源和工具交互。   一、智能体的基本概念1.1 什么是智能体？在广义的人工智能领域，智能体是一个能够自主地运作以影响其所处环境的实体。其核心能力体现在以下循环：  感知 (Perception)：接收来自环境的信息（传感器输入，如文本、图像、数据）。 思考&#...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-24.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/51de73c05326/" title="LangGraph 库核心组件与调用方法详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-26.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-15</div><div class="info-item-2">LangGraph 库核心组件与调用方法详解</div></div><div class="info-2"><div class="info-item-1"> LangGraph 是 LangChain 生态系统中的一个高级库，它允许开发者使用有向无环图 (DAG) 的方式构建健壮、有状态且可控的 LLM 应用。它特别适用于需要多步骤推理、代理 (Agent) 行为、循环和人工干预的复杂工作流。LangGraph 的核心优势在于其明确的状态管理和对图结构的直接建模能力，使得构建和调试复杂代理系统变得更加直观和可靠。  核心思想：将多步骤的 LLM 应用程序建模为状态机，其中每个节点代表一个操作（LLM 调用、工具调用、函数等），边代表状态转换。通过在节点之间传递和修改状态，实现复杂、有循环的工作流。它解决了传统 LangChain Chain 在处理复杂逻辑（特别是循环和条件分支）时的局限性。    一、LangGraph 核心概念LangGraph 的设计基于图论和状态机的思想。理解以下核心概念是使用 LangGraph 的基础：  State (状态)：  表示整个应用程序在某个时间点的数据快照。 通过 StateDict 对象传递，它是一个字典或类似字典的结构。 节点操作通常会接收当前状态，并返回一个表示状态更新的 StateD...</div></div></div></a><a class="pagination-related" href="/4b8301cdd035/" title="向量数据库 (Vector Database) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-03</div><div class="info-item-2">向量数据库 (Vector Database) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量数据库 (Vector Database &#x2F; Vector Store) 是一种专门设计用于高效存储、管理和检索向量嵌入 (Vector Embeddings) 的数据库。这些向量嵌入是高维的数值表示，由机器学习模型生成，能够捕捉文本、图像、音频或其他复杂数据的语义信息。向量数据库的核心能力在于通过计算向量之间的相似度 (Similarity) 来进行快速搜索，而非传统的精确匹配。  核心思想：将非结构化数据转化为机器可理解的低维或高维向量表示（嵌入），并在此基础上实现基于语义相似度的快速检索。它解决了传统数据库在处理语义搜索、推荐系统、多模态数据匹配等场景下的局限性。   一、什么是向量 (Vector)？在深入了解向量数据库之前，我们必须先理解“向量”这个核心概念。 1.1 向量的数学定义在数学和物理中，向量 (Vector) 是一个具有大小 (Magnitude) 和方向 (Direction) 的量。它可以被表示为一个有序的数值列表。  一维向量：一个标量，如 [5]。 二维向量：表示平面上的一个点或从原点指向该点的箭头，如 [x, y]。例如，[3, 4...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">466</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">227</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">81</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E9%9D%9E%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B-Text-Models-LLMs"><span class="toc-text">一、非对话模型 (Text Models &#x2F; LLMs)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%89%B9%E7%82%B9"><span class="toc-text">1.1 特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E4%BC%98%E7%82%B9"><span class="toc-text">1.2 优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E7%BC%BA%E7%82%B9"><span class="toc-text">1.3 缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">1.4 适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E7%A4%BA%E4%BE%8B-%E4%BD%BF%E7%94%A8-LangChain-%E7%9A%84-LLM-%E6%8E%A5%E5%8F%A3"><span class="toc-text">1.5 示例 (使用 LangChain 的 LLM 接口)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%AF%B9%E8%AF%9D%E6%A8%A1%E5%9E%8B-Chat-Models"><span class="toc-text">二、对话模型 (Chat Models)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%89%B9%E7%82%B9"><span class="toc-text">2.1 特点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BC%98%E7%82%B9"><span class="toc-text">2.2 优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%BC%BA%E7%82%B9"><span class="toc-text">2.3 缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">2.4 适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E7%A4%BA%E4%BE%8B-%E4%BD%BF%E7%94%A8-LangChain-%E7%9A%84-ChatModel-%E6%8E%A5%E5%8F%A3"><span class="toc-text">2.5 示例 (使用 LangChain 的 ChatModel 接口)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%AF%94%E8%BE%83%E6%80%BB%E7%BB%93"><span class="toc-text">三、比较总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/cecf96486ef4/" title="IPC (Inter-Process Communication) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPC (Inter-Process Communication) 详解"/></a><div class="content"><a class="title" href="/cecf96486ef4/" title="IPC (Inter-Process Communication) 详解">IPC (Inter-Process Communication) 详解</a><time datetime="2026-01-04T22:24:00.000Z" title="发表于 2026-01-05 06:24:00">2026-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/3059cdc5f529/" title="Golang sqlc 框架详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang sqlc 框架详解"/></a><div class="content"><a class="title" href="/3059cdc5f529/" title="Golang sqlc 框架详解">Golang sqlc 框架详解</a><time datetime="2026-01-01T22:24:00.000Z" title="发表于 2026-01-02 06:24:00">2026-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/4e0d929e572f/" title="如何防止 Golang Goroutine 泄漏"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-03.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何防止 Golang Goroutine 泄漏"/></a><div class="content"><a class="title" href="/4e0d929e572f/" title="如何防止 Golang Goroutine 泄漏">如何防止 Golang Goroutine 泄漏</a><time datetime="2025-12-30T22:24:00.000Z" title="发表于 2025-12-31 06:24:00">2025-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CFFI (C Foreign Function Interface for Python) 详解"/></a><div class="content"><a class="title" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解">CFFI (C Foreign Function Interface for Python) 详解</a><time datetime="2025-12-23T22:24:00.000Z" title="发表于 2025-12-24 06:24:00">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="行为驱动开发 (BDD) 详解"/></a><div class="content"><a class="title" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解">行为驱动开发 (BDD) 详解</a><time datetime="2025-12-21T22:24:00.000Z" title="发表于 2025-12-22 06:24:00">2025-12-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-04.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="🆕 2023">🆕 2023</a><a href="/archives/2024/" target="_blank" title="🆒 2024">🆒 2024</a><a href="/archives/2025/" target="_blank" title="👨‍👩‍👦 2025">👨‍👩‍👦 2025</a><a href="/archives/2026/" target="_blank" title="🆙 2026">🆙 2026</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="📁 分类">📁 分类</a><a href="/tags/" target="_blank" title="🔖 标签">🔖 标签</a><a href="/categories/" target="_blank" title="📽️ 时间线">📽️ 时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="💬 说说">💬 说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>