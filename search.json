[{"title":"从单机到哨兵，一张图理清redis架构演进！","url":"/2023/2023-01-26_%E4%BB%8E%E5%8D%95%E6%9C%BA%E5%88%B0%E5%93%A8%E5%85%B5%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%90%86%E6%B8%85redis%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B/","content":"Redis 的架构是逐步演进而来的，正所谓“罗马不是一天建成的”。\n\n\n2010 年：单机版 Redis\n\n当 Redis1.0在 2010 年首次发布时，整体架构非常简单，通常作为业务系统的缓存使用。不过，Redis 的数据是存储在内存中的，一旦重启，数据就会全部丢失，导致请求会直接打到数据库上，带来较大的压力。\n\n2013 年：持久化机制上线\n\n2013 年，Redis2.8版本发布，解决了之前“重启就丢数据”的问题。Redis 引入了 RDB（内存快照）机制，用于定时将内存中的数据持久化到磁盘。同时还支持 AOF（只追加文件）方式，将每一次写操作都记录到日志文件中，实现更高级别的持久化保障。\n\n2013 年：主从复制机制\n\n同样在 Redis2.8中，官方引入了主从复制功能，提升了系统的高可用性。主节点负责处理实时的读写请求，从节点则负责同步主节点的数据，起到备份和读扩展的作用。\n\n2013 年：Sentinel 哨兵机制上线\n\n在 Redis2.8版本中，引入了 Sentinel（哨兵）机制，用于实时监控 Redis 实例的运行状态。它主要负责以下几个方面的工作：\n\n\n监控 Redis 实例是否正常运行\n在发生故障时发出告警通知\n发生故障时自动完成主从切换（故障转移）\n\n2015 年：集群模式上线\n2015 年，Redis 发布了3.0版本，正式引入了 Redis Cluster（集群）功能。Redis 集群是一种分布式数据库解决方案，它通过“分片”机制管理数据。整个数据空间被划分为 16384 个槽（slot），每个节点负责其中一部分槽的数据。这样不仅提高了系统的可扩展性，还提升了整体的性能与容错能力。\n2017 年：Stream 数据类型\n\n到了 2017 年，Redis 发布了5.0版本，新增了Stream 数据类型，进一步拓展了 Redis 在实时消息处理等场景下的应用能力。\n\n2020 年：多线程\n\n2020 年，Redis 发布了6.0版本，引入了网络模块的多线程 I&#x2F;O 机制。Redis 的整体架构可以分为两个模块：网络模块和主处理模块。随着业务量的增长，开发者们发现网络模块开始逐渐成为系统的性能瓶颈。因此，在 6.0 版本中，对网络 I&#x2F;O 进行了多线程优化，从而提升了高并发场景下的网络处理能力。\n\n","categories":["中间件","Redis"],"tags":["2023","Redis","中间件"]},{"title":"MySQL 索引详解","url":"/2023/2023-01-27_MySQL%20%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3/","content":"\n索引是数据库中用于提高查询速度的一种数据结构。在 MySQL 中，合理有效地使用索引能够显著提升数据库的查询性能，减少 I&#x2F;O 操作。然而，不恰当的索引也可能带来额外的开销。理解 MySQL 索引的原理和优化策略，是数据库性能调优的关键。\n\n“好的索引，事半功倍；坏的索引，越帮越忙。” - 数据库优化格言\n\n\n一、什么是索引？索引（Index）是一种特殊的查找表，数据库搜索引擎可以利用它来快速定位数据。可以将其类比为一本书的目录，通过目录我们可以快速找到感兴趣的章节，而不需要通读整本书。\n在数据库中，没有索引的查询需要全表扫描，即逐行检查每条记录，直到找到符合条件的记录。当数据量非常大时，这种操作的效率会非常低下。索引通过创建指向数据物理位置的指针，使得数据库在查询时能够直接跳转到相关记录，从而大大加快查询速度。\n二、索引的优缺点优点\n显著提高数据检索速度：这是索引最核心、最主要的优点。\n加快表与表之间的连接速度：对于 JOIN 操作，索引可以加速连接条件的匹配。\n加快分组和排序操作：GROUP BY 和 ORDER BY 操作通常通过消除临时表和对文件进行排序来提高效率。\n保证数据的唯一性：唯一索引（Unique Index）可以强制列的数据不重复。\n\n缺点\n占用磁盘空间：索引本身也是一种数据结构，需要存储在磁盘上。\n降低更新速度：当对表中的数据进行 INSERT、UPDATE、DELETE 操作时，除了修改数据本身，还需要同时更新索引，这会增加数据库的写操作负担。\n维护成本：索引越多，维护成本越高，查询优化器选择索引的代价也可能增加。\n\n三、索引的分类MySQL 中索引可以从不同的维度进行分类：\n1. 按数据结构分类MySQL 主要支持两种索引结构，B+Tree 和 Hash。\n(1) B+Tree 索引 (默认，常用)\n特点:\nB+Tree 是一种多路平衡查找树，所有数据都存储在叶子节点，并且叶子节点之间通过指针连接，形成一个有序链表。\n非叶子节点只存储索引键，不存储数据，减少了树的高度，提高了查询效率。\n适合范围查询、模糊查询（前缀匹配）、排序等。\nMySQL 的 InnoDB 存储引擎默认使用 B+Tree 索引。\n\n\n适用场景: 几乎所有类型的查询，包括等值查询、范围查询、排序和分组操作。\n\n(2) Hash 索引\n特点:\n基于哈希表实现，通过哈希算法将索引列的值映射到哈希表中，存储行指针。\n查询速度极快，只需要进行一次哈希计算和一次指针查找。\n仅支持精确匹配查询（等值查询），不支持范围查询和排序。\n哈希冲突处理会影响性能。\nMySQL 的 Memory 存储引擎默认支持，InnoDB 存储引擎不支持显式创建 Hash 索引，但有自适应哈希索引 (Adaptive Hash Index)。\n\n\n适用场景: 等值查询，例如 WHERE id = 123。\n\n2. 按物理存储分类 (InnoDB 存储引擎)(1) 聚集索引 (Clustered Index)\n特点:\n一个表只有一个聚集索引。\n将数据行存储在索引的叶子节点中。也就是说，数据和索引是存储在一起的。\nInnoDB 存储引擎会自动为主键列创建聚集索引。如果表没有主键，MySQL 会选择第一个非空的唯一索引。如果也没有非空的唯一索引，InnoDB 会隐式地生成一个行 ID 作为聚集索引。\n查询效率极高，因为找到索引就意味着找到了数据。\n\n\n适用场景: 查询主键或按主键范围查询。\n\n(2) 非聚集索引 (Secondary Index &#x2F; Auxiliary Index)\n特点:\n一个表可以有多个非聚集索引。\n索引的叶子节点存储的是主键值，而不是数据行本身。\n当通过非聚集索引查询时，首先在非聚集索引中找到对应的主键值，然后（通过主键值）再去聚集索引中找到完整的数据行。这个过程称为回表查询。\n\n\n适用场景: 除了主键以外的所有索引，包括普通索引、唯一索引等。\n覆盖索引 (Covering Index): 当非聚集索引中包含查询所需的所有列时，就不需要回表查询完整数据行，这种索引被称为覆盖索引。覆盖索引能极大地提高查询性能。\n\n3. 按逻辑分类(1) 普通索引 (Normal &#x2F; Non-Unique Index)\n特点: 最基本的索引，没有任何限制，可重复。\n创建: CREATE INDEX index_name ON table_name (column_name);\n\n(2) 唯一索引 (Unique Index)\n特点: 要求索引列的值必须唯一，但允许有 NULL 值（且 NULL 值可以有多个）。\n创建: CREATE UNIQUE INDEX index_name ON table_name (column_name); 或 ALTER TABLE table_name ADD UNIQUE (column_name);\n\n(3) 主键索引 (Primary Key Index)\n特点: 一种特殊的唯一索引，一个表只能有一个主键。主键列的值必须唯一，且不能为 NULL。\n创建: ALTER TABLE table_name ADD PRIMARY KEY (column_name); 或在创建表时定义。\n在 InnoDB 中，主键索引就是聚集索引。\n\n(4) 全文索引 (Full-Text Index)\n特点: 用于在文本列（如 VARCHAR, TEXT）中进行关键词查找，支持自然语言查询。\n创建: CREATE FULLTEXT INDEX index_name ON table_name (column_name);\n适用场景: 博客文章内容搜索、商品描述搜索等。\n\n(5) 复合索引 (Composite &#x2F; Combination Index)\n特点: 在多个列上创建的索引。遵循“最左前缀原则”。\n最左前缀原则: 如果在一个 (col1, col2, col3) 的复合索引上，查询条件可以使用 col1、(col1, col2)、(col1, col2, col3) 来匹配索引，但不能直接使用 col2 或 col3。\n创建: CREATE INDEX index_name ON table_name (col1, col2, col3);\n\n四、索引的创建与删除创建索引\n创建表时指定\nCREATE TABLE users (    id INT PRIMARY KEY AUTO_INCREMENT,    username VARCHAR(50) UNIQUE,    email VARCHAR(100),    status TINYINT,    INDEX idx_status (status)  -- 普通索引);\n\n使用 CREATE INDEX 语句\nCREATE INDEX idx_email ON users (email);CREATE UNIQUE INDEX uidx_username ON users (username);CREATE INDEX idx_username_email ON users (username, email); -- 复合索引\n\n使用 ALTER TABLE 语句\nALTER TABLE users ADD INDEX idx_email (email);ALTER TABLE users ADD UNIQUE INDEX uidx_username (username);ALTER TABLE users ADD PRIMARY KEY (id); -- 添加主键（如果是新表）ALTER TABLE articles ADD FULLTEXT INDEX ft_content (content);\n\n删除索引DROP INDEX index_name ON table_name;ALTER TABLE table_name DROP INDEX index_name; -- 如果是唯一索引/普通索引ALTER TABLE table_name DROP PRIMARY KEY;     -- 如果是主键索引\n\n五、索引优化策略1. 选择合适的列创建索引\nWHERE 条件中经常使用的列：等值查询、范围查询的列。\nJOIN 连接条件中使用的列：ON 子句中的列。\nORDER BY 和 GROUP BY 子句中使用的列：可以避免文件排序。\n选择性高的列：列中值的重复率越低，索引的效果越好。例如，性别这种只有两种值的列，选择性很低，不适合单独建立索引。\n不为 NULL 的列：如果列可以为 NULL，索引可能会失效。\n\n2. 避免索引失效\n不要在 WHERE 子句中使用 OR 连接条件：除非 OR 连接的所有列都创建了索引。\n避免在索引列上进行函数操作：WHERE YEAR(create_time) = 2023 会导致索引失效。\n避免在索引列上进行类型转换：例如，将字符串与数字进行比较。\nLIKE 查询中，通配符 % 不要放在开头：WHERE column_name LIKE &#39;prefix%&#39; 会使用索引，而 WHERE column_name LIKE &#39;%suffix&#39; 或 &#39;%pattern%&#39; 不会。\n避免使用 != 或 &lt;&gt; 操作符：这些操作符通常会导致全表扫描。\nIS NULL 和 IS NOT NULL：在某些情况下可能使索引失效，取决于 MySQL 版本和优化器。通常最好让列始终有值。\n复合索引的“最左前缀原则”：查询条件必须从复合索引的最左边列开始使用，才能利用到该索引。\n\n3. 优化索引设计\n考虑使用覆盖索引：如果查询只需要索引中的列，就不需要回表，效率极高。\n创建短索引&#x2F;前缀索引：对于很长的字符串列，可以只索引其前缀。CREATE INDEX idx_long_text ON your_table (long_text(20)); -- 只索引前20个字符\n这样可以节省磁盘空间，提高索引效率，但可能会降低索引的选择性。\n利用联合索引：将经常一起查询的列创建为联合索引，并注意列的顺序（将选择性高的列放在前面）。\n考虑 InnoDB 的主键选择：如果业务 ID 是自增的，设为主键会减少页分裂和数据移动，提升性能。如果业务 ID 是UUID等随机值，考虑使用一个自增代理主键，业务 UUID 则作为唯一索引。\n定期维护索引：对索引进行优化和重建，例如 OPTIMIZE TABLE。\n\n4. 观察和分析\n使用 EXPLAIN 分析查询语句：这是最重要的工具，可以查看 MySQL 如何执行查询，是否使用了索引，使用了哪个索引，以及回表情况等。EXPLAIN SELECT * FROM users WHERE username = &#x27;Alice&#x27;;\n重点关注 type（访问类型）、key（实际使用的索引）、rows（大概扫描的行数）、Extra 等信息。\n慢查询日志：记录执行时间超过阈值的查询语句，方便定位性能瓶颈。\n监控数据库性能指标：如磁盘 I&#x2F;O、CPU 使用率、缓存命中率等。\n\n六、总结MySQL 索引是数据库性能优化的基石。正确理解和使用不同类型的索引，结合实际业务场景进行设计，并根据 EXPLAIN 等工具的分析结果进行迭代优化，才能真正发挥索引的威力。索引并非越多越好，它是一个需要合理权衡的过程，旨在在查询速度和写入速度之间取得最佳平衡。\n","categories":["中间件","MySQL"],"tags":["2023","中间件","MySQL"]},{"title":"Docker镜像构建与管理：打造标准化、可复用的容器镜像","url":"/2023/2023-02-01_Docker%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E4%B8%8E%E7%AE%A1%E7%90%86%EF%BC%9A%E6%89%93%E9%80%A0%E6%A0%87%E5%87%86%E5%8C%96%E3%80%81%E5%8F%AF%E5%A4%8D%E7%94%A8%E7%9A%84%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/","content":"\n本文由 简悦 SimpRead 转码， 原文地址 mp.weixin.qq.com\n\nDocker 镜像构建与管理：打造标准化、可复用的容器镜像开篇：你是否也在镜像管理上栽过跟头？凌晨 2 点，生产环境突然告警，新部署的容器启动失败。排查后发现：开发环境用的镜像 800MB，生产环境的却有 3.2GB，里面塞满了编译工具、测试数据，甚至还有开发同学的 SSH 私钥…\n这种 “镜像肥胖症” 你遇到过吗？或者更糟糕的：\n\n同一个服务，测试环境能跑，生产环境启动就报错\n镜像仓库里堆满了 latest、v1、v1-final、v1-final-final 这种让人崩溃的标签\n构建一次镜像要等 20 分钟，因为每次都要重新下载依赖包\n\n今天这篇文章，我会基于 5 年运维实战经验，教你构建一套标准化的镜像管理体系：从多阶段构建优化到镜像安全扫描，从版本管理策略到自动化构建流程，让你的镜像体积缩小 70%、构建速度提升 5 倍，并且永远不会再出现 “这个镜像到底能不能用” 的灵魂拷问。\n一、镜像构建的三大核心原则（90% 的人都忽略了）1. 最小化原则：镜像里只放 “必需品”很多人写 Dockerfile 就像搬家，什么都往里塞。我见过最离谱的：一个 Node.js 应用镜像，里面包含了完整的 gcc 编译工具链、Python3、甚至还有 vim 和 htop。\n正确做法：分清 “构建时依赖” 和 “运行时依赖”\n# ❌ 错误示例：单阶段构建，所有东西都打包进去FROM node:16WORKDIR /appCOPY . .RUN npm installRUN npm run buildCMD [&quot;npm&quot;, &quot;start&quot;]# 最终镜像大小：1.2GB\n\n# ✅ 正确示例：多阶段构建，只保留运行时必需# 构建阶段FROM node:16-alpine AS builderWORKDIR /appCOPY package*.json ./RUN npm ci --only=production# 运行阶段FROM node:16-alpineWORKDIR /appCOPY --from=builder /app/node_modules ./node_modulesCOPY . .CMD [&quot;node&quot;, &quot;index.js&quot;]# 最终镜像大小：180MB\n\n关键命令：docker history &lt;镜像名&gt; 查看每层大小，找出 “肥胖层”\n2. 可复现原则：今天能构建，明年也要能构建我曾经历过这样的生产事故：6 个月前的镜像需要重新构建（修复安全漏洞），结果构建失败了——因为 Dockerfile 里写的是 apt-get install nginx，没指定版本，新版本 nginx 配置格式变了。\n铁律：所有依赖必须锁定版本\n# ❌ 危险写法RUN apt-get update &amp;&amp; apt-get install -y nginxRUN pip install flask# ✅ 安全写法RUN apt-get update &amp;&amp; apt-get install -y \\    nginx=1.18.0-6ubuntu14.4 \\    &amp;&amp; rm -rf /var/lib/apt/lists/*    COPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt# requirements.txt 中明确版本：flask==2.3.2\n\n3. 安全原则：不要让镜像成为安全漏洞的温床血泪教训： 2023 年某次安全审计，发现生产环境 30% 的镜像存在高危漏洞，原因是基础镜像用的 ubuntu:latest，构建后就没更新过。\n安全加固清单：\n\n使用特定版本的基础镜像：FROM alpine:3.18.4 而非 FROM alpine:latest\n创建非 root 用户运行应用\n删除构建缓存和包管理器缓存\n定期扫描镜像漏洞\n\n# 安全镜像模板FROM python:3.11-slim-bullseye# 创建非root用户RUN groupadd -r appuser &amp;&amp; useradd -r -g appuser appuser# 安装依赖并清理缓存COPY requirements.txt .RUN pip install --no-cache-dir -r requirements.txt \\    &amp;&amp; rm -rf /root/.cache/pip# 切换到非root用户USER appuserCOPY --chown=appuser:appuser . /appWORKDIR /appCMD [&quot;python&quot;, &quot;app.py&quot;]\n\n二、实战：5 步打造生产级镜像构建体系Step 1：编写高效的 Dockerfile（附最佳实践模板）核心技巧：利用构建缓存机制，把变化频率低的放前面\n# 生产级 Dockerfile 模板（以 Java Spring Boot 为例）# 第一阶段：依赖下载（利用缓存）FROM maven:3.8.6-openjdk-11-slim AS depsWORKDIR /appCOPY pom.xml .RUN mvn dependency:go-offline -B# 第二阶段：构建应用FROM maven:3.8.6-openjdk-11-slim AS builderWORKDIR /appCOPY --from=deps /root/.m2 /root/.m2COPY . .RUN mvn clean package -DskipTests# 第三阶段：运行时镜像FROM openjdk:11-jre-slimRUN groupadd -r spring &amp;&amp; useradd -r -g spring spring# 安装监控工具（可选）RUN apt-get update &amp;&amp; apt-get install -y \\    curl=7.74.0-1.3+deb11u7 \\    &amp;&amp; rm -rf /var/lib/apt/lists/*# 复制 jar 包COPY --from=builder /app/target/*.jar app.jar# 健康检查HEALTHCHECK --interval=30s --timeout=3s --retries=3 \\    CMD curl -f http://localhost:8080/actuator/health || exit 1USER springEXPOSE 8080ENTRYPOINT [&quot;java&quot;, &quot;-Xmx512m&quot;, &quot;-jar&quot;, &quot;/app.jar&quot;]\n\nStep 2：构建参数化（一个 Dockerfile 适配多环境）# 使用 ARG 实现构建时参数化ARG APP_ENV=productionARG NODE_VERSION=16-alpineFROM node:$&#123;NODE_VERSION&#125; AS builder# 根据环境安装不同依赖ARG APP_ENVRUN if [ &quot;$APP_ENV&quot; = &quot;development&quot; ]; then \\        npm install; \\    else \\        npm ci --only=production; \\    fi\n\n构建命令：\n# 开发环境构建docker build --build-arg APP_ENV=development -t myapp:dev .# 生产环境构建docker build --build-arg APP_ENV=production -t myapp:prod .\n\nStep 3：自动化镜像扫描（提前发现安全隐患）实用脚本：镜像安全扫描自动化\n#!/bin/bash# scan_image.sh - 镜像安全扫描脚本IMAGE_NAME=$1REPORT_FILE=&quot;scan_report_$(date +%Y%m%d_%H%M%S).json&quot;echo &quot;🔍 开始扫描镜像: $IMAGE_NAME&quot;# 使用 Trivy 扫描（需提前安装：apt-get install trivy）trivy image --severity HIGH,CRITICAL \\          --format json \\          --output $REPORT_FILE \\          $IMAGE_NAME# 解析扫描结果CRITICAL_COUNT=$(jq &#x27;[.Results[].Vulnerabilities[]? | select(.Severity==&quot;CRITICAL&quot;)] | length&#x27; $REPORT_FILE)HIGH_COUNT=$(jq &#x27;[.Results[].Vulnerabilities[]? | select(.Severity==&quot;HIGH&quot;)] | length&#x27; $REPORT_FILE)echo &quot;📊 扫描结果：&quot;echo &quot;  - 严重漏洞: $CRITICAL_COUNT 个&quot;echo &quot;  - 高危漏洞: $HIGH_COUNT 个&quot;# 如果存在严重漏洞，阻止发布if [ $CRITICAL_COUNT -gt 0 ]; then    echo&quot;❌ 发现严重漏洞，禁止发布！&quot;    exit 1fiecho &quot;✅ 安全检查通过&quot;\n\nStep 4：镜像版本管理（告别 latest 地狱）标准化标签规范：\n# 版本标签格式：&lt;主版本&gt;.&lt;次版本&gt;.&lt;修订版本&gt;-&lt;构建号&gt;-&lt;git commit&gt;# 示例：v1.2.3-20231125-7a3b5c9#!/bin/bash# tag_image.sh - 自动生成镜像标签# 获取版本信息VERSION=$(cat VERSION)  # 从 VERSION 文件读取BUILD_DATE=$(date +%Y%m%d)GIT_COMMIT=$(git rev-parse --short HEAD)# 生成标签TAG=&quot;v$&#123;VERSION&#125;-$&#123;BUILD_DATE&#125;-$&#123;GIT_COMMIT&#125;&quot;# 构建并打标签docker build -t myapp:$&#123;TAG&#125; .docker tag myapp:$&#123;TAG&#125; myapp:latest# 推送到仓库docker push myapp:$&#123;TAG&#125;docker push myapp:latestecho &quot;✅ 镜像已发布: myapp:$&#123;TAG&#125;&quot;\n\nStep 5：构建流水线集成（CI&#x2F;CD 最佳实践）GitLab CI 配置示例：\n# .gitlab-ci.ymlstages:- build- scan- pushvariables:  DOCKER_REGISTRY:&quot;registry.company.com&quot;  IMAGE_NAME:&quot;$DOCKER_REGISTRY/myapp&quot;build:stage: buildscript:    -docker build-t $IMAGE_NAME:$CI_COMMIT_SHA.    -docker save$IMAGE_NAME:$CI_COMMIT_SHA &gt;image.tar  artifacts:    paths:      -image.tar    expire_in:1 hoursecurity_scan:  stage:scan  script:    - dockerload &lt;image.tar    -trivy image--exit-code 1--severity HIGH,CRITICAL$IMAGE_NAME:$CI_COMMIT_SHA  dependencies:    - buildpush_image:  stage:push  script:    - dockerload &lt;image.tar    -docker tag$IMAGE_NAME:$CI_COMMIT_SHA $IMAGE_NAME:latest    - dockerpush $IMAGE_NAME:$CI_COMMIT_SHA    - dockerpush $IMAGE_NAME:latestonly:    -main  dependencies:    - build\n\n三、进阶优化：让镜像构建效率翻倍1. 使用 BuildKit 加速构建# 开启 BuildKit（构建速度提升 30-50%）export DOCKER_BUILDKIT=1# 利用 BuildKit 的并行构建特性docker build --build-arg BUILDKIT_INLINE_CACHE=1 \\             --cache-from registry.company.com/myapp:latest \\             -t myapp:new .\n\n2. 构建缓存优化策略缓存优化脚本：\n#!/bin/bash# optimize_cache.sh - 智能缓存管理# 清理悬空镜像docker image prune -f# 清理超过7天未使用的镜像docker image prune -a --filter &quot;until=168h&quot; -f# 保留最近5个版本的镜像IMAGE_docker images --format &quot;&#123;&#123;.Repository&#125;&#125;:&#123;&#123;.Tag&#125;&#125;&quot; | \\  grep &quot;^$&#123;IMAGE_NAME&#125;:&quot; | \\sort -V | \\head -n -5 | \\  xargs -r docker rmiecho &quot;✅ 缓存优化完成&quot;\n\n3. 镜像体积极限压缩压缩技巧汇总：\n\n• 使用 Alpine 基础镜像（比 Ubuntu 小 90%）\n\n• 合并 RUN 指令减少层数\n\n• 使用 --no-install-recommends 参数\n\n• 删除不必要的文档和示例\n\n\n# 极限压缩示例（Go 应用）FROM golang:1.20-alpine AS builderWORKDIR /appCOPY . .RUN CGO_ENABLED=0 go build -ldflags=&quot;-s -w&quot; -o appFROM scratch  # 从零开始，终极精简COPY --from=builder /app/app /ENTRYPOINT [&quot;/app&quot;]# 最终大小：&lt; 10MB\n\n四、踩坑血泪史：这些错误你千万别犯坑 1：在镜像里存储敏感信息事故回放： 2022 年某次代码审计，发现镜像里包含数据库密码、AWS Access Key。虽然代码里用环境变量，但构建时的 .env 文件被 COPY 进去了。\n解决方案：\n# 使用 .dockerignore 排除敏感文件# .dockerignore 内容：*.env*.pem.git/.aws/\n\n坑 2：滥用 sudo 和 root 权限教训： 容器被攻破后，攻击者直接获得宿主机 root 权限。\n正确做法：\n# 永远不要在生产环境用 root 运行USER 1000:1000  # 使用 UID 而非用户名，避免用户不存在的问题\n\n坑 3：忽视时区问题症状： 日志时间总是差 8 小时，定时任务执行时间错乱。\n修复方法：\n# 设置时区ENV TZ=Asia/ShanghaiRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime &amp;&amp; echo $TZ &gt; /etc/timezone\n\n实用工具：一键镜像优化脚本#!/bin/bash# docker_optimize.sh - 一键优化 Docker 镜像set -eIMAGE_NAME=$1OPTIMIZED_NAME=&quot;$&#123;IMAGE_NAME&#125;_optimized&quot;echo &quot;🚀 开始优化镜像: $IMAGE_NAME&quot;# 1. 分析原始镜像大小ORIGINAL_SIZE=$(docker images --format &quot;&#123;&#123;.Size&#125;&#125;&quot; $IMAGE_NAME)echo &quot;原始大小: $ORIGINAL_SIZE&quot;# 2. 导出镜像并重新导入（去除历史层）docker save $IMAGE_NAME | docker load# 3. 使用 docker-slim 优化（需提前安装）docker-slim build --target $IMAGE_NAME --tag $OPTIMIZED_NAME \\                  --http-probe=false \\                  --continue-after=10# 4. 对比优化效果NEW_SIZE=$(docker images --format &quot;&#123;&#123;.Size&#125;&#125;&quot; $OPTIMIZED_NAME)echo &quot;✅ 优化完成&quot;echo &quot;  原始大小: $ORIGINAL_SIZE&quot;echo &quot;  优化后: $NEW_SIZE&quot;# 5. 运行测试echo&quot;🧪 运行测试...&quot;docker run --rm $OPTIMIZED_NAMEecho &quot;Test passed&quot;echo &quot;💡 优化后的镜像: $OPTIMIZED_NAME&quot;\n\n总结：掌握这 5 步，镜像管理不再是难题回顾今天的核心内容：\n\n三大原则：最小化、可复现、安全性\n\n五步体系：高效 Dockerfile → 参数化构建 → 安全扫描 → 版本管理 → CI&#x2F;CD 集成\n\n优化技巧：BuildKit 加速、缓存管理、极限压缩\n\n\n掌握这套方法论，你的镜像将实现：体积缩小 70%、构建速度提升 5 倍、安全漏洞降低 90%。 下次再遇到 “镜像太大”” 构建太慢 “”版本混乱” 的问题，10 分钟就能搞定。\n","categories":["Docker"],"tags":["2023","Docker","容器技术"]},{"title":"Docker镜像构建详解：从Dockerfile到高效实践","url":"/2023/2023-02-05_Docker%E9%95%9C%E5%83%8F%E6%9E%84%E5%BB%BA%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8EDockerfile%E5%88%B0%E9%AB%98%E6%95%88%E5%AE%9E%E8%B7%B5/","content":"\nDocker 镜像是 Docker 的核心组成部分之一。它是一个轻量级、独立、可执行的软件包，包含运行应用程序所需的一切：代码、运行时、系统工具、系统库和设置。构建 Docker 镜像是实现应用程序容器化的关键步骤，通过 Dockerfile 文件，我们可以定义镜像的构建过程。\n\n“Docker 镜像本质上是文件系统和配置的组合，它通过层（Layer）的概念实现了高效的存储和复用。理解 Dockerfile 的每一条指令以及如何优化构建过程，是成为 Docker 高手的必经之路。”\n\n\n一、Docker 镜像构建概述\nDockerfile：一个文本文件，包含一系列指令，用于自动化地在 Docker 环境中构建镜像。\n构建上下文 (Build Context)：在执行 docker build 命令时，你指定了一个路径（通常是当前目录）。这个路径下的所有文件和目录都会被发送到 Docker daemon，作为构建上下文。只有在构建上下文中包含的文件才能被 Dockerfile 中的指令（如 ADD, COPY）访问。\n镜像层 (Image Layer)：Docker 镜像由一系列只读层组成。Dockerfile 中的每条指令都会生成一个或多个新的镜像层。这种分层机制使得镜像的共享和缓存非常高效。\n\n二、Dockerfile 指令详解Dockerfile 包含一系列指令（Instruction），每个指令都表示一个构建步骤。\n2.1 FROM\n作用：指定基础镜像。Dockerfile 的第一条非注释指令必须是 FROM。\n格式：FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]\n示例：FROM ubuntu:22.04       # 使用 Ubuntu 22.04 作为基础镜像FROM node:18-alpine     # 使用 Node.js 18 的 Alpine 版本作为基础镜像\n最佳实践：选择尽可能小且功能足够的基础镜像，可以有效减小最终镜像的大小和攻击面。例如，优先选择 alpine 版本。\n\n2.2 LABEL\n作用：为镜像添加元数据。\n格式：LABEL &lt;key&gt;=&lt;value&gt; &lt;key&gt;=&lt;value&gt; ...\n示例：LABEL maintainer=&quot;your_email@example.com&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;My Super App&quot;\n最佳实践：为镜像添加有意义的标签，方便管理和查找。\n\n2.3 WORKDIR\n作用：设置工作目录。后续的 RUN, CMD, ENTRYPOINT, COPY, ADD 指令都会在这个目录下执行。\n格式：WORKDIR /path/to/workdir\n示例：WORKDIR /appCOPY package.json .  # 相当于 COPY package.json /app/package.json\n最佳实践：明确设置工作目录，方便管理文件路径，并提高可读性。可以使用多次，每次都会相对上一个 WORKDIR。\n\n2.4 COPY\n作用：从构建上下文复制文件或目录到镜像的文件系统。\n格式：COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt;\n示例：COPY . /app/          # 复制构建上下文所有内容到 /app/COPY src/index.js /app/src/ # 复制单个文件COPY web/dist /var/www/html/ # 复制目录内容\n最佳实践：\nCOPY 优于 ADD，因为 COPY 行为更明确，不支持自动解压等特殊功能。\n每次 COPY 只复制真正需要的文件，避免复制冗余文件或敏感信息。\n利用 .dockerignore 文件忽略不需要复制的文件（类似于 .gitignore）。\n\n\n\n2.5 ADD\n作用：类似于 COPY，但支持更多功能（不推荐在大多数情况下使用）。\n格式：ADD [--chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;... &lt;dest&gt;\n特殊功能：\n如果 &lt;src&gt; 是一个 URL，ADD 会下载这个文件到 &lt;dest&gt;。\n如果 &lt;src&gt; 是一个本地的 tar 压缩包（如 .tar, .gz, .bzip2, etc.），ADD 会自动解压到 &lt;dest&gt;。\n\n\n示例：ADD https://example.com/latest.tar.gz /tmp/ # 下载并解压ADD myapp.tar.gz /app/                   # 解压本地 tar 包\n最佳实践：\n优先使用 COPY，因为 ADD 的自动解压和下载功能可能带来意想不到的行为，且不利于缓存。\n对于下载文件，应该使用 RUN wget 或 RUN curl，这样可以更好地控制下载过程和清理。\n\n\n\n2.6 RUN\n作用：在当前镜像层中执行命令，创建新的镜像层。\n格式：\nRUN &lt;command&gt; (shell 形式，命令在 shell 中执行，如 sh -c)\nRUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec 形式，直接执行命令，不经过 shell)\n\n\n示例：RUN apt-get update &amp;&amp; apt-get install -y vim # shell 形式RUN [&quot;npm&quot;, &quot;install&quot;]                         # exec 形式\n最佳实践：\n合并多条 RUN 命令：将相关的 RUN 命令通过 &amp;&amp; 连接成一条，可以减少镜像层数，减小镜像大小。# 错误示例：会生成多层RUN apt-get updateRUN apt-get install -y curlRUN rm -rf /var/lib/apt/lists/*# 推荐：合并成一层RUN apt-get update \\    &amp;&amp; apt-get install -y curl \\    &amp;&amp; rm -rf /var/lib/apt/lists/*\n及时清理：在同一条 RUN 命令中，安装软件后立即清除缓存（如 apt-get clean, rm -rf /var/lib/apt/lists/*），避免无用数据被打包到镜像中。\n\n\n\n2.7 EXPOSE\n作用：声明容器运行时监听的端口。这仅仅是文档性质的声明，并不会真正发布端口。\n格式：EXPOSE &lt;port&gt; [&lt;port&gt;...]\n示例：EXPOSE 80         # 声明容器监听 80 端口EXPOSE 80/tcp 443/udp # 同时声明 TCP 和 UDP 端口\n使用：在 docker run 命令中使用 -p 或 -P 参数来实际发布端口。\n\n2.8 ENV\n作用：设置环境变量。这些变量在构建时和容器运行时都可用。\n格式：ENV &lt;key&gt;=&lt;value&gt; ...\n示例：ENV GREETING=&quot;Hello Docker!&quot;ENV HTTP_PROXY=&quot;http://proxy.example.com&quot;\n最佳实践：\n为应用程序提供必要的环境变量。\n避免在环境变量中存储敏感信息（如密码），应使用 Docker Secrets 或其他安全方案。\n\n\n\n2.9 ARG\n作用：定义构建时变量，仅在构建过程中可用。\n格式：ARG &lt;name&gt;[=&lt;default value&gt;]\n示例：ARG APP_VERSION=1.0.0RUN echo &quot;Building version: $&#123;APP_VERSION&#125;&quot;\n使用：在 docker build 命令中使用 --build-arg &lt;name&gt;=&lt;value&gt; 来传递值。\n区别于 ENV：ARG 仅在构建时有效，不会保留在最终镜像中，而 ENV 会。\n\n2.10 USER\n作用：设置运行容器的用户或用户组。\n格式：USER &lt;user&gt;[:&lt;group&gt;]\n示例：RUN adduser --system --group appuser # 创建一个系统用户USER appuser                      # 设置此用户运行后续命令\n最佳实践：\n避免使用 root 用户运行应用程序，以提高安全性。创建一个非特权用户来运行应用程序。\n\n\n\n2.11 VOLUME\n作用：声明容器中的一个挂载点，用于持久化数据或共享数据。\n格式：VOLUME [&quot;/path/to/mountpoint&quot;]\n示例：VOLUME [&quot;/var/log/myapp&quot;, &quot;/data&quot;]\n注意：VOLUME 只是一个声明，实际的数据挂载需要在 docker run 时使用 -v 参数指定。\n\n2.12 CMD\n作用：指定容器启动时要执行的默认命令。如果 docker run 命令中指定了其他命令，CMD 命令会被覆盖。\n格式：\nCMD [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec 形式，推荐)\nCMD [&quot;param1&quot;, &quot;param2&quot;] (作为 ENTRYPOINT 的默认参数)\nCMD command param1 param2 (shell 形式)\n\n\n同一个 Dockerfile 中只能有一条 CMD 指令。如果有多条，只有最后一条会生效。\n示例：CMD [&quot;npm&quot;, &quot;start&quot;]             # exec 形式CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo Hello &amp;&amp; npm start&quot;] # shell 形式的 exec (不推荐直接 shell 形式)\n最佳实践：使用 exec 形式，避免不必要的 shell 进程，提高效率。\n\n2.13 ENTRYPOINT\n作用：指定容器启动时要执行的命令。它不会被 docker run 的命令覆盖，而是作为该命令的补充或前缀。\n格式：ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] (exec 形式，推荐)\n同一个 Dockerfile 中只能有一条 ENTRYPOINT 指令。\n示例：ENTRYPOINT [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] # 启动 Nginx\n结合 CMD 使用：当 ENTRYPOINT 和 CMD 都存在时，CMD 的内容会作为 ENTRYPOINT 的参数。ENTRYPOINT [&quot;echo&quot;]CMD [&quot;Hello&quot;, &quot;World!&quot;]# 容器启动时执行：echo Hello World!# 如果运行 docker run myimage test_param，则执行：echo test_param\n最佳实践：\n当需要将容器作为可执行程序使用时（例如构建工具镜像），使用 ENTRYPOINT。\nENTRYPOINT 通常用于设置固定的启动命令，而 CMD 用于提供默认的参数。\n\n\n\n2.14 HEALTHCHECK\n作用：配置容器的健康检查。\n格式：HEALTHCHECK [OPTIONS] CMD command\n示例：HEALTHCHECK --interval=5s --timeout=3s --retries=3 \\    CMD curl -f http://localhost/ || exit 1\n最佳实践：为生产环境的容器配置健康检查，以便 Docker Daemon 知道容器是否正常运行，从而进行重启或调度。\n\n三、Docker 镜像构建的最佳实践3.1 使用 .dockerignore 文件\n与 .gitignore 类似，.dockerignore 文件用于指定在构建镜像时应忽略的文件和目录。\n好处：\n减少构建上下文的大小，加快构建速度。\n避免将敏感文件或不必要的文件（如 node_modules、.git、本地日志等）复制到镜像中。\n\n\n示例：.gitnode_modulesnpm-debug.logdisttmp/*.swp\n\n3.2 优化镜像层\n合并 RUN 指令：将多个相关的 RUN 命令合并为一条，用 &amp;&amp; 连接，并及时清理中间文件。这样可以减少镜像层数，每一层的大小也会更小。\n顺序优化：将不经常变动的指令放在 Dockerfile 的前面，这样 Docker 的构建缓存可以更好地发挥作用。一旦某一层发生变化，后续的所有层都需要重新构建。FROM node:18-alpineWORKDIR /app# 1. 复制 package.json 和 package-lock.json，确保只有当它们变动时才重新安装依赖# 这部分文件相对不常变动COPY package.json ./COPY package-lock.json ./# 2. 安装依赖 (如果 package.json 未变动，则会使用缓存)RUN npm install --production# 3. 复制应用代码 (这部分最常变动)COPY . .# 4. 构建应用 (如果代码变动，这层会重新构建)# RUN npm run build # 如果是前端应用，需要在容器内构建# 5. 暴露端口与定义启动命令EXPOSE 3000CMD [&quot;npm&quot;, &quot;start&quot;]\n\n3.3 多阶段构建 (Multi-stage Builds)\n概念：在 Dockerfile 中使用多个 FROM 指令，每个 FROM 都代表一个构建阶段。只将最终运行时所需的文件从一个阶段复制到下一个阶段，从而抛弃中间构建过程中产生的无用文件。\n好处：极大地减小最终镜像的大小，只包含生产环境所需的运行时依赖和应用程序代码。\n示例：构建一个前端 Vue 应用的镜像# 第一阶段：构建前端应用FROM node:18-alpine AS builderWORKDIR /appCOPY package.json ./RUN npm installCOPY . .RUN npm run build # 构建静态文件到 /app/dist 目录# 第二阶段：生产环境部署，使用 Nginx 作为 Web 服务器FROM nginx:stable-alpine# 复制第一阶段构建好的静态文件COPY --from=builder /app/dist /usr/share/nginx/html# 复制 Nginx 配置文件 (可选)# COPY nginx.conf /etc/nginx/conf.d/default.confEXPOSE 80CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;]\n\n3.4 最小化基础镜像\n使用 alpine 版本的基础镜像（如 ubuntu:alpine, node:18-alpine），它们基于 Alpine Linux，非常小巧。\n对于 Go、Rust 等编译型语言，可以直接使用 scratch 基础镜像，或者在一个构建阶段编译，在另一个 FROM scratch 的阶段中复制编译好的二进制文件。\n\n3.5 删除不必要的工具和缓存\n在 RUN 命令链中，安装完软件包后立即删除包管理器缓存（如 apt-get clean, yum clean all）。\n删除临时文件，例如：rm -rf /tmp/*。\n\n3.6 设置非 root 用户\n通过 USER 指令为应用程序创建一个非 root 用户，并使用该用户运行应用程序，提高安全性。\n\n3.7 使用固定标签的基础镜像\n避免使用 latest 标签作为基础镜像（如 FROM node:latest），因为 latest 标签可能会随时更新，导致构建结果不确定。\n应该使用具体的版本号，例如 FROM node:18.16.0-alpine，这有助于保证构建的可复现性。\n\n四、构建镜像使用 docker build 命令在 Dockerfile 所在的目录下构建镜像。\n\n基本命令：docker build -t my-app:1.0 .\n\n-t my-app:1.0：为镜像指定一个名称和标签。\n.：指定构建上下文的路径（当前目录）。\n\n\n指定 Dockerfile：docker build -f ./path/to/Dockerfile_alt -t my-app:2.0 .\n\n-f：指定 Dockerfile 的路径。\n\n\n传递构建参数：docker build --build-arg APP_VERSION=1.0.1 -t my-app:1.0 .\n\n五、总结Docker 镜像的构建是容器化工作流的基石。通过合理地编写 Dockerfile，并遵循上述最佳实践，你可以创建出：\n\n体积更小：减少存储空间，加快传输速度。\n构建更快：充分利用缓存机制。\n更安全：减少攻击面，避免以 root 运行。\n更可靠：保证构建的可复现性。\n\n深入理解每个 Dockerfile 指令的作用以及它们如何影响镜像的最终状态，是高效利用 Docker 的关键。不断实践和优化你的 Dockerfile，将使你的容器化应用程序更加健壮和高效。\n","categories":["Docker"],"tags":["2023","Docker","容器技术"]},{"title":"Python元类(Metaclass)深度解析","url":"/2023/2023-02-09_Python%E5%85%83%E7%B1%BB(Metaclass)%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","content":"Python 元类深度解析：从概念到实战\n\n“Everything is an object.” - Python之禅“Classes are objects too.” - 元类的核心思想\n\n在 Python 中，万物皆对象。你用 class 关键字定义的类，例如 str、int、list，它们本身也是对象。那么，是谁创建了这些类对象呢？答案就是“元类”(Metaclass)。元类是创建类的类，它允许我们在类被创建时对其行为进行定制，是 Python 中进行高级面向对象编程的强大工具。\n1. 什么是元类？在 Python 中，当你定义一个类 class MyClass: pass 的时候，Python 解释器会自动执行以下步骤：\n\n定义一个类对象：解释器读取 MyClass 的定义，并创建一个名为 MyClass 的类对象。\n将类对象绑定到命名空间：这个 MyClass 类对象被绑定到当前的命名空间中。\n\n然后，当你通过 my_instance = MyClass() 来创建实例时，MyClass 这个类对象就会被调用，从而创建并返回一个实例对象。\n元类就是用来创建这些类对象的。或者说，元类是类的模板，它控制着类的创建过程，可以拦截类的定义，修改类的属性、方法，甚至完全改变类的行为。\n简而言之：\n\n实例是由类创建的。\n类是由元类创建的。\n\n默认情况下，Python 中所有类的元类都是 type。type 是 Python 内置的元类，也是最基本的元类。\n2. type 元类：你的第一个元类type 不仅可以检查一个对象的类型（例如 type(1) 返回 &lt;class &#39;int&#39;&gt;），它更是一个功能强大的函数，可以动态地创建类。这是理解元类的关键。\ntype 函数有三种形式：\n\ntype(object)：返回 object 的类型。\ntype(name, bases, dict)：用于动态创建类。\n\n我们主要关注第二种形式：type(name, bases, dict)。\n\nname: 类的名称（字符串）。\nbases: 基类（父类）组成的元组。如果没有任何父类，传入一个空元组 ()。\ndict: 类的属性和方法组成的字典。键是属性&#x2F;方法名，值是属性值或方法函数。\n\n示例：使用 type 动态创建类\n# 常规方式定义一个类class MyClassRegular:    attr = 100    def method(self):        print(&quot;Hello from MyClassRegular!&quot;)# 使用 type 动态创建与 MyClassRegular 相同的类MyClassDynamic = type(    &#x27;MyClassDynamic&#x27;,  # name: 类的名称    (),                # bases: 基类元组，这里没有基类    &#123;                  # dict: 类的属性和方法字典        &#x27;attr&#x27;: 100,        &#x27;method&#x27;: lambda self: print(&quot;Hello from MyClassDynamic!&quot;)    &#125;)# 验证两个类行为一致print(MyClassRegular)       # &lt;class &#x27;__main__.MyClassRegular&#x27;&gt;print(MyClassDynamic)       # &lt;class &#x27;__main__.MyClassDynamic&#x27;&gt;instance_regular = MyClassRegular()instance_dynamic = MyClassDynamic()print(instance_regular.attr)  # 100instance_regular.method()     # Hello from MyClassRegular!print(instance_dynamic.attr)  # 100instance_dynamic.method()     # Hello from MyClassDynamic!# 确认它们的类型都是 typeprint(type(MyClassRegular)) # &lt;class &#x27;type&#x27;&gt;print(type(MyClassDynamic)) # &lt;class &#x27;type&#x27;&gt;\n\n这个例子清晰地表明，type 函数正是幕后创建类的“元类”。当我们使用 class 关键字时，Python 解释器实际上就是通过 type 来创建这个类对象的。\n3. 自定义元类：掌控类的创建过程现在我们知道 type 是默认的元类。那么，我们能否创建自己的元类，来定制类的创建过程呢？当然可以！\n一个自定义元类必须继承自 type。它的核心思想是：当你定义一个类时，如果你指定了一个自定义元类，那么 Python 不再调用 type 来创建你的类，而是会调用你指定的那个自定义元类。\n自定义元类通常会重写 __new__ 或 __init__ 方法。\n\n__new__(cls, name, bases, dct):\n\n在类对象被创建之前调用。\ncls: 元类本身（例如，如果你自定义的元类叫 MyMeta，那么 cls 就是 MyMeta）。\nname: 即将被创建的类的名称。\nbases: 即将被创建的类的基类元组。\ndct: 即将被创建的类的属性字典（包括方法）。\n职责：创建并返回新的类对象。通常会调用 super().__new__(cls, name, bases, dct) 来完成实际的类创建。在这个方法里，你可以在类创建前修改 name、bases 或 dct。\n\n\n__init__(cls, name, bases, dct):\n\n在类对象被创建之后，但实例被创建之前调用。\ncls: 已经创建好的类对象（比如 MyClass）。\nname, bases, dct: 与 __new__ 类似。\n职责：初始化已经创建好的类对象。通常用于在类创建后添加、修改或验证属性。\n\n\n\n3.1 定义一个简单的自定义元类# 1. 定义一个自定义元类，它必须继承自 typeclass MyMeta(type):    # __new__ 是在类对象创建之前被调用的    def __new__(cls, name, bases, dct):        print(f&quot;--- Meta: __new__ called for class &#123;name&#125; ---&quot;)        print(f&quot;Meta: Bases: &#123;bases&#125;&quot;)        print(f&quot;Meta: Dict: &#123;dct&#125;&quot;)        # 在这里可以修改 dct，例如添加一个属性        dct[&#x27;added_by_meta&#x27;] = &quot;This was added by MyMeta&quot;        dct[&#x27;upper_name&#x27;] = name.upper() # 添加大写类名属性        # 必须调用父类(type)的 __new__ 方法来实际创建类对象        return super().__new__(cls, name, bases, dct)    # __init__ 是在类对象创建之后被调用的    def __init__(cls_obj, name, bases, dct): # 注意：这里用 cls_obj 避免和前面参数名混淆        print(f&quot;--- Meta: __init__ called for class &#123;name&#125; ---&quot;)        print(f&quot;Meta: Class object created: &#123;cls_obj&#125;&quot;)        super().__init__(cls_obj, name, bases, dct) # 也要调用父类的 __init__# 2. 使用自定义元类创建类# 在 `class` 语句中，通过 `metaclass` 关键字参数指定元类class MyAdvancedClass(metaclass=MyMeta):    version = 1.0    def greeting(self):        print(f&quot;Hello from &#123;self.__class__.__name__&#125;, version &#123;self.version&#125;&quot;)# 3. 验证 MyAdvancedClass 的行为print(&quot;\\n--- After MyAdvancedClass definition ---&quot;)print(f&quot;MyAdvancedClass type is: &#123;type(MyAdvancedClass)&#125;&quot;) # &lt;class &#x27;__main__.MyMeta&#x27;&gt;# 确认元类添加的属性print(f&quot;MyAdvancedClass.added_by_meta: &#123;MyAdvancedClass.added_by_meta&#125;&quot;)print(f&quot;MyAdvancedClass.upper_name: &#123;MyAdvancedClass.upper_name&#125;&quot;)instance = MyAdvancedClass()instance.greeting()\n\n运行上述代码，你会看到输出的顺序：\n\nMyMeta.__new__ 会在 MyAdvancedClass 类定义被处理时立即执行。\nMyMeta.__init__ 紧接着执行，完成类对象的初始化。\n最后才是 MyAdvancedClass 自身的使用。\n\n这证明了元类确实在类创建的早期阶段就介入了。\n3.2 __prepare__ 方法 (Python 3.6+)在 Python 3.6 引入了 __prepare__(name, bases) 这个元类方法。它在 __new__ 和 __init__ 之前被调用，用于创建类的命名空间字典。\n\n职责：返回一个字典（或字典类对象），用于存储类的属性和方法。默认情况下，Python 使用普通的 dict。你可以返回一个 OrderedDict 等，确保属性的定义顺序得到保留。\n\nfrom collections import OrderedDictclass OrderedClassMeta(type):    @classmethod    def __prepare__(metacls, name, bases):        print(f&quot;--- Meta: __prepare__ called for class &#123;name&#125; ---&quot;)        return OrderedDict() # 返回一个有序字典    def __new__(metacls, name, bases, classdict):        print(f&quot;--- Meta: __new__ called for class &#123;name&#125; ---&quot;)        print(f&quot;Meta: classdict type in __new__: &#123;type(classdict)&#125;&quot;)        return super().__new__(metacls, name, bases, classdict)    def __init__(cls, name, bases, classdict):        print(f&quot;--- Meta: __init__ called for class &#123;name&#125; ---&quot;)        super().__init__(cls, name, bases, classdict)class MyOrderedClass(metaclass=OrderedClassMeta):    def method_a(self): pass    _property_x = 10    def method_b(self): pass# 此时 MyOrderedClass 的属性字典将保留定义顺序# 虽然通过 dir() 或 __dict__ 仍然会看到默认的排序，# 但在元类创建类时，__prepare__ 提供的有序字典确保了处理属性的顺序性。# 实际的应用场景可能在需要反射或代码生成时，依赖定义的顺序。\n\n4. __call__ 方法：控制实例创建我们已经看到元类的 __new__ 和 __init__ 控制着类的创建过程。但当我们通过 MyClass() 来创建实例时，幕后发生了什么呢？\n实际上，当你调用 MyClass() 时，Python 会调用 MyClass 这个类对象的 __call__ 方法。由于 MyClass 是由元类创建的，所以它的 __call__ 方法实际上继承自它的元类（type 或你的自定义元类）。\ntype 的 __call__ 方法做了三件事：\n\n调用 MyClass.__new__(cls, *args, **kwargs) 创建实例对象。\n如果 __new__ 返回的是 cls 的实例，则调用 MyClass.__init__(self, *args, **kwargs) 初始化实例。\n返回实例对象。\n\n因此，如果你想控制实例的创建过程（例如，实现单例模式、延迟加载等），你应该在自定义元类中重写 __call__ 方法。\nclass SingletonMeta(type):    _instances = &#123;&#125;    def __call__(cls, *args, **kwargs):        # 如果类的实例尚未被创建        if cls not in cls._instances:            # 调用 type.__call__ 来创建实例，并存储它            cls._instances[cls] = super().__call__(*args, **kwargs)        return cls._instances[cls] # 返回已有的实例class MySingleton(metaclass=SingletonMeta):    def __init__(self, data):        self.data = data        print(f&quot;MySingleton instance &#123;id(self)&#125; with data &#x27;&#123;self.data&#125;&#x27; created.&quot;)# 第一次创建实例s1 = MySingleton(&quot;first_data&quot;) # 会输出创建信息s2 = MySingleton(&quot;second_data&quot;) # 不会再次创建，直接返回s1print(f&quot;s1 is s2: &#123;s1 is s2&#125;&quot;) # Trueprint(f&quot;s1.data: &#123;s1.data&#125;&quot;)   # first_dataprint(f&quot;s2.data: &#123;s2.data&#125;&quot;)   # first_data\n这个例子展示了如何使用元类的 __call__ 方法轻松实现单例模式。\n5. 什么时候需要使用元类？元类是一个高级工具，通常在以下场景中考虑使用：\n\n框架级开发：在构建大型框架时，你可能需要对所有由该框架创建的类强制执行某些行为，例如：\n\n自动注册类：所有继承自特定基类的类都被自动注册到一个列表中。\n注入通用方法&#x2F;属性：确保所有类都拥有某些特定的方法或属性（如ORM模型类自动拥有 query 方法）。\n接口&#x2F;抽象类的验证：在类定义时检查它是否实现了所有必须的方法。\n修改类的行为：如强制所有方法名以特定前缀开头。\n\n\nAPI 定义：当你需要一个非常声明式的 API 时，元类能帮助你将一些“魔术”封装起来，让用户只需要声明性地定义类，而无需关心底层实现。\n\nORM (Object-Relational Mapping)：ORM 中经常用到元类来将 Python 类映射到数据库表。例如，Django ORM 的 models.Model 就是通过元类实现的。当你定义 class User(models.Model): ... 时，元类会解析你的字段定义，并为其生成对应的数据库列以及 save, filter 等方法。\n\n单例模式：如上例所示，可以强制一个类只能有一个实例。\n\n插件系统：可以动态地发现并加载所有继承某个基类的插件。\n\n\n然而，请记住：\n\n元类是强大的，但也是复杂的。它们会增加代码的复杂性和理解难度。\n不要过度使用元类。 大部分情况下，继承、类装饰器甚至普通的函数就能解决问题。\n只有当需要在类创建时修改类本身或其行为时，才考虑元类。\n\n6. 与类装饰器、继承的比较\n\n\n特性&#x2F;功能\n元类 (Metaclass)\n类装饰器 (Class Decorator)\n继承 (Inheritance)\n\n\n\n作用时机\n类创建时（在 class 语句执行时）\n类定义后（在类对象创建完成后）\n运行时，实例创建时\n\n\n影响范围\n控制如何创建类本身，影响所有实例和类本身的行为\n接受一个已创建的类，返回一个新类或修改后的类\n改变子类的行为，通过方法重写、属性覆盖\n\n\n修改能力\n可以修改 类 的 __dict__、基类、名称等，完全控制类创建过程\n对已创建的类进行修改（如添加方法、属性）\n通过子类定义，增加或修改父类的属性和方法\n\n\n应用场景\n框架级、ORM、自动注册、强制类结构、单例等\n常用工具、日志、权限、接口检查、添加 mixin\n代码复用、多态、LSP、组织代码结构\n\n\n复杂性\n高，引入了额外的抽象层\n中等\n低-中等\n\n\n推荐度\n仅在必要时使用（高级框架）\n常用，替代部分元类功能\n最常用，面向对象编程基石\n\n\n总结：\n\n继承是 Python 中最基本和常用的代码重用机制，用于定义“is-a”关系。\n类装饰器是在类已经完全创建之后，对其进行“包装”或“修改”。它比元类更简单，可以处理许多本需要元类才能解决的问题。\n元类则是在类诞生的那一刻就介入，控制着类的整个生产流程。\n\n如果你需要一个通用机制，让每个特定类或子类都能拥有一些额外的属性或方法，继承通常是最好的选择。如果需要对某个特定的类进行非侵入性的修改或增强，类装饰器更简洁。只有当你需要影响所有类的创建方式（无论它们是否通过继承共享基类，或是需要影响类的 __dict__、bases 等核心定义结构）时，才应该考虑元类。\n7. 实例与类创建的流程回顾理解元类，最好回顾一下 Python 对象、类和元类之间的关系及创建流程：\n\n定义一个类 MyClass：\n\nPython 解释器发现 class MyClass(metaclass=MyMeta): ...。\n它首先找到 MyMeta 这个元类。\n调用 MyMeta.__prepare__：准备类的字典，默认是 dict。\n执行类体代码：将 version = 1.0 和 greeting 方法加入到准备好的字典中。\n调用 MyMeta.__new__(MyMeta, &quot;MyClass&quot;, (object,), class_dict)：MyMeta 的 __new__ 方法被调用。它会在此时创建 MyClass 类对象。\n调用 MyMeta.__init__(MyClass_obj, &quot;MyClass&quot;, (object,), class_dict)：MyMeta 的 __init__ 方法被调用，用于初始化已经创建好的 MyClass 类对象。\n返回 MyClass 类对象。\n\n\n创建 MyClass 的实例 my_instance = MyClass()：\n\nPython 解释器发现 MyClass()，它会去调用 MyClass 这个类对象本身的 __call__ 方法。\n由于 MyClass 是由 MyMeta 创建的，MyClass 的 __call__ 方法继承自 MyMeta（或 type）。\n调用 MyMeta.__call__(MyClass, *args, **kwargs)：\n它首先会调用 MyClass.__new__(MyClass, *args, **kwargs) 来创建实例对象（MyClass 自己的 __new__ 方法，如果定义了）。\n如果 MyClass.__new__ 返回的是 MyClass 的实例，它会接着调用 MyClass.__init__(instance_obj, *args, **kwargs) 来初始化实例对象。\n返回实例对象 my_instance。\n\n\n\n\n\n这个流程图可以帮助你清晰地理解各个方法在哪个阶段发挥作用。\n结语元类是 Python 面向对象编程中最具魔力的特性之一，它将“一切皆对象”的哲学推向了极致。掌握元类，意味着你对 Python 对象的创建和生命周期有了更深层次的理解和掌控。然而，就像其他强大的工具一样，元类也需要谨慎使用。在决定使用元类之前，请始终评估是否可以通过继承或类装饰器来实现相同的功能。只有当你的需求确实落入元类的独特领域时，它才是你的最佳选择。\n","categories":["Python"],"tags":["2023","Python"]},{"title":"Python神库Pydantic深度解析：数据验证与设置管理的利器","url":"/2023/2023-02-10_Python%E7%A5%9E%E5%BA%93Pydantic%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81%E4%B8%8E%E8%AE%BE%E7%BD%AE%E7%AE%A1%E7%90%86%E7%9A%84%E5%88%A9%E5%99%A8/","content":"Python Pydantic 库深度解析：数据验证与设置管理的利器\nPydantic 是一个功能强大且广受欢迎的 Python 库，它使用 Python 类型提示来进行数据验证、序列化和反序列化。Pydantic 强制执行类型提示，并在数据无效时提供友好的错误报告，极大地简化了数据处理、API 请求体验证和配置管理等任务。\n\n\n1. 为什么选择 Pydantic？在现代 Python 应用开发中，数据从多种来源流入（API 请求、数据库查询、配置文件、第三方服务等），其结构和类型可能不完全符合预期。这导致了对数据验证的强烈需求。Pydantic 的出现，优雅地解决了这个问题：\n\n强制类型提示：利用 Python 3.6+ 的类型提示，在运行时进行数据验证。\n自动数据转换：在可能的情况下，Pydantic 会自动将数据转换为正确的类型（例如，将 &quot;123&quot; 转换为 123）。\n清晰的错误报告：当数据验证失败时，Pydantic 会生成详细、易于理解的错误信息。\n与 FastAPI 无缝集成：FastAPI 框架将 Pydantic 作为其核心组件，用于请求体、查询参数和响应模型的验证和序列化。\nSettings 管理：可以非常方便地从环境变量、.env 文件等加载配置。\nJSON 序列化&#x2F;反序列化：轻松将 Python 对象转换为 JSON 字符串，反之亦然。\n可扩展性：支持自定义验证器和类型。\n\n2. 核心概念：BaseModelPydantic 的核心是 BaseModel。你通过继承 pydantic.BaseModel 来定义你的数据模型。模型中的每个字段都是一个 Python 类型提示。\n2.1 定义一个基本模型from pydantic import BaseModelfrom typing import List, Optional# 定义一个 User 模型class User(BaseModel):    id: int    name: str = &quot;Anonymous&quot; # 带有默认值的字段    email: Optional[str] = None # Optional 表示该字段可以是 str 或 None    is_active: bool = True    tags: List[str] = [] # 列表类型，默认值为空列表# 2.2 创建模型实例并验证数据# 成功创建实例user_data = &#123;    &quot;id&quot;: 123,    &quot;name&quot;: &quot;Alice&quot;,    &quot;email&quot;: &quot;alice@example.com&quot;&#125;user = User(**user_data)print(user)# 输出: id=123 name=&#x27;Alice&#x27; email=&#x27;alice@example.com&#x27; is_active=True tags=[]# 默认值和Optionaluser_no_email = User(id=456)print(user_no_email)# 输出: id=456 name=&#x27;Anonymous&#x27; email=None is_active=True tags=[]# 自动类型转换user_str_id = User(id=&quot;789&quot;, name=&quot;Bob&quot;, is_active=&quot;False&quot;) # &quot;789&quot; -&gt; 789, &quot;False&quot; -&gt; Falseprint(user_str_id)# 输出: id=789 name=&#x27;Bob&#x27; email=None is_active=False tags=[]# 2.3 验证失败的例子try:    User(id=&quot;abc&quot;) # id 应该是整数，&quot;abc&quot; 无法转换为整数except Exception as e:    print(f&quot;\\nValidationError: &#123;e&#125;&quot;)    # 输出类似于:    # ValidationError: 1 validation error for User    # id    #   value is not a valid integer (type=type_error.integer)try:    User(id=1, email=123) # email 应该是字符串或None，而不是整数except Exception as e:    print(f&quot;\\nValidationError: &#123;e&#125;&quot;)    # 输出类似于:    # ValidationError: 1 validation error for User    # email    #   value is not a valid string (type=type_error.string)\n\n3. 嵌套模型Pydantic 可以很方便地处理复杂、嵌套的数据结构。\nfrom pydantic import BaseModel, HttpUrlfrom typing import List, Dictclass Item(BaseModel):    name: str    price: float    is_offer: Optional[bool] = Noneclass Order(BaseModel):    order_id: int    items: List[Item]    customer_name: str    delivery_address: str    total_amount: floatclass Company(BaseModel):    name: str    website: HttpUrl # 使用 Pydantic 的 HttpUrl 类型，自动验证URL格式# 创建嵌套模型实例order_data = &#123;    &quot;order_id&quot;: 1001,    &quot;items&quot;: [        &#123;&quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 1200.50&#125;,        &#123;&quot;name&quot;: &quot;Mouse&quot;, &quot;price&quot;: 25.00, &quot;is_offer&quot;: True&#125;    ],    &quot;customer_name&quot;: &quot;Charlie&quot;,    &quot;delivery_address&quot;: &quot;123 Main St&quot;,    &quot;total_amount&quot;: 1225.50&#125;order = Order(**order_data)print(order)# 输出: order_id=1001 items=[Item(name=&#x27;Laptop&#x27;, price=1200.5, is_offer=None), Item(name=&#x27;Mouse&#x27;, price=25.0, is_offer=True)] customer_name=&#x27;Charlie&#x27; delivery_address=&#x27;123 Main St&#x27; total_amount=1225.5# 验证 HttpUrlcompany_valid = Company(name=&quot;TechCorp&quot;, website=&quot;https://www.techcorp.com&quot;)print(company_valid)try:    Company(name=&quot;InvalidCo&quot;, website=&quot;not-a-url&quot;)except Exception as e:    print(f&quot;\\nValidationError: &#123;e&#125;&quot;) # 自动校验 URL 格式\n\n4. 数据导出与JSON操作Pydantic 模型提供了方便的方法来将数据导出为字典或 JSON 字符串。\nfrom pydantic import BaseModelimport jsonclass Product(BaseModel):    product_id: int    name: str    price: floatproduct_instance = Product(product_id=1, name=&quot;Gizmo&quot;, price=99.99)# 导出为字典product_dict = product_instance.dict()print(&quot;As dict:&quot;, product_dict)# As dict: &#123;&#x27;product_id&#x27;: 1, &#x27;name&#x27;: &#x27;Gizmo&#x27;, &#x27;price&#x27;: 99.99&#125;# 导出为JSON字符串product_json = product_instance.json()print(&quot;As JSON string:&quot;, product_json)# As JSON string: &#123;&quot;product_id&quot;: 1, &quot;name&quot;: &quot;Gizmo&quot;, &quot;price&quot;: 99.99&#125;# exclude 参数：导出时排除某些字段product_dict_no_price = product_instance.dict(exclude=&#123;&#x27;price&#x27;&#125;)print(&quot;Exclude price:&quot;, product_dict_no_price)# Exclude price: &#123;&#x27;product_id&#x27;: 1, &#x27;name&#x27;: &#x27;Gizmo&#x27;&#125;# include 参数：只导出指定字段product_dict_only_name = product_instance.dict(include=&#123;&#x27;name&#x27;&#125;)print(&quot;Only name:&quot;, product_dict_only_name)# Only name: &#123;&#x27;name&#x27;: &#x27;Gizmo&#x27;&#125;# by_alias 参数：如果使用了字段别名，导出时使用别名from pydantic import Fieldclass SensorData(BaseModel):    sensor_id: int = Field(alias=&#x27;id&#x27;) # 定义别名    value: floatsensor_data_instance = SensorData(id=101, value=25.5)print(&quot;SensorData as dict:&quot;, sensor_data_instance.dict()) # 默认使用原始字段名# SensorData as dict: &#123;&#x27;sensor_id&#x27;: 101, &#x27;value&#x27;: 25.5&#125;print(&quot;SensorData as dict by_alias:&quot;, sensor_data_instance.dict(by_alias=True)) # 使用别名# SensorData as dict by_alias: &#123;&#x27;id&#x27;: 101, &#x27;value&#x27;: 25.5&#125;\n\n5. 字段校验与自定义验证器除了 Python 内置类型和 Pydantic 提供的特殊类型（如 HttpUrl），你还可以使用 pydantic.Field 来添加更精细的字段验证，或定义自己的验证函数。\n5.1 Field 的使用Field 函数可以定义字段的默认值、别名、校验规则和额外说明。\nfrom pydantic import BaseModel, Field, EmailStrfrom typing import Listclass UserProfile(BaseModel):    username: str = Field(..., min_length=3, max_length=20, regex=&quot;^[a-zA-Z0-9_]+$&quot;) # `...` 表示该字段必须提供    email: EmailStr # EmailStr 是 Pydantic 内置的邮箱格式验证类型    age: int = Field(..., gt=0, lt=150) # gt: great than, lt: less than    bio: Optional[str] = Field(None, max_length=500) # 可以提供 None 作为默认值    tags: List[str] = Field(default_factory=list) # 列表默认值推荐使用 default_factory    # default_factory 参数提供了创建复杂类型默认值的方法，    # 每次创建实例时都会调用该工厂函数，避免多个实例共享同一个可变默认值的问题。# 有效数据profile = UserProfile(username=&quot;johndoe123&quot;, email=&quot;john@example.com&quot;, age=30)print(profile)# 无效数据示例try:    UserProfile(username=&quot;jd&quot;, email=&quot;invalid-email&quot;, age=160)except Exception as e:    print(f&quot;\\nValidation Error for UserProfile: &#123;e&#125;&quot;)\n\n5.2 @validator 自定义验证器当你需要更复杂的业务逻辑来验证字段时，可以使用 @validator 装饰器定义一个或多个验证函数。\nfrom pydantic import BaseModel, validator, ValidationErrorclass Post(BaseModel):    title: str    content: str    rating: int = 1 # 默认值    tags: List[str] = []    # 定义一个验证器来确保标题是驼峰式命名    @validator(&#x27;title&#x27;)    def title_must_be_camel_case(cls, v):        if not v[0].isupper():            raise ValueError(&#x27;title must start with an uppercase letter&#x27;)        return v    # 定义一个验证器来确保 rating 在 1 到 5 之间    # 可以在同一个 validator 中校验多个字段    @validator(&#x27;rating&#x27;)    def rating_must_be_in_range(cls, v):        if not 1 &lt;= v &lt;= 5:            raise ValueError(&#x27;rating must be between 1 and 5&#x27;)        return v    # 另一个验证器，在某个字段验证通过后再进行处理 (pre=False 是默认行为)    @validator(&#x27;content&#x27;)    def content_not_empty(cls, v):        if not v.strip():            raise ValueError(&#x27;content cannot be empty&#x27;)        return v# 成功创建post1 = Post(title=&quot;MyFirstPost&quot;, content=&quot;Some content here.&quot;, rating=4)print(post1)# 验证失败示例try:    Post(title=&quot;mySecondPost&quot;, content=&quot;xyz&quot;, rating=0)except ValidationError as e:    print(f&quot;\\nValidation Error for Post: &#123;e&#125;&quot;)    # 输出将显示多个验证错误\n\npre=True 的使用：如果你想在字段值被 Pydantic 的内置类型转换和验证之前运行自定义验证器，可以使用 pre=True。\nclass MyModel(BaseModel):    value: int    @validator(&#x27;value&#x27;, pre=True)    def check_value_is_string_convertible(cls, v):        &quot;&quot;&quot;这个验证器会在 Pydantic 尝试将 `v` 转成 int 之前执行&quot;&quot;&quot;        if isinstance(v, str) and not v.isdigit():            raise ValueError(&#x27;value string can only contain digits&#x27;)        return vtry:    MyModel(value=&quot;abc&quot;) # &quot;abc&quot; 会先经过 check_value_is_string_convertibleexcept ValidationError as e:    print(f&quot;\\nPre-validator error: &#123;e&#125;&quot;) # 看到的是自定义验证器的错误try:    MyModel(value=&quot;123&quot;) # &quot;123&quot; 会通过 pre 验证器，然后被 Pydantic 转换为 123    print(MyModel(value=&quot;123&quot;))except ValidationError as e:    pass\n\n6. 设置管理：BaseSettingsPydantic 提供了 BaseSettings 类，它是 BaseModel 的一个子类，专门用于从环境变量、.env 文件等加载应用程序配置。\nfrom pydantic import BaseSettings, Fieldfrom typing import Optionalclass AppSettings(BaseSettings):    app_name: str = &quot;My Awesome App&quot;    database_url: str    api_key: Optional[str] = None    debug_mode: bool = False    port: int = Field(8000, env=&quot;APP_PORT&quot;) # 可以为字段指定具体的环境变量名    class Config:        env_file = &quot;.env&quot; # 指定从 .env 文件加载配置        env_file_encoding = &#x27;utf-8&#x27;# 假设我们在项目根目录下有一个名为 `.env` 的文件，内容如下：# DATABASE_URL=&quot;postgresql://user:password@host:5432/dbname&quot;# API_KEY=&quot;your_secret_api_key_123&quot;# APP_PORT=8001# DEBUG_MODE=True# 创建设置实例settings = AppSettings()print(f&quot;App Name: &#123;settings.app_name&#125;&quot;)print(f&quot;Database URL: &#123;settings.database_url&#125;&quot;)print(f&quot;API Key: &#123;settings.api_key&#125;&quot;)print(f&quot;Debug Mode: &#123;settings.debug_mode&#125;&quot;)print(f&quot;Port: &#123;settings.port&#125;&quot;)# 注意：Pydantic 加载环境变量的优先级：# 1. 显式传递给 BaseSettings 构造函数的数据 (如 `AppSettings(api_key=&quot;manual_key&quot;)`)# 2. 环境变量 (如 `os.environ[&#x27;DATABASE_URL&#x27;]`)# 3. .env 文件 (如 `.env` 中的 `DATABASE_URL`)# 4. 字段的默认值 (如 `app_name=&quot;My Awesome App&quot;`)# 示例：通过环境变量覆盖 .env 文件中的值# export DATABASE_URL=&quot;mongodb://...&quot;# settings = AppSettings()# 此时 settings.database_url 会是 mongodb://...\n\nPydantic-Settings (Pydantic V2+)在 Pydantic V2 中，BaseSettings 被移到了单独的库 pydantic-settings 中。安装方式：pip install pydantic-settings。使用方式基本相同。\n7. 类型别名与泛型模型Pydantic 支持使用 typing 模块的各种高级类型提示，包括 Union, Literal, Dict, Set, Tuple 等。\n7.1 类型别名from pydantic import BaseModelfrom typing import Union, Literal, List# 定义一个类型别名UserID = intStatus = Literal[&quot;pending&quot;, &quot;completed&quot;, &quot;failed&quot;] # 限定只能是这三个字符串之一class Task(BaseModel):    task_id: UserID # 使用自定义类型别名    description: str    status: Status    assigned_to: Optional[UserID] = Nonetask1 = Task(task_id=1, description=&quot;Finish report&quot;, status=&quot;pending&quot;)print(task1)try:    # 状态必须是预定义的值    Task(task_id=2, description=&quot;Invalid task&quot;, status=&quot;in_progress&quot;)except ValidationError as e:    print(f&quot;\\nValidation Error for Task Status: &#123;e&#125;&quot;)\n\n7.2 泛型模型Pydantic 支持创建泛型模型，这在处理结构相同但内部数据类型可能不同的数据时非常有用。\nfrom pydantic import BaseModelfrom typing import TypeVar, Generic, ListT = TypeVar(&#x27;T&#x27;) # 定义一个类型变量class PaginatedResponse(BaseModel, Generic[T]):    page: int    page_size: int    total_count: int    items: List[T] # items 列表中的元素类型是 Tclass Product(BaseModel):    product_id: int    name: strclass User(BaseModel):    user_id: int    username: str# 创建包含 Product 列表的分页响应product_page = PaginatedResponse[Product](    page=1,    page_size=10,    total_count=100,    items=[        &#123;&quot;product_id&quot;: 1, &quot;name&quot;: &quot;Laptop&quot;&#125;,        &#123;&quot;product_id&quot;: 2, &quot;name&quot;: &quot;Mouse&quot;&#125;    ])print(&quot;Product Page:&quot;, product_page)# 创建包含 User 列表的分页响应user_page = PaginatedResponse[User](    page=2,    page_size=5,    total_count=50,    items=[        &#123;&quot;user_id&quot;: 10, &quot;username&quot;: &quot;Alice&quot;&#125;,        &#123;&quot;user_id&quot;: 11, &quot;username&quot;: &quot;Bob&quot;&#125;    ])print(&quot;User Page:&quot;, user_page)# 验证失败示例try:    PaginatedResponse[Product](        page=1,        page_size=10,        total_count=100,        items=[            &#123;&quot;product_id&quot;: 1, &quot;name&quot;: &quot;Laptop&quot;&#125;,            &#123;&quot;user_id&quot;: 20, &quot;username&quot;: &quot;Charlie&quot;&#125; # 类型不匹配 Product        ]    )except ValidationError as e:    print(f&quot;\\nValidation Error for Generic Model: &#123;e&#125;&quot;)\n\n8. 总结与最佳实践Pydantic 是一个现代 Python 不可或缺的库，它通过利用类型提示，极大地提升了数据验证和序列化的效率及可靠性。\n最佳实践：\n\n始终使用类型提示：这不仅是 Pydantic 的要求，也是良好的 Python 编程习惯。\n利用默认值：为可选字段提供默认值，使你的模型更健壮。\n处理可变默认值：对于列表、字典等可变类型，使用 Field(default_factory=list) 而不是 f: List[str] = []。\n善用 Optional 和 Union：明确表达字段可能为空或有多种类型的情况。\n使用 Pydantic 内置类型：如 EmailStr, HttpUrl, IPv4Address 等，省去了手动编写验证器的麻烦。\n自定义验证器：当需要复杂业务逻辑时，@validator 是强大的工具。\n集成 BaseSettings：简化应用程序的配置管理，统一从环境变量或 .env 文件加载。\n明确错误处理：在调用 .parse_obj() 或模型实例化时，始终考虑 ValidationError 异常的处理，给用户提供有意义的反馈。\n\nPydantic 不仅仅是一个验证库，它还是数据建模、API 契约定义和应用程序设置管理的强大统一工具。掌握 Pydantic，将让你的 Python 项目更加健壮、可靠和易于维护。\n","categories":["Python"],"tags":["2023","Python","Pydantic","数据校验"]},{"title":"发音记忆法：如何通过发音高效记忆英语单词的详细教程","url":"/2023/2023-03-04_%E5%8F%91%E9%9F%B3%E8%AE%B0%E5%BF%86%E6%B3%95%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E5%8F%91%E9%9F%B3%E9%AB%98%E6%95%88%E8%AE%B0%E5%BF%86%E8%8B%B1%E8%AF%AD%E5%8D%95%E8%AF%8D%E7%9A%84%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/","content":"\n许多人在学习英语单词时，习惯性地只看字母组合，导致背了就忘，而且容易拼写错误。这篇教程旨在改变这种低效的学习方式，教你如何以发音为核心，结合音标、自然拼读和听力，构建一套高效、持久的单词记忆策略。\n\n英语作为一门拼音文字，其单词的拼写和发音之间存在着内在的规律。掌握这些规律，将单词的“形”、“音”、“义”紧密结合，可以大大提升记忆效率和准确性。通过发音来记忆单词，不仅能帮助你更好地拼写，还能提高听力理解和口语表达能力。\n\n\n一、为什么只看字母记单词效率低下？传统的“死记硬背”方式通常关注字母顺序，例如：beautiful → B-E-A-U-T-I-F-U-L 美丽的\n这种方法存在以下问题：\n\n脱离语境：字母组合是抽象的，没有实际的语境或声音联系，大脑难以建立有效记忆。\n效率低下：每个单词都需要单独记忆字母序列，记忆量大，复习周期长。\n容易混淆：相似字母组合的单词（如 through, thorough, though）极易混淆。\n发音障碍：不了解发音规律，见到生词不敢读，听力理解也受到影响。\n拼写错误：由于不熟悉形音对应，拼写时容易出错。\n\n而通过发音记忆，我们关注的是：beautiful → &#x2F;‘bjuːtɪfl&#x2F; → 美丽的\n这将发音作为连接拼写和意义的桥梁。\n二、发音记忆法的核心原理发音记忆法的核心是利用语音规律，将单词的“音”作为记忆的中心，连接其“形”（拼写）和“义”（意义）。\n\n音形结合（Phonetic Awareness）：理解字母和字母组合如何发音。\n整体输入，立体记忆：通过听、读、写、说，多通道刺激大脑，形成更牢固的记忆。\n有意义的联结：发音提供了单词的“声音形象”，更容易与意义产生联结。\n高效检索：听到一个单词，能够快速联想到其拼写；看到一个单词，也能大概猜出其发音。\n\n三、发音记忆法的基石：音标与自然拼读要通过发音记单词，首先需要掌握发音的基础知识。\n3.1 国际音标 (IPA)重要性：音标是英语发音的“字母表”，它能准确无误地表示任何一个单词的发音，是纠正发音、独立学习发音的权威工具。\n学习方法：\n\n学习单个音标：逐一学习每个元音（长元音、短元音、双元音）和辅音（清辅音、浊辅音），掌握其正确发音口型和舌位。可以参考专业的音标教学视频或教程。\n推荐资源：YouTube 上的发音教学视频（搜索 “IPA English Pronunciation”）、英语字典中的音标发音示范。\n\n\n对比辨析相似音：特别注意那些容易混淆的音标，如 &#x2F;ɪ&#x2F; 与 &#x2F;iː&#x2F; (ship vs sheep)，&#x2F;æ&#x2F; 与 &#x2F;ʌ&#x2F; (cat vs cut)，&#x2F;θ&#x2F; 与 &#x2F;ð&#x2F; (thin vs this)。\n多听多模仿：反复听标准发音，并大声模仿，录下自己的声音与原音对比，找出差距并纠正。\n\n应用：当你遇到一个新单词时，第一步是查看其音标，并尝试读出来。\n3.2 自然拼读 (Phonics)重要性：自然拼读是学习英语读写的关键，它揭示了字母和字母组合与发音之间的常见规律。掌握好自然拼读，你可以“见词能读，听音能写”，极大地减少对音标的依赖。\n学习方法：\n\n单个字母发音：学习 26 个字母在单词中的基本发音（如 a 在 cat 中的发音）。\n常见字母组合发音：学习元音组合（如 ea &#x2F;iː&#x2F;, ai &#x2F;eɪ&#x2F;）、辅音组合（如 sh &#x2F;ʃ&#x2F;, ch &#x2F;tʃ&#x2F;, th &#x2F;θ&#x2F; &#x2F;ð&#x2F;）以及 R-controlled vowels (如 ar &#x2F;ɑːr&#x2F;, er &#x2F;ɜːr&#x2F;) 等。\n音节划分与重音：学习如何将单词划分为音节，并确定重音位置。重音是影响发音和听力理解的关键。\n练习拼读：结合各种自然拼读练习材料和 App，大量练习将字母组合拼读成词，将听到的词拆解成字母组合。\n\n与音标的关系：自然拼读是音标的应用。音标是精确的发音标记，而自然拼读是发音的规律总结。两者相辅相成。\n四、发音记忆法的具体步骤 (以一个新词为例)让我们以 exacerbate (加剧，恶化) 这个单词为例，演示发音记忆法的具体步骤。\n步骤 1：第一眼看词，不要急于拼读，而是听标准发音。\n工具：在线词典 (如剑桥词典、牛津词典、有道词典等)、翻译 App (如 Google 翻译) 都有发音功能。\n动作：点击发音按钮，至少听 3-5 遍，感受这个单词的整体声音、节奏和重音。闭上眼睛听，让声音进入大脑。\n\n步骤 2：分析音标，确认每个音素。\n工具：在线词典或纸质词典中的音标。\n动作：\n找到 exacerbate 的音标：&#x2F;ɪɡˈzæsəbeɪt&#x2F; (美式) 或 &#x2F;ɪɡˈzæsəbeɪt&#x2F; (英式)。\n划分音节：通常音标会自带音节划分，或根据发音自行划分：ex-ac-er-bate。\n找出重音：在音标中，重音符号 ˈ 会在重读音节的前面。例如美式中 /ɪɡˈzæsəbeɪt/，重音在第二个音节 /zæs/。这意味着这个音节要读得更响、更长、更高。\n逐个音素发音：\n&#x2F;ɪ&#x2F;：短促的“衣”\n&#x2F;ɡ&#x2F;：浊辅音“g”\n&#x2F;ˈzæs&#x2F;：重读音节，&#x2F;z&#x2F; + &#x2F;æ&#x2F; (像中文“啊”口张大) + &#x2F;s&#x2F;\n&#x2F;ə&#x2F;：非重读音节的弱读元音“呃”\n&#x2F;beɪt&#x2F;：&#x2F;b&#x2F; + &#x2F;eɪ&#x2F; (双元音，像中文“éi”) + &#x2F;t&#x2F;\n\n\n\n\n\n步骤 3：结合自然拼读，建立音形对应。现在，将音标的每个音素与单词的字母或字母组合对应起来。\n\nex- : 对应 &#x2F;ɪɡ&#x2F;。这里的 e 发 &#x2F;ɪ&#x2F;，x 在非重读音节中且后接元音时常发 &#x2F;ɡz&#x2F; 或 &#x2F;ks&#x2F;，这里是 &#x2F;ɡz&#x2F;。\n-ac- : 对应 &#x2F;ˈzæs&#x2F;。注意重音！ac 中的 a 发 &#x2F;æ&#x2F;。\n-er- : 对应 &#x2F;ə&#x2F;。er 在非重读音节中常弱读为 &#x2F;ə&#x2F;。\n-bate : 对应 &#x2F;beɪt&#x2F;。b &#x2F;b&#x2F;，ate 结尾的 e 不发音，使 a 发长音 &#x2F;eɪ&#x2F;。\n\n通过这种方式，你不仅能读出单词，还能理解为什么 x 发 &#x2F;ɡz&#x2F;，a 为什么发 &#x2F;æ&#x2F;，以及 e 为什么不发音等等。\n步骤 4：大声朗读，多通道记忆。\n动作：\n根据音标和自然拼读规律，大声、清晰、缓慢地朗读单词，确保重音和每个音素都正确。读 5-10 遍。\n逐渐加快语速，让发音自然流畅。\n录音对比：用手机录下自己的发音，与标准发音进行对比，找出并纠正发音上的偏差。这是非常关键的一步。\n\n\n\n步骤 5：联想记忆与例句，融入语境。\n动作：\n看释义：exacerbate (加剧，恶化)。\n造句或理解例句：\nThe new policy will exacerbate poverty. (新政策将加剧贫困。)\nHis rude comments only exacerbated the tension. (他粗鲁的评论只会加剧紧张气氛。)\n\n\n在造句或阅读例句时，再次大声朗读单词及其例句，强化“音”、“形”、“义”的联结。\n\n\n\n步骤 6：多重练习 (听、说、写)。\n听写 (Dictation)：\n听单词的音频，不看拼写，尝试写出来。一开始可能不准，但通过音标和自然拼读的辅助，会越来越准确。\n听句子，尝试写出句子中的目标单词。\n\n\n口语练习：\n将单词融入自己的口语表达中，尝试用这个词来描述事物或表达观点。\n\n\n拼写练习：\n不看单词，只凭记忆尝试拼写，然后核对。特别是对于容易错的字母组合，多加练习。\n\n\n\n步骤 7：定期复习，强化记忆链条。\n复习方法：采用艾宾浩斯记忆曲线原则，定期复习。\n复习时，先听发音，尝试回忆拼写和意义。\n再看拼写，尝试读出并回忆意义。\n最后，看意义，尝试回忆发音和拼写。\n\n\n工具：Anki (或类似的间隔重复软件) 是绝佳的复习工具。在 Anki 卡片上，正面可以放单词的发音（音频）和音标，背面放拼写和例句。\n\n五、提高发音记忆效率的实用技巧\n掌握音标优先：对于初学者或发音基础不牢固的人，优先系统学习音标。这是基石。\n善用工具：\n在线词典：提供真人发音、音标、例句。\n发音 App&#x2F;网站：如 YouGlish (通过 YouTube 视频听真实语境发音)、Pronunciation Dictionary。\n录音工具：手机录音功能非常方便。\n间隔重复软件 (Anki, Quizlet)：定制化卡片，科学安排复习。\n\n\n多听为王：输入决定输出。大量听英语材料 (播客、有声书、电影、剧集)，尤其是有文本对照的材料。在听的同时，刻意留意单词的发音。\n注意重音和语调：重音是单词的“灵魂”，语调是句子的“灵魂”。掌握它们能大大提高听感和口语表达。\n辨析同音异形词&#x2F;近音词：例如 hear &#x2F;hɪr&#x2F; vs here &#x2F;hɪr&#x2F;, waste &#x2F;weɪst&#x2F; vs waist &#x2F;weɪst&#x2F;。通过发音联系意义，通过例句理解用法。\n善用词根词缀：虽然不是直接关于发音，但词根词缀可以帮助你理解单词的结构和意义，减少“陌生感”，从而更容易将发音与意义关联。例如 bene- (好) + fic (做) + ent (的) &#x3D; beneficent 善良的。\n从小处着手，坚持不懈：每天坚持练习几个单词，而不是一次性学习大量。循序渐进，效果更佳。\n\n六、总结通过发音记忆单词，是英语学习中一项极其高效和值得投入的技能。它将枯燥的字母组合转化为有生命的声音，让大脑更容易捕捉和储存信息。\n从系统学习音标和自然拼读开始，到遵循“听音→析音→拼读→朗读→联想→实践→复习”的步骤，每一步都旨在强化音、形、义之间的多重联结。持之以恒地应用这种方法，你会发现单词不再是孤立的符号，而是鲜活的语流中的一部分，你的听力、口语、拼写能力将得到质的飞跃。\n让发音成为你记忆单词的引擎，打开英语学习的新篇章吧！\n","categories":["英语学习"],"tags":["2023","英语学习","单词记忆"]},{"title":"Dockerfile 常用指令详解","url":"/2023/2023-04-23_Dockerfile%E5%B8%B8%E7%94%A8%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/","content":"\nDockerfile 是一个文本文件，其中包含用户可以在命令行上调用以组装映像的所有命令。Docker 可以通过读取 Dockerfile 中的指令自动构建映像。它本质上是一个“可执行程序脚本”，用于自动化构建 Docker 镜像的过程。\n\n理解和熟练使用 Dockerfile 指令是 Docker 应用开发和部署的核心技能之一。一个优化良好、结构清晰的 Dockerfile 不仅能构建出高效、安全、体积小的镜像，还能提高构建速度和可维护性。\n\n\n一、Dockerfile 基础概念\n镜像 (Image)：一个只读的模板，包含了创建 Docker 容器所需的所有文件和配置。\n容器 (Container)：镜像运行时的实例。可以启动、停止、删除。\n层 (Layer)：Dockerfile 中的每个指令都会创建一个新的镜像层。这些层是只读的，可以被缓存和共享，是 Docker 镜像高效和可复用的关键。\n构建上下文 (Build Context)：当执行 docker build 命令时，它会向 Docker 守护进程发送一个目录（通常是当前目录）及其所有内容。这个目录被称为构建上下文。Dockerfile 和其中引用的所有文件都必须在这个上下文中。\n\n二、Dockerfile 常用指令详解以下是 Dockerfile 中常用的指令及其详细解释。\n1. FROM\n作用：指定基础镜像，是 Dockerfile 的第一个指令（除 ARG 之外）。所有的构建都必须基于一个基础镜像。\n语法：FROM &lt;image&gt; [AS &lt;name&gt;]FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;]FROM &lt;image&gt;@&lt;digest&gt; [AS &lt;name&gt;]\n示例：FROM ubuntu:22.04        # 基于 Ubuntu 22.04 镜像FROM python:3.9-slim-buster AS builder # 基于 Python 3.9 瘦身版镜像，并命名为 builder\n最佳实践：\n选择官方镜像，更可靠。\n选择尽可能小的基础镜像（如 alpine 或 slim 版本），以减小最终镜像体积。\n指定精确的标签（tag），避免使用 latest，确保构建的可复现性。\n使用多阶段构建（AS &lt;name&gt;），优化最终镜像。\n\n\n\n2. RUN\n作用：在当前镜像层之上执行命令，并提交结果作为新的镜像层。主要用于安装软件包、配置环境等。\n语法：RUN &lt;command&gt;           # shell 形式，命令在 shell 中运行 (默认是 `/bin/sh -c` on Linux, `cmd /S /C` on Windows)RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] # exec 形式，直接执行命令，不经过 shell\n示例：RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\    nginx \\    &amp;&amp; rm -rf /var/lib/apt/lists/* # shell 形式，安装 Nginx 并清理缓存RUN [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo hello&quot;] # exec 形式，指定 bash 运行\n最佳实践：\n将多个 RUN 命令合并成一个 RUN 命令，可以减少镜像层数，这是优化镜像大小的关键。使用 &amp;&amp; 连接命令，并在末尾添加清理命令（如 rm -rf /var/lib/apt/lists/*）。\n使用 exec 形式可以避免 shell 的额外开销，但通常 shell 形式更容易编写和理解。\n\n\n\n3. CMD\n作用：为执行中的容器提供默认的命令。如果 docker run 命令指定了其他命令，CMD 的命令会被覆盖。一个 Dockerfile 中只能有一个 CMD 指令，如果有多个，只有最后一个生效。\n语法：CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] # exec 形式 (推荐)CMD [&quot;param1&quot;,&quot;param2&quot;]              # 作为 ENTRYPOINT 的附加参数 (推荐)CMD command param1 param2            # shell 形式\n示例：CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] # 容器启动时运行 NginxCMD echo &quot;Hello Docker!&quot; # shell 形式\n最佳实践：\n建议使用 exec 形式，因为它能避免 shell 处理，更清晰地表示容器启动后的主要进程。\n当与 ENTRYPOINT 结合使用时，CMD 用于为 ENTRYPOINT 提供默认参数。\n\n\n\n4. ENTRYPOINT\n作用：配置一个容器作为可执行文件运行。它为容器提供了一个固定的命令和参数，而 CMD 只是为 ENTRYPOINT 提供默认参数，或者在没有 ENTRYPOINT 时提供默认命令。\n语法：ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;] # exec 形式 (推荐)ENTRYPOINT command param1 param2              # shell 形式 (不推荐)\n示例：ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;] # 容器作为 Java 应用运行CMD [&quot;--spring.profiles.active=prod&quot;] # 为 ENTRYPOINT 提供默认参数\n最佳实践：\n使用 exec 形式。\n与 CMD 结合使用时，ENTRYPOINT 定义了容器启动的固定行为，而 CMD 提供了可变的默认参数，方便通过 docker run ... 覆盖这些参数。\n\n\n\n5. COPY\n作用：从构建上下文复制文件或目录到镜像中指定路径。\n语法：COPY &lt;源路径&gt;... &lt;目标路径&gt;COPY [&quot;&lt;源路径&gt;&quot;, ..., &quot;&lt;目标路径&gt;&quot;]\n示例：COPY . /app/      # 将当前构建上下文所有文件复制到镜像的 /app 目录COPY src/main.go /app/main.go # 复制单个文件COPY --chown=user:group myapp /app/myapp # 复制并改变所有者\n最佳实践：\n只复制需要的文件，避免复制不必要的文件（如 .git 目录、日志文件等），通常通过 .dockerignore 文件来控制。\n指定精确的源和目标路径，减少不必要的拷贝。\n\n\n\n6. ADD\n作用：与 COPY 类似，但它有额外的功能：\n如果源路径是 URL，ADD 会下载该文件。\n如果源路径是 tar 压缩文件，ADD 会自动解压到目标路径。\n\n\n语法：ADD &lt;源路径&gt;... &lt;目标路径&gt;ADD [&quot;&lt;源路径&gt;&quot;, ..., &quot;&lt;目标路径&gt;&quot;]\n示例：ADD https://example.com/latest.tar.gz /app/ # 下载并解压ADD myapp.tar.gz /app/  # 解压本地压缩包\n最佳实践：\n由于 ADD 的自动解压和远程文件下载功能可能引入不确定性和安全风险，更推荐使用 COPY。只有当确实需要自动解压本地 tar 文件时才考虑 ADD。\n下载远程文件建议使用 RUN wget 或 RUN curl，这样可以更明确地控制下载过程和校验，并能清理下载缓存。\n\n\n\n7. WORKDIR\n作用：为 RUN, CMD, ENTRYPOINT, COPY, ADD 等指令设置工作目录。后续的指令都会在这个目录下执行。\n语法：WORKDIR /path/to/workdir\n示例：WORKDIR /appCOPY . . # 相当于 COPY . /app/.RUN npm installCMD [&quot;npm&quot;, &quot;start&quot;]\n最佳实践：\n为应用程序设置一个明确的工作目录，保持 Dockerfile 的整洁和可读性。\n避免每次都使用绝对路径，利用 WORKDIR 简化指令。\n\n\n\n8. EXPOSE\n作用：声明容器运行时监听的端口。这仅仅是一个文档作用，告诉用户容器服务监听哪个端口。它不会实际发布端口到宿主机，需要在使用 docker run 命令时通过 -p 或 -P 参数来映射。\n语法：EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;...]\n示例：EXPOSE 80      # 暴露 TCP 80 端口EXPOSE 80/tcp  # 明确指定 TCPEXPOSE 53/udp  # 暴露 UDP 53 端口EXPOSE 80 443  # 暴露多个端口\n最佳实践：\n声明应用程序监听的所有端口。\n这对于容器编排工具（如 Kubernetes）尤其有用，它们可以读取 Dockerfile 中的 EXPOSE 信息来配置服务。\n\n\n\n9. ENV\n作用：设置环境变量，这些变量在构建时和容器运行时都可用。\n语法：ENV &lt;key&gt;=&lt;value&gt; ...\n示例：ENV MY_ENV_VAR=&quot;hello&quot;ENV PATH=&quot;/usr/local/bin:$PATH&quot; # 添加 PATH\n最佳实践：\n定义应用程序所需的配置参数（如数据库连接字符串、API 密钥等）。\n环境变量会增加镜像层，如果设置了敏感信息，它们会保留在镜像历史中。敏感信息不应直接硬编码在 ENV 中，而应通过 docker run -e 或 secret manager 传递。\n\n\n\n10. ARG\n作用：定义构建时变量。这些变量在 docker build 命令中传递，并且只在构建阶段有效，容器运行时不可见。\n语法：ARG &lt;name&gt;[=&lt;default value&gt;]\n示例：ARG BUILD_VERSION=1.0.0ARG NODE_VERSIONRUN echo &quot;Building version: $BUILD_VERSION&quot; # 在 RUN 命令中使用 BUILD_VERSION\n构建时传递：docker build --build-arg NODE_VERSION=16.x .\n最佳实践：\n用于传递构建参数，如版本号、代理设置等。\nARG 在 FROM 之前定义可以影响 FROM 指令。\n注意，如果 ARG 定义的变量在 CMD&#x2F;ENTRYPOINT 中使用，需要用 ENV 重新声明，因为 ARG 只在构建时可见。\n\n\n\n11. VOLUME\n作用：创建一个挂载点，将宿主机的目录或 Docker 管理的卷挂载到容器中，绕过 Union File System。通常用于存储动态数据或共享数据。\n语法：VOLUME [&quot;/data&quot;]            # 推荐 exec 形式VOLUME /var/log/app\n示例：VOLUME [&quot;/var/www/html&quot;] # 声明此目录将用于存储 Web 服务器的数据\n最佳实践：\n声明应用程序可能需要外部持久化存储的目录。\n尽管 VOLUME 指令在 Dockerfile 中指定了挂载点，但实际的卷挂载是在 docker run -v 命令中完成的。\n\n\n\n12. USER\n作用：设置运行容器或执行 RUN, CMD, ENTRYPOINT 命令时使用的用户名或 UID&#x2F;GID。\n语法：USER &lt;user&gt;[:&lt;group&gt;]USER &lt;UID&gt;[:&lt;GID&gt;]\n示例：FROM ubuntu:22.04RUN useradd -ms /bin/bash appuser # 创建一个新用户USER appuser # 后续命令将以 appuser 身份运行CMD [&quot;echo&quot;, &quot;Hello from appuser&quot;]\n最佳实践：\n不要以 root 用户运行容器应用程序。这是安全最佳实践。创建并使用非 root 用户来运行应用程序。\n\n\n\n13. HEALTHCHECK\n作用：告诉 Docker 如何检测容器内的服务是否健康。\n语法：HEALTHCHECK [OPTIONS] CMD commandHEALTHCHECK NONE\nOptions:\n--interval=DURATION (default: 30s)\n--timeout=DURATION (default: 30s)\n--start-period=DURATION (default: 0s)\n--retries=N (default: 3)\n\n\n示例：HEALTHCHECK --interval=5s --timeout=3s --retries=3 \\    CMD curl --fail http://localhost/ || exit 1\n最佳实践：\n定义一个命令来检查应用程序的健康状况，例如检查 HTTP 端点是否返回 200 状态码，或者 TCP 端口是否响应。\n这对于容器编排系统（如 Kubernetes, Docker Compose）进行服务管理和自动恢复非常有用。\n\n\n\n14. LABEL\n作用：为镜像添加元数据。这允许用户添加自定义信息，如维护者、版本、描述等。\n语法：LABEL &lt;key&gt;=&quot;&lt;value&gt;&quot; [&lt;key&gt;=&quot;&lt;value&gt;&quot; ...]\n示例：LABEL maintainer=&quot;Your Name &lt;your.email@example.com&gt;&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;This is a sample web application.&quot;\n最佳实践：\n提供清晰的元数据，方便管理和识别镜像。\n可以使用多行 LABEL，但合并成一个 LABEL 指令可以减少镜像层数。\n\n\n\n三、Dockerfile 构建最佳实践\n使用 .dockerignore 文件：类似于 .gitignore，防止不必要的文件（如 node_modules, .git, .vscode 等）被复制到构建上下文中，加快构建速度并减小镜像体积。\n多阶段构建 (Multi-stage Builds)：\n将构建环境和运行时环境分离。\n例如，第一阶段编译代码，然后将编译好的二进制文件复制到第二阶段的轻量级运行时镜像中。\n这能显著减小最终镜像体积，提高安全性。\n\n# 第一阶段：构建FROM golang:1.20-alpine AS builderWORKDIR /appCOPY . .RUN go mod downloadRUN CGO_ENABLED=0 GOOS=linux go build -o myapp .# 第二阶段：运行FROM alpine:latestWORKDIR /appCOPY --from=builder /app/myapp .EXPOSE 8080CMD [&quot;./myapp&quot;]\n减少镜像层数：每个 RUN, COPY, ADD 指令都会创建一个新层。将相关的指令合并，特别是 RUN 命令，使用 &amp;&amp; 连接。\n利用缓存：Docker 会缓存每个指令的结果。将不变的指令放在 Dockerfile 的顶部，变化的指令放在底部，最大限度地利用缓存，加快重复构建的速度。\n指定精确的基础镜像标签：避免使用 latest，以确保构建的可复现性。\n非 Root 用户：尽量使用 USER 指令将容器运行为非 root 用户，提高安全性。\n清理中间文件：在 RUN 命令中，安装完软件包后立即清理包管理器的缓存 (apt-get clean, rm -rf /var/cache/apk/* 等)。\n明智地使用 VOLUME：只在需要持久化或共享数据时使用。\n\n四、总结Dockerfile 是容器化工作流的核心。通过深入理解其常用指令及其最佳实践，你可以：\n\n构建更小、更快的 Docker 镜像\n提高镜像的可维护性和安全性\n优化 CI&#x2F;CD 流程中的构建时间\n更好地管理应用程序的依赖和配置\n\n掌握这些知识，你就能更有效地利用 Docker，将你的应用程序打包、分发和部署到任何环境。\n","categories":["Docker"],"tags":["2023","Docker","容器技术"]},{"title":"Docker Compose 详解：定义和运行多容器 Docker 应用","url":"/2023/2023-04-27_Docker%20Compose%20%E8%AF%A6%E8%A7%A3%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%92%8C%E8%BF%90%E8%A1%8C%E5%A4%9A%E5%AE%B9%E5%99%A8%20Docker%20%E5%BA%94%E7%94%A8/","content":"\nDocker Compose 是一个用于定义和运行多容器 Docker 应用程序的工具。通过一个 YAML 文件（通常命名为 docker-compose.yml），你可以配置应用程序的所有服务（容器）、网络和卷。然后，只需一个命令，就可以从这个配置文件中启动、停止和管理整个应用程序。\n\n在实际的生产环境中，一个完整的应用程序通常由多个服务组成，例如一个 Web 应用可能包含一个 Web 服务器（Nginx&#x2F;Apache）、一个应用服务（Python&#x2F;Node.js&#x2F;Java）、一个数据库（PostgreSQL&#x2F;MySQL）和一个缓存服务（Redis）。手动管理这些独立容器的创建、网络连接和启动顺序非常繁琐且容易出错。Docker Compose 的出现正是为了解决这些多容器应用的管理复杂性。\n\n\n一、Docker Compose 简介与核心优势Docker Compose 简化了多容器应用的开发、测试和（小规模）部署。它将应用的整个拓扑结构描述在一个文件中，实现了“基础设施即代码”的理念。\nDocker Compose 的核心优势：\n\n单一文件，管理一切：用一个简单的 YAML 文件定义整个应用的架构，包括所有服务、它们的镜像、端口映射、卷挂载、环境变量和网络配置。\n易于启动和停止：通过 docker compose up 命令，可以一键启动所有服务并建立它们之间的网络连接；通过 docker compose down 可以一键停止并移除所有相关的容器、网络和卷。\n服务发现：Compose 会自动为你的服务创建内部网络，并使服务可以通过其服务名称相互通信，例如，Web 服务可以通过 database 宿主机名连接到数据库容器。\n环境隔离：每个项目（通常是一个目录）可以拥有独立的 Compose 配置，创建隔离的环境，避免不同项目之间的冲突。\n快速迭代：开发过程中，修改代码后可以快速重建并重启受影响的服务。\n跨平台：Compose 文件可以在任何支持 Docker 的平台上运行，保持开发、测试和生产环境的一致性。\n\n二、安装 Docker ComposeDocker Compose 的安装方式取决于你的 Docker Desktop 版本和操作系统。\n1. Docker Desktop (Windows &#x2F; macOS)如果你安装了 Docker Desktop，那么 Docker Compose 已经预装并集成在 Docker Engine 中。你可以直接使用 docker compose 命令。\n验证方式：\ndocker compose version\n\n2. Linux 系统对于 Linux，Docker Compose 作为一个独立二进制文件或插件提供。\n作为 Docker CLI 插件 (推荐，新版本)大多数新版本 Docker Engine (&gt;&#x3D; 20.10) 都会将 Docker Compose 作为 Docker CLI 的一个插件捆绑提供。如果你的 Docker Engine 较旧，可能需要单独安装。\n独立安装 (旧版本或特定需求)如果你的 Docker Engine 版本较旧，或者想安装旧版 docker-compose (注意是 docker-compose 带连字符)，可以手动下载：\n# 下载最新稳定版本的 Docker Composesudo curl -L &quot;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose# 赋予执行权限sudo chmod +x /usr/local/bin/docker-compose# 验证安装docker-compose --version # 注意是带连字符的命令\n注意： 新版本 Docker Compose (v2) 的命令是 docker compose (无连字符)，而旧版本 (v1) 的命令是 docker-compose (带连字符)。推荐使用新版本。\n三、Docker Compose 文件 (docker-compose.yml) 结构docker-compose.yml 文件是 Docker Compose 的核心。它是一个 YAML 格式的文件，定义了应用的所有服务。\n基本结构：\nversion: &#x27;3.8&#x27; # Compose 文件格式版本，推荐使用最新稳定版本services:     # 定义所有的服务 (容器)  web:        # 一个服务名称    image: nginx:latest # 使用的镜像    ports:    # 端口映射      - &quot;80:80&quot;    volumes:  # 卷挂载      - ./nginx.conf:/etc/nginx/nginx.conf # 宿主机文件:容器文件      - ./html:/usr/share/nginx/html # 宿主机目录:容器目录    depends_on: # 依赖关系，确保数据库先启动      - db    networks: # 指定服务加入的网络      - my-app-network  db:    image: postgres:13    environment: # 环境变量      POSTGRES_DB: mydb      POSTGRES_USER: user      POSTGRES_PASSWORD: password    volumes:      - db_data:/var/lib/postgresql/data # 命名卷挂载    networks:      - my-app-networknetworks:     # 定义网络  my-app-network:    driver: bridge # 桥接网络volumes:      # 定义命名卷  db_data:\n\n核心顶级键：\nversion (必需)：指定 Compose 文件格式版本。\n推荐使用 3.x 系列，目前最新稳定版是 3.8 或 3.9。不同版本支持的指令和功能有所差异。\n\n\nservices (必需)：定义应用程序包含的所有服务。每个服务都是一个独立的容器。\nnetworks (可选)：定义 Compose 应用中使用的网络。\nvolumes (可选)：定义 Compose 应用中使用的命名卷。\nconfigs (可选)：定义配置对象，通常用于存储敏感数据或配置信息。\nsecrets (可选)：定义敏感数据，通常用于数据库密码、API 密钥等。\n\nservices 下的常见指令：每个服务可以配置以下常用指令：\n\nimage：指定用于创建容器的镜像（如 ubuntu:latest, nginx:1.21）。\nbuild：如果需要从 Dockerfile 构建镜像，可以指定 Dockerfile 所在的上下文路径和 Dockerfile 文件名。service_name:  build: .           # 在当前目录查找 Dockerfile  # build: ./app       # 在 app 目录查找 Dockerfile  # build:  #   context: ./app   # 指定上下文路径  #   dockerfile: Dockerfile.web # 指定 Dockerfile 文件名  #   args:          # 构建参数 Arguments  #     version: 1.0\nports：端口映射，将容器的端口映射到宿主机的端口。\n&quot;宿主机端口:容器端口&quot; (如 &quot;80:80&quot;)\n&quot;宿主机IP:宿主机端口:容器端口&quot;\n\n\nenvironment：设置环境变量。environment:  - VAR1=value1  - VAR2=value2  # 或  VAR1: value1  VAR2: value2\nvolumes：卷挂载，用于持久化数据或将宿主机文件&#x2F;目录挂载到容器内。\n&quot;宿主机路径:容器路径&quot; (绑定挂载)\n&quot;卷名称:容器路径&quot; (命名卷挂载)\n&quot;./html:/usr/share/nginx/html:ro&quot; (只读挂载)\n\n\ndepends_on：定义服务之间的依赖关系。这会影响服务的启动顺序（例如，数据库服务会在 Web 服务之前启动）。注意：这只保证启动顺序，不保证服务完全可用。 （使用 healthcheck 更好地确保服务可用性）depends_on:  - db  - redis\nnetworks：指定服务要连接到的网络。定义在 networks 顶级键下。\ncontainer_name：指定容器的名称，而非 Compose 自动生成的名称。\ncommand：覆盖镜像中 CMD 指令定义的默认命令。\nentrypoint：覆盖镜像中 ENTRYPOINT 指令定义的默认入口点。\nextra_hosts：添加主机名到容器的 /etc/hosts 文件中。\nrestart：定义容器退出后的重启策略（no, on-failure, always, unless-stopped）。\nlabels：为容器添加元数据标签。\nhealthcheck：定义容器健康检查的方式。healthcheck:  test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost:8000/healthz&quot;]  interval: 10s  timeout: 5s  retries: 3  start_period: 30s # 在此期间如果检查失败不计入重试次数\ndeploy：部署相关的配置，例如在 Docker Swarm 模式下使用的副本数、资源限制等。\n\n四、Docker Compose 常用命令在包含 docker-compose.yml 文件的项目根目录下运行以下命令。\n1. 启动应用程序 (后台运行)docker compose up -d\n\n-d：在后台（detached mode）运行容器。\n此命令会解析 docker-compose.yml 文件，构建和&#x2F;或拉取所需的镜像，然后创建并启动所有服务。\n\n2. 停止并移除应用程序docker compose down\n\n此命令会停止并移除 docker compose up 启动的所有容器、网络和默认卷。\ndocker compose down -v：同时移除匿名卷和命名卷（小心使用，会删除数据）。\ndocker compose down --rmi all：移除所有服务创建的镜像。\n\n3. 查看服务状态docker compose ps\n\n列出 Compose 项目中所有服务的运行状态。\n\n4. 查看服务日志docker compose logs [service_name]\n\ndocker compose logs：显示所有服务的合并日志。\ndocker compose logs -f：实时跟踪日志输出。\ndocker compose logs web：只查看 web 服务的日志。\n\n5. 重启服务docker compose restart [service_name]\n\ndocker compose restart：重启所有服务。\ndocker compose restart web：只重启 web 服务。\n\n6. 构建或重建服务镜像docker compose build [service_name]\n\ndocker compose build：构建所有需要构建的服务的镜像。\ndocker compose build web：只构建 web 服务的镜像。\ndocker compose build --no-cache：构建时不使用缓存。\n\n7. 执行命令docker compose exec &lt;service_name&gt; &lt;command&gt;\n\n在正在运行的容器中执行命令。\ndocker compose exec web bash：在 web 服务容器中打开一个 Bash shell。\n\n8. 进入容器docker compose run &lt;service_name&gt; &lt;command&gt;\n\n在指定服务中运行一次性命令。与 exec 不同，run 会创建一个新容器来运行命令。\ndocker compose run web bash：创建一个新的 web 容器并进入 Bash shell。\ndocker compose run --rm web bash：运行完毕后自动移除容器。\n\n9. 移除停止的容器、网络和卷docker compose rm\n\n移除所有已停止的服务容器。\n\n五、Docker Compose 最佳实践\n为每个项目使用独立的 Compose 文件：将每个应用程序的 docker-compose.yml 文件放在其自己的项目目录中。这样可以确保环境隔离，并避免服务名称冲突。\n版本控制：将 docker-compose.yml 文件与你的代码一起进行版本控制。\n使用 volumes 进行数据持久化：对于数据库、日志等需要持久化的数据，务必使用命名卷或绑定挂载，防止容器删除时数据丢失。\n明确指定镜像版本：避免使用 latest 标签，以确保环境的可复现性。例如 nginx:1.21.6 而非 nginx:latest。\n利用 .env 文件管理环境变量：对于敏感信息（如数据库密码）或需要在不同环境（开发&#x2F;生产）中切换的变量，可以使用 .env 文件。\n在 docker-compose.yml 中：DB_PASSWORD: $&#123;DB_PASSWORD&#125;\n在 .env 文件中：DB_PASSWORD=mysecretpassword\n\n\n善用 depends_on 和 healthcheck：depends_on 用于服务启动顺序，healthcheck 用于更可靠地判断服务是否真的准备就绪。两者结合使用能提高应用启动的健壮性。\n多阶段构建配合 Compose：如果你的服务需要编译，可以在 Dockerfile 中使用多阶段构建，然后在 docker-compose.yml 中引用最终的小镜像。\n考虑使用 docker-compose.override.yml：在开发环境中，你可能需要一些与生产环境不同的配置（例如调试端口、开发服务器）。可以通过创建一个 docker-compose.override.yml 文件来覆盖主 docker-compose.yml 中的配置。\nCompose 会自动合并 docker-compose.yml 和 docker-compose.override.yml。\n例如，在 override 文件中可以添加 build 指令，或暴露更多端口。\n\n\n\n六、与 Docker Swarm &#x2F; Kubernetes 的关系\nDocker Compose：主要用于单机上多容器应用的开发、测试和（小规模）部署。它不提供自动伸缩、高可用性、滚动更新等生产级编排功能。\nDocker Swarm：Docker 官方的原生容器编排工具，提供了集群级别的容器管理，包括服务伸缩、负载均衡、滚动更新、故障恢复等。Compose 文件可以通过 docker stack deploy 命令直接部署到 Swarm 集群中。\nKubernetes (K8s)：目前业界最主流的容器编排平台，功能更全面、强大，但学习曲线较陡峭。Kubernetes 不直接使用 docker-compose.yml 文件，但有很多工具（如 kompose）可以将 Compose 文件转换成 Kubernetes 的资源定义。\n\n简单来说，Docker Compose 是你使用 Docker 进行多容器应用开发的起点，而当你的应用需要扩展到生产集群时，你可能会转向 Docker Swarm 或 Kubernetes。\n七、总结Docker Compose 是 Docker 生态中不可或缺的工具，它将复杂的 Docker 命令抽象化，通过一个简单的 YAML 文件就能定义和管理整个应用程序栈。无论是个人开发者进行本地开发测试，还是小团队进行应用部署，Docker Compose 都能极大地提高效率和便利性。\n掌握 Docker Compose，意味着你能够更优雅、更高效地构建、运行和管理你的多容器应用。\n","categories":["Docker"],"tags":["2023","Docker","容器技术"]},{"title":"Redis 各类数据结构指令详解","url":"/2023/2023-05-08_Redis%20%E5%90%84%E7%B1%BB%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%8C%87%E4%BB%A4%E8%AF%A6%E8%A7%A3/","content":"\nRedis 是一个开源（BSD 许可）的内存数据结构存储，可用作数据库、缓存和消息代理。它支持多种类型的数据结构，如字符串（strings）、哈希（hashes）、列表（lists）、集合（sets）、有序集合（sorted sets）等。理解并熟练使用这些数据结构及其相关指令，是高效利用 Redis 的关键。\n\nRedis 的强大之处在于其在内存中操作这些丰富的数据结构，使得读写速度极快。掌握每个数据结构的使用场景和对应指令，是进行高性能应用开发的基础。\n\n\n一、通用键指令 (Generic Commands)这些指令适用于所有数据类型的键。\n\n\n\n指令\n描述\n示例\n\n\n\nDEL key [key ...]\n删除一个或多个键。\nDEL mykey mylist\n\n\nEXISTS key [key ...]\n检查给定键是否存在。返回存在的键的数量。\nEXISTS mykey\n\n\nEXPIRE key seconds\n设置键的过期时间（秒）。\nEXPIRE mykey 60 (60秒后过期)\n\n\nTTL key\n获取键的剩余过期时间（秒）。-1 表示永久，-2 表示键不存在或已过期。\nTTL mykey\n\n\nPERSIST key\n移除键的过期时间，使其变为永久。\nPERSIST mykey\n\n\nTYPE key\n返回键存储值的类型。\nTYPE mykey (可能返回 string, list 等)\n\n\nKEYS pattern\n查找所有符合给定模式的键。应避免在生产环境中使用，会阻塞 Redis。\nKEYS user:*\n\n\nRENAME key newkey\n重命名键。\nRENAME oldkey newkey\n\n\nSCAN cursor [MATCH pattern] [COUNT count]\n用于迭代数据库中的键，避免 KEYS 的问题。\nSCAN 0 MATCH user:* COUNT 100\n\n\n二、字符串 (Strings)Redis 最基本的数据类型，可以存储文本、整数、浮点数，甚至是二进制数据。最大能存储 512MB。\n\n\n\n指令\n描述\n示例\n\n\n\nSET key value [EX seconds] [PX milliseconds] [NX|XX]\n设置键值对。EX:过期秒数, PX:过期毫秒数, NX:键不存在才设置, XX:键存在才设置。\nSET mykey &quot;hello&quot;SET mykey &quot;world&quot; EX 10SET mykey &quot;new&quot; NX\n\n\nGET key\n获取键的值。\nGET mykey\n\n\nMSET key value [key value ...]\n同时设置多个键值对。\nMSET key1 v1 key2 v2\n\n\nMGET key [key ...]\n同时获取多个键的值。\nMGET key1 key2\n\n\nINCR key\n将键存储的整数值加 1。如果键不存在，则初始化为 0 后再加 1。\nINCR counter\n\n\nDECR key\n将键存储的整数值减 1。\nDECR counter\n\n\nINCRBY key increment\n将键存储的整数值增加指定量。\nINCRBY counter 10\n\n\nDECRBY key decrement\n将键存储的整数值减少指定量。\nDECRBY counter 5\n\n\nGETSET key value\n设置键的新值并返回旧值。\nGETSET mykey &quot;new_value&quot;\n\n\nAPPEND key value\n将值追加到键的末尾。如果键不存在，则创建键并设置值。\nAPPEND mykey &quot; world&quot; (mykey 的值变为 hello world)\n\n\nGETRANGE key start end\n获取字符串的子字符串。\nGETRANGE mykey 0 4 (返回 hello)\n\n\nSETEX key seconds value\n设置键值对并指定过期时间（秒）。\nSETEX mykey 60 &quot;value&quot;\n\n\n场景示例: 用户会话存储、计数器、短 URL 映射。\n三、哈希 (Hashes)哈希是字段（field）和值（value）的映射表，非常适合存储对象。一个哈希可以存储多个字段-值对。\n\n\n\n指令\n描述\n示例\n\n\n\nHSET key field value [field value ...]\n设置哈希中一个或多个字段的值。\nHSET user:1 name &quot;Alice&quot; age 30 city &quot;New York&quot;\n\n\nHGET key field\n获取哈希中指定字段的值。\nHGET user:1 name\n\n\nHMSET key field value [field value ...]\n同时设置多个字段的值。（已被 HSET 替代，但仍兼容）\nHMSET user:2 name &quot;Bob&quot; age 25\n\n\nHMGET key field [field ...]\n同时获取多个字段的值。\nHMGET user:1 name city\n\n\nHGETALL key\n获取哈希中所有字段和值。\nHGETALL user:1\n\n\nHDEL key field [field ...]\n删除哈希中一个或多个字段。\nHDEL user:1 age\n\n\nHLEN key\n获取哈希中字段的数量。\nHLEN user:1\n\n\nHEXISTS key field\n检查哈希中是否存在指定字段。\nHEXISTS user:1 name\n\n\nHKEYS key\n获取哈希中所有字段名。\nHKEYS user:1\n\n\nHVALS key\n获取哈希中所有字段值。\nHVALS user:1\n\n\nHINCRBY key field increment\n将哈希中指定字段的值增加指定量。字段值必须是整数。\nHINCRBY user:1 visits 1\n\n\nHINCRBYFLOAT key field increment\n将哈希中指定字段的浮点数值增加指定量。\nHINCRBYFLOAT product:1 price 1.5\n\n\nHSETNX key field value\n只有当字段不存在时，才设置哈希中字段的值。\nHSETNX user:1 email &quot;alice@example.com&quot; (如果 email 字段已存在，则不会更新)\n\n\n场景示例: 存储用户对象信息、商品详情、配置设置。\n四、列表 (Lists)列表是值的有序集合。你可以向列表的两端（左侧或右侧）添加元素。\n\n\n\n指令\n描述\n示例\n\n\n\nLPUSH key value [value ...]\n将一个或多个值插入到列表的头部（左侧）。\nLPUSH mylist &quot;apple&quot; &quot;banana&quot; (列表: banana, apple)\n\n\nRPUSH key value [value ...]\n将一个或多个值插入到列表的尾部（右侧）。\nRPUSH mylist &quot;cherry&quot; (列表: banana, apple, cherry)\n\n\nLPOP key\n移除并返回列表的头部元素。\nLPOP mylist (返回 banana, 列表: apple, cherry)\n\n\nRPOP key\n移除并返回列表的尾部元素。\nRPOP mylist (返回 cherry, 列表: apple)\n\n\nLRANGE key start stop\n返回列表中指定范围内的元素。0 表示第一个元素，-1 表示最后一个元素。\nLRANGE mylist 0 -1 (返回所有)LRANGE mylist 0 0 (返回第一个)\n\n\nLLEN key\n获取列表的长度。\nLLEN mylist\n\n\nLINDEX key index\n通过索引获取列表中的元素。\nLINDEX mylist 0\n\n\nLREM key count value\n从列表中移除与指定值相等的元素。count &gt; 0: 从头开始移除 count 个。count &lt; 0: 从尾开始移除 &#96;\ncount\n\n\nLINSERT key BEFORE|AFTER pivot value\n在 pivot 元素之前或之后插入值。\nLINSERT mylist BEFORE &quot;apple&quot; &quot;pear&quot; (列表: banana, pear, apple, cherry)\n\n\nTRIM key start stop\n将列表修剪到指定范围，保留范围内的元素，移除范围外的元素。通常用于实现固定长度列表。\nLTRIM mylist 0 99 (只保留最新的100个元素)\n\n\nBLPOP key [key ...] timeout\n阻塞式左弹出。如果列表为空，则阻塞直到有元素可弹出或超时。 timeout 为 0 表示永远阻塞。\nBLPOP queue1 queue2 0\n\n\nBRPOP key [key ...] timeout\n阻塞式右弹出。\nBRPOP queue1 5 (阻塞最多 5 秒)\n\n\n场景示例: 消息队列、最新文章列表、关注者时间线、任务队列。\n五、集合 (Sets)集合是无序的、不重复的字符串元素集合。\n\n\n\n指令\n描述\n示例\n\n\n\nSADD key member [member ...]\n将一个或多个成员添加到集合。如果成员已存在，则忽略。\nSADD myset &quot;apple&quot; &quot;banana&quot;\n\n\nSMEMBERS key\n返回集合中的所有成员。\nSMEMBERS myset (返回 apple, banana，顺序不确定)\n\n\nSISMEMBER key member\n判断成员是否是集合的成员。\nSISMEMBER myset &quot;apple&quot; (返回 1)\n\n\nSCARD key\n获取集合的成员数量。\nSCARD myset\n\n\nSREM key member [member ...]\n从集合中移除一个或多个成员。\nSREM myset &quot;banana&quot;\n\n\nSPOP key [count]\n随机移除并返回集合中的一个或多个成员。\nSPOP myset\n\n\nSRANDMEMBER key [count]\n随机返回集合中的一个或多个成员，但不移除。\nSRANDMEMBER myset 2 (随机返回两个成员)\n\n\nSINTER key [key ...]\n返回所有给定集合的交集。\nSADD set1 a b cSADD set2 b c dSINTER set1 set2 (返回 b, c)\n\n\nSUNION key [key ...]\n返回所有给定集合的并集。\nSUNION set1 set2 (返回 a, b, c, d)\n\n\nSDIFF key [key ...]\n返回第一个集合与所有其他集合的差集。\nSDIFF set1 set2 (返回 a)\n\n\nSINTERSTORE destination key [key ...]\n将交集结果存储到目标集合。\nSINTERSTORE common_elements set1 set2\n\n\nSUNIONSTORE destination key [key ...]\n将并集结果存储到目标集合。\nSUNIONSTORE all_elements set1 set2\n\n\nSDIFFSTORE destination key [key ...]\n将差集结果存储到目标集合。\nSDIFFSTORE unique_to_set1 set1 set2\n\n\n场景示例: 标签系统、共同关注、抽奖程序、用户权限管理（例如，一个用户属于哪些角色）。\n六、有序集合 (Sorted Sets &#x2F; ZSETS)有序集合是集合的变种，每个成员都关联一个分数（score），集合中的成员是唯一的，但分数可以重复。元素按照分数从小到大排序。分数相同的元素，再根据成员的字典序排序。\n\n\n\n指令\n描述\n示例\n\n\n\nZADD key [NX|XX] [GT|LT] [CH] [INCR] score member [score member ...]\n将分数和成员添加到有序集合。NX: 成员不存在才添加, XX: 成员存在才更新, CH: 改变计数, INCR: 分数递增。\nZADD myzset 1 &quot;one&quot;ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;ZADD myzset INCR 1 &quot;one&quot; (将 “one” 的分数加 1)\n\n\nZRANGE key start stop [WITHSCORES]\n返回有序集合中指定排名范围内的成员。0 是第一个元素，-1 是最后一个元素。WITHSCORES 返回分数。\nZRANGE myzset 0 -1 WITHSCORES\n\n\nZREM key member [member ...]\n从有序集合中移除一个或多个成员。\nZREM myzset &quot;one&quot;\n\n\nZCARD key\n获取有序集合的成员数量。\nZCARD myzset\n\n\nZSCORE key member\n获取有序集合中指定成员的分数。\nZSCORE myzset &quot;two&quot;\n\n\nZRANK key member\n返回有序集合中指定成员的排名（分数从小到大排，排名从 0 开始）。\nZRANK myzset &quot;two&quot;\n\n\nZREVRANK key member\n返回有序集合中指定成员的逆序排名（分数从大到小排，排名从 0 开始）。\nZREVRANK myzset &quot;two&quot;\n\n\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]\n返回有序集合中指定分数范围内的成员。min 和 max 可以是 -inf (负无穷大) 或 +inf (正无穷大)。\nZRANGEBYSCORE myzset -inf 2 WITHSCORESZRANGEBYSCORE myzset (1 3 (排除分数 1 和 3)ZRANGEBYSCORE myzset 1 3 LIMIT 0 1\n\n\nZCOUNT key min max\n返回有序集合中指定分数范围内的成员数量。\nZCOUNT myzset 1 3\n\n\nZINCRBY key increment member\n对有序集合中指定成员的分数进行增量操作。\nZINCRBY myzset 1 &quot;one&quot;\n\n\nZREMRANGEBYRANK key start stop\n移除有序集合中指定排名范围内的所有成员。\nZREMRANGEBYRANK myzset 0 99 (移除排名前 100 的成员)\n\n\nZREMRANGEBYSCORE key min max\n移除有序集合中指定分数范围内的所有成员。\nZREMRANGEBYSCORE myzset baits 100 200\n\n\nZUNIONSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]\n计算给定多个有序集合的并集，并将结果存储到目标有序集合中。可指定权重和聚合方式。\nZADD zset1 1 &quot;a&quot; 2 &quot;b&quot; ZADD zset2 3 &quot;a&quot; 4 &quot;c&quot;ZUNIONSTORE zunion 2 zset1 zset2 AGGREGATE MAX (a:3, b:2, c:4)\n\n\nZINTERSTORE destination numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE SUM|MIN|MAX]\n计算给定多个有序集合的交集，并将结果存储到目标有序集合中。\nZINTERSTORE zinter 2 zset1 zset2 AGGREGATE SUM (a:4)\n\n\n场景示例: 排行榜（游戏积分榜、最热文章榜）、带有优先级的任务队列、根据分数范围筛选数据。\n七、Stream (流)Redis 5.0 引入了 Stream 数据结构，它是一个只追加的数据结构，用于处理日志流、事件流等时间序列数据。它支持多消费者组。\n\n\n\n指令\n描述\n示例\n\n\n\nXADD key ID field value [field value ...]\n添加新的条目到 Stream。ID 可以是 * (自动生成)，或手动指定。\nXADD mystream * sensor_id 123 temperature 25.5XADD mystream 1-0 event_type &quot;login&quot; user_id 456\n\n\nXRANGE key start end [COUNT count]\n获取 Stream 中指定 ID 范围内的条目。min 和 max 可以是 &quot;-&quot; (最小ID) 或 &quot;+&quot; (最大ID)。\nXRANGE mystream - +XRANGE mystream 1678881330000-0 1678881330999-999 COUNT 10\n\n\nXREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] ID [ID ...]\n从一个或多个 Stream 中读取条目。BLOCK 实现阻塞读取。\nXREAD COUNT 2 STREAMS mystream 0-0XREAD BLOCK 0 STREAMS mystream $ (阻塞读取最新条目)\n\n\nXGROUP CREATE key groupname ID [MKSTREAM]\n创建消费者组。ID 指定消费者组的起始 ID（例如 $ 表示从最新开始，0 表示从头开始）。MKSTREAM：如果 Stream 不存在则自动创建。\nXGROUP CREATE mystream mygroup $ MKSTREAM\n\n\nXREADGROUP GROUP groupname consumername COUNT count [BLOCK milliseconds] STREAMS key [key ...] ID [ID ...]\n从消费者组中读取条目。ID 为 &gt; 表示从未发送给当前消费者的条目开始。\nXREADGROUP GROUP mygroup myconsumer COUNT 1 STREAMS mystream &gt;\n\n\nXACK key groupname ID [ID ...]\n确认消费者已处理完某个条目。\nXACK mystream mygroup 1678881330000-0\n\n\nXPENDING key groupname [IDLE min-idle-time] [start end count] [consumer]\n获取消费者组中待处理消息列表。\nXPENDING mystream mygroup\n\n\nXCLAIM key groupname consumername min-idle-time ID [ID ...]\n夺回（claim）其他消费者已读取但长时间未确认的条目。\nXCLAIM mystream mygroup newconsumer 3600000 1678881330000-0\n\n\nXTRIM key MAXLEN [~] count\n裁剪 Stream，保留指定数量的最新条目。~ 大约保留。\nXTRIM mystream MAXLEN 1000\n\n\n场景示例: 实时消息系统、事件溯源、微服务间通信、物联网数据采集。\n八、HyperLogLog (HLL)HyperLogLog 是一种概率性数据结构，用于估算集合中元素的唯一数量（即基数）。它使用的内存非常少（固定 12KB），但会存在小部分误差。\n\n\n\n指令\n描述\n示例\n\n\n\nPFADD key element [element ...]\n添加一个或多个元素到 HyperLogLog。\nPFADD users:20231010 &quot;user1&quot; &quot;user2&quot; &quot;user3&quot;\n\n\nPFCOUNT key [key ...]\n返回 HyperLogLog 的近似基数。\nPFCOUNT users:20231010\n\n\nPFMERGE destkey sourcekey [sourcekey ...]\n将多个 HyperLogLog 合并到一个新的 HyperLogLog 中。\nPFMERGE users:total users:20231010 users:20231011\n\n\n场景示例: 网站独立访客数统计、用户日活&#x2F;月活统计、热门商品访问量统计。\n九、Geospatial (地理空间)\n\nRedis 3.2 引入了地理空间索引，允许存储和查询地理空间坐标，通常用于基于位置的服务 (LBS)。\n\n\n\n指令\n描述\n示例\n\n\n\nGEOADD key longitude latitude member [longitude latitude member ...]\n添加一个或多个地理空间成员（经度、纬度和名称）。\nGEOADD city_locations 13.361 38.084 &quot;Palermo&quot; 15.087 37.502 &quot;Catania&quot;\n\n\nGEOPOS key member [member ...]\n获取指定成员的经度和纬度。\nGEOPOS city_locations &quot;Palermo&quot;\n\n\nGEODIST key member1 member2 [UNIT]\n计算两个成员之间的距离。UNIT 可以是 m (米), km (千米), mi (英里), ft (英尺)。\nGEODIST city_locations &quot;Palermo&quot; &quot;Catania&quot; km\n\n\nGEORADIUS key longitude latitude radius IN|OUT [UNIT] [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]\n根据中心点和半径查询附近的成员。WITHCOORD 返回坐标，WITHDIST 返回距离。\nGEORADIUS city_locations 15 37 100 km WITHCOORD WITHDIST COUNT 5 ASC\n\n\nGEOSEARCH key [FROMMEMBER member|FROMLONLAT longitude latitude] [BYRADIUS radius unit|BYBOX width height unit] [ASC|DESC] [COUNT count [ANY]] [WITHCOORD] [WITHDIST] [WITHHASH]\n更灵活的地理空间查询指令 (Redis 6.2+)。\nGEOSEARCH city_locations FROMLONLAT 15 37 BYRADIUS 100 km\n\n\n场景示例: 查找附近的人&#x2F;店铺、地理围栏、LBS 游戏。\n十、总结Redis 凭借其丰富的数据结构和闪电般的读写速度，使其成为现代应用开发中不可或缺的工具。\n\n字符串：最简单，用于缓存、计数器、KV 存储。\n哈希：适合存储对象，如用户会话、商品信息。\n列表：实现消息队列、时间线、LIFO&#x2F;FIFO 队列。\n集合：去重、集合运算，如标签、共同兴趣、权限管理。\n有序集合：排行榜、带优先级队列、范围查询。\nStream：处理时间序列数据、消息队列、事件日志。\nHyperLogLog：大数据集的基数估算，节省内存。\nGeospatial：地理位置信息存储与查询，LBS 应用。\n\n通过理解每种数据结构的特性和适用场景，并熟练运用其相关指令，你将能够更好地设计和优化你的应用程序，充分发挥 Redis 的强大潜力。开始使用这些指令，构建你的高性能、高并发应用吧！\n","categories":["中间件","Redis"],"tags":["2023","Redis","中间件"]},{"title":"MySQL EXPLAIN 详解","url":"/2023/2023-06-01_MySQL%20%E7%B4%A2%E5%BC%95%E8%AF%A6%E8%A7%A3/","content":"\nEXPLAIN 是 MySQL 提供的一个非常强大的工具，用于分析 SELECT 语句的执行计划。通过 EXPLAIN 的输出结果，我们可以了解查询是如何执行的，包括使用了哪些索引、扫描了多少行、是否进行了文件排序等信息。这是数据库性能调优不可或缺的一环，能够帮助我们发现 SQL 语句中的性能瓶颈并进行优化。\n\n“优化前，先 EXPLAIN。没有 EXPLAIN 的优化都是盲人摸象。” - 数据库优化格言\n\n\n一、什么是 EXPLAIN？EXPLAIN 命令实际上是用来获取 MySQL 执行查询语句的执行计划的。执行计划描述了 MySQL 如何处理 SQL 语句，包括：\n\n表的连接顺序\n每个表使用的索引\n是否使用了临时表\n是否进行了文件排序\n扫描的行数预估\n\n通过分析这些信息，我们可以判断查询是否高效，是否可以进一步优化。\n二、如何使用 EXPLAIN？使用 EXPLAIN 非常简单，只需将 EXPLAIN 关键字放在任何 SELECT 语句的前面。\nEXPLAIN SELECT * FROM users WHERE username = &#x27;Alice&#x27;;EXPLAIN SELECT u.username, o.order_idFROM users u JOIN orders o ON u.id = o.user_idWHERE u.status = 1;\n\n执行后，结果会以表格的形式展示，每行代表一个表或一个操作。\n三、EXPLAIN 输出格式解读EXPLAIN 命令的输出结果通常包含以下列（不同版本或配置可能略有差异）：\n\n\n\n列名\n描述\n关键关注点\n\n\n\nid\nSELECT 查询的编号，表示查询中每个 SELECT 语句的序号。\n越大越优先执行，相同 ID 从上往下执行。\n\n\nselect_type\nSELECT 查询的类型。\nSIMPLE, PRIMARY, SUBQUERY, UNION 等。\n\n\ntable\n查询涉及的表名。\n关系到数据的来源。\n\n\npartitions\n匹配到的分区信息 (MySQL 5.6+), 对于未分区表显示 NULL。\n如果是分区表，查看是否正确选择分区。\n\n\ntype\n连接类型&#x2F;访问类型，非常重要，显示查询如何从表中查找行。\nALL (全表扫描) 最差，index, range, ref, eq_ref, const 较好。\n\n\npossible_keys\n可能使用的索引列表。\n供优化器选择的索引。\n\n\nkey\n实际使用的索引。\n优化器最终选择的索引。\n\n\nkey_len\n实际使用的索引长度（字节）。\n越短越好，看是否完全使用了联合索引。\n\n\nref\n显示索引的哪一列被用作查找依据。\n常量、其他表的列、函数等。\n\n\nrows\nMySQL 估计要扫描的行数。\n越小越好，直接影响查询性能。\n\n\nfiltered\nMySQL 估计将通过条件过滤的表行的百分比 (MySQL 5.7+)。\n过滤率越高，说明通过索引过滤的数据越多。\n\n\nExtra\n额外信息，包含许多重要的执行细节。\nUsing filesort, Using temporary, Using index (覆盖索引) 等，非常关键。\n\n\n接下来，我们详细解读其中几个最重要的列：\n1. id (SELECT Query ID)\n同一组的查询，id 相同。ID 越大，执行优先级越高。\n并发执行的查询，id 可能相同。\n如果存在子查询等嵌套查询，id 会不同。\nid 最大的语句块最先执行。\n如果 id 相同，则从上往下依次执行。\n\n\n\n示例：\nEXPLAIN SELECT * FROM users WHERE id IN (SELECT user_id FROM orders WHERE amount &gt; 100);-- id=2 (子查询) 会比 id=1 (外层查询) 先执行\n\n2. select_type (Query Type)表示每个 SELECT 语句的类型。常见的有：\n\nSIMPLE: 简单的 SELECT 查询，不包含 UNION 或子查询。\nPRIMARY: 最外层 SELECT 查询 (如果包含子查询)。\nSUBQUERY: 子查询中的第一个 SELECT 查询。\nDEPENDENT SUBQUERY: 依赖于外部查询的子查询。\nUNION: UNION 中的第二个或后续 SELECT 查询。\nDEPENDENT UNION: 依赖于外部查询的 UNION 中的第二个或后续 SELECT 查询。\nUNION RESULT: UNION 查询的结果集。\nDERIVED: 用于代表派生表（FROM 子句中的子查询）。\nMATERIALIZED: 已经物化（创建了临时表）的子查询（MySQL 5.6+）。\n\n3. table (Table Name)当前操作的表名。如果是派生表或 UNION 结果，会显示为 &lt;derivedN&gt; 或 &lt;unionM,N&gt;。\n4. type (Access Type) - 最重要的列之一这是判断查询性能的最关键指标之一，显示 MySQL 如何从表中查找行。从最好到最差的连接类型：\n\nsystem: 表只有一行记录（系统表），这是 const 类型的一个特例。\nconst: 通过主键或唯一索引查找，结果只有一行。非常快，因为只读一次。\nEXPLAIN SELECT * FROM users WHERE id = 1;\n\n\neq_ref: 对于每个来自先前的表的行，从当前表中读取一行。通常在连接操作中使用主键或唯一索引时发生。\nEXPLAIN SELECT * FROM users u JOIN orders o ON u.id = o.user_id WHERE o.order_id = 1;\n\n\nref: 非唯一性索引扫描，返回匹配某个单独值的多行。\nEXPLAIN SELECT * FROM users WHERE status = 1; (status 列有索引且值不唯一)\n\n\nrange: 范围扫描，适用于 WHERE 子句中使用 &lt;、&gt;、BETWEEN、IN 等操作符。\nEXPLAIN SELECT * FROM users WHERE id BETWEEN 1 AND 10;\n\n\nindex: 全索引扫描，扫描整个索引树，但由于不读取数据行，比 ALL 快（如果索引小于数据）。\nEXPLAIN SELECT username FROM users ORDER BY username; (如果 username 有索引)\n\n\nALL: 全表扫描，最差的访问类型。如果 Extra 列没有 Using where，那可能是在全表扫描后直接返回所有数据。如果 Extra 列有 Using where，那表示全表扫描后进行条件过滤。我们应该尽量避免。\nEXPLAIN SELECT * FROM users WHERE address LIKE &#39;%street%&#39;; (address 列没有索引)\n\n\n\n优化目标： 尽量将 type 优化到 ref、eq_ref、const 或 system 等，range 也是可以接受的。避免 ALL。\n5. possible_keys (Possible Keys)表示 MySQL 在当前查询中可能选择的索引列表。这只是一个候选列表，优化器最终可能不选择其中任何一个。\n6. key (Chosen Key) - 也很重要优化器最终决定实际使用的索引。\n\n如果为 NULL，表示没有使用索引。\n如果 key 显示的索引不在 possible_keys 中，说明 possible_keys 有误，或者 key 是通过隐式优化生成的（如自适应哈希索引）。\n\n7. key_len (Key Length)表示实际使用的索引的长度（字节数）。\n\n对于联合索引，key_len 可以帮你判断索引被用到了多少列。\n如果是一个 VARCHAR(100) CHARACTER SET utf8mb4 的列，其 key_len 会根据编码和是否允许 NULL 有所不同。\nkey_len 越小，说明索引用到的字段越少，或者字段的类型本身占用空间小。在保证索引效率的前提下，通常希望 key_len 尽可能小。\n\n8. ref (Reference)显示索引的哪一列或常量被用作查找索引的参考。\n\nconst: 表示与一个常量进行比较。\nfunc: 表示与表达式或函数的结果进行比较。\ndb.tbl.col_name: 表示与前一个表的某个列进行比较 (在连接查询中)。\n\n9. rows (Estimated Rows) - 非常重要MySQL 估计为了找到所需的行而需要读取的行数。这是一个非常重要的指标，值越小越好。它直接反映了查询的效率。\n即使 type 看起来不错，如果 rows 很大，也需要警惕。\n10. filtered (Filtered Percentage) - (MySQL 5.7+ 常用)通过条件过滤后的表行的百分比。\n\nfiltered 的值越高（越接近 100%），表示通过索引或 WHERE 条件过滤掉的数据越多，越高效。\n例如，rows 是 1000，filtered 是 10%，表示 MySQL 认为从这个表里取出 1000 行，经过 WHERE 过滤后，只有 100 行会传给上层。\n\n11. Extra (Extra Information) - 最重要的列之一包含不适合在其他列中显示但对查询优化非常重要的额外信息。以下是一些常见的 Extra 值及其含义：\n\nUsing index: 覆盖索引（Covering Index）。表示查询所需的所有数据都可以在索引中找到，而不需要回表查询数据行。这是非常高效的查询，值得追求。\nUsing where: 表明 WHERE 子句被用来限制哪些行与下一个表匹配，或者发送给客户端。如果 type 是 ALL 且 Extra 有 Using where，则表示在全表扫描后进行了过滤。\nUsing filesort: 文件排序。当查询需要对结果进行排序，但无法使用索引来完成排序时，MySQL 会在内存或磁盘上进行排序。这通常会导致性能问题，尤其是在大数据量时。应尽量避免。\n优化方法：为 ORDER BY 子句的列创建索引。\n\n\nUsing temporary: 使用临时表。通常发生在 GROUP BY 或 ORDER BY 子句无法使用索引优化时，或者多次 UNION 查询时。这也会导致性能问题，应尽量避免。\n优化方法：考虑优化 GROUP BY 或 UNION 语句，或增加内存。\n\n\nUsing join buffer (Block Nested Loop): 当两个表连接时，如果连接条件没有索引或者无法使用索引，MySQL 可能会使用连接缓冲区来处理。\nUsing index condition: 索引条件下推 (Index Condition Pushdown, ICP) (MySQL 5.6+)。在存储引擎层进行数据过滤，而不是在服务器层。这可以减少存储引擎返回给服务器层的行数，提高效率。\n例如，对于 idx(A, B)，查询 WHERE A &gt; 10 AND B &lt; 20，ICP 允许在遍历索引时就根据 B &lt; 20 条件进行过滤，而不是将所有 A &gt; 10 的行都取出来再过滤。\n\n\nUsing MRR: 多范围读取 (Multi-Range Read) (MySQL 5.6+)。当访问非聚集索引来获取数据时，MRR 可以将随机 I&#x2F;O 转换为顺序 I&#x2F;O，提高效率。\nBackward index scan: 反向索引扫描 (MySQL 8.0+)。查询以相反的顺序（降序）遍历索引，避免了额外的文件排序。\n\n四、EXPLAIN 的限制\nEXPLAIN 只能解释 SELECT 语句，不能解释 INSERT、UPDATE、DELETE。但可以通过将 UPDATE/DELETE 的 WHERE 子句提炼成 SELECT 语句进行分析。\nEXPLAIN 提供的是查询优化器估算的执行计划，在某些复杂查询或数据分布极端的情况下，实际执行计划可能与 EXPLAIN 有细微差异。\n当涉及到存储过程、触发器或用户自定义函数时，EXPLAIN 可能无法提供完整的执行计划信息。\n\n五、实际案例分析场景：用户表 users (id, username, email, status, create_time)，订单表 orders (order_id, user_id, amount, create_time)。\n案例 1: 无索引全表扫描EXPLAIN SELECT * FROM users WHERE email = &#x27;test@example.com&#x27;;\n\n\n\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n\n1\nSIMPLE\nusers\nNULL\nALL\nNULL\nNULL\nNULL\nNULL\n10000\n10.00\nUsing where\n\n\n分析:\n\ntype: ALL -&gt; 全表扫描，性能极差。\npossible_keys: NULL, key: NULL -&gt; 没有使用任何索引。\nrows: 10000 -&gt; 估计扫描 10000 行。\nExtra: Using where -&gt; 全表扫描后在服务器层进行条件过滤。\n\n优化: 为 email 列添加索引 CREATE INDEX idx_email ON users (email);\n案例 2: 使用普通索引EXPLAIN SELECT * FROM users WHERE email = &#x27;test@example.com&#x27;;\n\n\n\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n\n1\nSIMPLE\nusers\nNULL\nref\nidx_email\nidx_email\n302\nconst\n1\n100.00\nNULL\n\n\n分析:\n\ntype: ref -&gt; 这是一个良好的访问类型，表示通过非唯一索引查找。\nkey: idx_email -&gt; 成功使用了 email 索引。\nrows: 1 -&gt; 估计只扫描 1 行，效率极高。\nExtra: NULL -&gt; 没有额外的开销。\nkey_len: 302 -&gt; VARCHAR(100) 的索引长度（UTF8MB4 编码下，每个字符最多占 4 字节 + 2 字节长度前缀 + 1 字节 NULL 标识 &#x3D; 4*100 + 2 + 1 &#x3D; 403 字节，这里是 302，说明它可能只索引了部分长度或者编码不同）。\n\n案例 3: 使用覆盖索引EXPLAIN SELECT email FROM users WHERE email = &#x27;test@example.com&#x27;;\n\n\n\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n\n1\nSIMPLE\nusers\nNULL\nref\nidx_email\nidx_email\n302\nconst\n1\n100.00\nUsing index\n\n\n分析:\n\nExtra: Using index -&gt; 覆盖索引！ 查询的所有列（email）都可以在 idx_email 索引中获取，不需要回表查询数据行，效率最高。\n\n案例 4: 包含排序的文件排序EXPLAIN SELECT * FROM users ORDER BY create_time DESC;\n\n\n\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n\n1\nSIMPLE\nusers\nNULL\nALL\nNULL\nNULL\nNULL\nNULL\n10000\n100.00\nUsing filesort\n\n\n分析:\n\ntype: ALL -&gt; 全表扫描。\nExtra: Using filesort -&gt; 进行了文件排序，性能代价高。\n\n优化: 为 create_time 列添加索引 CREATE INDEX idx_create_time ON users (create_time);\nEXPLAIN SELECT * FROM users ORDER BY create_time DESC;\n\n\n\n\nid\nselect_type\ntable\npartitions\ntype\npossible_keys\nkey\nkey_len\nref\nrows\nfiltered\nExtra\n\n\n\n1\nSIMPLE\nusers\nNULL\nindex\nidx_create_time\nidx_create_time\n5\nNULL\n10000\n100.00\nBackward index scan (MySQL 8.0+) 或 NULL (旧版本)\n\n\n分析:\n\ntype: index -&gt; 全索引扫描，比全表扫描好。\nExtra: Backward index scan (MySQL 8.0+) 或 NULL (旧版本) -&gt; 说明利用索引进行排序，避免了文件排序。\n\n六、总结EXPLAIN 是 MySQL 性能调优的基石。掌握其输出结果的含义，并结合索引的知识进行分析，能够帮助我们：\n\n识别潜在的慢查询：特别是 type: ALL 和 Extra 中包含 Using filesort 或 Using temporary 的查询。\n验证索引的有效性：查看 key 字段是否使用了预期索引。\n优化索引设计和 SQL 语句：根据分析结果调整索引、重写 WHERE 或 JOIN 条件。\n\n记住，性能优化是一个持续的过程，EXPLAIN 是你在这个过程中最得力的助手。\n","categories":["中间件","MySQL"],"tags":["2023","中间件","MySQL","数据结构","算法"]},{"title":"MySQL B+树索引原理详解与对比","url":"/2023/2023-07-11_MySQL%20B+%E6%A0%91%E7%B4%A2%E5%BC%95%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AF%B9%E6%AF%94/","content":"\n数据库索引是提升查询性能的关键，而 MySQL 中最常见的索引结构就是 B+树。理解 B+树的原理对于优化数据库性能至关重要。本文将详细解析 B+树索引的内部工作机制，并将其与二叉查找树、平衡二二叉查找树、红黑树和 B 树进行对比，阐明 B+树在磁盘存储和数据库查询场景下的优势。\n\n“索引的本质是空间换时间，而 B+树是这种理念在磁盘存储场景下的极致优化。”\n\n\n一、为什么需要索引？想象一下，你有一本几百页的字典，如果要查找一个词，没有目录（索引）的话，你可能需要从头到尾翻阅。而有了目录（索引），你可以快速定位到词语的大致位置，大大提高查找效率。\n在数据库中，表是按照某种顺序（不一定是逻辑顺序）存储在磁盘上的。当数据量巨大时，如果没有索引，每次查询都需要进行全表扫描（Full Table Scan），这意味着数据库需要读取磁盘上的每一行数据并进行比较，效率极低。\n索引通过创建一种特殊的数据结构，可以快速定位到数据记录的位置，从而显著减少磁盘 I&#x2F;O 次数，提高查询速度。\n二、各种树结构简述与对比在深入 B+树之前，我们先回顾一下几种常见的树形数据结构，了解它们的优缺点，从而更好理解 B+树为何是数据库索引的优选。\n1. 二叉查找树 (Binary Search Tree - BST)\n特点：左子树所有节点的值小于根节点，右子树所有节点的值大于根节点。\n查找效率：平均情况下 O(logN)。\n缺点：在极端情况下（如插入的元素有序），二叉查找树会退化成链表，查找效率变为 O(N)。\n\n       50      /  \\    30    70   / \\    / \\  20 40  60 80// 退化情况 (左倾斜树)10 \\  20   \\    30     \\      40\n\n2. 平衡二叉查找树 (Balanced Binary Search Tree - BBST)\n特点：为了解决 BST 退化问题，BBST 引入平衡因子，确保树的高度尽可能小。任意节点的左右子树高度差不超过 1。常见的实现有 AVL 树。\n查找效率：始终保持 O(logN)。\n缺点：插入和删除操作时，可能需要进行多次旋转来维护平衡，增加了操作的复杂度。\n\n     40    /  \\  20    60 / \\    / \\10 30  50 70\n\n3. 红黑树 (Red-Black Tree - RBT)\n特点：一种自平衡二叉查找树，比 AVL 树的平衡条件更宽松，通过节点着色（红或黑）和旋转来保持平衡。\n查找效率：始终保持 O(logN)。\n优点：与 AVL 树相比，RBT 在插入和删除时进行旋转的次数更少，因此在读写混合的场景下表现更好。\n缺点：仍然是二叉树结构，每个节点只能有两个子节点。当数据量巨大时，树的高度依然会相对较高。\n\n4. B 树 (B-Tree)\n特点：多路平衡查找树。每个节点可以有多个子节点（通常是 2 个以上），而不仅仅是 2 个。节点内会存储多个关键字 (key)，并将搜索范围分割成多个子树。\n查找效率：O(log(k)N)，其中 k 是树的度（B 树中节点最大子节点数）。\n优点：\n降低树的高度：由于一个节点可以存储多个关键字和子节点指针，B 树的高度远低于二叉树。这对于磁盘存储至关重要，因为树的高度决定了磁盘 I&#x2F;O 的次数。\n适应磁盘 I&#x2F;O：B 树的节点大小通常设计为磁盘页（Page）的大小（如 4KB、16KB），一次磁盘 I&#x2F;O 可以读取整个节点，减少 I&#x2F;O 次数。\n\n\n缺点：每个节点既存储关键字又存储数据指针，数据指针可能分散在整个树中，导致范围查询效率相对较低。\n\nB 树节点结构概览：\n| Pointer1 | Key1 | Pointer2 | Key2 | Pointer3 | ... | KeyN | PointerN+1 |\n\n\nKey：索引值。\nPointer：指向子节点的指针。\n\n5. 对比总结\n\n\n特性\n二叉查找树\n平衡二叉查找树\n红黑树\nB 树\n\n\n\n节点子节点数\n2\n2\n2\nM (多于 2)\n\n\n平衡机制\n无\n严格平衡（AVL）\n宽松平衡\n自平衡（分裂与合并）\n\n\n树高\nO(N)~&#96;O(logN)&#96;\nO(logN)\nO(logN)\nO(log(M)N) (非常低)\n\n\n磁盘 I&#x2F;O 次数\n高\n高\n高\n低\n\n\n优点\n实现简单\n查找稳定 O(logN)\n读写均衡\n适合磁盘存储，降低 I&#x2F;O\n\n\n缺点\n易退化\n维护开销大\n仍然是二叉树结构\n范围查询效率略低\n\n\n从上表可以看出，对于数据库这种数据量大且存储在磁盘上的系统，B 树的多路、低高度特性使其比二叉树更具优势。但 B 树仍然有优化空间。\n三、B+树索引原理详解B+树是 B 树的变种，专门为文件系统和数据库设计，进一步优化了磁盘 I&#x2F;O 和范围查询。\n1. B+树的特点\n所有关键字（索引值）都出现在叶子节点中：非叶子节点只存储关键字和指向子节点的指针，不存储真正的数据。\n叶子节点包含了所有数据记录的指针，且互相连接（链表结构）：所有叶子节点构成一个有序链表，方便范围查询。\n非叶子节点（内节点）仅作为索引和分路，不存储数据：也称为索引节点。每个内节点中的关键字都是其子树中的最大（或最小）关键字。\n\nB+树节点结构概览：\n\n内节点 (非叶子节点)：| Pointer1 | Key1 | Pointer2 | Key2 | ... | KeyM-1 | PointerM |\n\nKey：索引值，仅用于指向子树，本身不带数据。\nPointer：指向子节点的指针。\n\n\n叶子节点：| Key1 | DataPointer1 | NextLeafPointer || Key2 | DataPointer2 | NextLeafPointer || ...                 | NextLeafPointer |\n\nKey：索引值。\nDataPointer：指向磁盘上实际数据记录的指针（对于聚簇索引，就是数据本身）。\nNextLeafPointer：指向下一个叶子节点的指针，形成有序链表。\n\n\n\n2. B+树的查找过程\n从根节点开始：根据要查找的关键字，在根节点内存中进行二分查找（或线性查找，取决于节点内关键字数量），找到对应的区间。\n沿指针下溯：根据找到的区间，获得指向子节点的指针，加载子节点到内存。\n重复步骤 1 和 2：直到达到叶子节点。\n在叶子节点中查找：在叶子节点内存中进行二分查找，找到目标关键字。\n获取数据指针&#x2F;数据：通过叶子节点存储的数据指针，定位并读取磁盘上的实际数据记录。\n\n示例：查找 Key &#x3D; 60\n        [50, 80]     (根节点，在内存中)       /    |    \\      /     |     \\   [1-40] [51-70] [81-100] (非叶子节点1，非叶子节点2，非叶子节点3)     |      |       |     V      V       V[10,20,30,40] [50,60,70] [80,90,100] (叶子节点，链表连接)\n\n查找 60：\n\n从根节点 [50, 80] 开始，60 &gt; 50 且 60 &lt; 80，走第二个指针。\n加载非叶子节点 [51-70] 到内存，60 在这个范围内，走指向 [50,60,70] 叶子节点的指针。\n加载叶子节点 [50,60,70] 到内存，找到 60。\n根据 60 对应的数据指针，获取数据。\n\n3. B+树的优势\n磁盘 I&#x2F;O 效率高：\n树的高度低：每个节点可以保存大量的关键字，使得 B+树的高度非常矮。通常 3-4 层深的 B+树就可以索引几十亿的数据。\n节点与磁盘页对应：B+树的节点大小通常等于一个磁盘页（如 16KB），一次磁盘 I&#x2F;O 可以读取整个节点，减少 I&#x2F;O 次数。大多数查询只需 3-4 次磁盘 I&#x2F;O 即可找到目标数据。\n\n\n范围查询友好：\n所有叶子节点组成一个有序链表。当查找完一个范围的起点后，只需沿着叶子节点的链表指针顺序遍历，而无需回溯到父节点，效率极高。\n\n\n查询性能稳定：\n所有查询都必须从根节点走到叶子节点，查询路径长度基本一致，因此查询性能稳定。\n\n\n有利于缓存：\n内节点只存储关键字和指针，占用空间小。当 B+树的内节点被加载到内存时，可以缓存更多的节点，进一步减少磁盘 I&#x2F;O。\n\n\n\n4. B+树的增删操作B+树的插入和删除操作相对复杂，需要保持树的平衡性、节点内关键字的有序性以及叶子节点链表的完整性。\n\n插入：\n找到合适的叶子节点插入新关键字。\n如果叶子节点未满，直接插入。\n如果叶子节点已满，则进行分裂：将一半关键字移到新的叶子节点，并将中间关键字（或其拷贝）提升到父节点。\n如果父节点也满了，则继续分裂，这个过程可能一直向上蔓延到根节点（导致树的高度增加）。\n\n\n删除：\n找到并删除叶子节点中的关键字。\n如果叶子节点关键字数量低于阈值，则尝试从兄弟节点借用关键字。\n如果无法借用，则进行合并：将该叶子节点与兄弟节点合并，并从父节点移除对应的关键字。\n合并过程也可能向上蔓延。\n\n\n\n四、MySQL 中的 B+树索引MySQL 主要存储引擎 InnoDB 实现了 B+树索引。它分为两种类型：聚簇索引和辅助索引（非聚簇索引）。\n1. 聚簇索引 (Clustered Index)\n定义：数据行本身就是存储在 B+树的叶子节点中。每个表只能有一个聚簇索引。\n特性：\n通常是表的主键（PRIMARY KEY）。如果表没有主键，InnoDB 会自动选择一个唯一的非空索引。如果没有这样的索引，InnoDB 会隐式地定义一个隐藏的行 ID 作为聚簇索引。\n数据的物理存储顺序与聚簇索引的逻辑顺序一致。\n优点：对于主键的查找和范围查询非常快，因为数据就在索引旁边，一步到位。\n缺点：数据插入顺序要尽可能和主键顺序一致，否则会造成大量的页分裂和数据挪动，导致性能下降和碎片化。\n\n\n数据结构：\nB+树的叶子节点存储完整的数据行。\n非叶子节点存储索引值和指向子节点的页指针。\n\n\n\n2. 辅助索引 &#x2F; 非聚簇索引 (Secondary Index &#x2F; Non-clustered Index)\n定义：除了聚簇索引之外的所有索引都是辅助索引。\n特性：\n不需要覆盖所有列，只包含索引列和聚簇索引的键值。\n优点：可以为不同的列创建多个辅助索引来优化不同查询。\n缺点：相比聚簇索引，它的查询过程是“回表”：\n通过辅助索引找到对应的聚簇索引键值。\n再通过聚簇索引键值去聚簇索引树中找到完整的数据行。\n\n\n\n\n数据结构：\nB+树的叶子节点存储索引值和对应的聚簇索引键值（主键值）。\n非叶子节点存储索引值和指向子节点的页指针。\n\n\n\n举例说明“回表”：\n假设 users 表有 id (主键, 聚簇索引), name (辅助索引), age 列。\n\n查询 SELECT * FROM users WHERE id = 100;\n直接通过聚簇索引 B+树查找，一次定位到叶子节点，获取完整数据。\n\n\n查询 SELECT * FROM users WHERE name = &#39;Alice&#39;;\n首先通过 name 辅助索引 B+树查找。\n在 name 索引的叶子节点找到 name=&#39;Alice&#39; 对应的 id 值（例如 id=100）。\n然后拿着 id=100，再去聚簇索引 B+树中查找，最终找到完整数据行。\n这个过程就是“回表”。\n\n\n\n索引覆盖 (Covering Index):\n如果辅助索引的叶子节点包含了所有查询需要的列，那么就不需要“回表”了。例如：SELECT id, name FROM users WHERE name = &#39;Alice&#39;;如果 name 列上有一个辅助索引，其叶子节点存储了 name 和 id，那么查询可以直接从辅助索引中获取 id 和 name，而无需回表。这种情况下，辅助索引成为“覆盖索引”，查询效率得到极大提升。\n五、总结B+树作为 MySQL 最核心的索引结构，凭借其独特的性质完美契合了磁盘存储和数据库查询的需求：\n\n多路结构、高度矮：极大地减少了磁盘 I&#x2F;O 次数，这是数据库性能的关键瓶颈。\n叶子节点链表：高效支持范围查询和全表扫描。\n内节点只存储索引：有助于将更多索引节点缓存在内存中。\n\n理解 B+树的这些原理，能够帮助我们：\n\n正确选择索引列：将经常用于 WHERE、ORDER BY、GROUP BY 的列作为索引。\n避免全表扫描：设计合适的索引以利用 B+树的快速查找能力。\n理解索引覆盖：通过创建覆盖索引来避免回表，进一步提高查询性能。\n优化插入顺序：对于聚簇索引，尽量使插入顺序与主键顺序一致，减少页分裂。\n\n总之，B+树是数据库查询性能的幕后英雄，深入理解其工作原理是数据库优化不可或缺的一环。\n","categories":["中间件","MySQL"],"tags":["2023","中间件","MySQL","数据结构","算法"]},{"title":"React 详解：核心 API 深度解读","url":"/2023/2023-07-27_React%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%A0%B8%E5%BF%83API%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BB/","content":"\nReact (也称为 React.js 或 ReactJS) 是一个由 Facebook 开发并维护的开源 JavaScript 库，用于构建用户界面 (User Interface)。本篇将深入剖析 React 的核心 API，涵盖了从组件定义到各种 Hooks 的详细用法，助您更全面地理解和运用 React。\n\n“React makes it painless to create interactive UIs. Design simple views for each state in your application, and React will efficiently update and render just the right components when your data changes.” —— React Official Documentation\n\n\n一、React 的核心模块与入口React 库被拆分为两个主要模块：react 和 react-dom。\n\nreact: 包含构建组件和定义其行为所需的核心 API（如 Component, useState, useEffect, createContext 等）。\nreact-dom: 提供与 DOM 交互的特定方法（如 render, createRoot 等），用于将 React 组件渲染到浏览器环境。\n\nreact-dom 主要 API1. createRoot(container) (React 18+)用途: 用于在客户端首次渲染 React 应用，是 React 18 引入的新的根 API，支持并发特性如 Concurrent Mode 和 Suspense。\n参数:\n\ncontainer: 一个 DOM 元素，React 将在该元素内部渲染您的组件。\n\n返回值: 一个根对象 (Root)。\n示例:\nimport React from &#x27;react&#x27;;import &#123; createRoot &#125; from &#x27;react-dom/client&#x27;; // 注意这里是从 &#x27;react-dom/client&#x27; 导入import App from &#x27;./App&#x27;;const container = document.getElementById(&#x27;root&#x27;);const root = createRoot(container); // 创建根root.render(&lt;App /&gt;); // 渲染应用// 或者在其他地方更新或卸载// root.unmount(); // 卸载组件树\n\n2. render(element, container, [callback]) (React 17 及以下)用途: 将一个 React 元素渲染到提供了的 container DOM 节点中，并返回对组件实例的引用（对于类组件）。\n参数:\n\nelement: 要渲染的 React 元素（通常是 JSX）。\ncontainer: DOM 元素，React 将在其内部渲染内容。\ncallback (可选): 在组件渲染或更新后执行的回调函数。\n\n示例:\nimport React from &#x27;react&#x27;;import ReactDOM from &#x27;react-dom&#x27;; // 注意这里是从 &#x27;react-dom&#x27; 导入import App from &#x27;./App&#x27;;ReactDOM.render(  &lt;React.StrictMode&gt;    &lt;App /&gt;  &lt;/React.StrictMode&gt;,  document.getElementById(&#x27;root&#x27;));\n\n3. unmountComponentAtNode(container) (React 17 及以下)用途: 从 DOM 中移除已挂载的 React 组件，清理其事件处理器和状态。\n示例:\nReactDOM.unmountComponentAtNode(document.getElementById(&#x27;root&#x27;));\n\n二、组件定义 APIReact 主要提供两种组件定义方式：函数组件 (Function Components) 和类组件 (Class Components)。随着 Hooks 的引入，函数组件已成为主流。\n1. 函数组件 (Function Components)定义: 普通的 JavaScript 函数，接收 props 对象作为参数，并返回一个 React 元素（通常是 JSX）。\n特点:\n\n无状态 (在 Hooks 出现之前)。\n更简洁、易于测试。\n配合 Hooks 使用，可以拥有状态和生命周期等功能。\n\n示例:\nimport React from &#x27;react&#x27;;// 简单函数组件function Greeting(props) &#123;  return &lt;h1&gt;Hello, &#123;props.name&#125;!&lt;/h1&gt;;&#125;// 箭头函数形式 (常见)const Farewell = (props) =&gt; &#123;  return &lt;p&gt;Goodbye, &#123;props.name&#125;.&lt;/p&gt;;&#125;;// 带解构的函数组件const Profile = (&#123; name, age &#125;) =&gt; &#123;  return (    &lt;div&gt;      &lt;p&gt;Name: &#123;name&#125;&lt;/p&gt;      &lt;p&gt;Age: &#123;age&#125;&lt;/p&gt;    &lt;/div&gt;  );&#125;;\n\n2. 类组件 (Class Components)定义: ES6 类，继承自 React.Component，且必须实现 render() 方法。\n特点:\n\n拥有自身的状态 (state)。\n可以通过生命周期方法 (componentDidMount, componentDidUpdate, componentWillUnmount 等) 响应组件的生命周期事件。\n在 React 16.8 (Hooks 引入) 之后，不建议在新项目中使用，但仍需了解其概念。\n\n示例:\nimport React, &#123; Component &#125; from &#x27;react&#x27;; // 导入 Componentclass Timer extends Component &#123;  constructor(props) &#123;    super(props);    this.state = &#123; count: 0 &#125;; // 初始化 state  &#125;  componentDidMount() &#123; // 组件挂载后执行    this.timerID = setInterval(() =&gt; this.tick(), 1000);  &#125;  componentWillUnmount() &#123; // 组件卸载前执行    clearInterval(this.timerID);  &#125;  tick() &#123;    this.setState(prevState =&gt; (&#123; // 使用函数形式更新 state      count: prevState.count + 1    &#125;));  &#125;  render() &#123; // 必须实现 render 方法    return &lt;p&gt;Count: &#123;this.state.count&#125;&lt;/p&gt;;  &#125;&#125;\n\n三、React Hooks API (React 16.8+)Hooks 是函数组件的核心。它们允许你在不编写 class 的情况下使用 state 和其他 React 特性。\n1. useState用途: 为函数组件添加状态。\n语法: const [state, setState] = useState(initialState);\n参数:\n\ninitialState: 状态的初始值。可以是任意类型，也可以是一个函数（该函数只会在首次渲染时执行，用于惰性初始化）。\n\n返回值: 一个数组，包含：\n\n当前状态值。\n一个用于更新状态的函数。\n\n示例:\n// Counter.jsximport React, &#123; useState &#125; from &#x27;react&#x27;;function Counter() &#123;  const [count, setCount] = useState(0); // number 状态  const [message, setMessage] = useState(&#x27;&#x27;); // string 状态  const [user, setUser] = useState(&#123; name: &#x27;Guest&#x27;, age: 0 &#125;); // object 状态  const increment = () =&gt; &#123;    setCount(prevCount =&gt; prevCount + 1); // 推荐使用函数式更新，避免闭包问题  &#125;;  const updateUserName = (newName) =&gt; &#123;    // 对于对象状态，setState 不会合并，需要手动合并    setUser(prevUser =&gt; (&#123; ...prevUser, name: newName &#125;));  &#125;;  return (    &lt;div&gt;      &lt;p&gt;Count: &#123;count&#125;&lt;/p&gt;      &lt;button onClick=&#123;increment&#125;&gt;Increment&lt;/button&gt;      &lt;p&gt;User Name: &#123;user.name&#125;&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; updateUserName(&#x27;Alice&#x27;)&#125;&gt;Set Alice&lt;/button&gt;      &lt;input type=&quot;text&quot; value=&#123;message&#125; onChange=&#123;(e) =&gt; setMessage(e.target.value)&#125; /&gt;      &lt;p&gt;Message: &#123;message&#125;&lt;/p&gt;    &lt;/div&gt;  );&#125;\n\n2. useEffect用途: 在函数组件中执行副作用操作（数据获取、订阅事件、手动修改 DOM、清理等）。它替代了类组件的 componentDidMount, componentDidUpdate, componentWillUnmount。\n语法: useEffect(setup, [dependencies]);\n参数:\n\nsetup: 包含副作用逻辑的函数。此函数可以返回一个清理函数（可选）。\ndependencies (可选数组): 一个依赖项数组。\n如果省略，useEffect 每次渲染后都会执行。\n如果为空数组 []，useEffect 只会在组件挂载时执行一次，并在组件卸载时执行清理函数（类似于 componentDidMount 和 componentWillUnmount）。\n如果包含依赖项，useEffect 会在依赖项发生变化时重新执行。\n\n\n\n返回值: 无。\n清理函数: useEffect 返回的函数会在下次 useEffect 执行前或组件卸载时执行，用于清理上次作用（如取消订阅、清除定时器）。\n示例:\nimport React, &#123; useState, useEffect &#125; from &#x27;react&#x27;;function DataFetcher(&#123; userId &#125;) &#123;  const [data, setData] = useState(null);  const [loading, setLoading] = useState(true);  const [error, setError] = useState(null);  useEffect(() =&gt; &#123;    console.log(`Fetching data for userId: $&#123;userId&#125;`);    setLoading(true);    setError(null);    setData(null); // 清空旧数据    const abortController = new AbortController(); // 用于取消请求    const signal = abortController.signal;    fetch(`https://jsonplaceholder.typicode.com/users/$&#123;userId&#125;`, &#123; signal &#125;)      .then(response =&gt; &#123;        if (!response.ok) &#123;          throw new Error(&#x27;Network response was not ok&#x27;);        &#125;        return response.json();      &#125;)      .then(json =&gt; &#123;        setData(json);      &#125;)      .catch(err =&gt; &#123;        if (err.name === &#x27;AbortError&#x27;) &#123; // 防止在组件卸载后更新状态          console.log(&#x27;Fetch aborted&#x27;);        &#125; else &#123;          setError(err);        &#125;      &#125;)      .finally(() =&gt; &#123;        setLoading(false);      &#125;);    // 清理函数    return () =&gt; &#123;      console.log(`Cleaning up for userId: $&#123;userId&#125;`);      abortController.abort(); // 取消未完成的请求    &#125;;  &#125;, [userId]); // 依赖项数组，当 userId 变化时重新执行 effect  if (loading) return &lt;div&gt;Loading user data...&lt;/div&gt;;  if (error) return &lt;div&gt;Error: &#123;error.message&#125;&lt;/div&gt;;  if (!data) return &lt;div&gt;No data found.&lt;/div&gt;;  return (    &lt;div&gt;      &lt;h2&gt;User Profile&lt;/h2&gt;      &lt;p&gt;Name: &#123;data.name&#125;&lt;/p&gt;      &lt;p&gt;Email: &#123;data.email&#125;&lt;/p&gt;    &lt;/div&gt;  );&#125;// 在 App 中使用// &lt;DataFetcher userId=&#123;1&#125; /&gt;// &lt;DataFetcher userId=&#123;2&#125; /&gt; // 切换 userId 会重新触发 effect\n\n3. useContext用途: 订阅 React Context 的值。这使得组件可以直接访问组件树中更高层组件提供的 Context 值，避免了 props 层层传递。\n语法: const value = useContext(MyContext);\n参数:\n\nMyContext: 由 React.createContext() 创建的 Context 对象。\n\n返回值: Context 对象的当前值。\n示例:\nimport React, &#123; createContext, useContext, useState &#125; from &#x27;react&#x27;;// 1. 创建 Context，并提供默认值const ThemeContext = createContext(&#x27;light&#x27;);// 2. 提供者组件function ThemeProvider(&#123; children &#125;) &#123;  const [theme, setTheme] = useState(&#x27;light&#x27;);  const toggleTheme = () =&gt; setTheme(t =&gt; (t === &#x27;light&#x27; ? &#x27;dark&#x27; : &#x27;light&#x27;));  const contextValue = &#123; theme, toggleTheme &#125;; // 包装成对象  return (    &lt;ThemeContext.Provider value=&#123;contextValue&#125;&gt;      &#123;children&#125;    &lt;/ThemeContext.Provider&gt;  );&#125;// 3. 消费者组件function ThemedButton() &#123;  const &#123; theme, toggleTheme &#125; = useContext(ThemeContext); // 从 Context 中获取值  return (    &lt;button className=&#123;theme&#125; onClick=&#123;toggleTheme&#125;&gt;      Current theme: &#123;theme&#125;    &lt;/button&gt;  );&#125;// 4. 应用中使用function App() &#123;  return (    &lt;ThemeProvider&gt;      &lt;div&gt;        &lt;h1&gt;My App&lt;/h1&gt;        &lt;ThemedButton /&gt;        &lt;p&gt;Some other content...&lt;/p&gt;      &lt;/div&gt;    &lt;/ThemeProvider&gt;  );&#125;\n\n4. useRef用途: 创建一个可变的 ref 对象，其 .current 属性可以在组件的整个生命周期中保存可变值，而不会导致重新渲染。最常见的用途是访问 DOM 元素。\n语法: const refContainer = useRef(initialValue);\n参数:\n\ninitialValue: ref 对象 .current 属性的初始值。\n\n返回值: 一个具有 current 属性的普通 JavaScript 对象。\n示例:\nimport React, &#123; useRef, useEffect &#125; from &#x27;react&#x27;;function FocusInput() &#123;  const inputRef = useRef(null); // 初始值为 null  useEffect(() =&gt; &#123;    // 确保 inputRef.current 存在（组件已挂载）    if (inputRef.current) &#123;      inputRef.current.focus(); // 自动聚焦 input 元素    &#125;  &#125;, []); // 空数组表示只在组件挂载时执行一次  const handleClick = () =&gt; &#123;    if (inputRef.current) &#123;      alert(`Input value: $&#123;inputRef.current.value&#125;`);    &#125;  &#125;;  return (    &lt;div&gt;      &lt;input type=&quot;text&quot; ref=&#123;inputRef&#125; /&gt; &#123;/* 将 ref 绑定到 DOM 元素 */&#125;      &lt;button onClick=&#123;handleClick&#125;&gt;Show Input Value&lt;/button&gt;    &lt;/div&gt;  );&#125;\n\n5. useReducer用途: useState 的替代方案，用于管理更复杂的 state 逻辑，例如涉及多个子值的 state，或者下一个 state 依赖于前一个 state。它与 Redux 的 reducer 概念相似。\n语法: const [state, dispatch] = useReducer(reducer, initialArg, init);\n参数:\n\nreducer(state, action): 一个纯函数，根据 state 和 action 计算新的 state。\ninitialArg: 初始状态。\ninit (可选): 一个惰性初始化函数，如果提供，则 initialArg 将作为其参数，其返回值作为初始状态。\n\n返回值: 一个数组，包含：\n\n当前状态值。\n一个 dispatch 函数，用于派发 action 来更新 state。\n\n示例:\nimport React, &#123; useReducer &#125; from &#x27;react&#x27;;// 1. 定义 reducer 函数const initialState = &#123; count: 0 &#125;;function reducer(state, action) &#123;  switch (action.type) &#123;    case &#x27;increment&#x27;:      return &#123; count: state.count + 1 &#125;;    case &#x27;decrement&#x27;:      return &#123; count: state.count - 1 &#125;;    case &#x27;reset&#x27;:      return initialState; // 重置到初始状态    case &#x27;set&#x27;:      return &#123; count: action.payload &#125;;    default:      throw new Error();  &#125;&#125;function CounterWithReducer() &#123;  const [state, dispatch] = useReducer(reducer, initialState);  return (    &lt;div&gt;      &lt;p&gt;Count: &#123;state.count&#125;&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; dispatch(&#123; type: &#x27;increment&#x27; &#125;)&#125;&gt;Increment&lt;/button&gt;      &lt;button onClick=&#123;() =&gt; dispatch(&#123; type: &#x27;decrement&#x27; &#125;)&#125;&gt;Decrement&lt;/button&gt;      &lt;button onClick=&#123;() =&gt; dispatch(&#123; type: &#x27;reset&#x27; &#125;)&#125;&gt;Reset&lt;/button&gt;      &lt;button onClick=&#123;() =&gt; dispatch(&#123; type: &#x27;set&#x27;, payload: 100 &#125;)&#125;&gt;Set to 100&lt;/button&gt;    &lt;/div&gt;  );&#125;\n\n6. useCallback用途: 记住（memoize）一个回调函数。当把回调函数作为 prop 传递给优化过的子组件时，或者作为 useEffect 的依赖项时，useCallback 可以避免不必要的重新创建函数实例，从而防止子组件不必要的重新渲染。\n语法: const memoizedCallback = useCallback(callback, [dependencies]);\n参数:\n\ncallback: 要记住的函数。\ndependencies (数组): 依赖项数组。只有当依赖项发生变化时，callback 才会重新创建。\n\n返回值: 记忆化的函数。\n示例:\nimport React, &#123; useState, useCallback, memo &#125; from &#x27;react&#x27;;// 子组件，使用 React.memo 进行性能优化const ChildComponent = memo((&#123; onClick, value &#125;) =&gt; &#123;  console.log(&#x27;ChildComponent rendered&#x27;);  return (    &lt;button onClick=&#123;onClick&#125;&gt;      Click me (&#123;value&#125;)    &lt;/button&gt;  );&#125;);function ParentComponent() &#123;  const [count, setCount] = useState(0);  const [name, setName] = useState(&#x27;Alice&#x27;);  // 每次 ParentComponent 渲染，handleClick 都会重新创建，导致 ChildComponent 重新渲染  // const handleClick = () =&gt; setCount(prevCount =&gt; prevCount + 1);  // 使用 useCallback 记住 handleClick。只有当 count 变化时，才会重新创建 handleClick。  const handleClick = useCallback(() =&gt; &#123;    setCount(prevCount =&gt; prevCount + 1);  &#125;, []); // 依赖项为空，表示函数只在首次渲染时创建一次  // 如果 handleClick 依赖 count，则需要将其加入依赖数组  // const handleClick = useCallback(() =&gt; &#123;  //   setCount(count + 1); // 这里的 count 依赖 state  // &#125;, [count]); // 当 count 变化时，重新创建 handleClick  return (    &lt;div&gt;      &lt;p&gt;Parent Count: &#123;count&#125;&lt;/p&gt;      &lt;ChildComponent onClick=&#123;handleClick&#125; value=&#123;count&#125; /&gt;      &lt;input type=&quot;text&quot; value=&#123;name&#125; onChange=&#123;(e) =&gt; setName(e.target.value)&#125; /&gt;      &lt;p&gt;Parent Name: &#123;name&#125;&lt;/p&gt;    &lt;/div&gt;  );&#125;\n\n7. useMemo用途: 记住（memoize）一个计算结果。它会在依赖项不变的情况下，避免重复执行昂贵的计算。\n语法: const memoizedValue = useMemo(() =&gt; computeExpensiveValue(a, b), [a, b]);\n参数:\n\ncomputeExpensiveValue: 一个在渲染期间执行的函数，返回要记住的值。\ndependencies (数组): 依赖项数组。只有当依赖项发生变化时，函数才会重新执行。\n\n返回值: 记忆化的计算结果。\n示例:\nimport React, &#123; useState, useMemo &#125; from &#x27;react&#x27;;function calculateFactorial(n) &#123;  console.log(`Calculating factorial for $&#123;n&#125;...`);  if (n &lt; 0) return -1;  if (n === 0) return 1;  let result = 1;  for (let i = 1; i &lt;= n; i++) &#123;    result *= i;  &#125;  return result;&#125;function FactorialCalculator() &#123;  const [number, setNumber] = useState(1);  const [incrementor, setIncrementor] = useState(0);  // 每次 incrementor 变化时，calculateFactorial 都会重新执行  // const factorial = calculateFactorial(number);  // 使用 useMemo，只有当 name 变化时，才会重新计算阶乘  const factorial = useMemo(() =&gt; calculateFactorial(number), [number]);  return (    &lt;div&gt;      &lt;p&gt;Factorial of &#123;number&#125; is: &#123;factorial&#125;&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; setNumber(number + 1)&#125;&gt;Increment Number (&#123;number&#125;)&lt;/button&gt;      &lt;p&gt;Incrementor: &#123;incrementor&#125;&lt;/p&gt;      &lt;button onClick=&#123;() =&gt; setIncrementor(incrementor + 1)&#125;&gt;Increment Incrementor (&#123;incrementor&#125;)&lt;/button&gt;    &lt;/div&gt;  );&#125;\n\n8. useImperativeHandle用途: 允许在 useRef 配合 forwardRef 使用时，自定义暴露给父组件的实例值，从而限制父组件可以访问的子组件内部功能。\n语法: useImperativeHandle(ref, createHandle, [dependencies]);\n参数:\n\nref: 由 React.forwardRef 提供的 ref 对象。\ncreateHandle: 一个函数，返回父组件将通过 ref.current 访问到的值。\ndependencies (数组): 当依赖项变化时，createHandle 会重新执行。\n\n示例:\nimport React, &#123; useRef, useImperativeHandle, forwardRef &#125; from &#x27;react&#x27;;// 子组件 SmallInput 必须用 forwardRef 包裹const SmallInput = forwardRef((&#123; label &#125;, ref) =&gt; &#123;  const inputEl = useRef(null);  // 使用 useImperativeHandle 自定义 ref.current 的值  useImperativeHandle(ref, () =&gt; (&#123;    focusInput: () =&gt; &#123; // 暴露一个 focusInput 方法给父组件      inputEl.current.focus();    &#125;,    clearInput: () =&gt; &#123; // 暴露一个 clearInput 方法      inputEl.current.value = &#x27;&#x27;;    &#125;,    getInputValue: () =&gt; inputEl.current.value // 暴露一个读取值的方法  &#125;));  return (    &lt;div&gt;      &lt;label&gt;&#123;label&#125;: &lt;/label&gt;      &lt;input type=&quot;text&quot; ref=&#123;inputEl&#125; /&gt;    &lt;/div&gt;  );&#125;);function ParentComponentWithInputHandle() &#123;  const inputRef = useRef(null);  const handleFocus = () =&gt; &#123;    if (inputRef.current) &#123;      inputRef.current.focusInput(); // 调用子组件暴露的方法    &#125;  &#125;;  const handleClear = () =&gt; &#123;    if (inputRef.current) &#123;      inputRef.current.clearInput();    &#125;  &#125;;  const handleAlertValue = () =&gt; &#123;    if (inputRef.current) &#123;      alert(`Input value is: $&#123;inputRef.current.getInputValue()&#125;`);    &#125;  &#125;;  return (    &lt;div&gt;      &lt;SmallInput label=&quot;My text&quot; ref=&#123;inputRef&#125; /&gt;      &lt;button onClick=&#123;handleFocus&#125;&gt;Focus Input&lt;/button&gt;      &lt;button onClick=&#123;handleClear&#125;&gt;Clear Input&lt;/button&gt;      &lt;button onClick=&#123;handleAlertValue&#125;&gt;Alert Value&lt;/button&gt;    &lt;/div&gt;  );&#125;\n\n9. useLayoutEffect用途: 与 useEffect 类似，但它在所有 DOM 变更后同步执行，浏览器在绘制前。适用于需要测量 DOM 布局（如滚动位置、元素尺寸）或执行与 DOM 视觉渲染紧密相关的副作用。\n语法: useLayoutEffect(setup, [dependencies]);\n特性:\n\n它的回调函数会在浏览器执行绘制之前执行，因此可以同步修改 DOM 布局。\n会阻塞浏览器的绘制，如果执行时间过长，可能导致性能问题。\n通常情况下，优先使用 useEffect，只有当需要同步操作 DOM 并且这会影响用户可见的布局时才使用 useLayoutEffect。\n\n示例:\nimport React, &#123; useState, useRef, useLayoutEffect &#125; from &#x27;react&#x27;;function Tooltip(&#123; children, position &#125;) &#123;  const [tooltipStyle, setTooltipStyle] = useState(&#123;&#125;);  const tooltipRef = useRef(null);  useLayoutEffect(() =&gt; &#123; // 同步测量，在浏览器绘制前调整位置    if (tooltipRef.current) &#123;      const &#123; width, height &#125; = tooltipRef.current.getBoundingClientRect();      if (position === &#x27;top&#x27;) &#123;        setTooltipStyle(&#123; transform: `translateY(-$&#123;height + 10&#125;px)` &#125;);      &#125; else if (position === &#x27;left&#x27;) &#123;        setTooltipStyle(&#123; transform: `translateX(-$&#123;width + 10&#125;px)` &#125;);      &#125;      // ... 更多位置计算    &#125;  &#125;, [position]);  return (    &lt;div style=&#123;&#123; position: &#x27;relative&#x27;, display: &#x27;inline-block&#x27; &#125;&#125;&gt;      &#123;children&#125;      &lt;div ref=&#123;tooltipRef&#125; style=&#123;&#123; ...tooltipStyle, position: &#x27;absolute&#x27;, background: &#x27;black&#x27;, color: &#x27;white&#x27; &#125;&#125;&gt;        I&#x27;m a tooltip!      &lt;/div&gt;    &lt;/div&gt;  );&#125;// &lt;Tooltip position=&quot;top&quot;&gt;&lt;button&gt;Hover Me&lt;/button&gt;&lt;/Tooltip&gt;\n\n10. useDebugValue用途: 用于在 React DevTools 中显示自定义 Hook 的标签。它不影响代码逻辑。\n语法: useDebugValue(value, [format])\n参数:\n\nvalue: 要显示的值。\nformat (可选): 一个函数，用于格式化 value，只在 DevTools 面板打开时执行，避免性能开销。\n\n示例:\nimport React, &#123; useState, useDebugValue &#125; from &#x27;react&#x27;;function useOnlineStatus() &#123;  const [isOnline, setIsOnline] = useState(true);  // 在 DevTools 中显示 &#x27;Online Status: Online&#x27; 或 &#x27;Online Status: Offline&#x27;  useDebugValue(isOnline, value =&gt; value ? &#x27;Online&#x27; : &#x27;Offline&#x27;);  React.useEffect(() =&gt; &#123;    const handleOnline = () =&gt; setIsOnline(true);    const handleOffline = () =&gt; setIsOnline(false);    window.addEventListener(&#x27;online&#x27;, handleOnline);    window.addEventListener(&#x27;offline&#x27;, handleOffline);    return () =&gt; &#123;      window.removeEventListener(&#x27;online&#x27;, handleOnline);      window.removeEventListener(&#x27;offline&#x27;, handleOffline);    &#125;;  &#125;, []);  return isOnline;&#125;function StatusBar() &#123;  const isOnline = useOnlineStatus();  return &lt;h1&gt;&#123;isOnline ? &#x27;✅ Online&#x27; : &#x27;❌ Offline&#x27;&#125;&lt;/h1&gt;;&#125;\n\n四、其他核心 API1. ReactDOM.createPortal(child, container)用途: 将子节点渲染到存在于父组件 DOM 层级之外的 DOM 节点。这在处理模态框 (Modals)、浮窗 (Tooltips)、加载指示器等需要脱离父元素样式或溢出限制的场景非常有用。\n参数:\n\nchild: 可以是任何可渲染的 React 子元素 (例如 JSX)。\ncontainer: 一个 DOM 元素，React 会将 child 挂载到这个 DOM 元素下。\n\n示例:\n// Modal.jsximport React from &#x27;react&#x27;;import &#123; createPortal &#125; from &#x27;react-dom&#x27;;const modalRoot = document.getElementById(&#x27;modal-root&#x27;); // 假设 HTML 中有一个 &lt;div id=&quot;modal-root&quot;&gt;&lt;/div&gt;function Modal(&#123; children, isOpen, onClose &#125;) &#123;  if (!isOpen) return null;  return createPortal(    &lt;div style=&#123;&#123;      position: &#x27;fixed&#x27;,      top: 0, left: 0, right: 0, bottom: 0,      backgroundColor: &#x27;rgba(0,0,0,0.5)&#x27;,      display: &#x27;flex&#x27;, alignItems: &#x27;center&#x27;, justifyContent: &#x27;center&#x27;    &#125;&#125;&gt;      &lt;div style=&#123;&#123;        background: &#x27;white&#x27;, padding: &#x27;20px&#x27;, borderRadius: &#x27;5px&#x27;      &#125;&#125;&gt;        &#123;children&#125;        &lt;button onClick=&#123;onClose&#125;&gt;Close Modal&lt;/button&gt;      &lt;/div&gt;    &lt;/div&gt;,    modalRoot // 将 Modal 的内容渲染到 modalRoot 节点  );&#125;// App.jsxfunction App() &#123;  const [showModal, setShowModal] = React.useState(false);  return (    &lt;div&gt;      &lt;h1&gt;My App&lt;/h1&gt;      &lt;button onClick=&#123;() =&gt; setShowModal(true)&#125;&gt;Open Modal&lt;/button&gt;      &lt;Modal isOpen=&#123;showModal&#125; onClose=&#123;() =&gt; setShowModal(false)&#125;&gt;        &lt;h2&gt;This is a modal!&lt;/h2&gt;        &lt;p&gt;It&#x27;s rendered outside the main app DOM tree.&lt;/p&gt;      &lt;/Modal&gt;    &lt;/div&gt;  );&#125;\n\n2. React.memo(Component, [arePropsEqual])用途: 是一种高阶组件 (HOC)，用于优化函数组件的性能。它会记住组件的渲染结果，如果 props 没有改变，则跳过重新渲染该组件。\n参数:\n\nComponent: 要进行性能优化的函数组件。\narePropsEqual (可选): 一个函数，用于自定义比较 props。如果返回 true，表示 props 相同，跳过重新渲染；否则重新渲染。默认是浅比较。\n\n示例:\nimport React, &#123; memo, useState &#125; from &#x27;react&#x27;;// 未优化的子组件const ExpensiveComponentUnoptimized = (&#123; count, name &#125;) =&gt; &#123;  console.log(&#x27;ExpensiveComponentUnoptimized rendered&#x27;);  return &lt;p&gt;Count: &#123;count&#125;, Name: &#123;name&#125;&lt;/p&gt;;&#125;;// 使用 memo 优化的子组件const ExpensiveComponent = memo((&#123; count, name &#125;) =&gt; &#123;  console.log(&#x27;ExpensiveComponent (memoized) rendered&#x27;);  return &lt;p&gt;Count: &#123;count&#125;, Name: &#123;name&#125;&lt;/p&gt;;&#125;);function ParentComponentOptimize() &#123;  const [parentCount, setParentCount] = useState(0);  const [parentName, setParentName] = useState(&#x27;World&#x27;);  return (    &lt;div&gt;      &lt;h1&gt;Parent Component&lt;/h1&gt;      &lt;button onClick=&#123;() =&gt; setParentCount(parentCount + 1)&#125;&gt;        Increment Parent Count (&#123;parentCount&#125;)      &lt;/button&gt;      &lt;button onClick=&#123;() =&gt; setParentName(parentName === &#x27;World&#x27; ? &#x27;React&#x27; : &#x27;World&#x27;)&#125;&gt;        Change Parent Name (&#123;parentName&#125;)      &lt;/button&gt;      &#123;/* 每次 ParentComponentOptimize 渲染，都会重新渲染 */&#125;      &lt;ExpensiveComponentUnoptimized count=&#123;parentCount&#125; name=&#123;parentName&#125; /&gt;      &#123;/* 仅当 props (count, name) 发生变化时才重新渲染 */&#125;      &lt;ExpensiveComponent count=&#123;parentCount&#125; name=&#123;parentName&#125; /&gt;    &lt;/div&gt;  );&#125;\n\n3. React.forwardRef(render)用途: 允许函数组件接收一个 ref，并将其向下转发给子组件内部的 DOM 节点或另一个 React 组件。\n参数:\n\nrender: 一个渲染函数，接收 props 和 ref 作为参数。\n\n示例: (见 useImperativeHandle 示例，SmallInput 组件就是用 forwardRef 包裹的)\n4. React.createContext(defaultValue)用途: 创建一个 Context 对象。当 React 渲染一个订阅了这个 Context 对象的组件时，它会从组件树中离这个组件最近的 Provider 获取当前 Context 值。\n参数:\n\ndefaultValue: 只有当组件没有对应的 Provider 时才会被使用。如果提供了 Provider，defaultValue 不起作用。\n\n返回值: 一个 Context 对象，包含 Provider 和 Consumer 组件。\n示例: (见 useContext 示例)\n5. React.lazy(loadComponent) + React.Suspense用途:\n\nReact.lazy: 允许你以动态导入（import()）的方式定义一个按需加载的组件。\nReact.Suspense: 允许在子组件（或组件树中的某个地方）完成异步加载时，展示一个回退 (fallback) UI。\n\n这对于代码分割和优化初始加载性能非常有用。\n语法:\n\nconst MyLazyComponent = React.lazy(() =&gt; import(&#39;./MyComponent&#39;));\n&lt;Suspense fallback=&#123;&lt;p&gt;Loading...&lt;/p&gt;&#125;&gt; ... &lt;/Suspense&gt;\n\n示例:\nimport React, &#123; Suspense &#125; from &#x27;react&#x27;;// 使用 React.lazy 动态导入组件const LazyLoadedComponent = React.lazy(() =&gt; import(&#x27;./LazyLoadedComponent&#x27;));function AppWithLazyLoading() &#123;  const [showLazy, setShowLazy] = React.useState(false);  return (    &lt;div&gt;      &lt;h1&gt;Main App&lt;/h1&gt;      &lt;button onClick=&#123;() =&gt; setShowLazy(true)&#125;&gt;Load Lazy Component&lt;/button&gt;      &#123;showLazy &amp;&amp; (        // Suspense 边界，当 LazyLoadedComponent 正在加载时，显示 fallback        &lt;Suspense fallback=&#123;&lt;div&gt;Loading lazy component...&lt;/div&gt;&#125;&gt;          &lt;LazyLoadedComponent /&gt;        &lt;/Suspense&gt;      )&#125;    &lt;/div&gt;  );&#125;// LazyLoadedComponent.jsx// export default function LazyLoadedComponent() &#123;//   return &lt;p&gt;I am a lazily loaded component!&lt;/p&gt;;// &#125;\n\n6. React.StrictMode用途: 一个用于突出显示应用中潜在问题的工具。它不会渲染任何可见 UI，但会为其后代激活额外的检查和警告。\n特性:\n\n在开发模式下，它会对以下行为发出警告：\n不安全的生命周期方法。\n使用过时的字符串 ref API。\n使用了废弃的 findDOMNode 方法。\n检测意外的副作用（双重调用 render 函数、useEffect 的 setup&#x2F;cleanup 函数）。\n遗留 Context API。\n\n\n不会对生产环境产生影响。\n\n示例:\nimport React from &#x27;react&#x27;;import &#123; createRoot &#125; from &#x27;react-dom/client&#x27;;import App from &#x27;./App&#x27;;const container = document.getElementById(&#x27;root&#x27;);const root = createRoot(container);root.render(  &lt;React.StrictMode&gt;    &lt;App /&gt;  &lt;/React.StrictMode&gt;);\n\n五、总结与展望React 的核心 API 旨在提供一套强大而灵活的工具集，以构建高性能和可维护的 UI。从基础的组件定义到现代的 Hooks，再到高级的 Portal、Memo 和 Suspense，React 持续演进，不断提升开发者的体验和应用的性能。\n\n函数组件 + Hooks: 已经成为 React 开发的首选范式，极大地简化了状态管理和副作用处理。\nVirtual DOM: 保证了高效的 UI 更新。\n声明式编程: 让 UI 逻辑更清晰、更易于理解。\n组件化: 促进了代码复用和可维护性。\n\n深入理解并熟练运用这些 API，是成为一名高效 React 开发者的关键。React 强大的生态系统和不断创新的特性，将继续为前端开发带来更多可能性。\n","categories":["前端技术","React"],"tags":["2023","TypeScript","React","前端技术"]},{"title":"TypeScript React 详解","url":"/2023/2023-08-01_TypeScript%20React%E8%AF%A6%E8%A7%A3/","content":"\nTypeScript + React 是现代前端开发中最强大的组合之一。TypeScript 为 React 应用带来了强大的类型系统，显著提高了代码质量、可维护性和开发效率。它在开发阶段就能捕获许多常见的错误，并提供出色的编辑器支持，使得构建大型、复杂的 React 应用变得更加可靠和愉快。\n\n“Adding TypeScript to your React project can feel like adding a safety net. It catches bugs early, improves code readability, and makes refactoring a breeze, especially as your application grows.”\n\n\n一、为什么在 React 中使用 TypeScript？React 本身是 JavaScript 库。虽然 JavaScript 灵活性高，但对于大型项目或多人协作，缺乏类型检查可能导致以下问题：\n\n难以发现的运行时错误: 许多类型相关的错误（例如，将一个字符串传递给期望数字的组件属性）只会在运行时报告，导致调试困难。\n代码可读性差: 开发者需要阅读大量代码或文档才能理解组件期望的属性 (props) 类型、状态 (state) 结构或函数参数。\n重构困难: 更改数据结构或组件接口时，很难快速准确地找出所有受影响的代码。\n有限的 IDE 支持: 没有类型信息，IDE 无法提供精准的自动补全、参数提示和错误检查。\n\nTypeScript (TS) 通过引入静态类型系统解决了这些问题：\n\n编译时错误检查: 在代码运行前捕获类型相关的错误。\n更好的代码可读性与自文档化: 类型定义本身就是文档，清晰地说明了数据结构。\n改进的代码重构: 编译器会检查所有受影响的地方，确保类型一致性。\n卓越的开发体验 (DX): 强大的 IDE 支持，包括自动补全、类型提示、重构工具和即时错误反馈。\n提升团队协作效率: 团队成员可以更快地理解和遵循代码约定。\n\n二、如何在 React 项目中启动 TypeScript？1. 新建项目使用 Create React App 或 Vite 等现代脚手架工具可以快速创建支持 TypeScript 的 React 项目：\n使用 Create React App (CRA):\nnpx create-react-app my-ts-app --template typescript# 或者yarn create react-app my-ts-app --template typescript\n\n使用 Vite (推荐，更快):\nnpm create vite@latest my-ts-app -- --template react-ts# 或者yarn create vite my-ts-app --template react-ts# 或者pnpm create vite my-ts-app --template react-ts\n\n2. 现有项目迁移\n安装 TypeScript:npm install --save-dev typescript @types/react @types/react-dom @types/node# 或者yarn add --dev typescript @types/react @types/react-dom @types/node\n\ntypescript: TypeScript 编译器本体。\n@types/react, @types/react-dom: React 和 ReactDOM 的类型定义。\n@types/node: Node.js 的类型定义 (如果使用 Node.js API)。\n\n\n添加 tsconfig.json: 在项目根目录创建 tsconfig.json 文件。&#123;  &quot;compilerOptions&quot;: &#123;    &quot;target&quot;: &quot;es5&quot;, // 编译为ES5，兼容性更好    &quot;lib&quot;: [&quot;dom&quot;, &quot;dom.iterable&quot;, &quot;esnext&quot;],    &quot;allowJs&quot;: true,    &quot;skipLibCheck&quot;: true,    &quot;esModuleInterop&quot;: true,    &quot;allowSyntheticDefaultImports&quot;: true,    &quot;strict&quot;: true, // 开启严格模式，强烈推荐    &quot;forceConsistentCasingInFileNames&quot;: true,    &quot;noFallthroughCasesInSwitch&quot;: true,    &quot;module&quot;: &quot;esnext&quot;,    &quot;moduleResolution&quot;: &quot;node&quot;,    &quot;resolveJsonModule&quot;: true,    &quot;isolatedModules&quot;: true,    &quot;noEmit&quot;: true, // 不生成JS文件，由构建工具（如Webpack/Vite）处理    &quot;jsx&quot;: &quot;react-jsx&quot; // 支持JSX  &#125;,  &quot;include&quot;: [    &quot;src&quot; // 告诉TS编译器检查src目录下的文件  ],  &quot;exclude&quot;: [    &quot;node_modules&quot; // 排除node_modules  ]&#125;\n重命名文件: 将 .js &#x2F; .jsx 文件重命名为 .ts &#x2F; .tsx。\n逐步添加类型: 根据 TypeScript 编译器的提示，逐步为组件属性 (props)、状态 (state) 和函数参数添加类型。\n\n三、React 组件中的类型定义1. 函数组件 (Functional Components)这是现代 React 中最常见的组件类型。\n1.1. Props 类型\n通过接口 (interface) 或类型别名 (type alias) 定义组件的 props。\n// 定义 Props 接口interface ButtonProps &#123;  label: string;  onClick: (event: React.MouseEvent&lt;HTMLButtonElement&gt;) =&gt; void;  primary?: boolean; // 可选属性  count?: number; // 也可以是联合类型&#125;// 使用 React.FC 或 React.VFC (推荐，更严格)// 或者直接在参数中解构并注解类型const MyButton: React.FC&lt;ButtonProps&gt; = (&#123; label, onClick, primary = false, count &#125;) =&gt; &#123;  const className = primary ? &#x27;button-primary&#x27; : &#x27;button-secondary&#x27;;  return (    &lt;button className=&#123;className&#125; onClick=&#123;onClick&#125;&gt;      &#123;label&#125; &#123;count !== undefined ? `($&#123;count&#125;)` : &#x27;&#x27;&#125;    &lt;/button&gt;  );&#125;;// 使用 MyButton&lt;MyButton label=&quot;Click Me&quot; onClick=&#123;() =&gt; console.log(&#x27;clicked&#x27;)&#125; primary /&gt;;&lt;MyButton label=&quot;Submit&quot; onClick=&#123;() =&gt; console.log(&#x27;submit&#x27;)&#125; count=&#123;5&#125; /&gt;;// 错误：遗漏 required 属性// &lt;MyButton primary /&gt;\n\n\nReact.FC (FunctionComponent): 提供 children 属性和一些静态属性（如 displayName）。在 React 18 之前广泛使用。\n\nReact.VFC (VoidFunctionComponent): 不自动提供 children 属性，更严格。已废弃并合并到 React.FC 和 React.Component 的类型定义中。\n\n直接注解参数: 推荐的方式，更简洁，且不包含隐式的 children 类型，如有需要可手动添加。\ninterface ButtonProps &#123;  label: string;  onClick: (event: React.MouseEvent&lt;HTMLButtonElement&gt;) =&gt; void;  children?: React.ReactNode; // 如果希望组件接收 children，需要明确声明&#125;const MyButton = (&#123; label, onClick, children &#125;: ButtonProps) =&gt; &#123;  return (    &lt;button onClick=&#123;onClick&#125;&gt;      &#123;label&#125; &#123;children&#125;    &lt;/button&gt;  );&#125;;&lt;MyButton label=&quot;Hello&quot; onClick=&#123;() =&gt; &#123;&#125;&#125;&gt;  &lt;span&gt;World&lt;/span&gt;&lt;/MyButton&gt;;\n\n1.2. State 类型 (使用 useState)\nuseState 钩子会尝试推断状态类型。如果初始值是 null 或 undefined，或希望更明确地指定复杂类型，可以手动指定泛型。\nimport React, &#123; useState &#125; from &#x27;react&#x27;;interface User &#123;  id: number;  name: string;  email: string;&#125;const UserProfile: React.FC = () =&gt; &#123;  // 初始值是 null，指定 User 或 null  const [user, setUser] = useState&lt;User | null&gt;(null);  const [loading, setLoading] = useState&lt;boolean&gt;(true);  const [error, setError] = useState&lt;string | null&gt;(null);  React.useEffect(() =&gt; &#123;    // 模拟数据加载    setTimeout(() =&gt; &#123;      if (Math.random() &gt; 0.5) &#123;        setUser(&#123; id: 1, name: &#x27;Alice&#x27;, email: &#x27;alice@example.com&#x27; &#125;);      &#125; else &#123;        setError(&#x27;Failed to load user data.&#x27;);      &#125;      setLoading(false);    &#125;, 1000);  &#125;, []);  if (loading) return &lt;div&gt;Loading user...&lt;/div&gt;;  if (error) return &lt;div&gt;Error: &#123;error&#125;&lt;/div&gt;;  if (!user) return &lt;div&gt;No user data.&lt;/div&gt;; // 在这里 user 是非 null 的  return (    &lt;div&gt;      &lt;h2&gt;User Profile&lt;/h2&gt;      &lt;p&gt;Name: &#123;user.name&#125;&lt;/p&gt;      &lt;p&gt;Email: &#123;user.email&#125;&lt;/p&gt;    &lt;/div&gt;  );&#125;;\n\n1.3. Effects 类型 (使用 useEffect)\nuseEffect 本身不需要类型参数，但回调函数中使用的变量应正确类型化。\n1.4. Context API 类型\n定义 Context 的值类型和默认值。\nimport React, &#123; createContext, useContext, useState, ReactNode &#125; from &#x27;react&#x27;;interface ThemeContextType &#123;  theme: &#x27;light&#x27; | &#x27;dark&#x27;;  toggleTheme: () =&gt; void;&#125;// 确保提供默认值，避免在使用时为 undefinedconst ThemeContext = createContext&lt;ThemeContextType | undefined&gt;(undefined);interface ThemeProviderProps &#123;  children: ReactNode;&#125;export const ThemeProvider: React.FC&lt;ThemeProviderProps&gt; = (&#123; children &#125;) =&gt; &#123;  const [theme, setTheme] = useState&lt;&#x27;light&#x27; | &#x27;dark&#x27;&gt;(&#x27;light&#x27;);  const toggleTheme = () =&gt; &#123;    setTheme((prevTheme) =&gt; (prevTheme === &#x27;light&#x27; ? &#x27;dark&#x27; : &#x27;light&#x27;));  &#125;;  const contextValue = &#123; theme, toggleTheme &#125;;  return &lt;ThemeContext.Provider value=&#123;contextValue&#125;&gt;&#123;children&#125;&lt;/ThemeContext.Provider&gt;;&#125;;export const useTheme = () =&gt; &#123;  const context = useContext(ThemeContext);  if (context === undefined) &#123;    throw new Error(&#x27;useTheme must be used within a ThemeProvider&#x27;);  &#125;  return context;&#125;;// 使用示例const ThemeButton: React.FC = () =&gt; &#123;  const &#123; theme, toggleTheme &#125; = useTheme();  return (    &lt;button onClick=&#123;toggleTheme&#125;&gt;      Current theme: &#123;theme&#125;. Click to switch.    &lt;/button&gt;  );&#125;;// 在 App.tsx 中// &lt;ThemeProvider&gt;//   &lt;ThemeButton /&gt;// &lt;/ThemeProvider&gt;\n\n2. 类组件 (Class Components)虽然函数组件更推荐，但理解类组件的类型定义也很重要。\nimport React, &#123; Component &#125; from &#x27;react&#x27;;interface WelcomeProps &#123;  name: string;  age?: number;&#125;interface WelcomeState &#123;  hasGreeted: boolean;  message: string;&#125;// 定义类组件时，通常传入两个泛型参数：Props类型 和 State类型class Welcome extends Component&lt;WelcomeProps, WelcomeState&gt; &#123;  constructor(props: WelcomeProps) &#123;    super(props);    this.state = &#123;      hasGreeted: false,      message: `Hello, $&#123;this.props.name&#125;!`    &#125;;  &#125;  componentDidMount() &#123;    // 模拟一些操作    setTimeout(() =&gt; &#123;      this.setState(&#123; hasGreeted: true, message: `Welcome $&#123;this.props.name&#125;!` &#125;);    &#125;, 1000);  &#125;  render() &#123;    const &#123; name, age &#125; = this.props;    const &#123; message &#125; = this.state;    return (      &lt;div&gt;        &lt;h1&gt;&#123;message&#125;&lt;/h1&gt;        &#123;age &amp;&amp; &lt;p&gt;You are &#123;age&#125; years old.&lt;/p&gt;&#125;        &#123;this.state.hasGreeted &amp;&amp; &lt;p&gt;I have greeted you!&lt;/p&gt;&#125;      &lt;/div&gt;    );  &#125;&#125;// 使用 Welcome&lt;Welcome name=&quot;Alice&quot; age=&#123;30&#125; /&gt;;&lt;Welcome name=&quot;Bob&quot; /&gt;;\n\n四、事件类型React 合成事件 (Synthetic Events) 具有自己的类型定义，通常可以通过 React.&lt;EventType&gt;Event&lt;HTMLElement&gt; 来指定。\nimport React from &#x27;react&#x27;;interface InputProps &#123;  onChange: (value: string) =&gt; void;  onSubmit: (e: React.FormEvent&lt;HTMLFormElement&gt;) =&gt; void;&#125;const MyForm: React.FC&lt;InputProps&gt; = (&#123; onChange, onSubmit &#125;) =&gt; &#123;  const handleChange = (e: React.ChangeEvent&lt;HTMLInputElement&gt;) =&gt; &#123;    // e.target.value 已经被正确推断为 string    onChange(e.target.value);  &#125;;  return (    &lt;form onSubmit=&#123;onSubmit&#125;&gt;      &lt;input type=&quot;text&quot; onChange=&#123;handleChange&#125; /&gt;      &lt;button type=&quot;submit&quot;&gt;Submit&lt;/button&gt;    &lt;/form&gt;  );&#125;;// 使用 MyForm&lt;MyForm  onChange=&#123;(value) =&gt; console.log(value)&#125;  onSubmit=&#123;(e) =&gt; &#123;    e.preventDefault();    console.log(&#x27;Form submitted&#x27;);  &#125;&#125;/&gt;;\n\n一些常见事件类型：\n\nReact.MouseEvent&lt;HTMLButtonElement&gt;: 按钮点击事件。\nReact.ChangeEvent&lt;HTMLInputElement&gt;: 输入框改变事件。\nReact.FormEvent&lt;HTMLFormElement&gt;: 表单提交事件。\nReact.KeyboardEvent&lt;HTMLInputElement&gt;: 键盘事件。\n\n五、Refs 类型使用 useRef 或 createRef 时，需要为其指定 DOM 元素的类型。\nimport React, &#123; useRef, useEffect &#125; from &#x27;react&#x27;;const MyInput: React.FC = () =&gt; &#123;  // 指定 ref 引用的是 HTMLInputElement 类型，初始值为 null  const inputRef = useRef&lt;HTMLInputElement&gt;(null);  useEffect(() =&gt; &#123;    // inputRef.current 在这里可能是 | null    if (inputRef.current) &#123;      inputRef.current.focus(); // 自动提示 focus() 方法    &#125;  &#125;, []);  return &lt;input type=&quot;text&quot; ref=&#123;inputRef&#125; /&gt;;&#125;;\n\n六、自定义 Hooks 类型自定义 Hooks 也应该正确地定义其参数和返回值的类型。\nimport &#123; useState, useEffect &#125; from &#x27;react&#x27;;interface UserData &#123;  id: number;  name: string;&#125;interface UseFetchResult&lt;T&gt; &#123;  data: T | null;  loading: boolean;  error: string | null;&#125;// 泛型自定义 Hookfunction useFetch&lt;T&gt;(url: string): UseFetchResult&lt;T&gt; &#123;  const [data, setData] = useState&lt;T | null&gt;(null);  const [loading, setLoading] = useState&lt;boolean&gt;(true);  const [error, setError] = useState&lt;string | null&gt;(null);  useEffect(() =&gt; &#123;    const fetchData = async () =&gt; &#123;      try &#123;        const response = await fetch(url);        if (!response.ok) &#123;          throw new Error(`HTTP error! status: $&#123;response.status&#125;`);        &#125;        const json = await response.json();        setData(json);      &#125; catch (e: any) &#123; // e 类型为 unknown，需要断言或检查        setError(e.message);      &#125; finally &#123;        setLoading(false);      &#125;    &#125;;    fetchData();  &#125;, [url]);  return &#123; data, loading, error &#125;;&#125;// 使用自定义 Hookconst UserFetcher: React.FC = () =&gt; &#123;  const &#123; data: user, loading, error &#125; = useFetch&lt;UserData&gt;(&#x27;/api/users/1&#x27;);  if (loading) return &lt;div&gt;Loading user...&lt;/div&gt;;  if (error) return &lt;div&gt;Error: &#123;error&#125;&lt;/div&gt;;  if (!user) return &lt;div&gt;No user data.&lt;/div&gt;;  return &lt;div&gt;User: &#123;user.name&#125;&lt;/div&gt;;&#125;;\n\n七、工具与最佳实践1. tsconfig.json 配置\nstrict: true: 强烈推荐开启，它会启用所有严格的类型检查选项，强制你编写更健壮的代码。\njsx: &quot;react-jsx&quot;: 适用于 React 17+ 新的 JSX 转换，无需在文件顶部导入 React。\nesModuleInterop: true: 改善 CommonJS 和 ES 模块之间的互操作性。\n\n2. 使用类型别名 vs 接口 (Type Alias vs Interface)\n接口 (interface): 更适合定义对象的形状，可以被合并 (declaration merging)。\n类型别名 (type): 可以定义任何类型（原始类型、联合类型、交叉类型、函数签名），更灵活。\n在 React 中，两者都可以用来定义 Props 和 State 的形状，选择哪个更多是个人偏好或团队约定。通常，对于对象形状，接口更常用。\n\n3. 类型推断让 TypeScript 尽可能地推断类型，只在必要时才明确添加类型注解。这能减少冗余代码。\n4. React.ReactNode当组件可能接收任意的 React 子元素时（字符串、数字、元素、组件数组等），使用 React.ReactNode 作为 children 的类型。\n5. 第三方库类型大多数流行库都有自己的类型定义，通常通过 @types/&lt;package-name&gt; 包提供。安装时会自动包含。\n6. ESLint 和 Prettier结合 ESLint 和 Prettier 可以进一步统一代码风格，并发现潜在的问题，例如使用 @typescript-eslint/eslint-plugin 来支持 TypeScript 特定的规则。\n八、总结将 TypeScript 引入 React 项目，就像为你的代码库增加了一层坚固的防护网。它在开发早期就能发现许多潜在错误，提供了无与伦比的编辑器支持，让代码变得更易读、易维护，并显著提升了开发效率和团队协作体验。虽然初期学习曲线可能存在，但长期来看，TypeScript 的加入会为 React 应用带来巨大的价值，尤其是在构建大型、复杂的企业级应用时，它几乎是不可或缺的。拥抱 TypeScript，享受更安全、更高效的 React 开发吧！\n","categories":["前端技术","React"],"tags":["2023","TypeScript","React","前端技术"]},{"title":"英语词根词缀系统性汇总：解锁词汇奥秘","url":"/2023/2023-08-21_%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%A0%B9%E8%AF%8D%E7%BC%80%E7%B3%BB%E7%BB%9F%E6%80%A7%E6%B1%87%E6%80%BB%EF%BC%9A%E8%A7%A3%E9%94%81%E8%AF%8D%E6%B1%87%E5%A5%A5%E7%A7%98/","content":"掌握英语词根词缀是扩大词汇量、提高阅读理解和词义猜测能力的关键。本系统性汇总旨在提供一个清晰、模块化的学习框架，帮助学习者高效记忆和运用这些词素。\n\n\n为什么要学习词根词缀？\n词义猜测能力 (Meaning Inference): 遇到生词时，通过识别其中的词根词缀，可以猜测其大致含义。\n词汇量爆发式增长 (Exponential Vocabulary Growth): 掌握一个词根或词缀，就能解锁一整个词汇家族。\n理解词源 (Etymological Insight): 深入了解单词的来源和演变，有助于长期记忆和文化理解。\n记忆效率提高 (Improved Retention): 相较于死记硬背，基于词根词缀的记忆更具逻辑性和结构性。\n提高阅读速度 (Enhanced Reading Speed): 减少因不认识单词而中断阅读的次数。\n\n\n词根词缀的构成一个英语单词通常由以下一个或多个部分构成：\n\n前缀 (Prefix): 位于词根前面，改变词根的含义，通常表示方向、否定、程度等。\n词根 (Root): 单词的核心部分，承载基本意义。\n后缀 (Suffix): 位于词根后面，改变词的词性或赋予特定语法功能。\n\n例如：un-believe-able\n\nun-: 前缀，表示否定\nbelieve: 词根，意为“相信”\n-able: 后缀，构成功能词，表示“能够…的”\nunbelievable: 难以置信的\n\n\n一、常见前缀 (Prefixes)前缀主要改变词根的含义。它们通常表示：\n\n否定 (Negation): un-, dis-, in-&#x2F;im-&#x2F;il-&#x2F;ir-, non-, a-&#x2F;an-, anti-, contra-\n方向&#x2F;位置 (Direction&#x2F;Position): ad-, de-, ex-, in-&#x2F;im-, pro-, re-, sub-, trans-, inter-, intra-, circum-\n程度&#x2F;数量 (Degree&#x2F;Quantity): over-, under-, hyper-, hypo-, multi-, mono-, uni-, bi-, tri-\n时间 (Time): pre-, post-, re-, fore-\n其他 (Others): co-&#x2F;com-&#x2F;con-, auto-, bene-, &#96;&#96;mal-/male-, omni-, pan-&#96;\n\n\n\n\n前缀\n含义\n示例（词族）\n\n\n\na-, an-\n无，不，非\nasexual, anarchy, atypical\n\n\nab-, abs-\n离开，不；非正常\nabnormal, absent, abduct, abstain\n\n\nad-\n去往，增加（常同化为 ac-, af-, ag-, al-, an-, ap-, ar-, as-, at-）\nadhere, admit, accord, affirm, aggression, allude, annex, appear, arrive, assist, attract\n\n\nambi-\n两者，周围\nambivalent, ambiguous, ambidextrous\n\n\nanti-\n反对，相反\nantifreeze, antisocial, antipathy\n\n\nauto-\n自己，自身\nautomatic, autobiography, autonomy\n\n\nbene-\n好，善\nbenefit, benevolent, benign\n\n\nbi-\n二，两\nbicycle, bilingual, bimonthly\n\n\ncircum-\n围绕\ncircumstance, circumvent, circumnavigate\n\n\nco-, com-, con-, cor-\n共同，一起\ncooperate, compose, connect, correlative\n\n\ncontra-, contro-\n反对，相反\ncontradict, controversy, contraband\n\n\nde-\n向下，离开，否定\ndecrease, deconstruct, deform, detract\n\n\ndis-\n不，相反，分离\ndisagree, disappear, discredit, dismiss\n\n\nen-, em-\n使…，进入，置于\nenable, embrace, empower, enlighten\n\n\nex-, e-\n出，超出，以前\nexit, exhale, ex-president, erase, eject\n\n\nextra-\n额外，超出\nextraordinary, extrapolate, extracurricular\n\n\nfore-\n前，预先\nforesee, forehead, foreground\n\n\nhetero-\n异，不同\nheterosexual, heterogeneous\n\n\nhomo-\n同，相同\nhomosexual, homogeneous, homograph\n\n\nhyper-\n超，过度\nhyperactive, hypertension\n\n\nhypo-\n低，不足\nhypothermia, hypothesis\n\n\nin-, im-, il-, ir-\n不，无，非\nincomplete, impossible, illegal, irregular\n\n\nin-, im-\n进入，向内\ninject, immerse, income\n\n\ninter-\n之间，相互\ninteract, international, interwoven\n\n\nintro-\n向内，内部\nintroverted, introspection, introduce\n\n\nmacro-\n大，宏观\nmacroeconomics, macroscopic\n\n\nmal-, male-\n坏，恶\nmalpractice, malfunction, malevolent\n\n\nmicro-\n小，微观\nmicroscope, microorganism, microchip\n\n\nmono-\n单一，独一\nmonochrome, monologue, monotheism\n\n\nmulti-\n多\nmultinational, multimedia, multiply\n\n\nnon-\n非，不\nnonstop, nonverbal, nonsense\n\n\nomni-\n全部，所有\nomnipresent, omniscient, omnivore\n\n\nout-\n超出，向外，胜过\noutnumbered, outstanding, outreach\n\n\nover-\n过度，在…上面\noverweight, oversleep, overflow\n\n\npan-\n全部，泛\npandemic, panorama, pan-African\n\n\nper-\n穿过，通过，彻底\nperceive, permeate, permanent\n\n\npost-\n后，以后\npostpone, postwar, postgraduate\n\n\npre-\n前，预先\nprepare, pretest, prefix\n\n\npro-\n向前，支持，赞成\nprogress, promote, proactive\n\n\nre-\n再，又，回\nrecall, review, rebuild\n\n\nretro-\n向后，倒退\nretrospective, retrograde, retroactive\n\n\nsemi-\n半，一部分\nsemicircle, semicolon, semifinal\n\n\nsub-\n在…之下，次一级\nsubway, subordinate, subconscious\n\n\nsuper-\n超级，在…上面\nsuperstar, supernatural, superficial\n\n\nsym-, syn-\n共同，一起\nsympathy, synthesis, synonym\n\n\ntele-\n远，远程\ntelephone, telescope, television\n\n\ntrans-\n穿过，转换\ntransport, transform, translucent\n\n\ntri-\n三\ntricycle, triangle, tripod\n\n\nun-\n不，非，相反\nunhappy, undo, unlock\n\n\nunder-\n在…下面，不足\nunderline, underprivileged, underdeveloped\n\n\nuni-\n单一，一个\nunicycle, uniform, unique\n\n\nvice-\n副，代理\nvice-president, viceroy\n\n\n\n二、核心词根 (Roots)词根是单词意义的核心。许多词根来源于拉丁语和希腊语。掌握这些词根，能够理解大量相关词汇。\n注意： 许多词根有多种拼写变体 (e.g., fac&#x2F;fect&#x2F;fic, duc&#x2F;duct)。\n\n\n\n词根\n含义\n示例（词族）\n\n\n\nact\n做，行动\nactive, react, action, actual, enact\n\n\naud\n听\naudio, audience, audible, audition\n\n\nbio\n生命\nbiology, biography, antibiotic, biome\n\n\ncap, capt, cept, cip\n拿，抓，头，包含\ncapture, receive, anticipate, capable, accept, concept, occupy, principal, anticipate\n\n\ncede, ceed, cess\n走，行\nproceed, success, recess, exceed, concede, intercede\n\n\nchron\n时间\nchronology, chronological, chronic, synchronize\n\n\ncred\n相信\ncredible, credit, creed, incredible, credentials\n\n\ndic, dict\n说，言\ndictionary, predict, contradict, verdict, dictate\n\n\nduc, duct\n引导，带领\nconduct, educate, induce, produce, reduction, aqueduct\n\n\nfac, fact, fect, fic\n做，制造\nfactory, effect, difficult, artificial, perfect, deficiency\n\n\nfer\n带来，携带\ntransfer, refer, fertile, defer, conference\n\n\nfin\n结束，限制\nfinish, define, finite, infinite, refine\n\n\nflect, flex\n弯曲\nreflect, flexible, deflect, inflection\n\n\nform\n形状，形成\nuniform, reform, formation, deform, inform\n\n\ngen\n产生，种类，出生\ngenerate, genetic, genus, gender, indigenous\n\n\ngeo\n地球\ngeography, geology, geometry, geothermal\n\n\ngraph, gram\n写，画\nphotograph, telegram, grammar, graphic, autograph\n\n\njud, jur, jus\n法律，判断\njudge, jury, justice, jurisdiction, perjury\n\n\nlect, leg\n选择，读，收集\ncollect, legible, election, lecture, intelligent\n\n\nlog, logue\n词语，思想，学说\nlogic, dialogue, apology, technology, monologue\n\n\nmanu\n手\nmanual, manuscript, manufacture, manipulate\n\n\nmedi\n中间\nmedium, mediate, intermediate, immediate\n\n\nmeter, metr\n测量\nthermometer, metric, symmetry, geometry\n\n\nmit, mis\n送，授予\ntransmit, mission, submit, dismiss, remit\n\n\nmort\n死亡\nmortal, mortgage, mortify, postmortem\n\n\nmov, mot, mob\n移动\nremove, motor, mobile, movement, motivate\n\n\nnounce, nunci\n报告，宣布\nannounce, pronounce, renounce, enunciate\n\n\nped\n脚，儿童\npedal, pedestrian, pedia, impediment, expedition\n\n\npel, puls\n推，驱使\ncompel, impulse, propel, repulsive, expel\n\n\npend, pens\n悬挂，称重，支付\npending, pension, suspend, depend, expensive\n\n\nphon\n声音\ntelephone, phonetics, symphony, cacophony\n\n\nphoto\n光\nphotograph, photosynthesis, photon, photogenic\n\n\nport\n携带\nportable, transport, import, report, support\n\n\npos, pon\n放置\nposition, compose, postpone, opponent, deposit\n\n\nrupt\n断裂，破裂\nrupture, interrupt, corrupt, abrupt, disrupt\n\n\nsci\n知道\nscience, conscious, omniscient, subconscious\n\n\nscrib, script\n写\ndescribe, script, prescribe, postscript, transcribe\n\n\nsec, sequ\n跟随\nsequence, consequently, prosecute, conduct\n\n\nsent, sens\n感觉，发送\nsensitive, consent, sensation, dissent, resent\n\n\nspec, spect\n看\ninspect, spectator, perspective, respect, suspect\n\n\nsta, sist, stit\n站立，放置\nstable, insist, constitute, status, resist\n\n\nstru, struct\n建造\nconstruct, structure, instruct, destroy, instrument\n\n\ntain, ten, tent\n保持，握住\ncontain, retain, sustenance, maintain, tenant\n\n\ntend, tens, tent\n伸展\nextend, tension, intention, contend, attend\n\n\nterr\n土地\nterrain, territory, subterranean, terrestrial\n\n\ntest\n证明，见证\ntestify, protest, testament, attest, detest\n\n\ntherm\n热\nthermometer, thermal, thermos, isotherm\n\n\ntract\n拉，拖\nattract, retract, tractor, extract, abstract\n\n\nven, vent\n来\nconvene, event, invention, prevent, conventional\n\n\nvers, vert\n转\nreverse, convert, introvert, avert, versatile\n\n\nvid, vis\n看\nvideo, vision, visible, revise, supervisor\n\n\nvoc, vok\n呼喊，声音\nvocal, invoke, provoke, advocate, revoke\n\n\nvolv, volu\n卷，转\nrevolve, evolve, volume, convoluted, involved\n\n\n\n三、后缀 (Suffixes)\n\n后缀主要改变单词的词性（名词、形容词、动词、副词）或赋予其特定的语法功能。\n1. 名词后缀 (Noun Suffixes)表示人、物、概念、状态、行为等。\n\n\n\n后缀\n含义 （通常是名词）\n示例（词族）\n\n\n\n-acy\n状态，性质\ndemocracy, accuracy, privacy\n\n\n-age\n行为，集合，状态\nmileage, breakage, courage, passage\n\n\n-al\n行为，过程\narrival, refusal, rehearsal\n\n\n-an, -arian\n人，做某事的人\nlibrarian, historian, comedian, vegetarian\n\n\n-ance, -ence\n状态，性质，行为\nperformance, excellence, dependence, importance\n\n\n-ancy, -ency\n状态，性质\nbuoyancy, efficiency, urgency\n\n\n-ant, -ent\n人，物，促成者\nparticipant, agent, student\n\n\n-ard\n具有某种特质的人\ncoward, drunkard, wizard\n\n\n-ation, -ition, -tion, -sion, -xion\n行为，过程，结果\ninformation, condition, nation, tension, complexion\n\n\n-cy\n状态，性质，职位\npresidency, urgency, candidacy\n\n\n-dom\n状态，领域\nfreedom, kingdom, wisdom\n\n\n-ee\n接受者，被…的人\nemployee, referee, nominee\n\n\n-eer\n从事某种职业的人\nengineer, volunteer, mountaineer\n\n\n-er, -or\n做某事的人，物，机器\nteacher, doctor, projector, actor, elevator\n\n\n-ess\n女性\nactress, hostess, waitress\n\n\n-hood\n状态，性质，时期\nchildhood, brotherhood, neighborhood\n\n\n-ian\n人，…地区的\nmusician, politician, Parisian\n\n\n-ibility, -ability\n能力，性质\nresponsibility, capability, credibility\n\n\n-ice\n行为，状态\njustice, service, avarice\n\n\n-ic\n人，学科\ncritic, public, rhetoric\n\n\n-ing\n行为，结果，事物\nbuilding, meeting, meaning, feeling\n\n\n-ism\n学说，主义，行为\ncapitalism, heroism, racism\n\n\n-ist\n从事者，信仰者\nartist, scientist, communist, capitalist\n\n\n-ity, -ty\n状态，性质\nelectricity, reality, loyalty, beauty\n\n\n-let\n小的\nbooklet, droplet, piglet\n\n\n-logy, -ology\n…学\nbiology, geology, psychology\n\n\n-ment\n行动，结果，状态\ngovernment, agreement, enjoyment, development\n\n\n-ness\n状态，性质\nkindness, happiness, darkness\n\n\n-ory\n场所，物品\nfactory, dormitory, laboratory\n\n\n-ship\n关系，状态，技能\nfriendship, leadership, internship, scholarship\n\n\n-th\n动作，状态\ngrowth, strength, depth, warmth\n\n\n-tude\n状态，性质\ngratitude, magnitude, solitude\n\n\n-ure\n行为，结果，状态\nculture, nature, exposure, closure\n\n\n-y\n状态，性质\nvictory, modesty, discovery\n\n\n2. 形容词后缀 (Adjective Suffixes)修饰名词，表示性质、特征等。\n\n\n\n后缀\n含义\n示例（词族）\n\n\n\n-able, -ible\n可…的，能…的，值得…的\nreadable, visible, incredible, accountable\n\n\n-al\n…的，具有…性质的\nnatural, musical, personal, universal\n\n\n-ant, -ent\n…的，…性质的\nobservant, different, dependent, persistent\n\n\n-ar\n…的，似…的\ncircular, regular, familiar, solar\n\n\n-ary\n…的，有关的，负责…的\nprimary, necessary, customary, honorary\n\n\n-ate\n…的，有…的\naccurate, compassionate, desolate, articulate\n\n\n-ful\n充满…的，有…的\nbeautiful, helpful, wonderful, insightful\n\n\n-ic, -ical\n…的，属于…的\neconomic, historical, identical, poetic\n\n\n-ile\n…的，易于…的\nfragile, sterile, infantile, versatile\n\n\n-ine\n…的，似…的\nmarine, feminine, aquiline, canine\n\n\n-ior\n比较级\nsuperior, interior, exterior, prior\n\n\n-ish\n像…的，有点…的\nchildish, reddish, selfish, foolish\n\n\n-ive\n有…倾向的，能…的\nactive, creative, destructive, descriptive\n\n\n-less\n无…的，不…的\ncareless, fearless, endless, harmless\n\n\n-like\n像…的\nchildlike, godlike, warlike\n\n\n-ly\n…的 (通常与名词结合)\nfriendly, costly, lovely (也有副词功能)\n\n\n-ous, -ious, -eous\n充满…的，有…性质的\nglorious, curious, courteous, adventurous\n\n\n-proof\n防…的\nwaterproof, foolproof, fireproof\n\n\n-some\n易于…的，有…倾向的\ntroublesome, handsome, awesome\n\n\n-y\n充满…的，有…的\nsunny, sleepy, rainy, witty, easy\n\n\n3. 动词后缀 (Verb Suffixes)使单词变为动词，表示“使成为”、“进行”等。\n\n\n\n后缀\n含义\n示例（词族）\n\n\n\n-ate\n使…，做\nactivate, elaborate, evaporate, articulate\n\n\n-en\n使…，变得…\nstrengthen, broaden, widen, heighten\n\n\n-ify, -fy\n使…，化\npurify, simplify, modify, classify\n\n\n-ize, -ise\n使…，化\ncivilize, fertilize, socialize, memorize\n\n\n4. 副词后缀 (Adverb Suffixes)修饰动词、形容词或其他副词，表示方式、程度等。\n\n\n\n后缀\n含义\n示例（词族）\n\n\n\n-ly\n…地\nquickly, slowly, happily, carefully\n\n\n-ward, -wards\n向…\nhomeward, backward, upward, downward\n\n\n-wise\n方式，方向\nclockwise, lengthwise, otherwise\n\n\n\n学习策略与技巧\n循序渐进： 从最常见、高频的词根词缀开始学习，不要试图一次性掌握所有。\n结合语境： 在实际句子和文章中去理解和应用词根词缀。\n多维记忆：\n视觉： 制作思维导图，将一个词根的词族可视化。\n听觉： 通过发音、跟读来记忆。\n书写： 反复抄写、造句。\n联想： 将词根词缀与已知的单词或形象联系起来。\n\n\n制作卡片 (Flashcards):\n正面：词根词缀 + 含义\n反面：3-5个常见例词及其中文释义\n\n\n主动测试： 遮住单词的词根或词缀，尝试猜测其含义；或者给出一个含义，尝试回想相关词根。\n善用工具： 词典、词源网站 (如 Etymonline.com) 是极佳的学习资源。\n阅读中发现： 在阅读英文书籍、文章时，有意识地去寻找并分析单词中的词根词缀。\n注意变体和歧义：\n拼写变体： 如 fer 和 ferr， pos 和 pon。\n同形异义： 少数词根或词缀可能有多个含义，需要结合语境判断。例如 de- 既可以表示“向下”（如 descend），也可以表示“否定”（如 deconstruct）。\n\n\n\n\n实践练习：分析单词尝试分析以下单词的构成，并理解其含义：\n\ncircumnavigate\nmalnutrition\nintrospective\nirresponsible\nphotograph\nchronicle\ndescriptive\nenable\nfortify\nbenevolent\n\n\n通过持续的练习和应用，你将能够逐步构建起庞大的英语词汇网络，让英语学习更加高效和有趣！\n","categories":["英语学习"],"tags":["2023","英语学习","单词记忆"]},{"title":"解析英语中的央元音：ə（schwa）和 ʌ","url":"/2023/2023-09-03_%E8%A7%A3%E6%9E%90%E8%8B%B1%E8%AF%AD%E4%B8%AD%E7%9A%84%E5%A4%AE%E5%85%83%E9%9F%B3%EF%BC%9A%C9%99%EF%BC%88schwa%EF%BC%89%E5%92%8C%20%CA%8C/","content":"英语中的元音系统复杂多样，其中央元音是一个非常特殊且重要的类别。它包括最常见的 schwa &#x2F;ə&#x2F;（非重读音节的元音）和 wedge &#x2F;ʌ&#x2F;（重读音节的元音）。掌握这两个音对于提高英语发音的自然度和理解口语至关重要。\n\n\n\n一、什么是央元音？央元音是指发音时舌头处于口腔中央位置（既不靠前也不靠后，既不高也不低）的元音。它们是口语中非常常见的音，尤其是在非重读音节中。\n1. 概念理解\n自然状态： 尝试放松口腔，自然地发出一个模糊的音，你的舌头和嘴唇都处于中立、放松的状态，这就是央元音的感觉。\n省力原则： 在非重读音节中，为了发音省力，许多元音都会弱化成央元音 &#x2F;ə&#x2F;。\n\n\n二、Schwa &#x2F;ə&#x2F;：最常见的元音&#x2F;ə&#x2F; 是英语中最常见的元音，我们称之为 “schwa”（弱读元音）。它总是出现在非重读音节中，发音短促、模糊、不清晰。它的存在使得英语的语流听起来自然、有节奏感。\n1. 发音要点\n舌位： 舌头处于口腔中央，放松，既不前也不后，不高也不低。\n唇形： 嘴唇放松，几乎呈中立状态，不圆也不扁。\n时长： 非常短促，是英语中最“弱”的元音。\n感觉： 像中文“啊”的短促、模糊、放松的版本，但不是“啊”那么清晰。\n\n2. &#x2F;ə&#x2F; 的常见拼写形式&#x2F;ə&#x2F; 可以由几乎所有元音字母或元音组合在非重读音节中表示：\n\na: sofa, about, again, banana\ne: taken, mother, broken, problem\ni: cousin, pencil, family, animal\no: history, common, second, factory\nu: success, autumn, circus, industry\nou: famous, enough\nea: occean\noi: porpoise\nar: dollar, regular\n\n3. &#x2F;ə&#x2F; 的重要性\n流利度： 正确使用 &#x2F;ə&#x2F; 能让你的英语听起来更自然，避免每个音节都读得太重。\n节奏感： 英语是重音计时语言 (stress-timed language)，有重音和非重音音节的交替。&#x2F;ə&#x2F; 的存在是重音模式的关键。\n理解native speakers： 英语母语者大量使用 &#x2F;ə&#x2F;，如果你不熟悉它，可能会难以识别很多单词。\n\n4. 练习技巧\n听音辨位： 听单词时，特别留意非重读音节里的模糊元音。\n对比重读元音： 听例如 present (礼物 - &#x2F;‘prɛzənt&#x2F;) 和 present (呈现 - &#x2F;prɪˈzɛnt&#x2F;) 的区别。\n跟读模仿： 模仿母语者发音时，重点体会他们如何弱化非重读音节。\n\n\n三、Wedge &#x2F;ʌ&#x2F;：重读音节的央元音&#x2F;ʌ&#x2F; 也是一个央元音，我们称之为 “wedge”（楔形音）。与 &#x2F;ə&#x2F; 不同，&#x2F;ʌ&#x2F; 总是出现在重读音节中，发音比 &#x2F;ə&#x2F; 更清晰、有力。它常被比作中文的“啊”或“呃”的短促音。\n1. 发音要点\n舌位： 舌头中部略微隆起，但仍然在口腔中央，比 &#x2F;ə&#x2F; 略高一些。\n唇形： 嘴唇放松，略微张开。\n时长： 短促而有力，是清晰的元音，但不是长元音。\n感觉： 类似于中文中快速说“啊”或“呃”时，口腔放松、快速发出的音。\n\n2. &#x2F;ʌ&#x2F; 的常见拼写形式&#x2F;ʌ&#x2F; 主要由以下字母表示，且通常在重读音节中：\n\nu: cut, but, run, sun, luck\no: some, love, money, done, glove (注意这里 o 发音变成了 &#x2F;ʌ&#x2F;，而不是 &#x2F;ɒ&#x2F; 或 &#x2F;ɔ:&#x2F;)\noo: flood, blood\nou: enough, tough, trouble\n\n3. &#x2F;ʌ&#x2F; 与 &#x2F;ə&#x2F; 的区别与联系\n区别：\n&#x2F;ʌ&#x2F;: 总是出现在重读音节，发音清晰有力。\n&#x2F;ə&#x2F;: 总是出现在非重读音节，发音短促模糊，是弱化元音。\n\n\n联系： 它们都是央元音，发音时舌位相对居中、放松。在某些方言或快速口语中，重读的 &#x2F;ʌ&#x2F; 可能会被弱化成 &#x2F;ə&#x2F;（尽管不常见）。\n\n4. 练习技巧\n掌握核心词： 记忆常见包含 &#x2F;ʌ&#x2F; 的单词，如 but, cut, run, love, money。\n对比相似音：\n&#x2F;ʌ&#x2F; vs &#x2F;ɑː&#x2F;: cut &#x2F;kʌt&#x2F; vs cart &#x2F;kɑːt&#x2F;\n&#x2F;ʌ&#x2F; vs &#x2F;ɔː&#x2F;: shut &#x2F;ʃʌt&#x2F; vs short &#x2F;ʃɔːt&#x2F;\n\n\n录音对比： 录下自己发 cut 和 about 的音，对比 &#x2F;ʌ&#x2F; 和 &#x2F;ə&#x2F; 的发音强度和清晰度。\n\n\n四、为什么掌握央元音很重要？\n听力理解： 母语者大量使用 &#x2F;ə&#x2F;，如果你不熟悉弱读，很多单词会听不出来。\n口语流利度： 避免“背诵腔”，让你的发音更接近母语者，语流更自然。\n减轻发音负担： 正确弱读非重读音节，能让你在说话时更轻松，减少口腔疲劳。\n区分单词： 某些单词的重音不同会导致意义不同，而弱读是重音模式的体现。\npresent (礼物 - &#x2F;‘prɛzənt&#x2F;) vs present (呈现 - &#x2F;prɪˈzɛnt&#x2F;)\ncontent (满足的 - &#x2F;kənˈtɛnt&#x2F;) vs content (内容 - &#x2F;ˈkɒntɛnt&#x2F; 或 &#x2F;ˈkɒntənt&#x2F;)\n\n\n\n\n五、综合练习\n单词练习：\n/ə/: about, teacher, doctor, banana, famous, celebrate, develop, attention\n/ʌ/: cup, money, dust, luck, run, hut, blood, touch\n\n\n句子练习：\nThe teacher will discuss about the money problem.\nHe loves to run under the sun.\nSome of the students are going to study for the exam.\n\n\n\n\n通过持续地练习和有意识地模仿，你将能够掌握这两个核心央元音，让你的英语发音迈上一个新台阶！\n","categories":["英语学习"],"tags":["2023","英语学习","单词记忆"]},{"title":"Git命令详解与实践","url":"/2023/2023-11-02_Git%E5%91%BD%E4%BB%A4%E8%AF%A6%E8%A7%A3%E4%B8%8E%E5%AE%9E%E8%B7%B5/","content":"\nGit 是一款免费、开源的分布式版本控制系统，旨在快速、高效地处理从小规模到超大规模的所有项目。它由 Linux 内核的创建者 Linus Torvalds 于 2005 年创建。Git 的核心理念是跟踪内容而非文件，并支持非线性开发（即多人并行开发，合并不同的工作流）。\n\n本文将深入介绍 Git 的核心概念、常用命令、工作流程、分支管理策略以及一些最佳实践。\n“Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency.” —— Git 官方 Slogan\n\n\n一、Git 核心概念在使用 Git 命令之前，理解其核心概念至关重要。\n1. 工作区 (Working Directory)\n你电脑上当前可见的目录，包含你项目的所有文件。\n你正在进行修改和编辑的地方。\n\n2. 暂存区 (Staging Area &#x2F; Index)\n一个文件，通常位于 .git/index，记录了你下次提交 (commit) 将要包含的所有文件修改。\n它是工作区和版本库之间的桥梁，用于选择性地添加修改。\n\n3. 本地仓库 (Local Repository)\n你项目 .git 目录下的所有内容，包含了项目的所有版本历史记录。\n所有的提交 (commit) 都存储在这里。\n\n4. 远程仓库 (Remote Repository)\n托管在网络上的仓库，如 GitHub、GitLab、Bitbucket 等。\n允许团队成员进行协作，共享代码。\n\n理解这三区一库 (工作区、暂存区、本地仓库、远程仓库) 的概念，是掌握 Git 的关键。\n示意图：Git 工作区、暂存区、本地仓库\n5. HEAD\n一个指向当前所在分支的指针。\n在分支上，它指向最新的一次提交 (commit)。\nHEAD 总是指向当前提交的快照 (snapshot)。\n\n二、Git 基础配置首次使用 Git 或在新机器上使用时，需要进行一些基本配置。\n# 配置用户名（重要，会记录到每次提交中）git config --global user.name &quot;Your Name&quot;# 配置用户邮箱（重要，会记录到每次提交中）git config --global user.email &quot;your.email@example.com&quot;# 设置默认的分支名称（Git 2.28+ 或更高版本，默认是 main）git config --global init.defaultBranch main# 查看所有配置git config --list# 查看某个配置项的值git config user.name\n\n三、Git 初始化与克隆1. 初始化本地仓库 (git init)在一个已存在的项目目录中初始化一个新的 Git 仓库。这会在当前目录下创建一个 .git 隐藏文件夹。\nmkdir my-new-projectcd my-new-projectgit init\n\n2. 克隆远程仓库 (git clone)从远程仓库下载一个项目到本地，并自动将其配置为本地仓库的远程源。\ngit clone &lt;remote-repository-url&gt; [local-directory-name]# 示例：git clone https://github.com/octocat/Spoon-Knife.git# 或者克隆到一个指定名称的目录git clone https://github.com/octocat/Spoon-Knife.git my-spoon-knife\n\n四、Git 文件管理1. 查看文件状态 (git status)查看工作区和暂存区文件的状态，哪些被修改了，哪些在暂存区等待提交。\ngit status# 简短输出，更适合快速查看git status -s\n\n2. 添加文件到暂存区 (git add)将工作区中文件的修改或新增的文件添加到暂存区。\n# 添加指定文件git add &lt;file1&gt; &lt;file2&gt; ...# 添加当前目录下所有修改和新增的文件（不包括删除的文件）git add .# 添加所有修改、新增和删除的文件（包括删除的，慎用）git add -A# 添加所有修改和新增的文件，包括被删除的（比较常用）git add -u\n\n3. 撤销暂存 (git reset)将暂存区的文件移回工作区（取消暂存）。\n# 将指定文件从暂存区中移除，但保留工作区修改git reset HEAD &lt;file&gt;# 将所有文件从暂存区中移除，但保留工作区修改git reset HEAD .\n\n4. 撤销工作区的修改 (git checkout &#x2F; git restore)丢弃工作区中文件的修改，恢复到上一次提交或暂存时的状态。\n\ngit checkout -- &lt;file&gt;: 传统方式，自 Git 2.23 起被 git restore 替代。\ngit restore &lt;file&gt;: 推荐方式（Git 2.23+），操作更清晰。\n\n# 丢弃指定文件在工作区的修改git restore &lt;file&gt;# 丢弃所有文件在工作区的修改（慎用，会丢失未提交的修改）git restore .\n\n5. 重命名或移动文件 (git mv)在 Git 中重命名或移动文件，并自动将这些更改暂存。\ngit mv &lt;old_path&gt; &lt;new_path&gt;# 示例：将 test.txt 重命名为 new_test.txtgit mv test.txt new_test.txt\n\n6. 删除文件 (git rm)将文件从工作区和暂存区中删除，并标记为待提交的删除操作。如果只想从 Git 追踪中移除文件，但保留在工作区（例如某些临时文件），可以使用 --cached 选项。\n# 从工作区和暂存区删除文件git rm &lt;file&gt;# 只从暂存区删除文件，保留工作区文件，不再被 Git 追踪git rm --cached &lt;file&gt;\n\n五、Git 提交与历史1. 提交修改 (git commit)将暂存区中的所有修改作为一个新的版本提交到本地仓库，并生成一个提交信息。\n# 使用默认编辑器输入提交信息git commit# 在命令行中直接输入提交信息git commit -m &quot;Your commit message here&quot;# 跳过暂存区，直接提交工作区中所有已追踪的修改（慎用，不推荐常规使用）# 等同于 git add -u &amp;&amp; git commit -m &quot;...&quot;git commit -am &quot;Your commit message here&quot;# 修改上一次提交（可用于修改提交信息或添加遗漏的文件）# 会打开编辑器让你修改提交信息，保存退出即可。如果想添加文件，先 git add，再 git commit --amendgit commit --amend\n\n2. 查看提交历史 (git log)查看本地仓库的提交历史记录。\n# 查看所有提交历史git log# 查看简短的提交历史git log --oneline# 以图谱形式查看提交历史（带分支信息，非常有用）git log --graph --oneline --decorate# 查看指定文件的提交历史git log &lt;file&gt;# 查看某个提交的具体修改内容git show &lt;commit_id&gt;\n\n3. 比较差异 (git diff)查看文件之间的差异。\n# 比较工作区和暂存区之间的差异git diff# 比较暂存区和最新提交之间的差异git diff --cached# 比较工作区和最新提交之间的差异git diff HEAD# 比较两个提交之间的差异git diff &lt;commit_id1&gt; &lt;commit_id2&gt;# 比较某个文件在工作区与暂存区的差异git diff &lt;file&gt;\n\n六、Git 分支管理分支是 Git 的核心功能之一，允许在不影响主线开发的情况下进行并行开发。\n1. 查看分支 (git branch)查看本地分支列表。\n# 列出所有本地分支，当前分支前有 *git branch# 列出所有本地和远程分支git branch -a# 列出所有远程分支git branch -r\n\n2. 创建分支 (git branch)创建一个新的分支。\ngit branch &lt;new-branch-name&gt;# 示例：创建一个名为 feature-x 的分支git branch feature-x\n\n3. 切换分支 (git checkout &#x2F; git switch)切换到指定分支。\n\ngit checkout &lt;branch-name&gt;: 传统方式。\ngit switch &lt;branch-name&gt;: Git 2.23+ 推荐，将切换操作与恢复操作 (git restore) 分离，使命令更清晰。\n\n# 切换到 feature-x 分支git switch feature-x# 切换到上一个分支git switch -# 创建并切换到新分支git switch -c &lt;new-branch-name&gt;# 等同于 git branch &lt;new-branch-name&gt; &amp;&amp; git switch &lt;new-branch-name&gt;\n\n4. 合并分支 (git merge)将指定分支的修改合并到当前分支。\n# 假设当前在 main 分支，要合并 feature-xgit switch maingit merge feature-x# 合并时强制执行 fast-forward（快速前进）模式（如果可能）git merge --ff-only &lt;branch&gt;# 合并时创建一个新的合并提交，即使可以 fast-forward（默认行为，推荐）git merge --no-ff &lt;branch&gt;# 终止正在进行的合并（例如发生冲突时）git merge --abort\n\n5. 删除分支 (git branch -d &#x2F; git branch -D)删除一个本地分支。\n# 安全删除分支（只删除已合并的分支）git branch -d &lt;branch-name&gt;# 强制删除分支（即使未合并也会删除，慎用！）git branch -D &lt;branch-name&gt;\n\n6. 变基 (git rebase)将一系列提交“移”到另一个基底提交之上。它可以使提交历史变得更线性、更整洁。\n\n优点: 产生线性的、整洁的提交历史，易于代码审查和追溯。\n缺点: 如果 rebase 了一个已经推送到远程的公共分支，会导致历史混乱，绝对禁止对公共分支进行 rebase。\n\n# 假设当前在 feature-x 分支，想将它 rebase 到 maingit switch feature-xgit rebase main# 交互式 rebase (用于修改、合并、删除提交等)git rebase -i &lt;commit_id&gt; # 从指定提交到当前 HEAD 之间的提交进行操作git rebase -i HEAD~3      # 操作最近的3个提交\n\n七、Git 远程仓库操作1. 查看远程仓库 (git remote)# 列出所有远程仓库git remote# 列出所有远程仓库及其 URLgit remote -v\n\n2. 添加远程仓库 (git remote add)git remote add &lt;name&gt; &lt;url&gt;# 示例：添加一个名为 origin 的远程仓库git remote add origin https://github.com/your/repo.git\n\n3. 从远程拉取 (git pull)从远程仓库获取最新修改并合并到当前本地分支。\n\ngit fetch + git merge: git pull 的底层操作。\ngit fetch: 只从远程获取修改，不合并。\n\n# 从 origin 远程仓库的 master 分支拉取并合并到当前本地分支git pull origin master# 获取所有远程分支的最新状态，但不合并git fetch origin# 获取所有远程仓库的所有分支的最新状态git fetch --all\n\n4. 推送到远程 (git push)将本地仓库的提交推送到远程仓库。\n# 将当前分支的修改推送到 origin 远程仓库的当前同名分支git push origin &lt;branch-name&gt;# 示例：将本地 main 分支推送到 origin 的 main 分支git push origin main# 首次推送时，设置上游分支（以后 git push 即可）git push -u origin main# 强制推送（慎用！会覆盖远程仓库的历史）git push -f origin &lt;branch-name&gt;\n\n5. 同步远程仓库 (git remote update)更新所有远程分支的引用。\ngit remote update\n\n八、Git 撤销操作Git 提供了多种撤销方式，但需谨慎使用。\n1. 撤销工作区修改 (git restore)前面已提过，用于丢弃工作区中文件的修改。\ngit restore &lt;file&gt;git restore . # 丢弃所有修改\n\n2. 撤销暂存区修改 (git reset HEAD)前面已提过，用于将暂存区文件移回工作区。\ngit reset HEAD &lt;file&gt;git reset HEAD . # 撤销所有暂存\n\n3. 撤销提交 (git reset)\ngit reset --soft &lt;commit_id&gt;:\n将 HEAD 移到 &lt;commit_id&gt;。\n保留工作区和暂存区的修改。\n你可以重新提交这些修改。\n\n\ngit reset --mixed &lt;commit_id&gt; (默认模式):\n将 HEAD 移到 &lt;commit_id&gt;。\n保留工作区修改。\n清空暂存区。\n你需要重新 git add 并 git commit。\n\n\ngit reset --hard &lt;commit_id&gt;:\n危险操作！ 将 HEAD 移到 &lt;commit_id&gt;。\n丢弃工作区和暂存区的所有修改，强制回到指定提交的状态。\n会丢失未保存的工作，请谨慎使用！\n\n\n\n# 撤销到上一次提交，保留工作区和暂存区git reset --soft HEAD~1 # 撤销到上一次提交，保留工作区，清空暂存区git reset HEAD~1 # 撤销到上一次提交，并丢弃所有修改（危险！）git reset --hard HEAD~1 # 撤销到指定提交 ID，并丢弃所有修改git reset --hard &lt;commit_id&gt; \n\n4. 反转提交 (git revert)\ngit revert &lt;commit_id&gt;:\n用于撤销一个已存在的提交。\n它会创建一个新的提交，这个新提交的内容是指定提交的逆向修改。\n优点是不会改写历史，非常适合在公共分支上撤销提交。\n\n\n\n# 反转指定提交，会创建一个新的反转提交git revert &lt;commit_id&gt; \n\n5. 找回丢失的提交 (git reflog)如果在使用 git reset --hard 或其他操作时不小心丢弃了提交，git reflog 可以帮助你找回。\ngit reflog# 查看 reflog 输出，找到你需要的 commit_id# 然后使用 git reset --hard &lt;commit_id&gt; 恢复\n\n九、Git 标签 (Tags)标签用于标记仓库历史中的重要里程碑，例如发布版本。\n# 列出所有标签git tag# 创建轻量标签（不含提交者信息，更像是一个分支指针）git tag &lt;tag-name&gt;# 创建附注标签（推荐，包含提交者、日期、信息等，对象在 Git 数据库中）git tag -a &lt;tag-name&gt; -m &quot;Tag message&quot;# 给特定提交创建标签git tag -a &lt;tag-name&gt; &lt;commit_id&gt; -m &quot;Tag message&quot;# 推送标签到远程仓库（默认 git push 不推送标签）git push origin &lt;tag-name&gt;# 推送所有标签到远程仓库git push origin --tags# 删除本地标签git tag -d &lt;tag-name&gt;# 删除远程标签git push origin --delete &lt;tag-name&gt;\n\n十、Git 临时存贮 (git stash)当你在一个分支上工作时，突然需要切换到另一个分支处理紧急问题，但又不希望提交当前未完成的工作，git stash 就能派上用场。它会保存当前工作区和暂存区的修改，然后将工作区恢复到 HEAD 的状态。\n# 存储当前修改（暂存区和工作区）git stash save &quot;Work in progress on feature X&quot;# 列出所有存储的 stashgit stash list# 应用最近的 stash（不会从列表中删除）git stash apply# 应用最近的 stash 并从列表中删除git stash pop# 应用指定的 stashgit stash apply stash@&#123;1&#125;# 删除最近的 stashgit stash drop# 删除指定的 stashgit stash drop stash@&#123;1&#125;# 删除所有 stashgit stash clear\n\n十一、Git 最佳实践\n提交粒度小而精: 每次提交只做一件事情，且提交信息清晰明了。\n提交信息规范: 遵循一定的提交信息规范，例如 feat: 新增功能, fix: 修复 bug, docs: 更新文档 等。\n频繁提交: 经常在本地进行提交，保持工作区干净。\n合理使用分支: 为每个功能、bug 修复或实验性任务创建独立的分支。\n主分支保持稳定: main (或 master) 等主分支应始终保持可发布状态。\n禁止对公共分支进行 rebase: rebase 适用于在本地清理提交历史，但切勿在已推送到远程的公共分支上强制 rebase，这会造成历史混乱。\n经常 pull &#x2F; fetch: 与团队成员协作时，保持本地仓库与远程仓库同步。\n代码审查 (Code Review): 通过 Pull Request&#x2F;Merge Request 进行代码审查，确保代码质量。\n忽略不必要的文件: 使用 .gitignore 文件忽略编译产物、日志、依赖模块等不应提交的文件。\n学习 git reflog: 它是你的后悔药，可以帮你找回“丢失”的提交。\n\n十二、总结Git 是现代软件开发不可或缺的工具。掌握了这些基础和进阶命令，你就能自信地管理项目代码，与团队高效协作。记住，实践是最好的学习方式，多加练习才能真正领会 Git 的强大之处。如果你有任何疑问，Git 官方文档和社区是获取帮助的最好资源。\n","categories":["开发工具","Git"],"tags":["2023","开发工具","Git"]},{"title":"常见网络攻击详解与预防：构建数字安全防线","url":"/2023/2023-10-12_%E5%B8%B8%E8%A7%81%E7%BD%91%E7%BB%9C%E6%94%BB%E5%87%BB%E8%AF%A6%E8%A7%A3%E4%B8%8E%E9%A2%84%E9%98%B2%EF%BC%9A%E6%9E%84%E5%BB%BA%E6%95%B0%E5%AD%97%E5%AE%89%E5%85%A8%E9%98%B2%E7%BA%BF/","content":"\n在数字时代，网络攻击已成为无处不在的威胁。从个人数据泄露到企业系统瘫痪，网络攻击的危害日益增长，形式也越来越多样化。理解这些S攻击类型、攻击原理以及如何有效预防它们，是构建强大数字安全防线的基石。本文将详细介绍一些最常见的网络攻击及其相应的防范措施。\n\n“网络安全不是一蹴而就的，而是一个持续不断的过程，需要技术、策略和人的共同努力。”\n\n\n一、概述：网络攻击的种类与威胁网络攻击通常利用系统、应用或协议的漏洞，试图破坏数据的机密性（Confidentiality）、完整性（Integrity）和可用性（Availability），即所谓的 CIA 三要素。\n根据攻击目标和手段，网络攻击可以分为多种类型：\n\n拒绝服务攻击 (DoS&#x2F;DDoS)：破坏系统的可用性。\n数据窃取&#x2F;泄露：破坏数据的机密性。\n数据篡改：破坏数据的完整性。\n恶意程序感染：破坏系统的可控性，窃取数据或进行其他恶意活动。\n社会工程学攻击：利用人性的弱点进行欺骗。\n\n接下来，我们将详细解析几种最常见的攻击类型。\n二、常见网络攻击详解与预防2.1 拒绝服务攻击 (DoS &#x2F; DDoS)2.1.1 攻击详解\nDoS (Denial of Service)：单点拒绝服务攻击。攻击者使用一台计算机向目标服务器发送大量无效或超负荷的请求，导致服务器资源耗尽，无法响应正常用户的请求。\nDDoS (Distributed Denial of Service)：分布式拒绝服务攻击。攻击者利用大量受感染的“僵尸”计算机（Botnet）同时向目标服务器发起攻击。这种攻击规模更大，更难防御，因为它来自不同的源IP地址，难以简单地通过封锁IP来解决。\n攻击目的：使目标服务器、网站或网络服务不可用，造成业务中断和经济损失。\n攻击类型：\n流量型攻击：通过发送海量流量淹没目标网络或服务器带宽。例如 SYN Flood、UDP Flood。\n资源耗尽型攻击：通过发送特定类型的请求耗尽服务器的CPU、内存、连接池等资源。例如 HTTP Flood、Slowloris。\n应用层攻击：针对应用程序的漏洞，通过少量请求就能耗尽资源，如大量查询数据库、触发复杂计算等。\n\n\n\n2.1.2 预防措施\n部署防火墙和入侵检测&#x2F;防御系统 (IDS&#x2F;IPS)：过滤恶意流量，检测异常模式。\nCDN (内容分发网络)：将流量分散到多个节点，并具备初步的DDoS清洗能力。\nDDoS 清洗服务：专业的DDoS防御服务提供商，可以在流量到达你的服务器之前进行过滤。\n负载均衡和冗余架构：分散流量，提高系统应对流量高峰的能力。\n流量异常监控：实时监控网络流量，及时发现异常峰值。\n限制请求频率和并发连接数：在反向代理（如 Nginx）和应用程序层面进行配置。\n及时修补系统和应用漏洞：防止攻击者利用已知漏洞进行攻击。\n\n2.2 SQL 注入 (SQL Injection)2.2.1 攻击详解\n攻击原理：应用程序在处理用户输入时，未对 &#39;、&quot;、\\ 等特殊字符进行充分过滤或转义，直接拼接到 SQL 查询语句中。攻击者可以通过插入恶意的 SQL 代码片段，改变原始查询逻辑。\n攻击目的：\n未经授权地访问、修改、删除数据库中的数据。\n绕过身份验证。\n甚至执行操作系统命令（取决于数据库和配置）。\n\n\n例子：\n原始查询：SELECT * FROM users WHERE username = &#39;&#123;$username&#125;&#39; AND password = &#39;&#123;$password&#125;&#39;\n攻击输入：username = &#39;admin&#39; -- (SQL 注释符)\n实际执行：SELECT * FROM users WHERE username = &#39;admin&#39; -- AND password = &#39;&#39;\n导致结果：成功绕过密码验证，以 admin 身份登录。\n\n\n\n2.1.2 预防措施\n使用参数化查询 (Prepared Statements)：这是最有效、最推荐的方法。将 SQL 语句与用户输入的数据分开处理，数据库引擎会明确区分代码和数据。\n例如在 Python 中使用 cursor.execute(&quot;SELECT * FROM users WHERE username = %s AND password = %s&quot;, (username, password))。\n\n\n输入验证和过滤：对所有用户输入进行严格的验证，只允许合法字符，禁止特殊字符。\n最小权限原则：数据库用户只分配其执行必要操作的最小权限，避免使用具有 DDL 或系统命令执行权限的用户。\n错误信息处理：不要向用户暴露详细的数据库错误信息，这可能暴露数据库结构。\nWAF (Web Application Firewall)：可以检测并拦截常见的 SQL 注入模式。\n\n2.3 跨站脚本攻击 (XSS - Cross-Site Scripting)2.3.1 攻击详解\n攻击原理：网站未能对用户提交的 HTML 或 JavaScript 代码进行过滤，导致这些恶意脚本在其他用户的浏览器中执行。\n攻击目的：\n窃取用户的 Session Cookie，劫持用户会话。\n在用户浏览器中执行恶意操作，例如修改页面内容、重定向到钓鱼网站。\n发送虚假请求。\n\n\n攻击类型：\n反射型 XSS (Reflected XSS)：恶意脚本通过 URL 参数注入，服务器将其反射回用户浏览器执行。\n存储型 XSS (Stored XSS)：恶意脚本被存储在服务器（如数据库），当其他用户访问包含恶意脚本的页面时，脚本被执行。危害最大。\nDOM XSS (DOM-based XSS)：漏洞存在于客户端代码中，恶意脚本不经过服务器，直接修改 DOM 结构。\n\n\n例子：用户发布留言时，输入 &lt;script&gt;alert(document.cookie)&lt;/script&gt;。如果网站没有过滤，其他用户访问此留言时，就会弹出包含他们 Cookie 的警告框。\n\n2.3.2 预防措施\n对用户输入进行严格的输出编码 (Output Encoding)：在将用户输入显示到网页之前，将所有可能被解释为 HTML 或 JavaScript 的特殊字符进行转义。\n例如将 &lt; 转义为 &amp;lt;，&gt; 转义为 &amp;gt;。\n\n\n内容安全策略 (CSP - Content Security Policy)：通过 HTTP 响应头配置浏览器，限制页面可以加载哪些资源（脚本、样式、图片等），并限制脚本的执行来源。\n输入验证和过滤：白名单机制，只允许已知安全的 HTML 标签和属性。\nHTTP-only Cookie：将敏感 Cookie 设置为 HttpOnly，防止 JavaScript 通过 document.cookie 访问。\n避免在 HTML 属性中直接使用用户输入。\n\n2.4 钓鱼攻击 (Phishing)2.4.1 攻击详解\n攻击原理：攻击者冒充合法机构（银行、社交媒体、电子邮件服务商、政府）或个人，通过电子邮件、短信、社交媒体等渠道发送虚假信息，诱骗受害者点击恶意链接、下载恶意附件或泄露个人敏感信息（用户名、密码、信用卡号）。\n攻击目的：窃取凭据、个人信息、财务数据，进行诈骗或进一步的攻击。\n常见手法：\n假冒网站：制作与真实网站 visually 相似的假网站。\n紧急&#x2F;诱惑信息：利用用户恐慌（账户异常）或贪婪（中奖信息）的心理。\n附件：包含恶意软件（木马、勒索软件）的附件。\n\n\n\n2.4.2 预防措施\n提高安全意识：\n警惕可疑邮件&#x2F;消息：检查发件人地址、邮件标题、邮件内容中的语法和拼写错误。\n不轻易点击链接：点击前将鼠标悬停在链接上，查看实际跳转地址是否与描述一致。\n不轻易下载附件：对于未知来源或可疑的附件，一律不打开。\n不随意透露个人信息：对于要求提供密码、银行卡号等敏感信息的请求，需高度警惕。\n\n\n使用多因素认证 (MFA)：即使密码被窃取，攻击者也难以登录。\n使用可靠的电子邮件服务和浏览器：它们通常内置了反钓鱼和恶意链接检测功能。\n定期更换密码，并使用强密码。\n\n2.5 恶意软件 (Malware - 木马、勒索软件、病毒等)2.5.1 攻击详解\n攻击原理：恶意软件是旨在损害、破坏或访问计算机系统而未经授权的任何软件。它们通常通过钓鱼邮件附件、感染的网站下载、捆绑在合法软件中或利用系统漏洞进行传播。\n常见类型：\n木马 (Trojan Horse)：伪装成合法程序，一旦运行，就会在受害者不知情的情况下执行恶意操作（如远程控制、窃取数据）。\n勒索软件 (Ransomware)：加密受害者文件或锁定系统，要求支付赎金才能恢复。\n病毒 (Virus)：通过感染其他程序或文件进行传播，具有自我复制能力。\n蠕虫 (Worm)：独立运行，通过网络自我复制传播，无需感染宿主程序。\n间谍软件 (Spyware)：秘密收集用户信息并发送给攻击者。\n后门 (Backdoor)：绕过正常安全验证，获得对系统的未授权访问。\n\n\n攻击目的：窃取数据、破坏系统、勒索钱财、利用资源进行其他攻击。\n\n2.5.2 预防措施\n安装并及时更新杀毒软件 (Antivirus)：定期扫描系统。\n保持操作系统和所有软件的最新状态：及时安装安全补丁，修复已知漏洞。\n使用防火墙：限制不必要的网络连接。\n谨慎下载和安装软件：只从官方或可信来源下载。\n禁用自动运行功能：如 USB 设备插入自动播放。\n定期备份重要数据：尤其是异地备份和离线备份，以防勒索软件攻击。\n提高安全意识：识别恶意链接、附件和可疑下载。\n\n2.6 缓冲区溢出 (Buffer Overflow)2.6.1 攻击详解\n攻击原理：当程序尝试写入超出其分配的固定大小缓冲区的数据时，多余的数据会覆盖相邻内存区域。攻击者可以精心构造输入，覆盖内存中的关键数据（如返回地址），从而控制程序的执行流程。\n攻击目的：执行任意恶意代码，提升权限，导致系统崩溃或程序异常。\n危害：这是一种底层攻击，但一旦成功，危害极大，可以完全控制目标系统。\n常见受害者：C&#x2F;C++ 等低级语言编写的程序，因为它们不提供内置的边界检查。\n\n2.6.2 预防措施\n安全的编程习惯：\n使用安全的库函数：避免使用不进行边界检查的 strcpy()、sprintf() 等函数，改用 strncpy()、snprintf() 等带有长度限制的函数。\n进行严格的输入验证和边界检查：在处理所有外部输入时，确保输入长度不会超出缓冲区大小。\n\n\n编译器和操作系统安全特性：\n数据执行保护 (DEP - Data Execution Prevention)：防止数据段中的代码被执行。\n地址空间布局随机化 (ASLR - Address Space Layout Randomization)：使攻击者难以预测内存地址。\n栈保护 (Stack Canaries)：在栈帧中插入特殊值，如果被覆盖则程序终止。\n\n\n使用内存安全的编程语言：如 Java, C#, Python, Go, Rust 等，它们提供了内存管理和边界检查，大大降低了缓冲区溢出的风险。\n\n三、通用网络安全预防原则除了针对特定攻击的措施外，以下普遍适用的安全原则至关重要：\n\n最小权限原则 (Principle of Least Privilege)：用户和进程只被授予完成任务所需的最小权限。\n纵深防御 (Defense in Depth)：部署多层安全措施，即使一层被攻破，还有其他层提供保护。\n定期安全审计和渗透测试：主动发现系统和应用的漏洞。\n数据加密：敏感数据在传输和存储时都进行加密。\n强大的身份验证和访问控制：使用强密码，结合 MFA，实施基于角色的访问控制 (RBAC)。\n及时更新与打补丁：对操作系统、应用程序、固件进行定期更新，修复已知的安全漏洞。\n安全意识培训：对所有员工进行定期安全培训，使其了解最新的威胁和安全最佳实践。\n备份和恢复计划：制定并测试完善的数据备份和灾难恢复计划。\n物理安全：保护服务器和网络设备的物理访问安全。\n日志记录和监控：收集、分析系统和应用程序的日志，及时发现异常行为。\n\n四、总结网络攻击是无休止的猫鼠游戏，攻击技术在不断演进，防御手段也必须随之升级。理解常见的攻击类型是有效防御的第一步。通过结合技术安全措施、严格的安全策略、员工安全意识培训，并坚持“防御即生存”的心态，我们可以共同努力，构建一个更安全、更可靠的数字环境。记住，没有绝对的安全，只有相对的安全，持续改进是网络安全永恒的主题。\n","categories":["网络安全"],"tags":["2023","网络安全"]},{"title":"JWT (JSON Web Tokens) 详解","url":"/2023/2023-12-21_JWT%20(JSON%20Web%20Tokens)%20%E8%AF%A6%E8%A7%A3/","content":"\nJWT (JSON Web Tokens) 是一种开放标准 (RFC 7519)，它定义了一种紧凑且自包含的方式，用于在各方之间安全地传输信息作为 JSON 对象。此信息可以通过数字签名进行验证，可以保证信息的完整性和真实性。JWT 通常用于认证和授权。\n\n“JSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object.” —— JWT.io\n\n\n一、为什么需要 JWT？在无状态的 Web 应用程序中，用户认证和授权是一个常见但复杂的问题。传统的基于 Session 的认证方式，在分布式系统（如微服务）和移动应用场景下遇到了挑战：\n\n跨域问题: Session 通常依赖 Cookie，而 Cookie 在跨域时有严格的限制。\n水平扩展性: Session 需要服务器端存储用户状态。当应用需要水平扩展时，Session 数据的共享和同步变得复杂（可能需要 Redis 等外部存储）。\n移动&#x2F;多客户端: 移动应用通常不使用 Cookie，给 Session 带来不便。\n性能开销: 每次请求都需要查询服务器端的 Session 存储。\n\nJWT 提供了一种替代方案，它允许服务器不再保存用户状态（无状态），将用户的认证信息和服务间授权信息通过 Token 的形式传递。\n二、JWT 的组成一个 JWT 实际上是一个字符串，通常由三部分组成，通过点 (.) 分隔：\nheader.payload.signature\n这三部分分别是：\n1. Header (头部)Header 通常包含两部分信息：\n\ntyp (Type): Token 的类型，固定为 JWT。\nalg (Algorithm): 签名 Token 所用的算法。常见的有 HMAC SHA256 (HS256) 或 RSA (RS256)。\n\n示例 (JSON 格式):\n&#123;  &quot;alg&quot;: &quot;HS256&quot;,  &quot;typ&quot;: &quot;JWT&quot;&#125;\n\n随后，这个 JSON 会被进行 Base64Url 编码，形成 JWT 的第一部分。\n2. Payload (负载)Payload 包含了一系列声明 (Claims)，这些声明是关于实体（通常是用户）以及附加元数据的语句。声明分为三种类型：\n\nRegistered Claims (注册声明):\n\n预定义的一些标准声明，并不是强制性的，但推荐使用，以提供互操作性。\n包括：\niss (Issuer): 签发人\nexp (Expiration Time): 过期时间（UNIX 时间戳），强烈推荐使用，以限制 Token 的有效期。\nsub (Subject): 主题\naud (Audience): 接收人（受众）\nnbf (Not Before): 在此时间之前，Token 不可用\niat (Issued At): 签发时间\njti (JWT ID): JWT 唯一标识\n\n\n\n\nPublic Claims (公有声明):\n\n可以自定义的声明，为了避免冲突，它们应该在 IANA JSON Web Token Registry 中注册，或者定义为包含 collision-resistant 命名空间的 URI。\n例如：&quot;name&quot;: &quot;John Doe&quot;\n\n\nPrivate Claims (私有声明):\n\n自定义的声明，用于在同意使用它们的各方之间交换信息。\n这些声明不是注册声明也不是公有声明。例如，一个应用可以定义 username 和 role 字段。\n例如：&quot;userId&quot;: &quot;123456&quot;, &quot;role&quot;: &quot;admin&quot;\n\n\n\n示例 (JSON 格式):\n&#123;  &quot;sub&quot;: &quot;1234567890&quot;,  &quot;name&quot;: &quot;John Doe&quot;,  &quot;admin&quot;: true,  &quot;exp&quot;: 1516239022, // 过期时间  &quot;iat&quot;: 1516239020  // 签发时间&#125;\n\n同样，这个 JSON 会被进行 Base64Url 编码，形成 JWT 的第二部分。\n注意: Payload 只是进行 Base64Url 编码，不是加密。这意味着任何人都可以在不解密的情况下读取到 Payload 中的信息。因此，敏感信息不应直接存储在 Payload 中。\n3. Signature (签名)签名部分用于验证 Token 的发送者，并确保 Token 没有被篡改。计算签名需要以下三个部分：\n\nBase64Url 编码后的 Header\nBase64Url 编码后的 Payload\n一个密钥 (secret)\n\n签名的计算公式通常是：\nSignature = HMACSHA256(base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret)\n其中 secret 是一个只有服务器知道的字符串。当 Token 在客户端和服务器之间传输时，服务器通过相同的算法和密钥重新计算签名。如果计算出的签名与 Token 中的签名匹配，则说明 Token 是有效且未被篡改的。\n三、JWT 的工作流程\n用户登录: 用户向认证服务器发送用户名和密码。\n验证凭据: 认证服务器验证用户的凭据。\n签发 Token: 验证成功后，认证服务器创建一个 JWT，其中包含用户的身份信息（Payload）和过期时间，并使用密钥 (secret) 对其进行签名。\n返回 Token: JWT 被发送回客户端。\n后续请求: 客户端在后续的每次请求中，将 JWT 附加到请求头中（通常是 Authorization 头，格式为 Bearer &lt;token&gt;）。\n验证 Token: 资源服务器接收到请求后，从请求头中获取 JWT。\n它首先验证 JWT 的签名是否有效（使用相同的算法和密钥重新计算签名）。\n接着，检查 Token 是否过期 (exp)，以及其他声明（如 iss, aud 等）是否符合预期。\n\n\n授权访问: 如果 Token 有效且未过期，服务器解析 Payload 获取用户身份信息，并据此决定是否授权用户访问请求的资源。\n\n四、JWT 的优点\n无状态 (Stateless): 服务器不需要存储 Session 信息，扩展性更好。\n易于扩展: 可以包含自定义的 Payload 信息，且可以在微服务架构中方便地共享认证信息。\n跨平台&#x2F;语言: 作为开放标准，支持多种编程语言和平台。\n安全性: 签名机制确保了 Token 的完整性，防止篡改。\n性能: 相较于查询数据库或缓存来获取 Session 信息，JWT 的验证通常更快。\n\n五、JWT 的缺点与安全考量\nPayload 不加密: Payload 只是 Base64Url 编码，而不是加密。不要在 Payload 中存储敏感信息。\nToken 一旦签发无法失效:\n除非过期，JWT 无法被“吊销”。\n如果 Token 被盗，攻击者可以滥用它直到过期。\n解决方案:\n设置较短的过期时间 (exp)，配合刷新 Token (Refresh Token) 机制。\n使用黑名单 (Blacklist) 或白名单 (Whitelist) 记录已登出或被吊销的 Token。\n限制 Payload 中的信息量，只包含必要的非敏感信息。\n\n\n\n\n密钥泄露: 如果用于签名的密钥泄露，攻击者可以伪造有效的 JWT。保护好密钥是重中之重。\nToken 存储: 客户端应安全地存储 JWT (例如在 HTTP Only 的 Cookie 中或 Web Storage，但要注意 XSS 风险)。\nLocal Storage&#x2F;Session Storage: 易受 XSS 攻击。\nHTTP Only Cookie: 可以有效防御 XSS，但 CSRF 风险依然存在。\n\n\n\n六、JWT 的使用场景\n认证 (Authentication): 最常见的用途。用户登录后，服务器返回 JWT，客户端在后续请求中携带它以证明身份。\n授权 (Authorization): 在 JWT 的 Payload 中包含用户的角色、权限等信息，资源服务器可以根据这些信息判断用户是否有权访问特定资源。\n信息交换: 在分布式系统或服务间调用时，JWT 可以用作信息交换的一种安全方式。例如，在微服务架构中，一个服务可以签发一个 JWT，包含特定请求的信息，传递给另一个服务进行处理。\n\n七、刷新 Token (Refresh Token) 机制为了解决 JWT 过期时间短和无法吊销的矛盾，通常会引入 Refresh Token 机制：\n\n登录成功: 服务器同时返回一个短生命周期的 Access Token (JWT) 和一个长生命周期的 Refresh Token。\nAccess Token 使用: 客户端使用 Access Token 访问受保护资源。\nAccess Token 过期: 当 Access Token 过期时，客户端检测到 401 Unauthorized 错误。\n使用 Refresh Token: 客户端发送 Refresh Token 到一个专门的刷新接口。\n验证 Refresh Token: 服务器验证 Refresh Token 的有效性（它通常存储在数据库中，可以被吊销）。\n签发新 Token: 如果 Refresh Token 有效，服务器签发新的 Access Token 和（可选地）新的 Refresh Token。\n继续访问: 客户端使用新的 Access Token 再次访问受保护资源。\n\nRefresh Token 可以存储在数据库中，并且可以被服务器吊销，从而实现对用户会话的控制。\n八、总结JWT 提供了一种高效、无状态的认证和授权机制，特别适用于分布式、跨平台和移动应用场景。然而，开发者必须充分理解其工作原理，尤其是 Payload 的可见性和 Token 无法直接失效的特性，并采取相应的安全措施（如短过期时间、刷新 Token、安全存储密钥、不在 Payload 存储敏感信息）来构建健壮安全的系统。\n","categories":["网络安全"],"tags":["2023","网络安全","JWT"]},{"title":"AWS Lambda与Serverless详解","url":"/2024/2024-01-13_AWS%20Lambda%E4%B8%8EServerless%E8%AF%A6%E8%A7%A3/","content":"前言\nServerless (无服务器) 是一种云计算执行模型，在这种模型中，云提供商动态地管理服务器资源的配置、部署、扩展和管理。开发者只需关注编写代码，而无需关心后端基础设施的运行和维护。AWS Lambda 是 Amazon Web Services (AWS) 提供的核心 Serverless 计算服务，它允许您运行代码而无需预置或管理服务器。\n\n“Serverless computing is a cloud-native development model that allows developers to build and run applications without having to manage servers.” —— AWS\n\n\n一、Serverless (无服务器) 架构概述1. 什么是 Serverless？Serverless 并非指“没有服务器”，而是指开发者无需关心或管理服务器。服务器仍然存在，但其运维工作（例如容量规划、补丁更新、操作系统维护、安全强化、负载均衡等）全部由云服务商负责。你的应用程序被解耦成一个个小的、独立的函数（或服务），这些函数在需要时才被执行。\n2. Serverless 的核心理念\n按需付费: 只为代码实际运行消耗的资源付费，代码没有运行时，不产生费用。\n自动伸缩: 根据请求量自动扩缩容量，无需人工干预。\n零服务器管理: 开发者无需管理底层服务器，专注于业务逻辑开发。\n事件驱动: 代码通常由特定的事件触发执行（例如 HTTP 请求、数据库变更、文件上传等）。\n\n3. Serverless 的优势\n降低运营成本: 无需管理服务器，减少运维开销。按需付费模式通常比预留实例更经济。\n简化开发: 开发团队可以专注于核心业务逻辑，提高开发效率。\n自动伸缩: 轻松应对流量峰谷，无需担心容量规划。\n高可用性: 云服务商通常在多个可用区部署 Serverless 服务，提供高可用性。\n更快的创新: 更快的部署周期，可以更快地将新功能推向市场。\n\n4. Serverless 的劣势&#x2F;挑战\n冷启动 (Cold Start): 函数在不活跃一段时间后，首次调用需要时间来启动执行环境，可能导致延迟。\n供应商锁定: 迁移到其他云服务商可能需要重构代码。\n受限的执行环境: 函数通常有执行时间、内存、存储等限制。\n调试和监控复杂: 分布式、无状态的特性使得调试和监控更加困难。\n成本预测: 在流量模式不确定的情况下，精确预估成本可能更具挑战性。\n\n5. Serverless 服务的类型Serverless 架构不仅仅是 FaaS (Function as a Service)，它还涵盖了其他无服务器服务：\n\nFaaS (Function as a Service): 最核心的 Serverless 服务，如 AWS Lambda, Azure Functions, Google Cloud Functions。\nBaaS (Backend as a Service): 提供预构建的后端服务，如身份验证、数据库、存储等，如 AWS Cognito, AWS S3, AWS DynamoDB, Google Firebase。\nServerless 数据库: 如 AWS DynamoDB, Aurora Serverless。\nServerless API 网关: 如 AWS API Gateway。\n\n二、AWS Lambda 详解AWS Lambda 是 AWS 的核心 FaaS 产品，它允许您将代码作为无服务器函数运行。\n1. Lambda 的工作原理\n上传代码: 您将代码（支持多种运行时，如 Node.js, Python, Java, Go, C#, Ruby, PowerShell, 自定义运行时）打包并上传到 Lambda。\n配置触发器: 设置一个或多个事件源来触发 Lambda 函数的执行（例如 API Gateway 的 HTTP 请求、S3 的文件上传、DynamoDB 的数据变更、CloudWatch 定时任务等）。\n按需执行: 当触发事件发生时，Lambda 服务会自动启动一个执行环境，运行您的代码，并将结果返回或处理。\n自动伸缩: 根据事件请求的并发量，Lambda 会自动扩展或收缩函数的执行实例。\n按实际使用付费: 您只需为函数运行的实际计算时间（以毫秒计）和请求次数付费。\n\n2. Lambda 的核心概念\n函数 (Function): 您的代码单元。\n运行时 (Runtime): Lambda 函数运行所需的环境（例如 Node.js 18, Python 3.9）。\n触发器 (Trigger): 定义什么事件会导致函数执行（如 API Gateway, S3, DynamoDB, SNS, SQS, CloudWatch Events, etc.）。\n事件 (Event): 触发器传递给函数的数据负载（JSON 格式）。\n执行环境 (Execution Environment): Lambda 为您的函数提供的安全且隔离的运行容器。\n内存 (Memory): 您为函数分配的内存量，它直接影响 CPU 和网络性能。\n超时 (Timeout): 函数允许运行的最长时间。\n并发 (Concurrency): 同时运行的函数实例数量。\n版本 (Versions): 可以为函数发布不同的版本，方便回滚和 A&#x2F;B 测试。\n别名 (Aliases): 指向特定版本的指针（例如 LATEST, PROD, DEV）。\n层 (Layers): 您可以打包第三方库、自定义运行时或其他依赖项作为层，供多个函数共享。\nDLQ (Dead-Letter Queue): 当函数处理失败时，将事件发送到的 SQS 队列或 SNS 主题，以便后续分析和重试。\nProvisioned Concurrency (预留并发): 预热 Lambda 函数实例，减少冷启动延迟。\nLambda@Edge: 在 AWS 全球内容分发网络（CloudFront）的边缘位置运行 Lambda 函数，以实现更低的延迟。\n\n3. Lambda 的常用触发器Lambda 的强大之处在于其与 AWS 生态系统中众多服务的集成：\n\nAPI Gateway: 构建 RESTful API 或 WebSocket API。\nS3 (Simple Storage Service): 图片上传、文件处理等事件。\nDynamoDB Streams: 实时处理数据库的变更事件。\nSQS (Simple Queue Service): 处理队列中的消息。\nSNS (Simple Notification Service): 订阅通知，处理消息。\nCloudWatch Events &#x2F; EventBridge: 定时任务、事件处理。\nKinesis: 实时数据流处理。\nALB (Application Load Balancer): 直接作为后端处理器。\nCognito: 用户身份验证、预注册等流程。\nRDS Proxy: 管理数据库连接池。\n\n4. Lambda 函数的最佳实践\n精简代码: 函数应该尽可能小，只做一件事 (单一职责原则)。\n无状态: 避免在函数实例内部存储状态。如果需要状态，请使用外部服务（如 DynamoDB, S3, RDS）。\n快速启动: 减少依赖包的大小，优化导入。\n调整内存: 内存设置会影响 CPU 和网络带宽。在测试环境中，逐渐增加内存直到性能不再显著提升，找到最佳平衡点。\n利用环境变量: 存储配置信息，而非硬编码。\n使用 Layers: 共享公共库和依赖。\n配置 DLQ: 捕获处理失败的事件。\n优化冷启动: 对于延迟敏感的应用，考虑预留并发 (Provisioned Concurrency)。\n日志和监控: 使用 CloudWatch Logs 和 Metrics 来监控函数运行状况。\n\n三、Serverless 架构实践案例Serverless 架构适用于多种应用场景：\n\nAPI 后端 (Web &#x2F; Mobile Backend):\n通过 API Gateway 暴露 RESTful API，Lambda 函数处理业务逻辑，后端使用 DynamoDB 或 RDS 存储数据。\n示例: 简单的 CRUD API, 用户认证服务。\n\n\n数据处理:\nS3 文件上传触发 Lambda 函数处理图像缩略图、视频转码、数据清洗和转换 (ETL)。\nKinesis Stream 实时数据流处理。\nDynamoDB Streams 实时数据分析和同步。\n\n\n定时任务:\n使用 CloudWatch Events (或 EventBridge) 定期触发 Lambda 函数，执行数据备份、报告生成、定时清理等任务。\n\n\n聊天机器人&#x2F;物联网 (IoT):\n处理来自聊天平台（如 Slack, Telegram）或 IoT 设备的实时消息。\n\n\n自动化运维:\n响应 AWS 资源变更事件，自动执行安全审计、资源管理、告警处理等任务。\n\n\n静态网站托管:\n结合 S3 (存储静态文件), CloudFront (CDN), API Gateway (API), Lambda (业务逻辑) 构建全栈无服务器应用。\n\n\n\n四、Serverless 工具链为了更高效地开发和部署 Serverless 应用，有许多工具可以辅助：\n\nAWS SAM (Serverless Application Model): AWS 官方提供的开源框架，用于定义和部署 Serverless 应用。基于 CloudFormation。\nServerless Framework: 一个流行的开源框架，支持 AWS Lambda、Azure Functions、Google Cloud Functions 等多个云平台。\nTerraform: 基础设施即代码 (IaC) 工具，可以定义和管理包括 Serverless 资源在内的云基础设施。\nCloudFormation: AWS 官方的 IaC 服务，所有 AWS 资源都可以通过 CloudFormation 模板定义。\n\n五、总结与展望Serverless 架构，特别是以 AWS Lambda 为代表的 FaaS 服务，正在改变我们构建和部署应用程序的方式。它通过将基础设施管理职责转移给云服务商，使开发者能够更加专注于核心业务逻辑，从而实现更快的开发迭代、更低的运营成本和更强大的伸缩性。\n尽管 Serverless 仍然面临冷启动、供应商锁定等挑战，但随着技术的发展和生态系统的完善，这些问题正逐步得到缓解。对于追求高效率、低成本和快速迭代的现代应用开发而言，Serverless 无疑是一个极具吸引力的选择。拥抱 Serverless，意味着更高的开发效能和更强大的业务敏捷性。\n","categories":["开发工具","云服务"],"tags":["2024","AWS","Serverless","云服务"]},{"title":"Vite配置详解：从入门到精通","url":"/2024/2024-01-26_Vite%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A/","content":"\nVite 是一款由 Vue.js 创始人尤雨溪开发的现代前端构建工具。它旨在通过原生 ES 模块提供极速的开发体验，并在生产环境中利用 Rollup 进行高效打包。Vite 的配置非常灵活，可以通过 vite.config.js（或 .ts）文件进行全面定制。\n\n“Vite 的配置就是对 vite.config.js 文件中导出的配置对象进行操作。这个配置文件提供了对开发服务器、构建过程、插件、别名等一切的控制。”\n\n\n一、Vite 配置文件的位置与类型\n文件名：通常是 vite.config.js、vite.config.ts、vite.config.mjs 或 vite.config.cjs。建议使用 .ts 文件以获得更好的类型提示。\n位置：通常位于项目根目录。\n\n1.1 基本结构Vite 配置文件默认导出一个配置对象。这个对象可以使用 defineConfig 辅助函数包裹，以获得更好的类型提示。\n// vite.config.tsimport &#123; defineConfig &#125; from &#x27;vite&#x27;;import vue from &#x27;@vitejs/plugin-vue&#x27;;// https://vitejs.dev/config/export default defineConfig(&#123;  // 这里是你的 Vite 配置选项  plugins: [vue()],  server: &#123;    port: 3000,  &#125;,  build: &#123;    outDir: &#x27;dist&#x27;,  &#125;&#125;);\n\n二、核心配置选项详解Vite 的配置对象包含多个顶级字段，每个字段控制着不同的方面。\n2.1 root\n类型：string\n默认值：process.cwd() (项目根目录)\n描述：项目根目录（index.html 所在的目录）。也可以通过命令行 vite --root ./some-dir 指定。\n\n2.2 base\n类型：string\n默认值：/\n描述：公共基础路径。\n开发环境：在开发服务器上，资源会从 http://localhost:port/ 加载。如果你需要部署在一个子路径下，如 example.com/my-app/，则 base 应该设置为 /my-app/。\n生产环境：用于打包后的资源路径。\n\n\n\n2.3 mode\n类型：string\n默认值：\n开发环境：development\n生产环境：production\n\n\n描述：指定项目运行的模式。这会影响 import.meta.env.MODE 的值。\n\n2.4 plugins\n类型：Array&lt;PluginOption | PluginOption[]&gt;\n描述：要使用的 Vite 插件数组。插件是扩展 Vite 功能的主要方式。\n示例：import &#123; defineConfig &#125; from &#x27;vite&#x27;;import vue from &#x27;@vitejs/plugin-vue&#x27;; // 官方 Vue 插件import eslintPlugin from &#x27;vite-plugin-eslint&#x27;; // 第三方 ESlint 插件export default defineConfig(&#123;  plugins: [    vue(),    eslintPlugin(&#123;      include: [&#x27;src/**/*.js&#x27;, &#x27;src/**/*.vue&#x27;, &#x27;src/**/*.ts&#x27;],      exclude: [&#x27;./node_modules/**&#x27;],    &#125;),  ],&#125;);\n注意：插件的顺序很重要，通常官方推荐的顺序已经是最优的。\n\n2.5 publicDir\n类型：string | false\n默认值：&#39;public&#39;\n描述：用于存放不需要构建处理的静态资源目录。这个目录下的文件会被直接复制到构建输出目录的根目录。\n在代码中可以通过 /文件名.扩展名 来访问这些文件。\n例如，public/favicon.ico 在 HTML 中就是 &lt;link rel=&quot;icon&quot; href=&quot;/favicon.ico&quot;&gt;。\n设置为 false 可以禁用此功能。\n\n\n\n2.6 resolve\n类型：object\n描述：配置模块解析规则。\n\n2.6.1 alias\n类型：Array&lt;&#123; find: string | RegExp, replacement: string &#125;&gt;\n描述：配置路径别名。这在处理深层嵌套的导入路径时非常有用。\n示例：import &#123; defineConfig &#125; from &#x27;vite&#x27;;import &#123; resolve &#125; from &#x27;path&#x27;; // NodeJS 路径模块export default defineConfig(&#123;  resolve: &#123;    alias: [      &#123; find: &#x27;@&#x27;, replacement: resolve(__dirname, &#x27;src&#x27;) &#125;,      &#123; find: &#x27;components&#x27;, replacement: resolve(__dirname, &#x27;src/components&#x27;) &#125;,      // 确保在 TypeScript 中也配置路径别名，在 tsconfig.json 中      // &quot;paths&quot;: &#123; &quot;@/*&quot;: [&quot;src/*&quot;], &quot;components/*&quot;: [&quot;src/components/*&quot;] &#125;    ],  &#125;,&#125;);\n\n2.6.2 dedupe\n类型：string[]\n描述：强制预绑定（预优化）的依赖。可以解决某些库在同一项目中出现多个实例的问题。\n\n2.6.3 extensions\n类型：string[]\n默认值：[&#39;.mjs&#39;, &#39;.js&#39;, &#39;.ts&#39;, &#39;.jsx&#39;, &#39;.tsx&#39;, &#39;.json&#39;, &#39;.vue&#39;]\n描述：导入时会尝试的扩展名列表。\n\n2.7 css\n类型：object\n描述：配置 CSS 相关的选项。\n\n2.7.1 preprocessorOptions\n类型：Record&lt;string, object&gt;\n描述：指定 CSS 预处理器的选项。\n示例：export default defineConfig(&#123;  css: &#123;    preprocessorOptions: &#123;      scss: &#123;        additionalData: `@import &quot;@/styles/variables.scss&quot;;`, // 全局引入 SCSS 变量      &#125;,      less: &#123;        javascriptEnabled: true, // 允许 Less 中使用 JavaScript      &#125;,    &#125;,  &#125;,&#125;);\n\n2.7.2 modules\n类型：CSSModulesOptions\n描述：配置 CSS Modules 的行为。\n\n2.7.3 postcss\n类型：string | (postcss.ProcessOptions &amp; &#123; plugins?: (postcss.Plugin | string)[] &#125;)\n描述：自定义 PostCSS 配置。\n\n2.8 json\n类型：object\n描述：配置 JSON 导入的行为。\n\n2.8.1 stringify\n类型：boolean\n默认值：false\n描述：导入的 JSON 会被字符串化为 export default JSON.parse(&quot;...&quot;)。这会禁用命名导入，但可以提供更好的性能。\n\n2.9 esbuild\n类型：ESBuildOptions | false\n描述：配置 esbuild 转换选项。Vite 使用 esbuild 进行 JavaScript&#x2F;TypeScript 的语法转换和压缩。\n示例：export default defineConfig(&#123;  esbuild: &#123;    jsxFactory: &#x27;h&#x27;,      // JSX 的工厂函数    jsxFragment: &#x27;Fragment&#x27;, // JSX 的片段    // 更多 esbuild 选项，参考其文档  &#125;,&#125;);\n设置为 false 可以禁用 esbuild（不推荐）。\n\n2.10 server\n类型：object\n描述：配置开发服务器选项。\n\n2.10.1 host\n类型：string | boolean\n默认值：&#39;localhost&#39;\n描述：指定服务器监听的 IP 地址。\ntrue：监听所有地址，包括局域网和公共地址（例如 0.0.0.0）。\nfalse：使用 localhost。\n&#39;0.0.0.0&#39; 或 true 允许通过局域网 IP 访问。\n\n\n\n2.10.2 port\n类型：number\n默认值：5173 (Vite 3+), 3000 (Vite 2)\n描述：指定开发服务器的端口。\n\n2.10.3 strictPort\n类型：boolean\n默认值：false\n描述：如果端口已被占用，是否严格退出。false 会自动尝试下一个可用端口。\n\n2.10.4 https\n类型：boolean | https.ServerOptions\n默认值：false\n描述：启用 TLS + HTTP&#x2F;2。可以传入 HTTPS 证书选项。\n\n2.10.5 open\n类型：string | boolean\n默认值：false\n描述：服务器启动时自动在浏览器中打开。\ntrue：打开项目根目录。\nstring：指定要打开的 URL 路径 (例如 /docs/index.html)。\n\n\n\n2.10.6 proxy\n类型：Record&lt;string, string | ProxyOptions&gt;\n描述：配置自定义代理规则。这对于跨域请求非常有用。\n示例：export default defineConfig(&#123;  server: &#123;    proxy: &#123;      &#x27;/api&#x27;: &#123;        target: &#x27;http://localhost:8000&#x27;, // 后端 API 地址        changeOrigin: true,            // 改变源（重要）        rewrite: (path) =&gt; path.replace(/^\\/api/, &#x27;&#x27;), // 重写路径, 去掉 &#x27;/api&#x27;      &#125;,      // 多个代理规则      &#x27;/another-api&#x27;: &#123;        target: &#x27;http://another-backend.com&#x27;,        changeOrigin: true,        // ...      &#125;,    &#125;,  &#125;,&#125;);\n\n2.10.7 cors\n类型：boolean | CorsOptions\n默认值：false\n描述：配置 CORS。\n\n2.11 build\n类型：object\n描述：配置生产环境构建选项。build 下的选项都会直接传递给 Rollup。\n\n2.11.1 target\n类型：string | string[]\n默认值：&#39;modules&#39;\n描述：esbuild 转换的最低目标浏览器版本（例如 &#39;es2015&#39;、[&#39;chrome58&#39;, &#39;firefox57&#39;]）。\n\n2.11.2 outDir\n类型：string\n默认值：&#39;dist&#39;\n描述：指定打包输出目录。\n\n2.11.3 assetsDir\n类型：string\n默认值：&#39;assets&#39;\n描述：指定静态资源（图片、字体等）的输出目录，相对于 outDir。\n\n2.11.4 assetsInlineLimit\n类型：number\n默认值：4096 (4KB)\n描述：小于此阈值的导入资源将内联为 base64 URLs。\n\n2.11.5 cssCodeSplit\n类型：boolean\n默认值：true\n描述：如果为 true，CSS 将会按异步模块的依赖在它们对应的块中进行代码分割。\n\n2.11.6 sourcemap\n类型：boolean | &#39;inline&#39; | &#39;hidden&#39;\n默认值：false\n描述：是否生成 sourcemap。\n\n2.11.7 minify\n类型：boolean | &#39;terser&#39; | &#39;esbuild&#39;\n默认值：&#39;esbuild&#39;\n描述：指定是否压缩代码。\n&#39;terser&#39;：使用 terser 进行压缩，功能更强大，但速度稍慢。\n&#39;esbuild&#39;：使用 esbuild 进行压缩，速度较快，但压缩率可能略低一点。\nfalse：不压缩。\n\n\n\n2.11.8 rollupOptions\n类型：RollupOptions (来自 Rollup 库的类型)\n描述：直接传递给 Rollup 的额外选项。用于更高级的打包定制。\n示例：export default defineConfig(&#123;  build: &#123;    rollupOptions: &#123;      output: &#123;        manualChunks(id) &#123;          if (id.includes(&#x27;node_modules&#x27;)) &#123;            // 将所有 node_modules 依赖打包到 vender.js            return &#x27;vendor&#x27;;          &#125;        &#125;,        // 控制 js 和 css 文件的命名        entryFileNames: &#x27;assets/[name]-[hash].js&#x27;,        chunkFileNames: &#x27;assets/[name]-[hash].js&#x27;,        assetFileNames: &#x27;assets/[name]-[hash].[ext]&#x27;,      &#125;,    &#125;,  &#125;,&#125;);\n\n2.12 define\n类型：Record&lt;string, string&gt;\n描述：定义全局常量替换。键会被自动字符串化。\n示例：export default defineConfig(&#123;  define: &#123;    __APP_VERSION__: JSON.stringify(&#x27;1.0.0&#x27;), // 注入应用版本号    &#x27;process.env.NODE_ENV&#x27;: JSON.stringify(&#x27;production&#x27;), // 兼容旧代码  &#125;,&#125;);\n\n2.13 envDir\n类型：string\n默认值：root\n描述：加载 .env 文件的目录。\n\n2.14 optimizeDeps\n类型：DepOptimizationOptions\n描述：配置依赖预构建（Dependency Pre-Bundling）选项。Vite 在开发服务器启动时会预构建 node_modules 中的 CommonJS&#x2F;UMD 模块为 ES 模块，以加速页面加载。\n\n2.14.1 include\n类型：string[]\n描述：强制预构建的依赖包。\n某些 ESM 兼容性较差的库可能需要手动添加到这里。\n例如：[&#39;lodash&#39;]\n\n\n\n2.14.2 exclude\n类型：string[]\n描述：从预构建中排除的依赖包。\n如果你确定某个库已经是纯 ESM 并且不需要预构建，可以将其排除。\n\n\n\n2.14.3 entries\n类型：string | string[]\n描述：指定分析依赖的入口文件。默认会分析 index.html。\n\n三、环境变量Vite 通过 import.meta.env 对象暴露环境变量。\n\nimport.meta.env.MODE：当前模式 (development 或 production)。\nimport.meta.env.BASE_URL：配置的 base 公共基础路径。\nimport.meta.env.PROD：是否在生产环境。\nimport.meta.env.DEV：是否在开发环境。\n以 VITE_ 开头的自定义环境变量：例如在 .env 文件中定义 VITE_API_URL=http://api.example.com，则可以通过 import.meta.env.VITE_API_URL 访问。\n\n// vite.config.ts 可以根据模式加载不同的配置import &#123; defineConfig, loadEnv &#125; from &#x27;vite&#x27;;export default defineConfig((&#123; command, mode &#125;) =&gt; &#123;  // 根据当前工作目录中的 `mode` 加载 .env 文件  // 设置 `process.env` 的键值对  const env = loadEnv(mode, process.cwd(), &#x27;&#x27;); // 第三个参数是前缀，&#x27;&#x27; 表示加载所有  return &#123;    // vite 配置    define: &#123;      __APP_ENV__: JSON.stringify(env.APP_ENV), // 可以将环境变量注入到代码中    &#125;,    // 根据 mode 条件性配置    server: &#123;      port: (mode === &#x27;development&#x27;) ? 3000 : undefined,    &#125;  &#125;;&#125;);\n\n四、条件式配置Vite 配置文件可以导出一个函数，该函数接收 command 和 mode 参数，允许你根据不同的环境或命令返回不同的配置对象。\n\ncommand：&#39;serve&#39; (开发环境) 或 &#39;build&#39; (生产环境)。\nmode：development 或 production，或通过 --mode 参数指定。\n\n// vite.config.tsimport &#123; defineConfig &#125; from &#x27;vite&#x27;;export default defineConfig((&#123; command, mode &#125;) =&gt; &#123;  if (command === &#x27;serve&#x27;) &#123;    // 开发环境专用配置    return &#123;      server: &#123;        open: true,        proxy: &#123;          &#x27;/api&#x27;: &#x27;http://localhost:8000&#x27;,        &#125;,      &#125;,      plugins: [/* dev plugins */],    &#125;;  &#125; else &#123;    // `command === &#x27;build&#x27;`    // 生产环境专用配置    return &#123;      build: &#123;        sourcemap: false,        minify: &#x27;terser&#x27;,        // ...      &#125;,      plugins: [/* prod plugins */],    &#125;;  &#125;&#125;);\n\n五、总结Vite 的配置文件 vite.config.js (.ts等) 是定制整个构建和开发流程的控制中心。通过对 plugins、server、build、resolve 等核心选项的灵活配置，可以满足绝大多数前端项目的需求。\n\n原生 ES 模块：理解 Vite 在开发模式下直接利用浏览器原生 ES 模块的特性，有助于理解其极速启动的原因。\nRollup 生产打包：生产构建时，Vite 内部使用 Rollup，因此其许多 build 选项都直接映射到 Rollup 的配置。\n插件生态：Vite 的强大功能很大程度上依赖于其丰富的插件生态。\n环境变量：善用 import.meta.env 和 .env 文件来管理不同环境下的配置。\n\n熟练掌握 Vite 配置，将大大提升你的开发效率和项目构建质量。\n","categories":["前端技术","项目构建"],"tags":["TypeScript","2024","JavaScript","Vite","项目构建"]},{"title":"开源协议详解：理解与选择的艺术","url":"/2024/2024-02-22_%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE%E8%AF%A6%E8%A7%A3%EF%BC%9A%E7%90%86%E8%A7%A3%E4%B8%8E%E9%80%89%E6%8B%A9%E7%9A%84%E8%89%BA%E6%9C%AF/","content":"\n在开源软件的世界里，开源协议 (Open Source License) 扮演着至关重要的角色。它定义了你对开源代码的权利和义务：你可以做什么，不能做什么，以及当你修改或分发代码时需要遵守哪些规则。理解这些协议对于开发者、公司和代码使用者来说都至关重要，它不仅关乎合法合规，更影响着项目的成长、社区的形成以及商业模式的选择。\n\n“开源协议是开源世界的宪法，明确了游戏规则，确保了开放与合作的平衡。”\n\n\n一、什么是开源协议？为什么需要它？开源协议是一份法律文件，它授予用户使用、修改和分发开源软件的权利，但同时也会施加一定的条件和限制。\n为什么需要开源协议？\n\n界定权利与义务：明确使用者可以对代码做什么（使用、修改、分发），以及必须做什么（保留版权信息、公开源码等）。\n保护贡献者：允许贡献者保留版权，同时授权他人使用，确保其辛勤工作不会被恶意独占。\n促进创新：降低了他人基于现有代码进行二次开发和创新的门槛。\n建立信任：协议的公开透明有助于社区形成共识，促进协作。\n避免法律纠纷：明确的协议条款可以减少因代码使用引起的所有权、责任和版权争议。\n\n核心问题：任何没有明确开源协议的代码，默认情况下都受版权法保护，意味着未经授权，任何人无权使用、修改和分发。因此，开源项目必须选择并声明一个开源协议。\n二、开源协议的分类：宽松与Copyleft开源协议通常分为两大类：\n2.1 宽松Lassive&#x2F;Permissive License (BSD、MIT、Apache等)\n特点：对使用者施加的限制最少，允许高度自由地使用、修改和分发代码，通常包括闭源和商业化。它们被称为“放宽型”或“自由型”协议。\n主要限制：\n保留版权和许可声明：你必须在分发代码时包含原始的版权和许可声明。\n免责声明：不提供任何担保，软件按“原样”提供。\n\n\n适用场景：\n希望自己的代码能够被广泛采用，包括商业产品。\n不强制下游项目也必须开源。\n适合作为库、框架或组件。\n\n\n\n2.2 强Copyleft &#x2F; Weak Copyleft (GPL、LGPL、AGPL等)\n特点：强制性的开源协议，旨在维护软件的“自由”属性。其核心原则是：如果你修改并分发了基于此协议的代码，那么你的修改部分也必须以相同的协议开源。这种“传染性”是其主要特征。\nCopyleft 的含义：”Copying permitted, but changes must be shared.” (允许复制，但修改必须共享。) 这是对传统版权 (Copyright) 概念的文字游戏。\n主要限制：\n公开源码：分发（或特定条件下使用）基于 Copyleft 协议的代码时，必须提供对应的源代码。\n协议保持一致：你的派生作品必须沿用相同的或兼容的 Copyleft 协议。\n免责声明和保留版权等与宽松协议类似。\n\n\n适用场景：\n希望其软件能够永远保持开源状态，防止他人修改后闭源独占。\n对于注重软件自由和社区共享的项目。\n\n\n\n三、主流开源协议详解3.1 宽松型协议 (Permissive Licenses)3.1.1 MIT License (麻省理工学院许可证)\n特点：最简洁、最宽松的协议之一。\n主要限制：\n必须在所有副本或重要部分中包含版权和许可声明。\n\n\n优点：几乎没有任何限制，开发者和公司可以自由使用、修改、合并、发布、分发、再许可（sub-license）、销售软件，甚至将其用于闭源和商业项目。\n缺点：不保证派生作品的持续开源。\n适用场景：个人项目、库、前端框架、移动应用组件，或任何希望最大化代码采用率的项目。\n知名项目：jQuery, Ruby on Rails, React\n\n3.1.2 BSD License (伯克利软件发行版许可证)\n特点：与 MIT 类似，非常宽松。有 2-clause, 3-clause 和 4-clause 版本。\n2-clause (FreeBSD License)：与 MIT 几乎等价，只要求保留版权信息和免责声明。\n3-clause (New &#x2F; Revised BSD License)：在 2-clause 基础上增加了一个不能用项目或贡献者名称为产品背书的条款。\n4-clause (Original BSD License)：额外的广告条款，要求在所有广告材料中提及原作者，现已不推荐使用。\n\n\n主要限制（以 3-clause 为例）：\n重新分发时必须保留版权声明、许可证列表和免责声明。\n未经事先书面许可，不得使用贡献者姓名或项目名称为您的产品背书或推广。\n\n\n优点：与 MIT 类似，高度自由，兼容性极好。\n缺点：不保证派生作品的持续开源。\n适用场景：科学计算库、操作系统组件、网络工具。\n知名项目：Netflix 的很多开源项目，Golang (部分)，Redis (部分)。\n\n3.1.3 Apache License 2.0 (Apache 许可证 2.0)\n特点：相对 MIT 和 BSD 来说，提供了一定的专利保护，是商业友好的宽松协议。\n主要限制：\n保留版权和许可声明：必须包含原始的版权和许可声明。\n修改声明：如果你修改了代码，必须声明这些修改。\n专利授权：如果贡献者贡献的代码中包含专利，则会自动授权给用户使用（非常重要，避免专利流氓）。\n禁止使用商标：不能使用 Apache 社区的商标为你的产品做宣传。\n\n\n优点：宽松自由，但对个人或公司使用开源代码时可能涉及的专利问题提供了较好的解决方案。\n缺点：不保证派生作品的持续开源。\n适用场景：大型企业级项目、Web 服务器、大数据组件、云原生项目。\n知名项目：Android, Apache Hadoop, Apache Kafka, Kubernetes (遵循 Apache 协议)。\n\n3.2 强Copyleft 协议 (Strong Copyleft Licenses)3.2.1 GNU General Public License (GPL)\n特点：最著名的强 Copyleft 协议，分为 GPLv2 和 GPLv3。\n主要限制 (核心)：\n分发源代码：如果你分发（包括以商业形式销售）使用了 GPL 代码的软件，无论是否修改，你的整个软件（包括你自己的私有代码）都必须以 GPL 协议开源其源代码。\n协议保持一致：派生作品必须采用相同的 GPL 协议。\n需附带版权信息、许可证文本和免责声明。\n\n\n优点：最大程度地保障软件的自由，鼓励所有开发者共享改进。\n缺点：“传染性” 极强。如果你将 GPL 代码集成到你的专有（闭源）产品中，你将被迫开源你的整个产品，这通常是公司不愿意看到的。\n适用场景：操作系统内核、核心工具、GNU 项目。\n知名项目：Linux Kernel (GPLv2), GNU Compiler Collection (GCC)。\n\n3.3 弱Copyleft 协议 (Weak Copyleft Licenses)3.3.1 GNU Lesser General Public License (LGPL)\n特点：GPL 的一个“弱化”版本，Copyleft 效果较弱，主要用于库。\n主要限制：\n动态链接：如果你将 LGPL 库动态链接到你的专有（闭源）代码中，你的专有代码无需开源。\n静态链接&#x2F;直接修改：如果你直接修改了 LGPL 库的源代码，或者将其静态链接到你的项目中，那么你修改的部分或你集成的整个库的重新分发版本仍然必须以 LGPL 协议开源。\n用户必须能够替换掉 LGPL 部分。\n需附带版权信息、许可证文本和免责声明。\n\n\n优点：允许闭源软件使用 LGPL 库，使其成为一种更适合作为通用库的 Copyleft 协议。平衡了代码自由和商业使用的需求。\n缺点：在使用时仍需注意其限制，尤其是对库的直接修改或静态链接。\n适用场景：代码库、框架，希望被广泛使用但又不想完全放弃 Copyleft 精神的项目。\n知名项目：GNU C Library (glibc)，FFmpeg (部分)。\n\n3.3.2 Mozilla Public License 2.0 (MPLv2)\n特点：文件级 Copyleft。比 LGPL 更强，但比 GPL 弱。\n主要限制：\n文件级开源：如果你修改了 MPL 许可的代码文件，那么你修改后的文件也必须以 MPL 协议开源。\n兼容闭源：你可以将 MPL 许可的文件与你的闭源文件组合在一起，而无需开源你的闭源部分。\n需附带版权信息、许可证文本和免责声明。\n\n\n优点：对于只修改了部分源码文件的贡献者，强制他们将修改部分回馈社区，但又不对整个应用程序做强制开源要求。\n缺点：比宽松协议更复杂，在整合时需要注意文件粒度。\n适用场景：Web 浏览器、特定模块或插件。\n知名项目：Firefox。\n\n3.4 较新的强Copyleft 协议3.4.1 GNU Affero General Public License (AGPL)\n特点：GPL 的一个扩展版本，旨在解决“网络服务空白”问题。\n主要限制（在 GPL 基础上增加）：\n网络交互：如果用户通过网络与 AGPL 软件进行交互（例如，通过 SaaS 服务），即使没有“分发”软件本身，也必须提供相应的源代码。这解决了 GPL 软件作为网络服务时，使用者无法获得源代码的问题。\n其他与 GPL 类似。\n\n\n优点：确保即使是提供网络服务的软件，其用户也能获得并修改源代码，最大化软件自由。\n缺点：“传染性”最强。对于提供 SaaS 服务的公司来说，使用 AGPL 代码意味着其整个服务代码都可能需要开源。\n适用场景：对软件自由度有极端高要求的网络服务、数据库。\n知名项目：MongoDB (早期版本，后切换到 SSPL), Nextcloud。\n\n四、如何选择开源协议？选择一个合适的开源协议取决于你的项目目标和期望：\n\n如果目标是最大化代码的采用率和兼容性，不介意他人闭源使用你的代码：\n\nMIT (最宽松，最简单)\nBSD 3-Clause (与 MIT 类似，多一条背书限制)\nApache 2.0 (宽松且提供专利保护，适合企业使用)\n\n\n如果目标是确保你的代码及其派生作品始终保持开源，避免被他人闭源独占：\n\nGPLv3 (最强 Copyleft，应用于整个项目)\nAGPLv3 (比 GPLv3 更强，覆盖网络服务使用场景)\n\n\n如果你想让你的库被闭源软件使用，但又希望对库本身的修改能回馈社区：\n\nLGPLv3 (弱 Copyleft，适合库，区分动态链接和静态链接&#x2F;修改)\nMPLv2 (文件级 Copyleft，更关注单个文件的修改)\n\n\n如果你是用户，需要评估集成开源代码的风险：\n\n宽松协议：风险最低，可以自由集成和闭源。\nLGPL&#x2F;MPL：需要仔细阅读协议条款，了解链接类型和修改责任。\nGPL&#x2F;AGPL：风险最高，如果集成到闭源产品中，可能需要开源你的整个产品或服务。对于商业公司，通常应避免在闭源产品中直接依赖强 Copyleft 代码。\n\n\n\n签署与声明一旦选择了协议，你需要在项目的根目录下添加一个名为 LICENSE 或 LICENSE.txt 的文件，并在其中包含完整的协议文本。同时，在项目的 README 文件中明确声明你选择的协议。\n五、协议兼容性 (License Compatibility)将不同协议的软件组合在一起时，协议兼容性是关键。一个常见的规则是“向下兼容”：\n\n宽松协议通常可以与几乎所有协议兼容，因为它们施加的限制最少。\n强 Copyleft 协议（如 GPLv2）通常只兼容相同或更强的 Copyleft 协议。例如，GPLv2 代码不能与 GPLv3 代码合并，因为 GPLv3 增加了额外的限制。而 GPLv3 更灵活，通常可以包含 GPLv2 代码。AGPL 兼容 GPL。\n组合不同协议的代码时，最终的组合作品必须遵守所有涉及协议的最严格的限制。\n\n在不确定时，请咨询专业的法律意见。\n六、总结开源协议是开源生态系统健康运行的基石。它们平衡了代码共享的自由和权利保护的必要性。无论是作为开源项目的贡献者还是使用者，理解不同协议的特点、约束和兼容性都至关重要。正确选择和使用开源协议，不仅能确保你的项目合法合规，更能促进开放创新，推动软件技术持续发展。\n","categories":["开发工具","开源协议"],"tags":["2024","开源协议"]},{"title":"Vercel介绍","url":"/2024/2024-03-03_Vercel%E4%BB%8B%E7%BB%8D/","content":"\nVercel 是一个为前端开发和部署量身定制的云平台，由 Next.js 框架的创建者开发并维护。它致力于提供极致的开发者体验，通过集成的 CI&#x2F;CD、自动扩展的 Serverless 功能和全球 CDN，使开发者能够快速部署现代化网站和 Web 服务。Vercel 的核心理念是让部署变得简单、快速且高效。\n\n“Vercel is the platform for frontend developers, providing the speed and reliability innovators need to create at the moment of inspiration.” —— Vercel Official\n\n\n一、Vercel 核心概念与愿景Vercel 将自身定位为 “Frontend Cloud”，旨在解决现代前端应用面临的挑战，尤其是与后端、API、数据源的集成以及复杂的部署流程。其核心愿景是让开发者能够专注于代码，而将基础设施的复杂性完全交给 Vercel 处理。\n1. 技术栈偏好Vercel 对基于 React, Vue, Svelte 等构建的现代前端框架，尤其是它自身创造的 Next.js，提供了无与伦比的优化和支持。它不仅仅是一个静态站点托管服务，更是一个能够处理动态内容和 Serverless 后端逻辑的平台。\n2. 核心特征\n一体化的部署平台: 从 Git 仓库连接，到自动构建、部署、CDN 分发，再到 Serverless 函数的执行，Vercel 提供了一站式服务。\n零配置部署: 大多数现代前端项目（如 Next.js, Create React App, Vue CLI 等）可以直接从 Git 仓库导入，Vercel 会自动检测框架并进行相应配置。\n开发者体验 (DX) 优先: 专注于简化工作流程，提供友好直观的 UI，丰富的命令行工具 (Vercel CLI) 和实时的构建日志。\n高性能: 利用全球 CDN (Content Delivery Network)、Serverless Functions、边缘部署等技术，确保应用快速响应和高可用性。\n\n二、Vercel 的主要功能和优势1. 自动 CI&#x2F;CD (Continuous Integration &#x2F; Continuous Deployment)\nGit 集成: 与 GitHub, GitLab, Bitbucket 深度集成。\n预览部署 (Preview Deployments): 每次向 main 分支之外的分支提交代码或创建 Pull Request (PR) 时，Vercel 都会自动创建一个独立的、可分享的预览 URL。这极大地简化了团队协作、UI&#x2F;UX 审查和 Bug 测试过程。\n生产部署 (Production Deployments): 当代码合并到主分支时，Vercel 会自动将其部署到生产环境，并且支持原子部署（即新版本完全部署成功后才切换流量，保证零停机时间）。\n即时回滚: 如果生产部署出现问题，可以即时回滚到任何之前的部署版本。\n\n2. Serverless Functions (无服务器函数)\n集成: Vercel 内置了对其平台上的 Serverless Functions (在 AWS Lambda 上运行) 的支持。开发者可以在前端项目中直接编写 API 路由，Vercel 会自动将其部署为 Serverless Functions。\n支持语言: Node.js, Go, Python, Ruby (支持自定义运行时)。\n优势: 自动扩展、按需付费、无需管理服务器。无需单独部署和管理后端服务，极大地简化了全栈应用的开发。\n边缘函数 (Edge Functions): Vercel 的最新技术，允许开发者在 CDN 边缘节点执行 JavaScript 函数，离用户更近，响应更快（基于 V8 引擎，比传统 Serverless 更轻量、更快）。\n主要用于身份验证、A&#x2F;B 测试、重定向、SSR 数据的预热等。\n\n\n\n3. 全球 CDN 与边缘部署\n高性能内容分发: Vercel 利用其全球网络边缘节点缓存静态资产，确保用户从离他们最近的服务器获取内容，从而加快加载速度。\n边缘网络: Serverless Functions 可以在靠近最终用户的地方运行，减少延迟。\n\n4. 数据集成 (Data Cache &#x2F; Data Storage)Vercel 提供了多种与数据源交互的方式，包括：\n\n缓存: 内置了强大的缓存机制，用于提升静态生成（SSG）和服务器端渲染（SSR）的性能。\nVercel KV: 基于 Redis 的键值存储服务，专为 Serverless 和 Edge 函数优化。\nVercel Postgres: 托管的 PostgreSQL 数据库服务。\nVercel Blob: 托管的文件存储服务，专为边缘和 Serverless 环境优化。\n集成第三方数据源: 通过 Serverless Functions 可以轻松连接到外部数据库（如 MongoDB, PlanetScale, Supabase 等）或 API。\n\n5. 开发者工具与生态\nVercel CLI: 强大的命令行界面工具，用于本地开发、部署预览和管理项目。\nDev Server (Next.js): 与 Next.js 等框架的开发服务器无缝集成，提供热模块替换 (HMR) 等功能。\n监控与日志: 提供实时的部署日志、函数执行日志和性能监控。\nMarketplace: 提供了大量第三方集成，如分析工具、CMS、数据库等。\n\n三、Next.js 与 Vercel 的关系Vercel 是 Next.js 的创造者和主要维护者。这种深度集成是 Vercel 平台的最大优势之一：\n\n极致优化: Vercel 的基础设施是为 Next.js 量身打造的，能够充分利用 Next.js 的各种特性，例如：\n静态站点生成 (SSG): 将页面的 HTML、CSS、JS 文件在构建时生成，并通过 CDN 分发，实现极快的加载速度。\n服务器端渲染 (SSR): 按需在服务器上渲染页面，处理动态内容，并将其发送给客户端。\n增量静态再生 (ISR): 重新生成旧的静态内容，而不是每次部署都重建整个站点。\nAPI 路由 (API Routes): Next.js 内置的 Serverless Functions，Vercel 可以直接部署。\n边缘运行时 (Edge Runtime): Next.js 12+ 引入的特性，可以在 Vercel 的 Edge Functions 上运行。\n\n\n无缝衔接: Next.js 项目可以直接导入 Vercel，无需额外配置，开箱即用。\n\n两者共同构建了现代全栈 Web 应用开发和部署的理想生态。\n四、如何使用 Vercel\n连接 Git 仓库: 登录 Vercel 账户，选择导入 Git 仓库（GitHub, GitLab, Bitbucket）。\n选择项目: 选择要部署的仓库和一个分支。\n自动配置: Vercel 会自动检测项目类型（例如 Next.js, React, Vue 等），并推荐构建设置。如有需要可手动修改。\n部署: 点击 “Deploy” 按钮。\n预览 URL: Vercel 会生成一个唯一的预览 URL。\n生产部署: 合并到主分支后，会自动触发生产部署，并更新项目的自定义域名（如果已配置）。\n\n五、Vercel 的计费模式Vercel 提供免费 tier (Hobby 计划) 和付费计划 (Pro, Enterprise)。\n\nHobby 计划: 适用于个人项目、开源项目。提供慷慨的构建时间、带宽、Serverless 函数执行时间等免费额度。\nPro &#x2F; Enterprise 计划: 提供更高的额度，更多的并发部署、团队功能、高级支持、更长的函数执行时间等。\n\n其按使用量付费的模式意味着您只为您实际使用的资源付费，这对于许多项目来说是非常经济高效的。\n六、总结与展望Vercel 通过将复杂的基础设施抽象化，提供了一个高度集成、自动化和优化的平台，显著降低了现代前端应用的部署门槛，并提升了开发者体验和应用性能。\n对于使用 Next.js 等前端框架构建的网站、PWA、API 后端等应用，Vercel 无疑是部署的首选平台之一。它不仅简化了部署流程，更通过全球 CDN、Serverless Functions 和 Edge Functions 等技术，为您的应用提供了极致的速度、可用性和扩展性。随着前端技术栈的不断演进，以及对更快、更分布式应用的需求增长，Vercel 在“Frontend Cloud”领域的地位将愈发巩固。\n","categories":["开发工具","云服务"],"tags":["2024","Serverless","云服务","Vercel","CI/CD"]},{"title":"Vercel.json详解","url":"/2024/2024-03-11_Vercel.json%E8%AF%A6%E8%A7%A3/","content":"\nvercel.json 是 Vercel 平台用于配置项目部署行为的核心文件。它允许开发者精细地控制构建过程、路由规则、Serverless Functions、环境变量、域名设置等。理解和熟练使用 vercel.json 对于优化 Vercel 上的应用性能、实现复杂的路由逻辑和管理部署具有至关重要的作用。\n\n“The vercel.json file is a powerful tool for configuring your Vercel Project. It allows you to customize various aspects of your deployments, from build settings to routing rules and Serverless Functions.” —— Vercel Documentation\n\n\n一、vercel.json 的作用与重要性vercel.json 是一个位于项目根目录的 JSON 配置文件。当您将代码部署到 Vercel 时，Vercel 会读取此文件来获取构建、部署和运行时行为的指示。\n主要作用包括：\n\n自定义构建过程: 指定构建命令、输出目录等。\n路由重写与重定向: 实现友好的 URL、A&#x2F;B 测试、多语言站点的路由等。\nServerless Functions 配置: 控制函数的运行时、内存、超时、环境变量等。\n环境变量管理: 区分不同环境（开发、预览、生产）的变量。\nHTTP 响应头配置: 增加安全头、缓存控制头等。\n域名与别名管理: 配置自定义域名和别名。\n项目类型检测覆盖: 强制指定项目框架。\n\n二、vercel.json 结构概览一个典型的 vercel.json 文件可能包含以下顶级字段：\n&#123;  &quot;version&quot;: 2,  &quot;name&quot;: &quot;my-vercel-project&quot;,  &quot;builds&quot;: [    // ... 构建配置  ],  &quot;routes&quot;: [    // ... 路由规则  ],  &quot;env&quot;: &#123;    // ... 环境变量  &#125;,  &quot;regions&quot;: [&quot;sfo1&quot;], // Serverless Functions 部署区域  &quot;functions&quot;: &#123;    // ... Serverless Functions 特定配置  &#125;,  &quot;headers&quot;: [    // ... 全局HTTP响应头  ],  &quot;cleanUrls&quot;: true, // 移除 .html 扩展名  &quot;rewrites&quot;: [    // ... 重写规则 (routes 数组的简化语法)  ],  &quot;redirects&quot;: [    // ... 重定向规则 (routes 数组的简化语法)  ],  &quot;trailingSlash&quot;: true // URL 末尾是否添加斜杠&#125;\n\n接下来，我们详细讲解一些常用和重要的字段。\n三、常用配置字段详解1. version\n类型: number\n默认值: 2\n说明: 指定 vercel.json 配置文件的版本。目前推荐使用 2。\n\n2. name\n类型: string\n说明: 可选字段，用于在 Vercel UI 中显示的项目名称。\n\n3. builds (构建配置)\n类型: Array&lt;Object&gt;\n\n说明: 定义构建步骤和构建器的配置。每个对象代表一个构建器。\n\nsrc: 要处理的源文件或 glob 模式 (例如 *.js 或 api/*.py)。\nuse: 使用的构建器名称 (例如 @vercel/static-build, @vercel/node, @vercel/python)。\nconfig: 传递给构建器的特定配置。\n\n示例:\n&#123;  &quot;builds&quot;: [    &#123;      &quot;src&quot;: &quot;package.json&quot;,      &quot;use&quot;: &quot;@vercel/static-build&quot;,      &quot;config&quot;: &#123; &quot;distDir&quot;: &quot;build&quot; &#125; // 告诉 Vercel 静态文件输出在 build 目录    &#125;,    &#123;      &quot;src&quot;: &quot;api/**/*.js&quot;,      &quot;use&quot;: &quot;@vercel/node&quot;    &#125;  ]&#125;\n\n注意: 对于 Next.js 项目，通常不需要手动配置 builds，Vercel 会自动识别并使用 @vercel/next 作为构建器。\n\n\n\n4. routes (路由规则)\n类型: Array&lt;Object&gt;\n\n说明: 这是 vercel.json 中最强大的字段之一，允许定义复杂的请求处理逻辑。Vercel 会按序处理这些规则。\n每个路由对象可以包含以下属性：\n\nsrc: 匹配传入请求的路径（正则表达式）。\ndest: 请求的目标路径（可以包含捕获组）。\nstatus: HTTP 状态码 (用于重定向，如 301, 302, 307, 308)。\nheaders: 要添加到响应的 HTTP 头部。\nmethods: 匹配的 HTTP 方法 (例如 [&quot;GET&quot;, &quot;POST&quot;])。\ncontinue: 是否继续匹配后续路由规则。\nhandle: 特殊处理类型，如 filesystem (忽略文件系统路径)、rewrite (重写)、redirect (重定向)、hit (匹配但不处理)。\nlocale: 匹配特定语言环境。\n\n示例:\n&#123;  &quot;routes&quot;: [    // 重定向 http://example.com/old-path 到 http://example.com/new-path    &#123; &quot;src&quot;: &quot;/old-path&quot;, &quot;status&quot;: 301, &quot;dest&quot;: &quot;/new-path&quot; &#125;,    // 重写 /api/* 到 Serverless Functions 的 /api 目录    &#123; &quot;src&quot;: &quot;/api/(.*)&quot;, &quot;dest&quot;: &quot;/api/$1&quot; &#125;,    // 匹配所有非文件系统路径，重写到 /index.html (SPA 路由)    &#123; &quot;src&quot;: &quot;/(.*)&quot;, &quot;dest&quot;: &quot;/index.html&quot;, &quot;handle&quot;: &quot;filesystem&quot; &#125;,    // 添加安全HTTP头    &#123;      &quot;src&quot;: &quot;/(.*)&quot;,      &quot;headers&quot;: &#123;        &quot;Content-Security-Policy&quot;: &quot;default-src &#x27;self&#x27; data: https: &quot;      &#125;,      &quot;continue&quot;: true    &#125;  ]&#125;\n\n\n优先级: routes 数组中的规则按顺序匹配。一旦一个规则匹配并执行了 dest 或 status 操作，后续规则通常不再执行（除非设置了 continue: true）。\nrewrites 和 redirects 简化: 对于简单的重写和重定向，Vercel 提供了 rewrites 和 redirects 顶级字段，它们的内部实现也是基于 routes。\n\n\n\n5. env (环境变量)\n类型: Object&lt;string, string&gt;\n\n说明: 定义部署时注入到构建过程和 Serverless Functions 的环境变量。\n\n注意: 这些变量仅在部署时有效，不会在客户端代码中暴露。对于客户端环境变量，请在前端框架的配置中处理 (例如 Next.js 的 NEXT_PUBLIC_*)。\n\n推荐: 敏感信息（如 API 密钥）应在 Vercel UI 或 CLI 中作为 Secret 变量管理，而不是直接写入 vercel.json。\n示例:\n&#123;  &quot;env&quot;: &#123;    &quot;DATABASE_URL&quot;: &quot;@my_database_url&quot;, // 使用Vercel Secret    &quot;ANALYTICS_ID&quot;: &quot;UA-XXXXX-Y&quot;  &#125;&#125;\n\n6. functions (Serverless Functions 特定配置)\n类型: Object&lt;string, Object&gt;\n\n说明: 允许对部署为 Serverless Functions 的特定文件或 glob 模式应用配置。\n\nmemory: 分配给函数的内存（MB）。\nmaxDuration: 函数允许运行的最长时间（秒）。\nruntime: 指定函数的运行时版本 (例如 @vercel/node@20.x)。\nincludeFiles, excludeFiles: 包含或排除特定文件。\n\n示例:\n&#123;  &quot;functions&quot;: &#123;    &quot;api/heavy-task.js&quot;: &#123;      &quot;memory&quot;: 1024,      &quot;maxDuration&quot;: 60    &#125;,    &quot;api/**/*.py&quot;: &#123;      &quot;runtime&quot;: &quot;@vercel/python@3.11&quot;    &#125;  &#125;&#125;\n\n7. headers (全局 HTTP 响应头)\n类型: Array&lt;Object&gt;\n\n说明: 定义添加到匹配路由的 HTTP 响应头。常用于缓存控制和安全设置。\n\nsource: 匹配的 URL 路径。\nheaders: 要添加的 HTTP 头键值对。\n\n示例:\n&#123;  &quot;headers&quot;: [    &#123;      &quot;source&quot;: &quot;/(.*)&quot;,      &quot;headers&quot;: [        &#123; &quot;key&quot;: &quot;Cache-Control&quot;, &quot;value&quot;: &quot;s-maxage=1, stale-while-revalidate&quot; &#125;,        &#123; &quot;key&quot;: &quot;X-Frame-Options&quot;, &quot;value&quot;: &quot;DENY&quot; &#125;      ]    &#125;  ]&#125;\n\n8. rewrites (重写)\n类型: Array&lt;Object&gt;\n\n说明: 语法更简单的重写规则，与 routes 中的重写行为相同，但更简洁。\n\nsource: 匹配的路径。\ndestination: 重写到的目标路径。\n\n示例:\n&#123;  &quot;rewrites&quot;: [    &#123; &quot;source&quot;: &quot;/old-path&quot;, &quot;destination&quot;: &quot;/new-path&quot; &#125;,    &#123; &quot;source&quot;: &quot;/blog/(.*)&quot;, &quot;destination&quot;: &quot;/posts/$1&quot; &#125;  ]&#125;\n\n9. redirects (重定向)\n类型: Array&lt;Object&gt;\n\n说明: 语法更简单的重定向规则。\n\nsource: 匹配的路径。\ndestination: 重定向到的目标路径。\npermanent: true 表示 308 (永久重定向)，false 表示 307 (临时重定向)。\n\n示例:\n&#123;  &quot;redirects&quot;: [    &#123; &quot;source&quot;: &quot;/legacy-page&quot;, &quot;destination&quot;: &quot;/modern-page&quot;, &quot;permanent&quot;: true &#125;,    &#123; &quot;source&quot;: &quot;/old-docs/(.*)&quot;, &quot;destination&quot;: &quot;https://docs.newsite.com/$1&quot;, &quot;permanent&quot;: false &#125;  ]&#125;\n\n10. cleanUrls\n类型: boolean\n默认值: true\n说明: 如果设置为 true，Vercel 会自动移除 .html 文件扩展名 (例如 /about.html 变为 /about)。\n\n11. trailingSlash\n类型: boolean\n默认值: false\n说明: 控制 URL 路径末尾是否强制存在或移除斜杠。\ntrue: /pathname 将重定向到 /pathname/。\nfalse: /pathname/ 将重定向到 /pathname。\n\n\n\n12. app (Vercel App Directory 支持)\n类型: Object\n\n说明: 针对 Next.js app 目录的特定配置。\n\nanalytics: 控制 Vercel 性能分析 (Web Analytics) 的收集，例如 enabled: true。\n\n\n\n四、vercel.json 最佳实践\n从简单开始: 对于大多数 Next.js 等框架项目，您甚至不需要一个 vercel.json 文件，Vercel 会自动处理。\n逐步添加配置: 仅在需要自定义构建、路由或函数行为时才添加和修改 vercel.json。\n使用 Vercel CLI 本地测试: 使用 vercel dev 命令可以在本地模拟 Vercel 的生产环境，包括 vercel.json 中的路由和函数。\n版本控制: 将 vercel.json 文件纳入您的 Git 版本控制，确保团队成员和部署环境之间配置的一致性。\n利用环境变量和 Secret: 避免将敏感信息硬编码到 vercel.json 中，而是通过 Vercel 的环境变量和 Secret 功能进行管理。\n理解路由优先级: routes 数组中的规则按顺序匹配，先匹配的规则会优先执行。仔细测试，避免意外的路由行为。\n文档参考: Vercel 官方文档是学习 vercel.json 最新和最详细信息的最佳资源。\n\n五、总结vercel.json 是 Vercel 开发者手中的一把瑞士军刀，它赋予了您对基于该平台的应用程序部署行为进行精细控制的能力。无论是实现复杂的路由，优化 Serverless Functions 的性能，还是仅仅为了启用一些安全头，vercel.json 都是不可或缺的。掌握它的用法，将使您能够更充分地利用 Vercel 平台的强大功能，构建和部署高性能、高可用的现代化 Web 应用。\n","categories":["开发工具","云服务"],"tags":["2024","Serverless","云服务","Vercel"]},{"title":"深入理解JavaScript原型链（Prototype Chain）","url":"/2024/2024-03-27_%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3JavaScript%E5%8E%9F%E5%9E%8B%E9%93%BE%EF%BC%88Prototype%20Chain%EF%BC%89/","content":"\nJavaScript 是一门基于**原型（Prototype）**的语言，而非传统的基于类（Class）的语言（尽管 ES6 引入了 class 语法糖）。理解原型链是深入掌握 JavaScript 面向对象、继承以及对象属性查找机制的关键。它解释了为什么一个对象可以访问到它自身没有定义的方法和属性。\n\n“JavaScript 的一切皆对象，而原型链是这些对象连接的纽带。”\n\n\n一、什么是原型（Prototype）？在 JavaScript 中，每个对象都有一个内部属性，指向它的原型（Prototype）。这个原型又是一个对象，它也有自己的原型，这样一层一层向上，直到最后是 null。这个由一系列原型组成的链条就是原型链。\n1. [[Prototype]] 和 __proto__\n[[Prototype]]：这是对象内部隐藏的属性，它指向该对象的原型。在 ES5 之前，开发者无法直接访问这个内部属性。\n__proto__：这是大多数现代 JavaScript 引擎提供的一个非标准的 getter&#x2F;setter，用于访问或设置对象的 [[Prototype]]。虽然它现在已经被标准化为 Object.prototype.__proto__，但由于其历史遗留问题和潜在的性能影响，不推荐在生产代码中直接使用它来修改原型链。\nObject.getPrototypeOf() 和 Object.setPrototypeOf()：ES6 引入的标准方法，用于获取和设置对象的原型，推荐使用。\n\nconst obj = &#123;&#125;;console.log(obj.__proto__ === Object.prototype); // trueconsole.log(Object.getPrototypeOf(obj) === Object.prototype); // true\n\n2. prototype 属性除了普通对象有 __proto__ 之外，**函数（Function）**对象还拥有一个特殊的属性：prototype。\n\n函数.prototype：这个属性指向一个对象，这个对象会成为所有通过该函数构造出来的实例的 [[Prototype]]。\n\nfunction Person(name) &#123;  this.name = name;&#125;// Person.prototype 是一个对象console.log(typeof Person.prototype); // &quot;object&quot;const p1 = new Person(&#x27;Alice&#x27;);// p1 是 Person 构造函数的实例// p1 的原型指向 Person.prototypeconsole.log(Object.getPrototypeOf(p1) === Person.prototype); // trueconsole.log(p1.__proto__ === Person.prototype);             // true\n\n区分 __proto__ 和 prototype 是理解原型链的关键：\n\n__proto__ 是所有对象都拥有的，指向其自身的原型。\nprototype 只有函数对象才拥有的，用于指定它所创建出来的实例的原型。\n\n二、原型链是如何工作的？—— 属性查找机制当访问一个对象的属性时，JavaScript 引擎会遵循以下查找规则：\n\n首先在对象自身寻找：检查对象本身是否拥有这个属性（通过 hasOwnProperty() 方法可以判断）。\n如果找不到，沿着原型链向上查找：如果对象自身没有这个属性，引擎会沿着 __proto__ 指向的原型对象继续查找。\n重复步骤 1 和 2：如果原型对象也没有，就查找原型的原型，一直向上搜索。\n直到 null 为止：如果最终查找到原型链的顶端（通常是 Object.prototype 的原型，即 null），仍然没有找到该属性，那么就返回 undefined。\n\n示例：\nfunction Animal(name) &#123;  this.name = name; // 自身属性&#125;Animal.prototype.sayName = function() &#123;  console.log(`My name is $&#123;this.name&#125;`);&#125;;const dog = new Animal(&#x27;Buddy&#x27;);dog.sayName(); // &quot;My name is Buddy&quot;// 查找过程：// 1. dog 对象自身没有 `sayName` 属性。// 2. 沿着 dog.__proto__ (即 Animal.prototype) 向上查找。// 3. 在 Animal.prototype 中找到了 `sayName` 方法。// 4. 执行该方法。console.log(dog.hasOwnProperty(&#x27;name&#x27;));      // true (自身属性)console.log(dog.hasOwnProperty(&#x27;sayName&#x27;));   // false (原型上的属性)console.log(&#x27;sayName&#x27; in dog);                // true (通过原型链找到)\n\n三、原型链的构建过程原型链的构建主要通过以下两种方式：\n1. 构造函数模式 (new 操作符)当使用 new 操作符调用一个函数（作为构造函数）时，会发生以下步骤：\n\n创建一个新的空对象：这个新对象是 new 调用的结果。\n设置新对象的原型：将这个新创建的对象的 __proto__ 属性，指向构造函数 Function.prototype 属性所指向的对象。\n将构造函数的作用域赋给新对象：使得构造函数内部的 this 关键字指向这个新对象。\n执行构造函数内部的代码：为新对象添加属性和方法。\n返回新对象：如果构造函数没有显式地返回另一个对象，则返回这个新创建的对象。\n\nfunction Car(brand) &#123;  this.brand = brand;&#125;Car.prototype.drive = function() &#123;  console.log(`$&#123;this.brand&#125; is driving.`);&#125;;const myCar = new Car(&#x27;Tesla&#x27;);// 此时：// 1. myCar.__proto__ === Car.prototype// 2. Car.prototype.__proto__ === Object.prototype// 3. Object.prototype.__proto__ === nullconsole.log(myCar.brand);    // &quot;Tesla&quot; (自身属性)myCar.drive();               // &quot;Tesla is driving.&quot; (在 Car.prototype 上找到)console.log(myCar.toString()); // &quot;[object Object]&quot; (在 Object.prototype 上找到)\n\n图示原型链：\nmyCar  ---&gt;  Car.prototype  ---&gt;  Object.prototype  ---&gt;  null    (brand)       (drive)          (toString)\n\n2. 通过 Object.create()Object.create() 方法可以创建一个新对象，并将其 __proto__ 属性设置为指定对象。这是实现原型继承的更纯粹的方式。\nconst protoObj = &#123;  greeting: &#x27;Hello&#x27;,  sayHello: function() &#123;    console.log(`$&#123;this.greeting&#125;, I am $&#123;this.name&#125;`);  &#125;&#125;;const personA = Object.create(protoObj);personA.name = &#x27;Alice&#x27;;personA.sayHello(); // &quot;Hello, I am Alice&quot;const personB = Object.create(protoObj);personB.name = &#x27;Bob&#x27;;personB.greeting = &#x27;Hi&#x27;; // 自身添加属性，覆盖原型链上的personB.sayHello(); // &quot;Hi, I am Bob&quot;// 此时：// 1. personA.__proto__ === protoObj// 2. protoObj.__proto__ === Object.prototype// 3. Object.prototype.__proto__ === null\n\n四、理解 Object.prototype 和 Function.prototype这两个是 JavaScript 中非常重要的原型对象。\n1. Object.prototypeObject.prototype 是所有普通对象的终极原型（除非你特意创建不带原型的对象 Object.create(null)）。它包含了所有对象共享的基本方法，如 toString(), hasOwnProperty(), valueOf() 等。\nconst obj = &#123;&#125;;console.log(obj.__proto__ === Object.prototype);        // trueconsole.log(Object.getPrototypeOf(obj) === Object.prototype); // true// 任何普通对象最终都会继承 Object.prototype 上的方法obj.toString(); // &quot;[object Object]&quot;\n\n2. Function.prototypeFunction.prototype 是所有函数（包括构造函数、普通函数、箭头函数）的原型。它提供了一些函数共有的方法，如 call(), apply(), bind() 等。\nfunction myFunc() &#123;&#125;console.log(myFunc.__proto__ === Function.prototype); // true// Function.prototype 也是一个对象，所以它也有自己的原型console.log(Function.prototype.__proto__ === Object.prototype); // true// 这意味着函数也是对象，它们也继承了 Object.prototype 的方法myFunc.toString(); // &quot;function myFunc() &#123;&#125;&quot; (被 Function.prototype 上的 toString 覆盖)\n\n一个完整的原型链例子：\n创建一个普通对象字面量: `const o = &#123;&#125;;`o ---&gt; Object.prototype ---&gt; null使用构造函数创建一个实例: `const arr = [1,2];` (等同于 `new Array()`)arr ---&gt; Array.prototype ---&gt; Object.prototype ---&gt; null创建一个自定义构造函数: `function Foo() &#123;&#125;`Foo ---&gt; Function.prototype ---&gt; Object.prototype ---&gt; null使用自定义构造函数创建一个实例: `const f = new Foo();`f ---&gt; Foo.prototype ---&gt; Object.prototype ---&gt; null其中，Foo.prototype 是一个普通对象:Foo.prototype ---&gt; Object.prototype ---&gt; null\n\n五、原型链在继承中的应用在 ES6 class 语法糖出现之前，原型链是 JavaScript 实现继承的主要方式。\n示例：经典的原型链继承\n// 父类构造函数function SuperType(name) &#123;  this.name = name;  this.colors = [&#x27;red&#x27;, &#x27;blue&#x27;];&#125;SuperType.prototype.sayName = function() &#123;  console.log(this.name);&#125;;// 子类构造函数function SubType(name, age) &#123;  SuperType.call(this, name); // 继承父类的实例属性  this.age = age;&#125;// 核心：设置原型链实现方法继承// 方式一：Object.create() (推荐)SubType.prototype = Object.create(SuperType.prototype);// 修复 constructor 指向 (Good Practice)SubType.prototype.constructor = SubType;// 方式二：直接赋值（不推荐，会修改 SuperType.prototype）// SubType.prototype = new SuperType(); // 这种方式也会继承父类的实例属性，可能导致意外共享SubType.prototype.sayAge = function() &#123;  console.log(this.age);&#125;;const instance1 = new SubType(&#x27;Alice&#x27;, 25);instance1.colors.push(&#x27;green&#x27;); // 修改 instance1 的 colors 属性console.log(instance1.colors);  // [&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]instance1.sayName();            // &quot;Alice&quot; (继承自 SuperType.prototype)instance1.sayAge();             // 25 (自身方法)const instance2 = new SubType(&#x27;Bob&#x27;, 30);console.log(instance2.colors);  // [&quot;red&quot;, &quot;blue&quot;] (没有被 instance1 的修改影响)\n\n六、ES6 class 语法糖下的原型链ES6 的 class 关键字仅仅是原型链的语法糖，它并没有引入真正的类继承机制，底层仍然是基于原型链实现的。\nclass Parent &#123;  constructor(name) &#123;    this.name = name;  &#125;  sayHello() &#123;    console.log(`Hello, I&#x27;m $&#123;this.name&#125;`);  &#125;&#125;class Child extends Parent &#123;  constructor(name, age) &#123;    super(name); // 调用父类构造函数    this.age = age;  &#125;  sayAge() &#123;    console.log(`I&#x27;m $&#123;this.age&#125; years old.`);  &#125;&#125;const child = new Child(&#x27;Tom&#x27;, 10);child.sayHello(); // &quot;Hello, I&#x27;m Tom&quot; (继承自 Parent.prototype)child.sayAge();   // &quot;I&#x27;m 10 years old.&quot; (自身方法)// 实际上，底层原型链如下：console.log(Object.getPrototypeOf(Child) === Parent);          // true (Child 构造函数继承 Parent 构造函数)console.log(Object.getPrototypeOf(Child.prototype) === Parent.prototype); // true (Child.prototype 继承 Parent.prototype)console.log(Object.getPrototypeOf(child) === Child.prototype); // true (child 实例的原型是指向 Child.prototype)\n\n七、注意事项和最佳实践\n添加原型方法&#x2F;属性的时机：通常在构造函数定义之后立即添加原型属性和方法。\n避免直接修改 __proto__：修改 __proto__ 会对性能产生负面影响，因为它会扰乱 JavaScript 引擎内部的优化。使用 Object.setPrototypeOf() 也要谨慎。\n使用 hasOwnProperty()：在遍历对象属性时，使用 obj.hasOwnProperty(prop) 可以判断属性是否是对象自身的，而不是从原型链继承的。\nfor...in 循环：for...in 循环会遍历对象及原型链上所有可枚举的属性。为了避免遍历到原型链上的属性，通常会配合 hasOwnProperty() 使用。\nObject.create() 优于 new Parent() 进行原型继承：Object.create() 更纯粹地创建了一个指定原型的对象，而 new Parent() 会创建 Parent 的实例属性，这在某些情况下可能不是我们想要的。\n\n八、总结JavaScript 原型链是其面向对象机制的基石。它定义了对象如何继承属性和方法，是属性查找的根本机制。\n核心要点：\n\n__proto__: 所有对象都有，指向其原型。\nprototype: 只有函数有，指向一个对象，这个对象是其构造出的实例的原型。\n属性查找: 当访问一个对象属性时，会沿着原型链向上查找，直到找到或到达 null。\n继承: 原型链是 JavaScript 实现继承的本质。\nObject.prototype 和 Function.prototype: 两个核心的原型对象，分别对应所有对象的基石和所有函数的基石。\n\n掌握原型链，是理解 JavaScript 高级特性（如继承、闭包、作用域）的关键一步，也是成为一名优秀的 JavaScript 开发者的必备知识。\n","categories":["前端技术","JavaScript"],"tags":["2024","JavaScript","原型链"]},{"title":"SQLite 详细教程：从入门到实践","url":"/2024/2024-05-17_SQLite%20%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B%EF%BC%9A%E4%BB%8E%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/","content":"\nSQLite 是一个非常流行且强大的嵌入式关系型数据库管理系统。它与其他数据库（如 MySQL、PostgreSQL）最大的不同在于，它不是一个独立的服务器进程，而是以库的形式被集成到应用程序中。这意味着 SQLite 数据库是一个单一的文件，易于部署、备份和传输。它零配置、无服务器、自包含的特性，使其成为移动应用、桌面应用、小型网站、物联网设备以及开发测试等场景的理想选择。\n\n“轻量级却不失强大，SQLite 让数据库操作变得前所未有的简单。”\n\n\n一、什么是 SQLite？SQLite 是一个 C 语言库，实现了一个小型、快速、自包含的 SQL 数据库引擎。它的名字“Lite”就说明了它的轻量级特性。\n核心特点：\n\n无服务器 (Serverless): 与传统的客户端-服务器模式数据库不同，SQLite 应用程序直接读写磁盘上的数据库文件，无需独立的数据库服务器进程。\n零配置 (Zero-configuration): 无需安装、配置或管理。你只需直接使用其库。\n自包含 (Self-contained): 作为一个单一的文件，整个数据库都存储在这个文件中。\n事务支持 (Transactional): 完全支持 ACID (Atomicity, Consistency, Isolation, Durability) 特性，确保数据完整性。\nSQL 标准 (SQL Standard): 遵循大部分 SQL92 标准，支持常见的 SQL 语句。\n跨平台 (Cross-platform): 可以在几乎所有操作系统上运行，包括 Windows, macOS, Linux, Android, iOS 等。\n\n常见应用场景：\n\n移动应用：Android 和 iOS 内置 SQLite 作为本地数据存储。\n桌面应用：如 Firefox、Chrome 浏览器、Skype 等使用 SQLite 存储数据。\n小型网站：流量不大的网站可以使用 SQLite 作为后端数据库。\n物联网设备：资源受限的设备非常适合。\n嵌入式系统：各种设备中作为本地数据存储。\n开发测试：作为快速原型开发和测试的本地数据库。\n\n二、安装与入门SQLite 无需传统意义上的“安装”。你只需要下载其命令行工具或将其库集成到你的项目中。\n1. 下载 SQLite 命令行工具访问 SQLite 官方网站：https://www.sqlite.org/download.html\n在 “Precompiled Binaries for …” 部分，根据你的操作系统下载对应的文件。\n\nWindows: 下载 sqlite-tools-win32-x86-...zip。解压后会得到 sqlite3.exe (或 sqlite.exe) 文件。将其路径添加到系统环境变量 PATH 中，或者直接在解压目录中使用。\nmacOS &#x2F; Linux: 通常系统会自带 sqlite3。如果没有，可以下载 sqlite-tools-linux-x86-...zip 并解压，或者通过包管理器安装：\nmacOS (Homebrew): brew install sqlite\nUbuntu&#x2F;Debian: sudo apt-get install sqlite3\nFedora&#x2F;CentOS: sudo yum install sqlite\n\n\n\n2. 启动 SQLite 命令行界面 (CLI)打开命令行&#x2F;终端，输入 sqlite3。\n\n创建新数据库文件或连接现有数据库：sqlite3 mydatabase.db\n如果 mydatabase.db 不存在，它会被创建。如果存在，则会连接到该数据库。\n不指定数据库文件，进入内存模式（数据库内容不会保存）：sqlite3\n\n进入 CLI 后，你会看到 sqlite&gt; 提示符。\n3. SQLite CLI 特殊命令 (以 . 开头)在 sqlite&gt; 提示符下，除了标准的 SQL 语句，你还可以使用一些以 . 开头的内置命令来管理数据库。\n\n.help: 显示帮助信息。\n.databases: 列出当前连接的数据库。\n.tables: 列出当前数据库中的所有表。\n.schema &lt;table_name&gt;: 显示表的创建 SQL 语句。\n.quit 或 .exit: 退出 SQLite CLI。\n.mode &lt;mode&gt;: 设置输出模式 (e.g., list, csv, column)。\n.headers on/off: 开启&#x2F;关闭列名显示。\n.open &lt;filename&gt;: 关闭当前数据库并打开另一个数据库。\n.read &lt;filename&gt;: 从文件中执行 SQL 语句。\n.dump: 导出整个数据库为 SQL 脚本。\n\n示例：\nsqlite&gt; .databasesmain: /path/to/mydatabase.dbsqlite&gt; .tables# 暂时没有表sqlite&gt; .quit\n\n三、基本 SQL 操作SQLite 遵循标准的 SQL 语法。下面是一些基本的 SQL 操作示例。\n1. 创建表 (CREATE TABLE)创建一个名为 users 的表，包含 id, name, email 字段。\nCREATE TABLE users (    id INTEGER PRIMARY KEY AUTOINCREMENT,    name TEXT NOT NULL,    email TEXT UNIQUE);\n\nINTEGER PRIMARY KEY AUTOINCREMENT: id 将是一个自动递增的整数主键。\nTEXT: 字符串类型。\nNOT NULL: 字段不能为 NULL。\nUNIQUE: 字段值必须唯一。\n\n在 CLI 中执行：\nsqlite&gt; CREATE TABLE users (   ...&gt;     id INTEGER PRIMARY KEY AUTOINCREMENT,   ...&gt;     name TEXT NOT NULL,   ...&gt;     email TEXT UNIQUE   ...&gt; );sqlite&gt; .tablesuserssqlite&gt; .schema usersCREATE TABLE users (    id INTEGER PRIMARY KEY AUTOINCREMENT,    name TEXT NOT NULL,    email TEXT UNIQUE);\n\n2. 插入数据 (INSERT INTO)向 users 表插入几条记录。\nINSERT INTO users (name, email) VALUES (&#x27;Alice&#x27;, &#x27;alice@example.com&#x27;);INSERT INTO users (name, email) VALUES (&#x27;Bob&#x27;, &#x27;bob@example.com&#x27;);INSERT INTO users (name, email) VALUES (&#x27;Charlie&#x27;, &#x27;charlie@example.com&#x27;);\n\n3. 查询数据 (SELECT)\n查询所有字段所有记录：SELECT * FROM users;\n查询特定字段：SELECT name, email FROM users;\n按条件查询：SELECT * FROM users WHERE id = 1;SELECT * FROM users WHERE name LIKE &#x27;A%&#x27;; -- 名字以 A 开头的用户\n排序：SELECT * FROM users ORDER BY name ASC; -- 按名字升序\n限制结果：SELECT * FROM users LIMIT 1 OFFSET 1; -- 跳过第一条，取第二条\n\n在 CLI 中执行：\nsqlite&gt; INSERT INTO users (name, email) VALUES (&#x27;Alice&#x27;, &#x27;alice@example.com&#x27;);sqlite&gt; INSERT INTO users (name, email) VALUES (&#x27;Bob&#x27;, &#x27;bob@example.com&#x27;);sqlite&gt; INSERT INTO users (name, email) VALUES (&#x27;Charlie&#x27;, &#x27;charlie@example.com&#x27;);sqlite&gt; .mode columnsqlite&gt; .headers onsqlite&gt; SELECT * FROM users;id          name        email----------  ----------  -----------------1           Alice       alice@example.com2           Bob         bob@example.com3           Charlie     charlie@example.comsqlite&gt; SELECT name FROM users WHERE id = 2;name----------Bob\n\n4. 更新数据 (UPDATE)修改 id 为 1 的用户的邮箱。\nUPDATE users SET email = &#x27;alice.new@example.com&#x27; WHERE id = 1;\n验证：\nsqlite&gt; SELECT * FROM users WHERE id = 1;id          name        email----------  ----------  --------------------1           Alice       alice.new@example.com\n\n5. 删除数据 (DELETE FROM)删除 id 为 2 的用户。\nDELETE FROM users WHERE id = 2;\n验证：\nsqlite&gt; SELECT * FROM users;id          name        email----------  ----------  --------------------1           Alice       alice.new@example.com3           Charlie     charlie@example.com\n\n6. 删除表 (DROP TABLE)删除整个 users 表。\nDROP TABLE users;\n\n四、数据类型SQLite 支持的 SQL 数据类型非常灵活。与其他数据库不同，SQLite 采用的是动态类型系统。这意味着你可以在任何列中存储任何类型的值。\nSQLite 提供了以下五种主要的数据类型（Storage Classes）：\n\nNULL: 值是 NULL。\nINTEGER: 带符号的整数，根据大小存储为 1, 2, 3, 4, 6 或 8 字节。\nREAL: 浮点数值，存储为 8 字节的 IEEE 浮点数。\nTEXT: 字符串，以 UTF-8, UTF-16BE 或 UTF-16LE 编码存储。\nBLOB: 二进制大对象，存储为原始字节数据。\n\n重要概念：Type Affinity (类型亲和性)\n当你创建表时指定的类型（例如 INT, VARCHAR, DATETIME）在 SQLite 中被称为 Type Affinity。它只是一个建议，并不强制特定类型的存储。\n例如：\n\nINTEGER, INT, BIGINT 都会被赋予 INTEGER 亲和性。\nTEXT, VARCHAR, NVARCHAR 都会被赋予 TEXT 亲和性。\nREAL, DOUBLE, FLOAT 都会被赋予 REAL 亲和性。\nBLOB 会被赋予 BLOB 亲和性。\nDATETIME, BOOLEAN 等没有直接对应的存储类，它们通常会根据亲和性存为 TEXT 或 INTEGER。\n\n示例：即使将列定义为 INTEGER，你仍然可以尝试插入字符串：\nCREATE TABLE mixed_data (    id INTEGER PRIMARY KEY,    my_int_col INTEGER,    my_text_col TEXT);INSERT INTO mixed_data (my_int_col, my_text_col) VALUES (123, &#x27;Hello&#x27;);INSERT INTO mixed_data (my_int_col, my_text_col) VALUES (&#x27;abc&#x27;, &#x27;World&#x27;); -- 仍然可以插入！INSERT INTO mixed_data (my_int_col, my_text_col) VALUES (3.14, 456);     -- 浮点数会被截断或转换为整数\n\n建议： 尽管 SQLite 具有动态类型，但为了数据的一致性和可预测性，强烈建议在创建表时为列指定合理的类型，并在插入数据时遵循这些类型。\n五、索引 (Indexes)索引是提高查询速度的关键手段。为经常用于 WHERE 子句、JOIN 条件或 ORDER BY 子句的列创建索引。\nCREATE INDEX idx_users_email ON users (email);\n\n主键 (PRIMARY KEY) 列会自动创建唯一索引。\nUNIQUE 约束也会自动创建唯一索引。\n\n六、事务 (Transactions)事务是数据库操作的原子性、一致性、隔离性和持久性 (ACID) 的保证。SQLite 完全支持事务。\nBEGIN TRANSACTION;  -- 或者 BEGIN; 或 BEGIN DEFERRED;  -- 执行一系列 SQL 语句  INSERT INTO ...;  UPDATE ...;  -- 如果有错误或需要回滚  -- ROLLBACK;COMMIT;             -- 提交所有更改\n\nBEGIN TRANSACTION: 开始一个事务。\nCOMMIT: 提交事务，所有更改永久保存。\nROLLBACK: 回滚事务，所有更改被撤销，数据库回到事务开始前的状态。\n\n示例：\nsqlite&gt; BEGIN;sqlite&gt; INSERT INTO users (name, email) VALUES (&#x27;David&#x27;, &#x27;david@example.com&#x27;);sqlite&gt; SELECT * FROM users; -- 在当前事务中可见 Davidsqlite&gt; ROLLBACK;sqlite&gt; SELECT * FROM users; -- David 被回滚，不再可见\n\n七、与编程语言集成SQLite 的强大之处在于它可以方便地集成到各种编程语言中。以下是一些常见语言的示例：\n1. Node.js使用 sqlite3 模块。\nnpm install sqlite3\n\nconst sqlite3 = require(&#x27;sqlite3&#x27;).verbose();const db = new sqlite3.Database(&#x27;./mydatabase.db&#x27;); // 连接数据库db.serialize(() =&gt; &#123;  db.run(&quot;CREATE TABLE IF NOT EXISTS greetings (id INTEGER PRIMARY KEY, message TEXT)&quot;);  const stmt = db.prepare(&quot;INSERT INTO greetings (message) VALUES (?)&quot;);  for (let i = 0; i &lt; 10; i++) &#123;      stmt.run(&quot;Hello world &quot; + i);  &#125;  stmt.finalize();  db.all(&quot;SELECT id, message FROM greetings&quot;, [], (err, rows) =&gt; &#123;    if (err) &#123;      throw err;    &#125;    rows.forEach((row) =&gt; &#123;      console.log(`$&#123;row.id&#125;: $&#123;row.message&#125;`);    &#125;);  &#125;);&#125;);db.close((err) =&gt; &#123;  if (err) &#123;    console.error(err.message);  &#125;  console.log(&#x27;Close the database connection.&#x27;);&#125;);\n\n2. Python使用内置的 sqlite3 模块。\nimport sqlite3# 连接到数据库文件 (如果不存在则创建)conn = sqlite3.connect(&#x27;mydatabase.db&#x27;)cursor = conn.cursor()# 创建表cursor.execute(&#x27;&#x27;&#x27;    CREATE TABLE IF NOT EXISTS products (        id INTEGER PRIMARY KEY AUTOINCREMENT,        name TEXT NOT NULL,        price REAL    )&#x27;&#x27;&#x27;)# 插入数据cursor.execute(&quot;INSERT INTO products (name, price) VALUES (?, ?)&quot;, (&#x27;Laptop&#x27;, 1200.00))cursor.executemany(&quot;INSERT INTO products (name, price) VALUES (?, ?)&quot;,                   [(&#x27;Mouse&#x27;, 25.50), (&#x27;Keyboard&#x27;, 75.00)])conn.commit() # 提交事务# 查询数据cursor.execute(&quot;SELECT * FROM products WHERE price &gt; ?&quot;, (50,))rows = cursor.fetchall()for row in rows:    print(row)# 更新数据cursor.execute(&quot;UPDATE products SET price = ? WHERE name = ?&quot;, (1300.00, &#x27;Laptop&#x27;))conn.commit()# 关闭连接conn.close()\n\n3. Java使用 JDBC 驱动（需要下载 sqlite-jdbc.jar 并添加到项目中）。\nimport java.sql.*;public class SQLiteDemo &#123;    public static void main(String[] args) &#123;        String url = &quot;jdbc:sqlite:mydatabase.db&quot;; // 数据库文件路径        try (Connection conn = DriverManager.getConnection(url)) &#123;            if (conn != null) &#123;                DatabaseMetaData meta = conn.getMetaData();                System.out.println(&quot;The driver name is &quot; + meta.getDriverName());                System.out.println(&quot;A new database has been connected.&quot;);                Statement stmt = conn.createStatement();                // 创建表                stmt.execute(&quot;CREATE TABLE IF NOT EXISTS tasks (id INTEGER PRIMARY KEY, name TEXT)&quot;);                // 插入数据                stmt.execute(&quot;INSERT INTO tasks (name) VALUES (&#x27;Learn SQLite&#x27;)&quot;);                stmt.execute(&quot;INSERT INTO tasks (name) VALUES (&#x27;Write Markdown&#x27;)&quot;);                // 查询数据                ResultSet rs = stmt.executeQuery(&quot;SELECT id, name FROM tasks&quot;);                while (rs.next()) &#123;                    System.out.println(rs.getInt(&quot;id&quot;) + &quot;\\t&quot; +                                       rs.getString(&quot;name&quot;));                &#125;            &#125;        &#125; catch (SQLException e) &#123;            System.out.println(e.getMessage());        &#125;    &#125;&#125;\n\n八、高级特性和注意事项1. 外键约束 (Foreign Keys)SQLite 默认情况下不强制执行外键约束。你需要手动启用它。\nPRAGMA foreign_keys = ON;\n这条语句需要在每次连接到数据库时执行 (PRAGMA 是 SQLite 的特殊命令)。\n然后就可以创建带外键的表：\nCREATE TABLE IF NOT EXISTS categories (    id INTEGER PRIMARY KEY AUTOINCREMENT,    name TEXT NOT NULL UNIQUE);CREATE TABLE IF NOT EXISTS posts (    id INTEGER PRIMARY KEY AUTOINCREMENT,    title TEXT NOT NULL,    content TEXT,    category_id INTEGER,    FOREIGN KEY (category_id) REFERENCES categories (id));\n\n2. JOIN 操作连接多个表进行查询。\nSELECT    p.title,    c.name AS category_nameFROM    posts AS pJOIN    categories AS c ON p.category_id = c.id;\n\n3. 用户权限&#x2F;安全SQLite 没有内置的用户账户和权限管理系统。所有连接到数据库文件的程序都具有对该文件的完全读写权限（取决于操作系统文件系统权限）。因此，安全需要通过文件系统权限、应用程序逻辑或加密来保证。\n4. 并发性 (Concurrency)SQLite 支持并发读写，但在并发写入方面有一些限制。\n\n多个读者可以同时访问数据库。\n只有一个写入者可以在任何给定时间写入数据库。\n\n当一个进程尝试写入时，它会获取一个写锁。其他写入尝试会等待，直到锁被释放。在高并发写入场景下，这可能成为性能瓶颈。对于需要高并发写入的场景，传统的客户端-服务器数据库（如 PostgreSQL, MySQL）是更好的选择。\n5. 加密SQLite 本身不提供数据加密功能。要加密 SQLite 数据库，你需要使用第三方扩展（如 SQLCipher）或在应用程序层面进行数据加密。\n6. 可视化工具除了命令行，还有许多图形界面工具可以方便地管理 SQLite 数据库：\n\nDB Browser for SQLite: 免费、开源，功能强大，跨平台。强烈推荐。\nDataGrip (JetBrains): 商业多数据库 IDE，支持 SQLite。\nVS Code 扩展: 许多 VS Code 扩展也支持 SQLite 数据库的浏览和查询。\n\n九、总结SQLite 以其独特的嵌入式、零配置、无服务器特性，在众多数据库中独树一帜。它非常适合那些资源受限、不需要高并发写入、或需要简单部署和管理的应用场景。从移动应用到桌面软件，再到小型个人项目，SQLite 都展现了其作为一款强大而又轻量级数据库的优秀品质。\n掌握 SQLite 的基本操作和特性，将大大拓宽你的技术栈，并为许多项目中数据存储问题提供一个简单而高效的解决方案。\n","categories":["中间件","SQLite"],"tags":["中间件","2024","SQLite"]},{"title":"深入理解 JavaScript Fetch：为什么需要两次 await？","url":"/2024/2024-06-11_%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%20JavaScript%20Fetch%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E4%B8%A4%E6%AC%A1%20await/","content":"\nJavaScript 中的 fetch API 提供了一种现代、强大的方式来发送网络请求。然而，初学者在使用 async/await 语法处理 fetch 请求时，经常会遇到一个困惑：为什么需要两次 await 才能获取到实际的数据？本文将深入探讨 fetch API 的设计原理，解释这“两次等待”背后的逻辑。\n\n“Fetch API 的设计哲学：将 HTTP 响应的元数据与实际数据流分离处理。”\n\n\n一、fetch API 概览fetch API 是 Web API 的一部分，用于替代老旧的 XMLHttpRequest 对象，提供了一个更强大、更灵活的用于获取资源的接口。它基于 Promise，使得异步请求的处理更加简洁。\n一个典型的 fetch 请求（不使用 async/await）看起来是这样的：\nfetch(&#x27;https://api.example.com/data&#x27;)  .then(response =&gt; &#123;    // 第一次 then: 处理响应头和状态    if (!response.ok) &#123;      throw new Error(`HTTP error! status: $&#123;response.status&#125;`);    &#125;    return response.json(); // 或 .text(), .blob(), .arrayBuffer() 等  &#125;)  .then(data =&gt; &#123;    // 第二次 then: 处理实际数据    console.log(data);  &#125;)  .catch(error =&gt; &#123;    console.error(&#x27;There was a problem with the fetch operation:&#x27;, error);  &#125;);\n\n当使用 async/await 语法糖时，上述代码变成了：\nasync function fetchData() &#123;  try &#123;    // 第一次 await: 等待获取到响应头信息    const response = await fetch(&#x27;https://api.example.com/data&#x27;);    if (!response.ok) &#123;      throw new Error(`HTTP error! status: $&#123;response.status&#125;`);    &#125;    // 第二次 await: 等待响应体数据解析完成    const data = await response.json();    console.log(data);  &#125; catch (error) &#123;    console.error(&#x27;Error fetching data:&#x27;, error);  &#125;&#125;fetchData();\n\n正是这里的 await response.json() 引起了许多人的疑惑：为什么 fetch 返回的 response 对象不是直接包含数据的？\n二、第一次 await：获取 Response 对象当 fetch 函数执行时，它会立即向服务器发送请求。fetch 函数本身返回一个 Promise，这个 Promise 会在接收到服务器的响应头信息时被解决 (resolved)，而不是在接收到完整的响应体数据时。\n所以，const response = await fetch(&#39;...&#39;) 中的第一次 await 实际上是等待：\n\n网络请求完成。\n服务器发送回 HTTP 响应头（例如状态码、响应类型、各种 Cache-Control 等 HTTP 头）。\n\n此时，你得到了一个 Response 对象。这个 Response 对象包含了请求的元数据（response.status, response.ok, response.headers 等），但它并不包含服务器返回的实际数据（响应体）。\nResponse 对象的 body 属性是一个 ReadableStream。这意味着响应体数据是以流的形式到达的，可能是一个很大的文件，浏览器并不会立即将其全部加载到内存中。\n三、第二次 await：解析响应体数据Response 对象提供了一系列方法来解析其响应体（body）数据，这些方法都返回 Promise：\n\nresponse.json(): 将响应体解析为 JSON 对象。\nresponse.text(): 将响应体解析为纯文本。\nresponse.blob(): 将响应体解析为 Blob (Binary Large Object) 对象，通常用于处理二进制文件，如图片。\nresponse.arrayBuffer(): 将响应体解析为 ArrayBuffer，用于处理更低级别的二进制数据。\nresponse.formData(): 将响应体解析为 FormData 对象，通常用于处理 HTML 表单数据。\n\n第二次 await 的作用就是等待其中一个解析方法（例如 response.json()）的 Promise 解决。这个 Promise 的解决时机是：\n\n整个响应体数据已经从网络上完整接收完毕。\n响应体数据已经成功地被解析成指定的格式（例如 JSON）。\n\n所以，const data = await response.json() 中的第二次 await 实际上是在等待：\n\n响应流（ReadableStream）完全读取完毕。\n读取到的数据被成功转换为 JavaScript 对象（或字符串、Blob 等）。\n\n四、为什么这样设计？这种“两次等待”的设计并非出于偶然，而是 fetch API 灵活性和效率的体现：\n\n分步处理，提前判断：\n\n在接收到响应头之后，你就可以立即检查请求是否成功（response.ok 或 response.status）。如果状态码是 4xx 或 5xx，你可以提前抛出错误，无需下载和解析整个响应体，从而节省带宽和处理时间。\n例如，一个 404 错误通常会有一个很小的响应体（甚至没有），提前判断可以避免不必要的解析。\n\n\n处理大型文件和数据流：\n\n如果 fetch 在接收到响应头时就直接返回解析好的数据，那么对于非常大的文件（如视频、图片、PDF），浏览器必须等待整个文件下载完成并解析后才能执行后面的代码。这可能导致主线程长时间阻塞。\n通过流式处理（ReadableStream），开发者可以更灵活地处理数据。虽然 response.json() 等方法会等待整个流读取完毕，但理论上，你可以直接操作 response.body 这个流来分块处理数据，尤其适用于处理大量数据时需要显示进度或在数据到达时立即开始处理的场景（尽管这通常需要更高级的 API 或自定义流处理器）。\n\n\n支持不同数据类型：\n\n服务器可以返回 JSON、文本、二进制文件等多种类型的数据。Response 对象提供不同的解析方法，允许开发者根据 Content-Type 或其他业务逻辑选择最合适的解析方式。\n如果 fetch 直接返回解析好的数据，它将不得不猜测（或依赖某个 HTTP 头）如何解析，这会降低灵活性。\n\n\n\n五、总结JavaScript fetch API 需要两次 await 的原因是：\n\n第一次 await (await fetch(url)): 等待网络请求完成，获取到包含 HTTP 响应头和元信息的 Response 对象。此时响应体数据可能尚未完全下载，也未被处理。\n第二次 await (await response.json()): 等待 Response 对象的 body 流完全读取完毕，并根据所选方法（如 json(), text(), blob() 等）将其内容解析成可用的 JavaScript 数据结构。\n\n这种设计使得 fetch 接口既高效又灵活，允许开发者在接收到响应头后立即对请求结果进行初步判断，从而优化网络资源的使用和用户体验。理解这个机制对于有效地使用 fetch API，编写健壮、高性能的网络请求代码至关重要。\n","categories":["前端技术","JavaScript"],"tags":["前端技术","2024","JavaScript"]},{"title":"手写Promise：深入解析JS Promise原理","url":"/2024/2024-04-06_%E6%89%8B%E5%86%99Promise%EF%BC%9A%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90JS%20Promise%E5%8E%9F%E7%90%86/","content":"\nJavaScript Promise 是异步编程的核心，它解决了回调地狱（Callback Hell）的问题，让异步代码的编写更加优雅和可维护。然而，Promises 究竟是如何工作的？它背后隐藏了哪些状态管理和回调机制？本文将通过从零开始手写一个简化的 Promise 实现，来深入解析其核心原理。\n\n“理解 Promise 的精髓，就是理解异步状态管理和时序控制。”\n\n\n一、Promise 的基本概念复习在开始手写之前，我们先快速回顾 Promise 的几个核心概念：\n\n三种状态 (States):\n\npending (待定): 初始状态，既没有成功，也没有失败。\nfulfilled (已成功&#x2F;已兑现): 操作成功完成。\nrejected (已失败&#x2F;已拒绝): 操作失败。\nPromise 的状态一旦从 pending 变为 fulfilled 或 rejected，就不可逆转，称为 settled (已敲定)。\n\n\n构造函数: new Promise(executor)\n\nexecutor 是一个执行器函数，它在 Promise 构造时同步执行。\nexecutor 接收两个参数：resolve (成功回调) 和 reject (失败回调)。\nresolve(value): 将 Promise 的状态从 pending 变为 fulfilled，并将 value 传递给后续的 then 回调。\nreject(reason): 将 Promise 的状态从 pending 变为 rejected，并将 reason 传递给后续的 catch 回调。\n\n\n链式调用: promise.then(onFulfilled, onRejected).catch(onRejected)\n\nthen() 和 catch() 方法都返回一个新的 Promise，从而允许链式调用。\nonFulfilled: Promise 成功时的回调函数。\nonRejected: Promise 失败时的回调函数。\ncatch(onRejected) 是 then(null, onRejected) 的语法糖。\n\n\n\n二、手写一个简化的 MyPromise我们将一步步构建一个名为 MyPromise 的类，使其具备 Promise 的核心功能。\n1. 骨架和状态管理首先，定义 MyPromise 的基本结构，包括状态 (status)、成功值 (value)、失败原因 (reason)，以及用于存储待执行回调的数组。\nclass MyPromise &#123;  // 定义 Promise 的三种状态  static PENDING = &#x27;pending&#x27;;  static FULFILLED = &#x27;fulfilled&#x27;;  static REJECTED = &#x27;rejected&#x27;;  constructor(executor) &#123;    this.status = MyPromise.PENDING; // 初始状态为 pending    this.value = undefined;          // 存储成功后的值    this.reason = undefined;         // 存储失败后的原因    // 存储 pending 状态下，需要执行的成功和失败回调    // 为什么需要数组？因为一个 Promise 可能被多次 then()    this.onFulfilledCallbacks = [];    this.onRejectedCallbacks = [];    // 定义 resolve 函数    const resolve = (value) =&gt; &#123;      // 只有在 pending 状态下才能改变状态      if (this.status === MyPromise.PENDING) &#123;        this.status = MyPromise.FULFILLED;        this.value = value;        // 状态改变后，执行所有待执行的成功回调        this.onFulfilledCallbacks.forEach(callback =&gt; &#123;          callback(this.value);        &#125;);      &#125;    &#125;;    // 定义 reject 函数    const reject = (reason) =&gt; &#123;      // 只有在 pending 状态下才能改变状态      if (this.status === MyPromise.PENDING) &#123;        this.status = MyPromise.REJECTED;        this.reason = reason;        // 状态改变后，执行所有待执行的失败回调        this.onRejectedCallbacks.forEach(callback =&gt; &#123;          callback(this.reason);        &#125;);      &#125;    &#125;;    // 执行 executor 函数    // 捕获 executor 中的错误，直接调用 reject    try &#123;      executor(resolve, reject);    &#125; catch (error) &#123;      reject(error);    &#125;  &#125;  // then 方法的实现  then(onFulfilled, onRejected) &#123;    // 确保 onFulfilled 和 onRejected 总是一个函数    onFulfilled = typeof onFulfilled === &#x27;function&#x27; ? onFulfilled : value =&gt; value;    onRejected = typeof onRejected === &#x27;function&#x27; ? onRejected : reason =&gt; &#123; throw reason; &#125;;    // then 方法必须返回一个新的 Promise，实现链式调用    const promise2 = new MyPromise((resolve, reject) =&gt; &#123;      // 如果当前 Promise 已经是 fulfilled 状态      if (this.status === MyPromise.FULFILLED) &#123;        // 使用 setTimeout 模拟异步，确保在下一个微任务队列中执行        // 这是为了符合 Promise/A+ 规范：onFulfilled 和 onRejected 必须异步执行        setTimeout(() =&gt; &#123;          try &#123;            const x = onFulfilled(this.value);            // 处理 onFulfilled 的返回值，这是 then 链式调用的核心            // x 可能是普通值，也可能是另一个 Promise            resolvePromise(promise2, x, resolve, reject);          &#125; catch (error) &#123;            reject(error);          &#125;        &#125;, 0);      &#125;      // 如果当前 Promise 已经是 rejected 状态      else if (this.status === MyPromise.REJECTED) &#123;        setTimeout(() =&gt; &#123;          try &#123;            const x = onRejected(this.reason);            resolvePromise(promise2, x, resolve, reject);          &#125; catch (error) &#123;            reject(error);          &#125;        &#125;, 0);      &#125;      // 如果当前 Promise 仍然是 pending 状态      else &#123;        // 将回调函数存储起来，等待 resolve/reject 调用时执行        this.onFulfilledCallbacks.push(() =&gt; &#123;          setTimeout(() =&gt; &#123;            try &#123;              const x = onFulfilled(this.value);              resolvePromise(promise2, x, resolve, reject);            &#125; catch (error) &#123;              reject(error);            &#125;          &#125;, 0);        &#125;);        this.onRejectedCallbacks.push(() =&gt; &#123;          setTimeout(() =&gt; &#123;            try &#123;              const x = onRejected(this.reason);              resolvePromise(promise2, x, resolve, reject);            &#125; catch (error) &#123;              reject(error);            &#125;          &#125;, 0);        &#125;);      &#125;    &#125;);    return promise2;  &#125;  // catch 方法是 then(null, onRejected) 的语法糖  catch(onRejected) &#123;    return this.then(null, onRejected);  &#125;&#125;\n\n2. 核心辅助函数：resolvePromisethen 方法的核心在于它的返回值。then 回调的返回值 x 决定了下一个 Promise (promise2) 的状态。这部分逻辑是 Promise&#x2F;A+ 规范中最为复杂，但也是最关键的部分 —— Promise 解决过程 (Promise Resolution Procedure)。\n/** * Promise 解决过程 (Promise Resolution Procedure) * 这是一个核心辅助函数，用于处理 then 回调的返回值 x * 规范：https://promisesaplus.com/#the-promise-resolution-procedure * * @param &#123;MyPromise&#125; promise2   then 方法返回的新 Promise * @param &#123;any&#125; x                onFulfilled 或 onRejected 的返回值 * @param &#123;Function&#125; resolve    promise2 的 resolve 方法 * @param &#123;Function&#125; reject     promise2 的 reject 方法 */function resolvePromise(promise2, x, resolve, reject) &#123;  // 2.3.1 如果 promise2 和 x 指向同一个对象，则以 TypeError 为据因拒绝 promise  if (promise2 === x) &#123;    return reject(new TypeError(&#x27;Chaining cycle detected for promise&#x27;));  &#125;  // 2.3.2 If x is a Promise, adopt its state  // 如果 x 是一个 Promise，则将 promise2 的状态与 x 的状态保持一致  if (x instanceof MyPromise) &#123;    // x.then 可能会被多次调用，或者被调用多次 resolve(y) / reject(r)    // 确保只处理一次    x.then(function(y) &#123;      resolvePromise(promise2, y, resolve, reject); // 递归解析 x 的结果    &#125;, reject); // 如果 x 失败了，则 promise2 也失败    return;  &#125;  // 2.3.3 If x is an object or function  // 如果 x 是对象或函数 (非 null)，则可能它是一个 thenable 对象  if (x &amp;&amp; (typeof x === &#x27;object&#x27; || typeof x === &#x27;function&#x27;)) &#123;    let called = false; // 防止 then 被多次调用，即防止 resolve 或 reject 被多次调用    try &#123;      // 2.3.3.1 Let then be x.then      // 尝试获取 x 的 then 方法      const then = x.then;      // 2.3.3.3 If then is a function, call it with x as this,      // 如果 then 是一个函数，则将其作为 Promise 执行器调用      if (typeof then === &#x27;function&#x27;) &#123;        // then.call(x, resolvePromiseFn, rejectPromiseFn)        // 这个 resolve/reject 函数与 MyPromise 的 resolve/reject 不同，        // 它们是用于决定 promise2 状态的，且需要递归调用 resolvePromise        then.call(x,          y =&gt; &#123;            if (called) return; // 确保只处理一次            called = true;            resolvePromise(promise2, y, resolve, reject); // 递归解析 y          &#125;,          r =&gt; &#123;            if (called) return; // 确保只处理一次            called = true;            reject(r); // 如果 thenable 失败了，则 promise2 也失败          &#125;        );      &#125; else &#123;        // 2.3.3.4 If then is not a function, fulfill promise with x        // 如果 then 不是函数，则直接以 x 填充 promise2        resolve(x);      &#125;    &#125; catch (error) &#123;      // 2.3.3.2 If retrieving the property x.then results in a thrown exception e,      // 2.3.3.3.4.1 If calling then throws an exception e,      // 如果获取 x.then 或调用 then 时出错，则拒绝 promise      if (called) return; // 防止重复拒绝      called = true;      reject(error);    &#125;    return;  &#125;  // 2.3.4 If x is not an object or function, fulfill promise with x  // 如果 x 是普通值（非对象、非函数），则直接以 x 填充 promise2  resolve(x);&#125;\n\n三、测试 MyPromise现在，我们可以用一些例子来测试我们的 MyPromise 实现。\n1. 基本同步&#x2F;异步示例// 同步执行 resolveconsole.log(&#x27;--- Test 1: Sync Resolve ---&#x27;);new MyPromise((resolve, reject) =&gt; &#123;  console.log(&#x27;Executor starts (sync)&#x27;);  resolve(&#x27;Sync Data&#x27;);  console.log(&#x27;Executor ends (sync)&#x27;);&#125;).then(data =&gt; &#123;  console.log(&#x27;Sync Resolve Result:&#x27;, data);&#125;);console.log(&#x27;After sync promise creation&#x27;);// 异步执行 resolveconsole.log(&#x27;\\n--- Test 2: Async Resolve ---&#x27;);new MyPromise((resolve, reject) =&gt; &#123;  console.log(&#x27;Executor starts (async)&#x27;);  setTimeout(() =&gt; &#123;    resolve(&#x27;Async Data&#x27;);    console.log(&#x27;Executor resolves (async)&#x27;);  &#125;, 100);  console.log(&#x27;Executor ends (async)&#x27;);&#125;).then(data =&gt; &#123;  console.log(&#x27;Async Resolve Result:&#x27;, data);&#125;);console.log(&#x27;After async promise creation&#x27;);// 异步执行 rejectconsole.log(&#x27;\\n--- Test 3: Async Reject ---&#x27;);new MyPromise((resolve, reject) =&gt; &#123;  setTimeout(() =&gt; &#123;    reject(&#x27;Async Error&#x27;);  &#125;, 50);&#125;).then(null, error =&gt; &#123; // 或 .catch(error =&gt; ...)  console.log(&#x27;Async Reject Result:&#x27;, error);&#125;);\n\n预期输出：\n--- Test 1: Sync Resolve ---Executor starts (sync)Executor ends (sync)After sync promise creationSync Resolve Result: Sync Data--- Test 2: Async Resolve ---Executor starts (async)Executor ends (async)After async promise creationAsync Resolve Result: Async DataExecutor resolves (async)--- Test 3: Async Reject ---Async Reject Result: Async Error\n注意：console.log(&#39;Executor resolves (async)&#39;) 会在回调执行后才输出，因为回调被 setTimeout 延迟了，即使是 0ms 也是调度到微任务队列（或宏任务，这里我们用 setTimeout 模拟，实际 Promise 是微任务）。\n2. 链式调用console.log(&#x27;\\n--- Test 4: Chaining ---&#x27;);new MyPromise((resolve, reject) =&gt; &#123;  setTimeout(() =&gt; resolve(1), 50);&#125;).then(value =&gt; &#123;  console.log(&#x27;First then:&#x27;, value); // 1  return value + 1; // 返回普通值&#125;).then(value =&gt; &#123;  console.log(&#x27;Second then:&#x27;, value); // 2  return new MyPromise(r =&gt; setTimeout(() =&gt; r(value + 10), 50)); // 返回一个新的 Promise&#125;).then(value =&gt; &#123;  console.log(&#x27;Third then:&#x27;, value); // 13  throw new Error(&#x27;Something went wrong!&#x27;); // 抛出错误&#125;).then(value =&gt; &#123; // 这个 then 不会被执行  console.log(&#x27;Fourth then:&#x27;, value);&#125;, error =&gt; &#123;  console.log(&#x27;Caught Error in then:&#x27;, error.message); // Something went wrong!  return &#x27;Recovered&#x27;; // 错误处理后返回普通值，链式继续&#125;).then(value =&gt; &#123;  console.log(&#x27;Fifth then:&#x27;, value); // Recovered  return new MyPromise((res, rej) =&gt; rej(&#x27;Chain Rejected!&#x27;)); // 返回一个失败的 Promise&#125;).catch(error =&gt; &#123;  console.log(&#x27;Caught Error in catch:&#x27;, error); // Chain Rejected!&#125;);\n预期输出：\n--- Test 4: Chaining ---First then: 1Second then: 2Third then: 13Caught Error in then: Something went wrong!Fifth then: RecoveredCaught Error in catch: Chain Rejected!\n\n3. thenable 对象console.log(&#x27;\\n--- Test 5: Thenable Object ---&#x27;);const thenable = &#123;  then(resolve, reject) &#123;    console.log(&#x27;Thenable then called&#x27;);    setTimeout(() =&gt; resolve(&#x27;From Thenable&#x27;), 50);  &#125;&#125;;new MyPromise(resolve =&gt; resolve(thenable))  .then(data =&gt; &#123;    console.log(&#x27;Resolved with thenable data:&#x27;, data);  &#125;);console.log(&#x27;After thenable promise creation&#x27;);\n\n预期输出：\n--- Test 5: Thenable Object ---After thenable promise creationThenable then calledResolved with thenable data: From Thenable\n\n四、核心原理总结通过手写 MyPromise，我们揭示了 Promise 的几个关键原理：\n\n状态机管理: Promise 的核心是维护其三种状态 (pending, fulfilled, rejected)，并且状态只能从 pending 转换为 fulfilled 或 rejected 一次，之后状态不可变。\n回调存储机制: 在 pending 状态下，then 方法会将回调函数（onFulfilled, onRejected）存储起来。一旦 Promise 状态变成 fulfilled 或 rejected，这些存储的回调就会被异步执行。\n异步执行: then 方法中的回调函数必须被异步执行（即使 Promise 状态已经确定），这是通过 setTimeout(..., 0) 来模拟微任务队列的机制。这是 Promise&#x2F;A+ 规范强制规定的，确保了宏任务和微任务的执行顺序。\n链式调用的实现: then 方法总是返回一个新的 Promise (promise2)。这个 promise2 的状态和值取决于 then 方法中回调函数（onFulfilled 或 onRejected）的返回值 x。\nPromise 解决过程 (resolvePromise): 这是最精妙的部分。它处理 then 回调的返回值 x。\n如果 x 是一个普通值，promise2 会成功并以 x 为值。\n如果 x 是一个 Promise，promise2 的状态会“跟随” x 的状态。\n如果 x 是一个 “thenable” 对象（即有一个 then 方法的对象），promise2 会尝试像 Promise 一样处理 x，调用其 then 方法并以 x 的处理结果来决定自身的最终状态。\n处理过程中，严格避免 Promise 循环引用和多次调用 resolve/reject。\n\n\n错误捕获: executor 中的同步错误和 then 回调中抛出的错误都会被 catch 捕获，导致 Promise 变为 rejected 状态。\n\n通过手动实现这些机制，我们不仅理解了 Promise 的内部工作流程，也掌握了异步操作如何通过状态和回调协同，实现顺序执行和错误处理的精髓。这对于编写和调试复杂的异步 JavaScript 代码至关重要。\n","categories":["前端技术","JavaScript"],"tags":["前端技术","2024","JavaScript","Promise","源码分析"]},{"title":"TypeScript高级类型","url":"/2024/2024-07-26_TypeScript%E9%AB%98%E7%BA%A7%E7%B1%BB%E5%9E%8B/","content":"TypeScript高级类型是增强类型系统灵活性和精确性的核心工具，主要包括以下关键类型及其应用场景：\n\n\n一、交叉类型(Intersection Types)通过&amp;合并多个类型，新类型需同时满足所有成员类型的特性。典型应用包括混入(Mixin)模式和对象属性合并：\ninterface A &#123; a: number &#125;interface B &#123; b: string &#125;type C = A &amp; B; // 必须包含a和b属性\n该特性在混合类功能时尤其有用，例如合并Person和Programmer类的属性和方法\n二、联合类型(Union Types)使用|声明变量可接受多种类型中的任意一种，需配合类型保护确保类型安全：\nfunction printId(id: string | number) &#123;  if (typeof id === &#x27;string&#x27;) console.log(id.toUpperCase());  else console.log(id.toFixed(2));&#125;\n\n三、映射类型(Mapped Types)通过keyof和泛型实现类型转换，内置工具类型包括：\n\nPartial：使所有属性可选\nReadonly：使属性只读\nPick&lt;T, K&gt;：选取部分属性\nOmit&lt;T, K&gt;：排除指定属性\n\n# Partial&lt;T&gt; type Partial&lt;T&gt; = &#123; [P in keyof T]?: T[P] &#125;; # Readonly&lt;T&gt;type Readonly&lt;T&gt; = &#123; readonly [P in keyof T]: T[P] &#125;; # Pick&lt;T, K&gt;type Pick&lt;T, K extends keyof T&gt; = &#123; [P in K]: T[P] &#125;;# Omit&lt;T, K&gt;type Omit&lt;T, K extends keyof any&gt; = Pick&lt;T, Exclude&lt;keyof T, K&gt;&gt;;\n\n四、条件类型(Conditional Types)通过T extends U ? X : Y实现动态类型推导，典型应用包括：\n\nExclude&lt;T, U&gt;：从T中排除U类型\nExtract&lt;T, U&gt;：提取T中符合U的类型\nNonNullable：排除null&#x2F;undefined\n\n# Exclude&lt;T, U&gt;type Exclude&lt;T, U&gt; = T extends U ? never : T;# Extract&lt;T, U&gt;type Extract&lt;T, U&gt; = T extends U ? T : never;# NonNullable&lt;T&gt;type NonNullable&lt;T&gt; = T extends null | undefined ? never : T;\n\n五、模板字面量类型TypeScript 4.1+支持基于字符串模板的类型操作：\ntype HttpMethod = &#x27;GET&#x27; | &#x27;POST&#x27; | &#x27;PUT&#x27;;type ApiPath = `/api/$&#123;string&#125;`;\n\n可用于精确约束路由格式或API路径\n\n六、类型推断与泛型约束通过infer关键字提取嵌套类型，结合泛型约束实现高级模式：\n# ReturnType&lt;T&gt;type ReturnType&lt;T&gt; = T extends (...args: any[]) =&gt; infer R ? R : never;\n该机制广泛用于工具类型库开发\n","categories":["前端技术","TypeScript"],"tags":["TypeScript","前端技术","2024","JavaScript"]},{"title":"Electron 开发详解","url":"/2024/2024-07-04_Electron%E5%BC%80%E5%8F%91%E8%AF%A6%E8%A7%A3/","content":"\nElectron 是 GitHub 开发的一个开源框架，它允许你使用 Web 技术 (HTML, CSS, JavaScript) 构建跨平台的桌面应用程序。这意味着你可以利用已有的前端技能，开发出像 VS Code、Slack、Discord 等专业桌面应用。本文将深入探讨 Electron 的核心概念、开发流程、最佳实践和常见问题。\n\n“Build cross-platform desktop apps with JavaScript, HTML, and CSS.” —— Electron 官方 Slogan\n\n\n一、Electron 简介Electron 结合了 Chromium 用于渲染页面和 Node.js 用于操作底层系统。\n\nChromium: 提供强大的 Web 渲染能力，负责界面显示。\nNode.js: 提供访问操作系统底层 API 的能力，例如文件系统、网络、进程管理等。\n\n这种结合使得 Web 开发者能够轻松地构建功能丰富的桌面应用程序，并且这些应用可以运行在 Windows、macOS 和 Linux 三大主流操作系统上。\n二、核心概念Electron 应用主要由以下几个核心概念构成：\n1. 主进程 (Main Process)\n唯一性: 一个 Electron 应用只有一个主进程。\n入口点: 应用程序的入口文件 (main.js 或你配置的其他文件) 运行在主进程中。\nNode.js 环境: 主进程是一个完整的 Node.js 环境，可以访问所有 Node.js API 和 Electron 提供的特定模块（如 app, BrowserWindow, Menu 等）。\n管理窗口: 主进程负责创建和管理渲染进程（即浏览器窗口）。\n不能直接访问 DOM: 主进程没有浏览器环境，也无法直接访问 DOM。\n全局应用生命周期: 管理应用的整个生命周期，包括启动、关闭、最小化、最大化等。\n\n2. 渲染进程 (Renderer Process)\n多重性: 每个 Electron 窗口（BrowserWindow 实例）都运行一个独立的渲染进程。\nWeb 环境: 渲染进程本质上就是一个 Chromium 浏览器实例，用于加载和渲染 Web 页面（HTML, CSS, JavaScript）。\n有限的 Node.js 环境: 默认情况下，渲染进程中的 JavaScript 代码不能直接访问 Node.js API。为了安全考虑，需要通过 contextBridge 等方式暴露特定功能。\n可访问 DOM: 与普通浏览器环境一样，可以直接访问 DOM。\n独立的沙箱: 每个渲染进程都是独立的，一个渲染进程崩溃不会影响其他渲染进程。\n\n3. IPC (Inter-Process Communication)由于主进程和渲染进程运行在不同的环境中，它们之间需要一种机制来通信，这就是 IPC。\n\nipcMain: 用于主进程发送和接收消息。\nipcRenderer: 用于渲染进程发送和接收消息。\n\n通信方式:\n\n渲染进程向主进程发送消息（异步）:\n渲染进程: ipcRenderer.send(&#39;some-channel&#39;, arg1, arg2)\n主进程: ipcMain.on(&#39;some-channel&#39;, (event, arg1, arg2) =&gt; &#123; /* 处理 */ event.sender.send(&#39;reply-channel&#39;, &#39;reply-data&#39;); &#125;)\n\n\n渲染进程向主进程发送消息并等待回复（同步，不推荐）:\n渲染进程: const result = ipcRenderer.sendSync(&#39;some-sync-channel&#39;, arg)\n主进程: ipcMain.on(&#39;some-sync-channel&#39;, (event, arg) =&gt; &#123; event.returnValue = &#39;some-result&#39;; &#125;)\n警告: 同步 IPC 会阻塞渲染进程，可能导致界面卡顿，应尽量避免使用。\n\n\n主进程向渲染进程发送消息:\n主进程: mainWindow.webContents.send(&#39;some-channel&#39;, arg1, arg2)\n渲染进程: ipcRenderer.on(&#39;some-channel&#39;, (event, arg1, arg2) =&gt; &#123; /* 处理 */ &#125;)\n\n\n\n4. 预加载脚本 (Preload Script)\n角色: 这是一个特殊的 JavaScript 文件，在渲染进程加载网页内容之前，于一个独立的、安全的上下文 (isolated world) 中运行。\n目的:\n桥接主进程和渲染进程，安全地将 Node.js API 或自定义函数暴露给渲染进程的 window 对象，而不会污染全局环境或给予渲染进程完全的 Node.js 访问权限。\n在加载页面内容之前，进行一些必要的初始化操作。\n\n\n配置: 在 BrowserWindow 的 webPreferences.preload 选项中指定。\n重要API: contextBridge 用于安全地暴露 API。\n\n示例（preload.js）:\nconst &#123; contextBridge, ipcRenderer &#125; = require(&#x27;electron&#x27;);contextBridge.exposeInMainWorld(&#x27;electronAPI&#x27;, &#123;  sendMessageToMain: (message) =&gt; ipcRenderer.send(&#x27;msg-from-renderer&#x27;, message),  onReplyFromMain: (callback) =&gt; ipcRenderer.on(&#x27;msg-from-main-reply&#x27;, (_event, value) =&gt; callback(value))&#125;);\n示例（渲染进程）:\n// 在你的Web页面脚本中window.electronAPI.sendMessageToMain(&#x27;Hello from renderer!&#x27;);window.electronAPI.onReplyFromMain((reply) =&gt; &#123;  console.log(&#x27;Received reply from main:&#x27;, reply);&#125;);\n\n三、开发一个简单的 Electron 应用1. 初始化项目mkdir my-electron-appcd my-electron-appnpm init -ynpm install electron --save-dev\n\n2. 创建主进程文件 (main.js)const &#123; app, BrowserWindow, ipcMain &#125; = require(&#x27;electron&#x27;);const path = require(&#x27;path&#x27;);function createWindow () &#123;  const mainWindow = new BrowserWindow(&#123;    width: 800,    height: 600,    webPreferences: &#123;      preload: path.join(__dirname, &#x27;preload.js&#x27;), // 引入预加载脚本      nodeIntegration: false, // 重要的安全考量：禁用 Node.js 集成      contextIsolation: true // 重要的安全考量：启用上下文隔离    &#125;  &#125;);  // 加载应用的 index.html 文件  mainWindow.loadFile(&#x27;index.html&#x27;);  // 打开开发者工具 (可选)  // mainWindow.webContents.openDevTools();  // 示例：主进程接收渲染进程消息  ipcMain.on(&#x27;msg-from-renderer&#x27;, (event, message) =&gt; &#123;    console.log(&#x27;Message from renderer:&#x27;, message);    // 回复渲染进程    event.sender.send(&#x27;msg-from-main-reply&#x27;, &#x27;Hello from main process!&#x27;);  &#125;);&#125;// 当 Electron 应用准备就绪时创建窗口app.whenReady().then(() =&gt; &#123;  createWindow();  app.on(&#x27;activate&#x27;, () =&gt; &#123;    // 在 macOS 上，当点击 dock 中的应用图标时，如果没有其他打开的窗口，则通常在应用程序中重新创建一个窗口。    if (BrowserWindow.getAllWindows().length === 0) &#123;      createWindow();    &#125;  &#125;);&#125;);// 当所有窗口被关闭时退出应用app.on(&#x27;window-all-closed&#x27;, () =&gt; &#123;  if (process.platform !== &#x27;darwin&#x27;) &#123;    app.quit();  &#125;&#125;);\n\n3. 创建预加载脚本 (preload.js)const &#123; contextBridge, ipcRenderer &#125; = require(&#x27;electron&#x27;);contextBridge.exposeInMainWorld(&#x27;electronAPI&#x27;, &#123;  sendMessageToMain: (message) =&gt; ipcRenderer.send(&#x27;msg-from-renderer&#x27;, message),  onReplyFromMain: (callback) =&gt; ipcRenderer.on(&#x27;msg-from-main-reply&#x27;, (_event, value) =&gt; callback(value))&#125;);\n\n4. 创建渲染进程文件 (index.html)&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;title&gt;My Electron App&lt;/title&gt;    &lt;link rel=&quot;stylesheet&quot; href=&quot;style.css&quot;&gt;&lt;/head&gt;&lt;body&gt;    &lt;h1&gt;Hello Electron!&lt;/h1&gt;    &lt;p&gt;This is a simple Electron application.&lt;/p&gt;    &lt;button id=&quot;send-btn&quot;&gt;Send Message to Main&lt;/button&gt;    &lt;p id=&quot;reply-status&quot;&gt;&lt;/p&gt;    &lt;script src=&quot;renderer.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;\n\n5. 创建渲染进程脚本 (renderer.js)// 注意：这里我们通过预加载脚本暴露的 &#x27;electronAPI&#x27; 访问主进程功能document.getElementById(&#x27;send-btn&#x27;).addEventListener(&#x27;click&#x27;, () =&gt; &#123;  window.electronAPI.sendMessageToMain(&#x27;Button clicked!&#x27;);  document.getElementById(&#x27;reply-status&#x27;).innerText = &#x27;Message sent to main process...&#x27;;&#125;);window.electronAPI.onReplyFromMain((reply) =&gt; &#123;  document.getElementById(&#x27;reply-status&#x27;).innerText = `Received reply: &quot;$&#123;reply&#125;&quot;`;  console.log(&#x27;Reply from main:&#x27;, reply);&#125;);console.log(&#x27;Renderer process loaded.&#x27;);\n\n6. 配置 package.json在 package.json 中添加一个 main 字段指向主进程文件，并添加启动脚本：\n&#123;  &quot;name&quot;: &quot;my-electron-app&quot;,  &quot;version&quot;: &quot;1.0.0&quot;,  &quot;description&quot;: &quot;A minimal Electron application&quot;,  &quot;main&quot;: &quot;main.js&quot;, // &lt;-- 这里指向你的主进程文件  &quot;scripts&quot;: &#123;    &quot;start&quot;: &quot;electron .&quot;, // &lt;-- 添加启动脚本    &quot;build&quot;: &quot;electron-builder&quot; // for packaging, will discuss later  &#125;,  &quot;keywords&quot;: [],  &quot;author&quot;: &quot;Your Name&quot;,  &quot;license&quot;: &quot;MIT&quot;,  &quot;devDependencies&quot;: &#123;    &quot;electron&quot;: &quot;^29.0.1&quot;  &#125;&#125;\n\n7. 运行应用npm start\n\n四、安全考量由于 Electron 应用运行在桌面环境中，并且可以访问 Node.js API，安全性是至关重要的。\n\n禁用 nodeIntegration: 在 BrowserWindow 的 webPreferences 中，始终将 nodeIntegration 设置为 false。这是最基本的安全措施，可以防止渲染进程直接访问 Node.js API。\n启用 contextIsolation: 在 BrowserWindow 的 webPreferences 中，始终将 contextIsolation 设置为 true。这会确保你的预加载脚本和网页内容运行在完全隔离的 JavaScript 上下文中，防止恶意脚本通过原型链攻击或全局变量污染来获取 Node.js 访问权限。\n使用 contextBridge: 通过预加载脚本中的 contextBridge 来安全地暴露你需要给渲染进程使用的功能，而不是直接将 Node.js 模块赋值给 window 对象。\n限制 remote 模块: remote 模块（在 Electron 12.0.0 之后已被废弃，并拆分为 @electron/remote）允许渲染进程直接使用主进程模块，这带来了巨大的安全隐患。如果必须使用，请严格控制其提供的功能。\n验证外部内容: 如果你的应用需要加载外部的或用户生成的内容，务必对其进行严格的沙箱隔离，或者只使用 webview 标签且不启用 Node.js 集成。\n内容安全策略 (CSP): 使用 Content-Security-Policy HTTP 头来限制网页可以加载的资源（脚本、样式、图片等），可以有效防御 XSS 攻击。&lt;meta http-equiv=&quot;Content-Security-Policy&quot; content=&quot;script-src &#x27;self&#x27; &#x27;unsafe-inline&#x27;; object-src &#x27;self&#x27;&quot;&gt;\n会话管理: 使用 session 模块来管理 cookies, 缓存, 下载等，并可以设置自定义协议和权限。\n\n五、打包与分发 (Packaging)当应用开发完成后，你需要将其打包成可执行文件，以便在不同操作系统上分发。常用的打包工具是 electron-builder 或 electron-packager。electron-builder 功能更强大，支持自动更新、NSIS 安装包等。\n使用 electron-builder\n安装:npm install electron-builder --save-dev\n配置 package.json:在 package.json 中添加 build 字段，进行打包配置。&#123;  &quot;name&quot;: &quot;my-electron-app&quot;,  &quot;version&quot;: &quot;1.0.0&quot;,  &quot;description&quot;: &quot;A minimal Electron application&quot;,  &quot;main&quot;: &quot;main.js&quot;,  &quot;scripts&quot;: &#123;    &quot;start&quot;: &quot;electron .&quot;,    &quot;build&quot;: &quot;electron-builder -mwl&quot; // -mwl 分别代表打包 Windows, macOS, Linux  &#125;,  &quot;devDependencies&quot;: &#123;    &quot;electron&quot;: &quot;^29.0.1&quot;,    &quot;electron-builder&quot;: &quot;^23.6.0&quot;  &#125;,  &quot;build&quot;: &#123;    &quot;appId&quot;: &quot;com.yourname.yourapp&quot;, // 你的应用唯一标识符    &quot;productName&quot;: &quot;MyElectronApp&quot;,  // 产品名称    &quot;directories&quot;: &#123;      &quot;output&quot;: &quot;dist&quot; // 输出目录    &#125;,    &quot;files&quot;: [      &quot;main.js&quot;,      &quot;preload.js&quot;,      &quot;index.html&quot;,      &quot;renderer.js&quot;,      &quot;package.json&quot;,      &quot;assets/**&quot;, // 如果有图片等资源      &quot;node_modules/**/*&quot; // 依赖通常会自动包含，但可以明确指定    ],    &quot;win&quot;: &#123;      &quot;target&quot;: [&quot;nsis&quot;, &quot;zip&quot;],      &quot;icon&quot;: &quot;build/icon.ico&quot; // Windows 图标路径    &#125;,    &quot;mac&quot;: &#123;      &quot;target&quot;: [&quot;dmg&quot;, &quot;zip&quot;],      &quot;icon&quot;: &quot;build/icon.icns&quot; // macOS 图标路径    &#125;,    &quot;linux&quot;: &#123;      &quot;target&quot;: [&quot;AppImage&quot;, &quot;deb&quot;], // 通常 AppImage 兼容性较好      &quot;icon&quot;: &quot;build/icon.png&quot; // Linux 图标路径    &#125;  &#125;&#125;\n创建图标: 准备 build 文件夹和对应的图标文件 (icon.ico, icon.icns, icon.png)。\n运行打包命令:npm run build\n打包完成后，会在 dist 目录下找到生成的可执行安装文件。\n\n六、最佳实践与常见问题1. 结构化项目随着应用功能的增加，建议对项目进行模块化，将不同的功能或组件分离到不同的文件或文件夹中。\nmy-electron-app/├── main.js         # 主进程入口├── preload.js      # 预加载脚本├── package.json├── index.html      # 渲染进程 HTML├── renderer.js     # 渲染进程 JavaScript├── assets/         # 静态资源 (图片, 字体等)├── src/│   ├── main/       # 主进程相关模块│   │   ├── windows/    # 窗口管理器│   │   └── ipc/        # IPC 处理器│   └── renderer/   # 渲染进程相关模块 (例如 React/Vue 组件)│       ├── components/│       └── views/└── build/          # 图标文件\n\n2. 使用框架或库对于复杂的 UI，你可以将 React, Vue, Angular 等前端框架集成到 Electron 的渲染进程中，像开发普通网页一样进行开发。\n3. 应用启动性能优化\n懒加载: 仅在需要时才加载某些模块或组件。\n减小包体积: 优化 Webpack 配置，移除不必要的依赖，进行代码分割。\n使用缓存: 缓存启动资源。\n显示启动画面: 在应用加载时显示一个 splash screen，提高用户体验。\n\n4. 调试\n主进程: 可以使用 VS Code 的调试功能（配置 launch.json）或 Node.js 的 inspector 模式 (electron --inspect .)。\n渲染进程: 直接在应用的窗口中使用 Chromium 开发者工具（mainWindow.webContents.openDevTools()）。\n\n5. 自动更新electron-builder 内置了对自动更新的支持（基于 electron-updater）。你需要提供一个更新服务器或使用第三方服务（如 Squirrel.Windows, Squirrel.Mac）来托管更新文件。\n6. 系统托盘 (Tray) 和菜单 (Menu)Electron 提供了 Tray 和 Menu 模块，可以在主进程中创建系统托盘图标和自定义应用菜单，增加桌面应用的原生感。\n7. Node.js process 对象process 对象在主进程和渲染进程都被 Electron 修改过。在渲染进程中，process.type 为 &#39;renderer&#39;，在主进程中为 &#39;browser&#39;。其他像 process.platform, process.arch 等可以用来判断应用运行环境。\n七、总结Electron 为 Web 开发者打开了桌面应用开发的大门。它使得一次编写、多平台部署成为可能，极大地提高了开发效率。然而，其便利性也伴随着安全性、性能优化等挑战。通过理解 Electron 的核心概念（主进程、渲染进程、IPC、预加载脚本）、遵循安全最佳实践，并善用其提供的强大工具和模块，你将能够构建出高质量、功能丰富的跨平台桌面应用程序。\n","categories":["桌面开发"],"tags":["TypeScript","前端技术","2024","JavaScript","Electron"]},{"title":"Vue3响应式原理深度解析","url":"/2024/2024-08-11_Vue3%E5%93%8D%E5%BA%94%E5%BC%8F%E5%8E%9F%E7%90%86%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","content":"\nVue 3 响应式系统是其 MVVM 框架的核心基石，它让前端开发者能够以声明式的方式构建用户界面，而无需手动操作 DOM。与 Vue 2 基于 Object.defineProperty 的实现不同，Vue 3 借助 ES6 的 Proxy 对象，彻底重构了响应式系统，带来了更高性能、更强大的功能和更灵活的 API。\n\n“Vue 3 的响应式系统是一个优雅而强大的解决方案，它通过 Proxy 和一套高效的依赖追踪机制，实现了数据与视图的紧密双向绑定，极大地提升了开发体验。”\n\n\n一、响应式系统的核心概念在深入 Vue 3 响应式原理之前，我们需要理解几个核心概念：\n\n数据劫持 (Data Interception)：当访问或修改数据时，能够执行自定义逻辑。\n依赖收集 (Dependency Collection)：追踪哪些组件或函数正在使用哪些响应式数据。\n派发更新 (Trigger Update)：当响应式数据发生变化时，通知所有依赖于该数据的组件或函数进行更新。\n\n二、Vue 2 与 Vue 3 响应式原理对比理解 Vue 3 的优势，最好从对比 Vue 2 开始。\n2.1 Vue 2：基于 Object.defineProperty\n实现方式：在初始化时遍历数据的每个属性，使用 Object.defineProperty 为每个属性设置 getter 和 setter。\n优点：\n在 ES5 环境下工作良好，兼容性好。\n\n\n缺点：\n无法检测到属性的添加或删除：由于 defineProperty 只能劫持已存在的属性，直接添加或删除对象属性无法触发视图更新。需要使用 $set 或 $delete。\n无法监听数组索引和长度变化：对于 arr[index] = newValue 或 arr.length = 0 这样的操作，无法被 defineProperty 捕获。Vue 2 通过劫持数组的原生方法（如 push, pop, splice 等）来解决。\n深层嵌套对象性能开销大：初始化时需要深层递归遍历所有属性，如果数据层级很深或数据量很大，性能开销较大。\n编码复杂：需要处理各种边界情况和数组方法的劫持。\n\n\n\n2.2 Vue 3：基于 Proxy\n实现方式：使用 ES6 的 Proxy 对象，代理整个目标对象，拦截所有对该对象的操作 (如 get、set、deleteProperty、has 等)。\n优点：\n能够检测到属性的添加和删除：Proxy 可以拦截 set 和 deleteProperty 操作，因此无论是修改已有属性还是添加&#x2F;删除新属性，都能被捕获。\n能够监听数组索引和长度变化：Proxy 可以拦截get （当访问数组索引时）和 set（修改索引或长度时）操作。\n惰性求值 (Lazy Evaluation)：Proxy 只在访问数据时劫持，不需要在初始化时深层递归遍历，大大减少了初始化开销。只有当属性被访问时，才会被“代理”。\n原生 API 支持：直接使用原生 Proxy，API 更简洁，更易于维护。\n更好的 TypeScript 支持：Proxy 的类型推导能力更强。\n\n\n缺点：\n浏览器兼容性要求：Proxy 是 ES6 新特性，无法在 IE 浏览器中工作。\n性能开销：虽然初始化开销小，但在某些极端情况下，Proxy 的拦截器调用可能会比直接的 defineProperty 多一些，但通常在现代浏览器中性能表现更优。\n\n\n\n三、Vue 3 响应式系统的核心 APIVue 3 响应式系统通过 reactive 和 ref 这两个核心 API 暴露给开发者。\n3.1 reactive() 函数reactive() 用于创建一个响应式对象或数组。\nimport &#123; reactive &#125; from &#x27;vue&#x27;;const state = reactive(&#123;  count: 0,  user: &#123;    name: &#x27;Vue&#x27;,    age: 3,  &#125;,  items: [&#x27;apple&#x27;, &#x27;banana&#x27;]&#125;);console.log(state.count); // 0state.count++;           // 触发更新state.user.age = 4;      // 触发更新state.items.push(&#x27;orange&#x27;); // 触发更新state.newProp = &#x27;hello&#x27;; // 也可以响应式地添加新属性delete state.user.name;  // 也可以响应式地删除属性\n\n特点：\n\n深层响应式：reactive 会深层地转换其所有嵌套属性为响应式。\n只能作用于对象类型：参数必须是对象 (plain objects, arrays, Map, Set)。对于原始类型（如 string, number, boolean），请使用 ref。\n解构丢失响应性：直接解构 state 对象会使其属性失去响应性，因为解构出的变量不再是 Proxy 对象的属性了。let &#123; count &#125; = state; // count 此时是 0 (原始值)，不再是响应式的count++; // 不会影响 state.count，也不会触发更新\n解决办法是使用 toRefs 或 toRef。\n\n3.2 ref() 函数ref() 用于创建一个包装原始类型值（或对象）的响应式引用。\nimport &#123; ref &#125; from &#x27;vue&#x27;;const count = ref(0);const message = ref(&#x27;Hello Vue 3&#x27;);const user = ref(&#123; name: &#x27;Ref User&#x27; &#125;); // 也可以包装对象console.log(count.value); // 访问值时需要 .valuecount.value++;            // 修改值时需要 .value，并触发更新console.log(message.value);message.value = &#x27;New message&#x27;;console.log(user.value.name);user.value.name = &#x27;Updated Ref User&#x27;; // 内部对象仍由 reactive 处理\n\n特点：\n\n包装原始值：主要用于使原始类型值具有响应性。\n通过 .value 访问和修改：在 JavaScript 中访问或修改 ref 的值时，必须使用 .value 属性。\n在模板中自动解包：在 Vue 模板中，如果 ref 处于顶层属性位置，会自动解包，无需 .value。&lt;template&gt;  &lt;div&gt;Count: &#123;&#123; count &#125;&#125;&lt;/div&gt; &lt;!-- 模板中直接使用 count --&gt;&lt;/template&gt;\n内部 reactive 转换：如果 ref 包装的是一个对象，Vue 会自动地将这个对象用 reactive 转换，使其内部属性也具有深层响应性。\n\n3.3 toRefs() &#x2F; toRef() 函数\ntoRefs(reactiveObject)：将一个响应式对象的所有顶层属性转换为 ref 对象。这在解构响应式对象时非常有用，可以保持响应性。import &#123; reactive, toRefs &#125; from &#x27;vue&#x27;;const state = reactive(&#123;  foo: 1,  bar: 2&#125;);const stateAsRefs = toRefs(state); // stateAsRefs 是 &#123; foo: Ref&lt;1&gt;, bar: Ref&lt;2&gt; &#125;let &#123; foo, bar &#125; = stateAsRefs; // foo 和 bar 都是 Ref 对象，可以被解构console.log(foo.value); // 访问时仍需 .valuefoo.value++;            // 触发更新console.log(state.foo); // 2\ntoRef(reactiveObject, key)：为响应式对象的一个属性创建 ref。import &#123; reactive, toRef &#125; from &#x27;vue&#x27;;const state = reactive(&#123;  foo: 1,  bar: 2&#125;);const fooRef = toRef(state, &#x27;foo&#x27;);console.log(fooRef.value); // 1fooRef.value++;console.log(state.foo); // 2\n\n四、Vue 3 响应式原理内部实现Vue 3 的响应式系统由 @vue/reactivity 包提供，其核心是 Proxy 和一套高效的依赖追踪机制。\n4.1 reactive 内部工作原理当我们调用 reactive(obj) 时：\n\n创建 Proxy：Vue 会返回一个 obj 的 Proxy 实例。这个 Proxy 会拦截对 obj 的所有操作。\ntrack (依赖收集)：\n当 Proxy 对象的属性被读取 (通过 get 拦截器) 时，Vue 会检查当前是否存在一个活跃的“副作用函数” (effect function，也就是需要响应式更新的函数或组件渲染函数)。\n如果存在，Vue 就会将这个副作用函数与当前被读取的属性建立依赖关系。这个关系存储在一个全局的 WeakMap 和 Map 结构中。\ntargetMap (WeakMap): target -&gt; Map (每个响应式对象)\ndepsMap (Map): key -&gt; Set (每个属性对应的副作用函数集合)\n\n\n\n\ntrigger (派发更新)：\n当 Proxy 对象的属性被修改 (set 拦截器) 或删除 (deleteProperty 拦截器) 时，Vue 会查找 depsMap，找到所有依赖于该属性的副作用函数。\n然后，Vue 会执行这些副作用函数，通常会导致组件重新渲染。\n\n\n\n简化的伪代码：\nconst targetMap = new WeakMap(); // 存储对象及其属性的依赖function track(target, key) &#123;  if (!activeEffect) return; // 没有活跃的副作用函数，无需收集  let depsMap = targetMap.get(target);  if (!depsMap) &#123;    targetMap.set(target, (depsMap = new Map()));  &#125;  let dep = depsMap.get(key);  if (!dep) &#123;    depsMap.set(key, (dep = new Set()));  &#125;  dep.add(activeEffect); // 将当前副作用函数添加到依赖集合中&#125;function trigger(target, key) &#123;  const depsMap = targetMap.get(target);  if (!depsMap) return;  const dep = depsMap.get(key);  if (dep) &#123;    dep.forEach(effect =&gt; effect()); // 执行所有依赖的副作用函数  &#125;&#125;function reactive(obj) &#123;  return new Proxy(obj, &#123;    get(target, key, receiver) &#123;      const res = Reflect.get(target, key, receiver);      track(target, key); // 依赖收集      // 如果是对象，继续对内部对象进行 reactive 转换      return typeof res === &#x27;object&#x27; &amp;&amp; res !== null ? reactive(res) : res;    &#125;,    set(target, key, value, receiver) &#123;      const res = Reflect.set(target, key, value, receiver);      trigger(target, key); // 派发更新      return res;    &#125;,    deleteProperty(target, key) &#123;      const res = Reflect.deleteProperty(target, key);      trigger(target, key); // 派发更新      return res;    &#125;  &#125;);&#125;\n\n4.2 ref 内部工作原理ref() 的实现比 reactive() 稍微复杂一点：\n\n创建 RefImpl 实例：ref 返回一个 RefImpl 类的实例。这个实例有一个 _value 属性来存储实际的值。\ngetter 和 setter：RefImpl 的 value 属性通过 getter 和 setter 实现了依赖收集和派发更新。\n自动 reactive 转换：\n在 setter 中，如果新设置的值是一个对象，Vue 会自动将其转换为 reactive 对象。\n这意味着 ref(obj) 实际上是 reactive(obj) 加上一个 RefImpl 包装。\n\n\n模板自动解包：Vue 编译器在处理模板时，会识别出顶层的 ref 对象，并在编译时自动添加 .value，所以你在模板中无需手动写 .value。\n\n简化的伪代码：\nclass RefImpl &#123;  constructor(value) &#123;    this._value = convert(value); // 如果是对象，会用 reactive() 转换    this.dep = new Set(); // 存储依赖这个 ref 的副作用函数  &#125;  get value() &#123;    trackRef(this); // 收集依赖    return this._value;  &#125;  set value(newValue) &#123;    if (newValue !== this._value) &#123;      this._value = convert(newValue); // 如果是对象，再次转换      triggerRef(this); // 派发更新    &#125;  &#125;&#125;function ref(raw) &#123;  return new RefImpl(raw);&#125;function convert(val) &#123;  return typeof val === &#x27;object&#x27; &amp;&amp; val !== null ? reactive(val) : val;&#125;// trackRef 和 triggerRef 类似于 track 和 trigger，但作用于 RefImpl 实例的 depfunction trackRef(refInstance) &#123;    if (activeEffect) &#123;        refInstance.dep.add(activeEffect);    &#125;&#125;function triggerRef(refInstance) &#123;    refInstance.dep.forEach(effect =&gt; effect());&#125;\n\n五、深入理解依赖追踪 (Effect Functions)在 Vue 3 响应式系统中，组件的渲染函数和 watchEffect、watch、computed 等 API 内部的函数都被视为“副作用函数”（或者可称为“响应式作用” Effect Function）。\n\neffect 函数：Vue 内部有一个 effect 函数，它接收一个函数作为参数，并在执行该函数时，将其设置为当前的 activeEffect。当 activeEffect 存在时，所有被访问的响应式属性都会将 activeEffect 添加为自己的依赖。import &#123; effect, reactive &#125; from &#x27;vue&#x27;;const state = reactive(&#123; count: 0 &#125;);effect(() =&gt; &#123;  // 这是一个副作用函数  // 在这里访问 state.count，就会将这个 effect 函数添加到 state.count 的依赖集合中  console.log(&#x27;Count changed:&#x27;, state.count);&#125;);state.count++; // 输出 &quot;Count changed: 1&quot;\n调度器 (Scheduler)：当一个响应式数据被修改并触发更新时，绑定的 effect 函数并不会立即执行。Vue 内部有一个调度器，它会将所有触发的 effect 函数放入一个工作队列中，并在下一个微任务（microtask）或宏任务（macrotask）周期统一执行，以优化性能，避免不必要的重复渲染。\n\n六、总结Vue 3 的响应式系统凭借 ES6 Proxy 的强大能力，彻底解决了 Vue 2 中 Object.defineProperty 的痛点，带来了：\n\n更全面的响应式支持：能够监听属性的添加、删除和数组的变化。\n更高的性能：初始化时无需深层递归，采用惰性求值。\n更简洁的 API：通过 reactive 和 ref 提供了清晰的响应式声明方式。\n更强大的功能：为 Composition API 提供了坚实的基础，使得逻辑复用和组织更加灵活。\n\n理解 Proxy 的拦截机制、track (依赖收集) 和 trigger (派发更新) 的过程，以及 reactive 和 ref 这两个核心 API 的作用和内部实现，是掌握 Vue 3 并高效开发的关键。\n","categories":["前端技术","Vue"],"tags":["前端技术","2024","JavaScript","Vue"]},{"title":"GoLang gRPC 详解：构建高性能、跨语言的微服务","url":"/2024/2024-09-02_GoLang%20gRPC%20%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%9E%84%E5%BB%BA%E9%AB%98%E6%80%A7%E8%83%BD%E3%80%81%E8%B7%A8%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BE%AE%E6%9C%8D%E5%8A%A1y/","content":"\n本文将带你深入了解 Google Remote Procedure Call (gRPC) 在 Go 语言中的应用。我们将从 gRPC 的核心概念、工作原理讲起，逐步讲解其与 Protocol Buffers 的关系、四种通信模式，并通过 Go 语言示例代码，帮助你构建高性能、跨语言的微服务。\n\n随着微服务架构的流行，服务间通信变得愈发重要。传统的 RESTful API 虽然普及，但在性能、类型安全、多语言支持等方面存在一些局限。gRPC 作为 Google 开源的高性能 RPC (Remote Procedure Call) 框架，以其基于 HTTP&#x2F;2、Protocol Buffers 和多种语言支持的优势，迅速成为构建分布式系统和服务间通信的有力选择，尤其在 Go 语言生态中备受青睐。\n\n\n一、什么是 gRPC？gRPC (gRPC Remote Procedure Calls) 是一个现代的开源高性能 RPC 框架，可以在任何环境中运行。它允许客户端和服务端透明地通信，并使构建连接系统变得容易。\ngRPC 的核心特征：\n\n高性能：基于 HTTP&#x2F;2 协议，支持多路复用、头部压缩、服务器推送等特性，减少了网络开销和延迟。\nProtocol Buffers (ProtoBuf)：默认使用 ProtoBuf 作为接口定义语言 (IDL) 和数据序列化格式。ProtoBuf 是一种高效、紧凑的序列化协议，比 JSON 或 XML 更小、更快。\n多语言支持：通过 ProtoBuf 生成器，gRPC 支持 Go, Java, Python, C++, C#, Node.js 等多种主流编程语言，实现了天然的跨语言互操作性。\n接口定义：通过 .proto 文件定义服务接口、方法和消息结构，确保了客户端和服务端之间严格的类型契约。\n四种服务方法：支持 Unary (一元)、Server-Side Streaming (服务端流)、Client-Side Streaming (客户端流) 和 Bidirectional Streaming (双向流) 四种通信模式。\n插件式架构：支持插拔式的认证、负载均衡、可观测性等。\n\n二、gRPC 与 Go 语言的结合优势Go 语言天生为并发和网络编程而设计，与 gRPC 结合具有以下优势：\n\n高性能：Go 的协程 (goroutine) 和 Channel 机制非常适合处理高并发的 gRPC 请求，能够充分发挥 HTTP&#x2F;2 的多路复用能力。\n简洁的并发编程：Go 语言的并发模型使得编写流式 gRPC 服务变得简单直观。\n静态类型安全：ProtoBuf 生成的 Go 代码是强类型的，减少了运行时错误，提高了代码质量。\n开发效率：Go 语言的编译速度快，加上 ProtoBuf 自动代码生成，大大提高了开发效率。\n丰富的生态：Go 社区对 gRPC 有良好的支持和丰富的库。\n\n三、Protocol Buffers (ProtoBuf) 详解Protocol Buffers 是一种由 Google 开发的语言无关、平台无关、可扩展的结构化数据序列化机制，用于结构化数据。gRPC 默认使用它来定义服务接口和消息结构。\n3.1 .proto 文件语法一个 .proto 文件定义了 gRPC 服务及其消息。\n示例：user.proto\n// 指定 Protobuf 语法版本，目前通常是 proto3syntax = &quot;proto3&quot;;// Go 语言相关的包选项，生成代码的包名option go_package = &quot;.;pb&quot;; // .;pb 表示在当前目录下的pb子目录生成文件// 定义 gRPC 服务接口service UserService &#123;  // 定义 gRPC 方法。一个请求可以得到一个响应 (一元调用)  rpc GetUser (GetUserRequest) returns (UserResponse) &#123;&#125;  // 服务端流式调用：客户端发送一个请求，服务端持续返回多个响应  rpc ListUsers (ListUsersRequest) returns (stream UserResponse) &#123;&#125;  // 客户端流式调用：客户端持续发送多个请求，服务端返回一个响应  rpc CreateUsers (stream UserRequest) returns (CreateUsersResponse) &#123;&#125;  // 双向流式调用：客户端和服务端都可以持续发送和接收消息  rpc Chat (stream ChatMessage) returns (stream ChatMessage) &#123;&#125;&#125;// 消息定义：GetUserRequestmessage GetUserRequest &#123;  int32 id = 1; // 字段类型 字段名 = 字段编号&#125;// 消息定义：UserResponsemessage UserResponse &#123;  int32 id = 1;  string name = 2;  string email = 3;  // 嵌套消息  Address address = 4;&#125;// 消息定义：Addressmessage Address &#123;  string street = 1;  string city = 2;  string zip_code = 3;&#125;// 消息定义：ListUsersRequestmessage ListUsersRequest &#123;  // 可选字段，例如分页参数  int32 page_size = 1;  int32 page_token = 2;&#125;// 消息定义：UserRequest (用于创建用户，包含不带ID的用户信息)message UserRequest &#123;  string name = 1;  string email = 2;  Address address = 3;&#125;// 消息定义：CreateUsersResponsemessage CreateUsersResponse &#123;  repeated int32 created_user_ids = 1; // repeated 关键字表示一个可重复的字段（数组）  string message = 2;&#125;// 消息定义：ChatMessage (用于双向流)message ChatMessage &#123;  string sender = 1;  string content = 2;  int64 timestamp = 3;&#125;\n\n3.2 编译 .proto 文件为了在 Go 语言中使用这些定义，你需要使用 protoc (Protocol Buffers 编译器) 工具来生成 Go 代码。\n安装 Protoc 编译器：\n下载对应操作系统的 protoc 最新版本：https://github.com/protocolbuffers/protobuf/releases 并将其添加到系统 PATH。\n安装 Go 的 ProtoBuf 和 gRPC 插件：\ngo install google.golang.org/protobuf/cmd/protoc-gen-go@latestgo install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest\n请确保 $GOPATH/bin 路径已添加到 $PATH 环境变量中，这样 protoc 才能找到这些插件。\n编译文件：\n在 user.proto 文件所在的目录执行：\nprotoc --go_out=. --go_opt=paths=source_relative \\       --go-grpc_out=. --go-grpc_opt=paths=source_relative \\       user.proto\n\n这会生成两个文件：\n\nuser.pb.go：包含 ProtoBuf 消息结构、序列化&#x2F;反序列化方法等。\nuser_grpc.pb.go：包含 gRPC 服务接口（UserServiceClient 和 UserServiceServer）以及客户端&#x2F;服务端实现所需的桩 (stub) 代码。\n\n四、gRPC 四种通信模式 (Go 语言实现)4.1 Unary RPC (一元 RPC)客户端发送一个请求，服务器返回一个响应。最常见的 RPC 模式。\n4.1.1 定义 .proto (已在上面给出)service UserService &#123;  rpc GetUser (GetUserRequest) returns (UserResponse) &#123;&#125;&#125;message GetUserRequest &#123;  int32 id = 1;&#125;message UserResponse &#123;  int32 id = 1;  string name = 2;  string email = 3;  Address address = 4;&#125;\n\n4.1.2 服务端实现// server/main.gopackage mainimport (\t&quot;context&quot;\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;net&quot;\t&quot;google.golang.org/grpc&quot;\t&quot;google.golang.org/grpc/codes&quot;\t&quot;google.golang.org/grpc/status&quot;\tpb &quot;your_project_path/pb&quot; // 替换为你的项目路径)type server struct &#123;\tpb.UnimplementedUserServiceServer // 嵌入生成的Unimplemented...，用于向前兼容\tusers                           map[int32]*pb.UserResponse&#125;func newServer() *server &#123;\treturn &amp;server&#123;\t\tusers: map[int32]*pb.UserResponse&#123;\t\t\t1: &#123;Id: 1, Name: &quot;Alice&quot;, Email: &quot;alice@example.com&quot;, Address: &amp;pb.Address&#123;Street: &quot;123 Main St&quot;, City: &quot;Anytown&quot;, ZipCode: &quot;10001&quot;&#125;&#125;,\t\t\t2: &#123;Id: 2, Name: &quot;Bob&quot;, Email: &quot;bob@example.com&quot;, Address: &amp;pb.Address&#123;Street: &quot;456 Oak Ave&quot;, City: &quot;Otherville&quot;, ZipCode: &quot;20002&quot;&#125;&#125;,\t\t&#125;,\t&#125;&#125;// 实现 GetUser 方法func (s *server) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.UserResponse, error) &#123;\tlog.Printf(&quot;Received GetUser request for ID: %d&quot;, req.GetId())\tuser, ok := s.users[req.GetId()]\tif !ok &#123;\t\t// 返回一个带有gRPC特定错误码的错误\t\treturn nil, status.Errorf(codes.NotFound, &quot;User with ID %d not found&quot;, req.GetId())\t&#125;\treturn user, nil&#125;func main() &#123;\tlis, err := net.Listen(&quot;tcp&quot;, &quot;:50051&quot;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;failed to listen: %v&quot;, err)\t&#125;\ts := grpc.NewServer()\t// 注册服务实现\tpb.RegisterUserServiceServer(s, newServer())\tlog.Printf(&quot;server listening at %v&quot;, lis.Addr())\tif err := s.Serve(lis); err != nil &#123;\t\tlog.Fatalf(&quot;failed to serve: %v&quot;, err)\t&#125;&#125;\n\n4.1.3 客户端调用// client/main.gopackage mainimport (\t&quot;context&quot;\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;time&quot;\t&quot;google.golang.org/grpc&quot;\t&quot;google.golang.org/grpc/credentials/insecure&quot;\tpb &quot;your_project_path/pb&quot; // 替换为你的项目路径)func main() &#123;\t// 连接到 gRPC 服务器\tconn, err := grpc.Dial(&quot;:50051&quot;, grpc.WithTransportCredentials(insecure.NewCredentials()))\tif err != nil &#123;\t\tlog.Fatalf(&quot;did not connect: %v&quot;, err)\t&#125;\tdefer conn.Close()\t// 创建 UserService 客户端\tc := pb.NewUserServiceClient(conn)\t// 调用 GetUser 方法\tctx, cancel := context.WithTimeout(context.Background(), time.Second)\tdefer cancel()\t// 请求存在的用户\tuserResp, err := c.GetUser(ctx, &amp;pb.GetUserRequest&#123;Id: 1&#125;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;could not get user: %v&quot;, err)\t&#125;\tfmt.Printf(&quot;User 1: %s (%s)\\n&quot;, userResp.GetName(), userResp.GetEmail())\t// 请求不存在的用户\tuserResp2, err := c.GetUser(ctx, &amp;pb.GetUserRequest&#123;Id: 99&#125;)\tif err != nil &#123;\t\tlog.Printf(&quot;could not get user 99 (expected error): %v\\n&quot;, err)\t&#125; else &#123;\t\tfmt.Printf(&quot;User 99 (unexpected): %s (%s)\\n&quot;, userResp2.GetName(), userResp2.GetEmail())\t&#125;&#125;\n\n4.2 Server-Side Streaming RPC (服务端流式 RPC)客户端发送一个请求，服务器返回一系列响应消息。\n4.2.1 定义 .proto (已在上面给出)service UserService &#123;  rpc ListUsers (ListUsersRequest) returns (stream UserResponse) &#123;&#125;&#125;message ListUsersRequest &#123;  int32 page_size = 1;  int32 page_token = 2; // 模拟分页的起始点&#125;\n\n4.2.2 服务端实现// server/main.go (在现有代码基础上新增方法)// ...func (s *server) ListUsers(req *pb.ListUsersRequest, stream pb.UserService_ListUsersServer) error &#123;\tlog.Printf(&quot;Received ListUsers request: PageSize=%d, PageToken=%d&quot;, req.GetPageSize(), req.GetPageToken())\tpageSize := req.GetPageSize()\tif pageSize == 0 &#123;\t\tpageSize = 2 // 默认分页大小\t&#125;\tstartIndex := req.GetPageToken() // 简单模拟 page_token 为起始ID\tcount := 0\tfor id := startIndex + 1; ; id++ &#123;\t\tuser, ok := s.users[id]\t\tif !ok &#123;\t\t\tbreak // 没有更多用户了\t\t&#125;\t\tif count &gt;= int(pageSize) &#123;\t\t\tbreak // 达到分页大小\t\t&#125;\t\tif err := stream.Send(user); err != nil &#123;\t\t\tlog.Printf(&quot;Error sending user %d: %v&quot;, id, err)\t\t\treturn err\t\t&#125;\t\tlog.Printf(&quot;Sent user: %s&quot;, user.GetName())\t\tcount++\t\t// 模拟处理延迟\t\ttime.Sleep(100 * time.Millisecond)\t&#125;\treturn nil&#125;// ...\n\n4.2.3 客户端调用// client/main.go (在现有代码基础上新增调用)// ...func main() &#123;\t// ... (连接服务器，创建客户端)\t// 调用 ListUsers (服务端流)\tfmt.Println(&quot;\\n--- Calling ListUsers (Server-Side Streaming) ---&quot;)\tlistCtx, listCancel := context.WithTimeout(context.Background(), 5*time.Second)\tdefer listCancel()\tstream, err := c.ListUsers(listCtx, &amp;pb.ListUsersRequest&#123;PageSize: 3, PageToken: 0&#125;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;could not list users: %v&quot;, err)\t&#125;\tfor &#123;\t\tuser, err := stream.Recv()\t\tif err == io.EOF &#123; // 服务器发送完成信号\t\t\tbreak\t\t&#125;\t\tif err != nil &#123;\t\t\tlog.Fatalf(&quot;error receiving user from stream: %v&quot;, err)\t\t&#125;\t\tfmt.Printf(&quot;Received user from stream: %s (%s)\\n&quot;, user.GetName(), user.GetEmail())\t&#125;\tfmt.Println(&quot;ListUsers stream completed.&quot;)&#125;// ...\n\n4.3 Client-Side Streaming RPC (客户端流式 RPC)客户端发送一系列请求消息，服务器接收所有请求后返回一个响应。\n4.3.1 定义 .proto (已在上面给出)service UserService &#123;  rpc CreateUsers (stream UserRequest) returns (CreateUsersResponse) &#123;&#125;&#125;message UserRequest &#123;  string name = 1;  string email = 2;  Address address = 3;&#125;message CreateUsersResponse &#123;  repeated int32 created_user_ids = 1;  string message = 2;&#125;\n\n4.3.2 服务端实现// server/main.go (在现有代码基础上新增方法)// ...func (s *server) CreateUsers(stream pb.UserService_CreateUsersServer) error &#123;\tlog.Println(&quot;Received CreateUsers (Client-Side Streaming) request&quot;)\tvar createdIDs []int32\tlastID := int32(len(s.users)) // 模拟获取当前最大ID\tfor &#123;\t\treq, err := stream.Recv()\t\tif err == io.EOF &#123; // 客户端发送完成信号\t\t\tresp := &amp;pb.CreateUsersResponse&#123;\t\t\t\tCreatedUserIds: createdIDs,\t\t\t\tMessage:        fmt.Sprintf(&quot;Successfully created %d users&quot;, len(createdIDs)),\t\t\t&#125;\t\t\tlog.Printf(&quot;Client stream finished. Sending response: %s&quot;, resp.GetMessage())\t\t\treturn stream.SendAndClose(resp) // 发送最终响应并关闭流\t\t&#125;\t\tif err != nil &#123;\t\t\tlog.Fatalf(&quot;error receiving user from client stream: %v&quot;, err)\t\t&#125;\t\tlastID++\t\tnewUser := &amp;pb.UserResponse&#123;\t\t\tId:      lastID,\t\t\tName:    req.GetName(),\t\t\tEmail:   req.GetEmail(),\t\t\tAddress: req.GetAddress(),\t\t&#125;\t\ts.users[lastID] = newUser // 添加到内存中的用户列表\t\tcreatedIDs = append(createdIDs, lastID)\t\tlog.Printf(&quot;Created user: %s (ID: %d)&quot;, newUser.GetName(), newUser.GetId())\t\t// 模拟处理延迟\t\ttime.Sleep(50 * time.Millisecond)\t&#125;&#125;// ...\n\n4.3.3 客户端调用// client/main.go (在现有代码基础上新增调用)// ...func main() &#123;\t// ... (连接服务器，创建客户端)\t// 调用 CreateUsers (客户端流)\tfmt.Println(&quot;\\n--- Calling CreateUsers (Client-Side Streaming) ---&quot;)\tcreateCtx, createCancel := context.WithTimeout(context.Background(), 5*time.Second)\tdefer createCancel()\tcreateStream, err := c.CreateUsers(createCtx)\tif err != nil &#123;\t\tlog.Fatalf(&quot;could not create users stream: %v&quot;, err)\t&#125;\tusersToCreate := []*pb.UserRequest&#123;\t\t&#123;Name: &quot;Charlie&quot;, Email: &quot;charlie@example.com&quot;, Address: &amp;pb.Address&#123;Street: &quot;789 Pine Ln&quot;, City: &quot;Newtown&quot;, ZipCode: &quot;30003&quot;&#125;&#125;,\t\t&#123;Name: &quot;David&quot;, Email: &quot;david@example.com&quot;, Address: &amp;pb.Address&#123;Street: &quot;101 Maple Rd&quot;, City: &quot;Oldtown&quot;, ZipCode: &quot;40004&quot;&#125;&#125;,\t\t&#123;Name: &quot;Eve&quot;, Email: &quot;eve@example.com&quot;, Address: &amp;pb.Address&#123;Street: &quot;202 Birch Blvd&quot;, City: &quot;Midtown&quot;, ZipCode: &quot;50005&quot;&#125;&#125;,\t&#125;\tfor _, user := range usersToCreate &#123;\t\tif err := createStream.Send(user); err != nil &#123;\t\t\tlog.Fatalf(&quot;failed to send user %s: %v&quot;, user.GetName(), err)\t\t&#125;\t\tfmt.Printf(&quot;Sent user to create: %s\\n&quot;, user.GetName())\t\ttime.Sleep(100 * time.Millisecond) // 模拟发送间隔\t&#125;\t// 客户端完成发送，并等待服务器的最终响应\tcreateResp, err := createStream.CloseAndRecv()\tif err != nil &#123;\t\tlog.Fatalf(&quot;error closing stream and receiving response: %v&quot;, err)\t&#125;\tfmt.Printf(&quot;CreateUsers response: %s (IDs: %v)\\n&quot;, createResp.GetMessage(), createResp.GetCreatedUserIds())&#125;// ...\n\n4.4 Bidirectional Streaming RPC (双向流式 RPC)客户端和服务端都可以独立地发送和接收消息。两者都是流。\n4.4.1 定义 .proto (已在上面给出)service UserService &#123;  rpc Chat (stream ChatMessage) returns (stream ChatMessage) &#123;&#125;&#125;message ChatMessage &#123;  string sender = 1;  string content = 2;  int64 timestamp = 3;&#125;\n\n4.4.2 服务端实现// server/main.go (在现有代码基础上新增方法)// ...func (s *server) Chat(stream pb.UserService_ChatServer) error &#123;\tlog.Println(&quot;Received Chat (Bidirectional Streaming) request&quot;)\tfor &#123;\t\t// 1. 接收客户端消息\t\treq, err := stream.Recv()\t\tif err == io.EOF &#123;\t\t\tlog.Println(&quot;Client chat stream closed.&quot;)\t\t\treturn nil // 客户端关闭，服务端也处理完毕\t\t&#125;\t\tif err != nil &#123;\t\t\tlog.Printf(&quot;Error receiving chat message from client: %v&quot;, err)\t\t\treturn err\t\t&#125;\t\tlog.Printf(&quot;[Server Received] From %s: %s&quot;, req.GetSender(), req.GetContent())\t\t// 2. 模拟服务端响应\t\tserverMsg := &amp;pb.ChatMessage&#123;\t\t\tSender:    &quot;Server&quot;,\t\t\tContent:   fmt.Sprintf(&quot;Hello %s, I received your message: \\&quot;%s\\&quot;&quot;, req.GetSender(), req.GetContent()),\t\t\tTimestamp: time.Now().Unix(),\t\t&#125;\t\tif err := stream.Send(serverMsg); err != nil &#123;\t\t\tlog.Printf(&quot;Error sending chat message to client: %v&quot;, err)\t\t\treturn err\t\t&#125;\t\tlog.Printf(&quot;[Server Sent] To %s: %s&quot;, req.GetSender(), serverMsg.GetContent())\t\ttime.Sleep(100 * time.Millisecond) // 模拟处理延时\t&#125;&#125;// ...\n\n4.4.3 客户端调用// client/main.go (在现有代码基础上新增调用)// ...func main() &#123;\t// ... (连接服务器，创建客户端)\t// 调用 Chat (双向流)\tfmt.Println(&quot;\\n--- Calling Chat (Bidirectional Streaming) ---&quot;)\tchatCtx, chatCancel := context.WithCancel(context.Background())\tdefer chatCancel()\tchatStream, err := c.Chat(chatCtx)\tif err != nil &#123;\t\tlog.Fatalf(&quot;could not create chat stream: %v&quot;, err)\t&#125;\t// 协程用于接收服务端消息\tgo func() &#123;\t\tfor &#123;\t\t\tresp, err := chatStream.Recv()\t\t\tif err == io.EOF &#123;\t\t\t\tlog.Println(&quot;Server chat stream closed.&quot;)\t\t\t\treturn\t\t\t&#125;\t\t\tif err != nil &#123;\t\t\t\tlog.Printf(&quot;Error receiving chat message from server: %v&quot;, err)\t\t\t\treturn\t\t\t&#125;\t\t\tfmt.Printf(&quot;[Client Received] From %s: %s\\n&quot;, resp.GetSender(), resp.GetContent())\t\t&#125;\t&#125;()\t// 主协程用于发送客户端消息\tclientMessages := []*pb.ChatMessage&#123;\t\t&#123;Sender: &quot;ClientA&quot;, Content: &quot;Hi there!&quot;, Timestamp: time.Now().Unix()&#125;,\t\t&#123;Sender: &quot;ClientA&quot;, Content: &quot;How are you?&quot;, Timestamp: time.Now().Unix()&#125;,\t\t&#123;Sender: &quot;ClientA&quot;, Content: &quot;Just checking in.&quot;, Timestamp: time.Now().Unix()&#125;,\t&#125;\tfor _, msg := range clientMessages &#123;\t\tif err := chatStream.Send(msg); err != nil &#123;\t\t\tlog.Fatalf(&quot;failed to send chat message: %v&quot;, err)\t\t&#125;\t\tfmt.Printf(&quot;[Client Sent] From %s: %s\\n&quot;, msg.GetSender(), msg.GetContent())\t\ttime.Sleep(500 * time.Millisecond) // 模拟发送间隔\t&#125;\t// 等待一段时间，让接收协程处理完消息\ttime.Sleep(2 * time.Second)\tchatStream.CloseSend() // 客户端完成发送\tfmt.Println(&quot;Client chat stream closed send operation.&quot;)\t// 等待接收协程彻底结束\ttime.Sleep(2 * time.Second)\tfmt.Println(&quot;Chat stream completed.&quot;)&#125;// ...\n注意： 运行这些代码时，请确保将 pb &quot;your_project_path/pb&quot; 替换为实际的项目路径，例如 pb &quot;github.com/your_username/your_project_name/pb&quot;。\n五、错误处理与拦截器 (Interceptors)5.1 错误处理gRPC 推荐使用 google.golang.org/grpc/status 包提供的 API 进行规范的错误处理。\n\nstatus.Error(codes.Code, msg string)：创建带有 gRPC 错误码的错误。\nstatus.Errorf(codes.Code, format string, a ...any)：格式化创建错误。\ncodes.NotFound, codes.InvalidArgument, codes.Internal 等：预定义的 gRPC 错误码。\nstatus.FromError(err).Code()：在客户端解析接收到的 gRPC 错误码。\n\n5.2 拦截器 (Interceptors)拦截器类似于 HTTP 中间件，可以在 RPC 调用之前或之后执行逻辑，用于实现日志记录、认证、监控、错误处理等横切关注点。\nUnary Interceptors (一元拦截器)：\n// Server-Side Unary Interceptorfunc serverUnaryInterceptor(ctx context.Context, req interface&#123;&#125;, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface&#123;&#125;, error) &#123;\tlog.Printf(&quot;Server Unary Interceptor: Method %s called&quot;, info.FullMethod)\t// 在 RPC 调用前执行逻辑\tstart := time.Now()\tresp, err := handler(ctx, req) // 调用实际的 RPC 业务逻辑\t// 在 RPC 调用后执行逻辑\tlog.Printf(&quot;Server Unary Interceptor: Method %s finished in %v, error: %v&quot;, info.FullMethod, time.Since(start), err)\treturn resp, err&#125;// Client-Side Unary Interceptorfunc clientUnaryInterceptor(ctx context.Context, method string, req, reply interface&#123;&#125;, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error &#123;\tlog.Printf(&quot;Client Unary Interceptor: Calling Method %s&quot;, method)\t// 在 RPC 调用前执行逻辑\tstart := time.Now()\terr := invoker(ctx, method, req, reply, cc, opts...) // 调用实际的 RPC 请求发送\t// 在 RPC 调用后执行逻辑\tlog.Printf(&quot;Client Unary Interceptor: Method %s finished in %v, error: %v&quot;, method, time.Since(start), err)\treturn err&#125;\n\nStream Interceptors (流式拦截器)：\n流式拦截器相对复杂，需要包装 grpc.ServerStream 或 grpc.ClientStream。\n应用拦截器：\n// Server 注册拦截器s := grpc.NewServer(    grpc.UnaryInterceptor(serverUnaryInterceptor),    // grpc.StreamInterceptor(serverStreamInterceptor), // 如果有流式拦截器)// Client 注册拦截器conn, err := grpc.Dial(    &quot;:50051&quot;,    grpc.WithTransportCredentials(insecure.NewCredentials()),    grpc.WithUnaryInterceptor(clientUnaryInterceptor),    // grpc.WithStreamInterceptor(clientStreamInterceptor), // 如果有流式拦截器)\n\n六、更多特性与最佳实践\nDeadline (截止时间) 与超时：通过 context.WithTimeout 或 context.WithDeadline 在客户端设置 RPC 的截止时间，避免长时间阻塞。服务端会收到这个截止时间，并在超时后自动取消请求。\nTLS&#x2F;SSL 加密：生产环境中强烈建议使用 TLS&#x2F;SSL 对 gRPC 通信进行加密，确保数据安全。通过 grpc.WithTransportCredentials(credentials.NewClientTLSFromFile/NewServerTLSFromFile) 配置。\n认证：gRPC 支持多种认证机制，如基于 Token 的认证 (grpc.WithPerRPCCredentials)、SSL&#x2F;TLS 客户端证书认证等。\nMetadata (元数据)：可以在请求和响应中附加键值对形式的元数据，用于传递额外的非业务数据，如认证信息、跟踪 ID 等。\n错误重试与负载均衡：通常通过服务网格 (Service Mesh) 如 Istio 或客户端侧的负载均衡库实现。\n代码组织：将 .proto 文件、生成的 pb 文件、服务端实现和客户端调用分别放在合理的目录结构中。例如：.├── proto/│   └── user.proto├── pb/                 # 存放 protoc 生成的 go 代码│   ├── user.pb.go│   └── user_grpc.pb.go├── server/│   └── main.go└── client/    └── main.go\n版本管理：在 .proto 文件中使用 package 或添加版本字段来管理服务和消息的版本。\n\n七、总结Go 语言与 gRPC 的结合为构建高性能、可伸缩、跨语言的微服务提供了强大的解决方案。通过理解 Protocol Buffers 的接口定义、gRPC 的四种通信模式，以及 Go 语言的并发特性，开发者可以高效地构建出健壮的分布式系统。\n从简单的一元调用到复杂的双向流，gRPC 提供了灵活的通信模型，拦截器和错误处理机制则进一步增强了服务的可维护性和可靠性。无论你是要开发内部服务、API 网关，还是需要高并发的数据流处理，gRPC 都是一个值得深入学习和掌握的强大工具。\n希望本文能为你使用 Golang gRPC 开启微服务之旅提供坚实的基础！\n","categories":["Golang","微服务"],"tags":["2024","Golang","gRPC","微服务"]},{"title":"TanStack Query Vue 深度解析：优化你的 Vue 3 数据请求与状态管理","url":"/2024/2024-10-06_TanStack%20Query%20Vue%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BC%98%E5%8C%96%E4%BD%A0%E7%9A%84%20Vue%203%20%E6%95%B0%E6%8D%AE%E8%AF%B7%E6%B1%82%E4%B8%8E%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86/","content":"\n本文将带你深入了解如何在 Vue 3 项目中高效使用 TanStack Query（前身为 Vue Query 或 React Query），从而告别传统数据请求的烦恼，迎接更优雅、高效、智能的数据管理方式。\n\n在现代前端应用中，数据请求和状态管理是核心且复杂的任务。传统的 fetch 或 axios + useState&#x2F;ref 组合在处理缓存、刷新、分页、错误重试、乐观更新等方面常常力不从心，导致代码冗余、逻辑复杂、用户体验不佳。TanStack Query（以前称作 Vue Query 或 React Query）应运而生，它提供了一套强大的工具集，旨在解决这些痛点，让数据请求变得像客户端状态管理一样简单而强大。\n\n\n一、为什么选择 TanStack Query？TanStack Query 提供了一套在 Vue 3 应用中处理服务器状态（Server State）的强大工具。它与客户端状态（Client State，如 ref 或 reactive）管理有显著区别，专门针对以下痛点进行了优化：\n\n数据缓存 (Caching)：自动管理数据缓存，减少不必要的网络请求，提高应用响应速度。\n数据同步 (Synchronization)：确保UI始终显示最新数据，支持后台数据更新，实现“Stale-While-Revalidate”策略。\n请求去重 (Deduplication)：自动合并短时间内相同的请求，避免重复发送。\n后台刷新 (Background Refetching)：在用户不察觉的情况下，静默地更新旧数据，保持数据新鲜。\n离线支持 (Offline Support)：优化离线回退和重连后的数据同步。\n错误重试 (Retries)：内置失败请求的自动重试机制。\n分页与无限滚动 (Pagination &amp; Infinite Scroll)：简化复杂的数据加载模式。\n乐观更新 (Optimistic Updates)：提供平滑的用户体验，即时响应用户操作，即使网络请求仍在后台进行。\nDevtools 支持：强大的调试工具，让你清晰看到数据状态和请求过程。\n\n总之，TanStack Query 帮助你将精力集中在业务逻辑上，而不是繁琐的数据管理细节。\n二、核心概念速览在使用 TanStack Query 之前，理解几个核心概念至关重要：\n\nQuery (查询)：用于读取数据。它是 TanStack Query 最基本也是最常用的单位。通常对应 GET 请求。\nQuery Key (查询键)：一个唯一的数组或字符串，用于标识和缓存 Query。它是 TanStack Query 缓存系统的核心。\nQuery Function (查询函数)：一个返回 Promise 的函数，负责实际的数据请求。\n\n\nMutation (变更)：用于创建、更新、删除数据（即写入操作）。通常对应 POST, PUT, DELETE 请求。\nCallback (回调函数)：包含 onMutate, onError, onSuccess, onSettled 等，用于处理 Mutation 的生命周期，常用于乐观更新。\n\n\nQuery Client (查询客户端)：TanStack Query 的核心实例，管理所有 Query 和 Mutation 的缓存、状态和行为。\n\n三、安装与基本配置首先，我们需要在 Vue 3 项目中安装 TanStack Query 的 Vue 版本。\n# 使用 npmnpm install @tanstack/vue-query @tanstack/query-core# 使用 yarnyarn add @tanstack/vue-query @tanstack/query-core# 使用 pnpmpnpm add @tanstack/vue-query @tanstack/query-core\n\n接下来，在你的 Vue 应用入口文件（通常是 main.js 或 main.ts）中进行配置：\n// main.tsimport &#123; createApp &#125; from &#x27;vue&#x27;import App from &#x27;./App.vue&#x27;import &#123;  VueQueryPlugin,  QueryClient,  QueryClientConfig,&#125; from &#x27;@tanstack/vue-query&#x27; // 引入VueQueryPlugin和QueryClientconst app = createApp(App)// 1. 创建 QueryClient 实例const queryClient = new QueryClient(&#123;  defaultOptions: &#123;    queries: &#123;      // 全局配置：Query失败时自动重试3次      retry: 3,       // 全局配置：数据在1分钟内保持新鲜，1分钟后变为stale（陈旧），下次请求会触发后台刷新      staleTime: 1000 * 60,      // 全局配置：非活跃（无组件使用）的Queries在5分钟后会被垃圾回收      gcTime: 1000 * 60 * 5,     &#125;,    mutations: &#123;      // 全局配置：Mutation失败时不重试      retry: false,     &#125;,  &#125;,&#125;)// 2. 注册 VueQueryPlugin，并传入 QueryClient 实例app.use(VueQueryPlugin, &#123;  queryClient,  // 可选：启用 Devtools  // devtools: &#123;  //   initialIsOpen: false, // 默认不打开  //   position: &#x27;bottom-right&#x27;,  // &#125;,&#125;)app.mount(&#x27;#app&#x27;)\n\n✨ TanStack Query Devtools强烈推荐安装 TanStack Query Devtools。它是一个用于调试 Query 状态、缓存和性能的强大工具。\nnpm install @tanstack/query-devtools# 或 yarn add @tanstack/query-devtools# 或 pnpm add @tanstack/query-devtools\n\n在你的 main.ts 或 App.vue 中引入并使用：\n// main.ts (或者根据你的情况，在App.vue中引入)import &#123; VueQueryPlugin, QueryClient &#125; from &#x27;@tanstack/vue-query&#x27;;import &#123; VueQueryDevtools &#125; from &#x27;@tanstack/query-devtools&#x27;; // 引入 Devtoolsconst app = createApp(App);const queryClient = new QueryClient();app.use(VueQueryPlugin, &#123; queryClient &#125;);// 在开发环境中显示 Devtoolsif (import.meta.env.NODE_ENV === &#x27;development&#x27;) &#123;  app.component(&#x27;VueQueryDevtools&#x27;, VueQueryDevtools);&#125;app.mount(&#x27;#app&#x27;);\n\n然后在你的 App.vue 或其他根组件模板中添加：\n&lt;!-- App.vue --&gt;&lt;template&gt;  &lt;router-view /&gt;  &lt;template v-if=&quot;import.meta.env.NODE_ENV === &#x27;development&#x27;&quot;&gt;    &lt;!-- 使用 Devtools 组件 --&gt;    &lt;VueQueryDevtools :initialIsOpen=&quot;false&quot; /&gt;  &lt;/template&gt;&lt;/template&gt;\n\n这将显示一个可切换的面板，让你洞察所有 Query 的状态，包括数据、错误、加载状态、缓存时间等。\n四、使用 useQuery 进行数据查询useQuery 是 TanStack Query 中用于获取服务端数据的核心 Hook。\n4.1 基本查询示例&lt;!-- components/PostsList.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;文章列表&lt;/h1&gt;    &lt;p v-if=&quot;isLoading&quot;&gt;加载中...&lt;/p&gt;    &lt;p v-else-if=&quot;isError&quot;&gt;加载失败: &#123;&#123; error.message &#125;&#125;&lt;/p&gt;    &lt;ul v-else&gt;      &lt;li v-for=&quot;post in data&quot; :key=&quot;post.id&quot;&gt;        &#123;&#123; post.title &#125;&#125;      &lt;/li&gt;    &lt;/ul&gt;    &lt;button @click=&quot;refetch&quot; :disabled=&quot;isFetching&quot;&gt;      &#123;&#123; isFetching ? &#x27;刷新中...&#x27; : &#x27;手动刷新&#x27; &#125;&#125;    &lt;/button&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useQuery &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;; // 假设使用axios进行数据请求interface Post &#123;  id: number;  title: string;  body: string;&#125;// 异步查询函数，返回一个Promiseconst fetchPosts = async (): Promise&lt;Post[]&gt; =&gt; &#123;  const &#123; data &#125; = await axios.get(&#x27;https://jsonplaceholder.typicode.com/posts&#x27;);  return data;&#125;;// 使用 useQuery Hookconst &#123;  data,       // 查询到的数据  isLoading,  // 第一次加载时为 true  isFetching, // 只要有任何请求激活就为 true (包括后台静默刷新)  isError,    // Query 失败时为 true  error,      // 错误对象  refetch,    // 手动触发查询刷新&#125; = useQuery(&#123;  queryKey: [&#x27;posts&#x27;],    // 唯一的查询键，用于缓存  queryFn: fetchPosts,    // 查询函数  // 可选配置，会覆盖全局配置  staleTime: 1000 * 10,   // 该Query在10秒后变为stale  gcTime: 1000 * 60 * 30, // 非活跃30分钟后垃圾回收&#125;);&lt;/script&gt;\n\n解析：\n\nqueryKey: [&#39;posts&#39;]：这是这个 Query 的唯一标识符。TanStack Query 会使用它来存储、获取和管理缓存。强烈建议使用数组，因为你可以通过向数组添加更多元素来创建更精细的 Query Key（例如 [&#39;posts&#39;, postId]）。\nqueryFn: fetchPosts：执行数据请求的异步函数，必须返回一个 Promise。\nisLoading：指示查询是否处于首次加载状态（stale 且 fetching）。\nisFetching：指示查询是否正在进行中（即数据正在从后端获取）。即使数据已存在于缓存中，但在后台刷新时，isFetching 也会是 true。\nrefetch：一个函数，可以手动调用来重新获取数据。\n\n4.2 依赖查询键 (Dynamic Query Keys)Query Key 可以包含动态参数，这对于查询特定资源非常有用。\n&lt;!-- components/PostDetail.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;文章详情&lt;/h1&gt;    &lt;input type=&quot;number&quot; v-model=&quot;selectedPostId&quot; min=&quot;1&quot; max=&quot;10&quot; /&gt;    &lt;p v-if=&quot;isLoading&quot;&gt;加载中...&lt;/p&gt;    &lt;p v-else-if=&quot;isError&quot;&gt;加载失败: &#123;&#123; error.message &#125;&#125;&lt;/p&gt;    &lt;div v-else-if=&quot;data&quot;&gt;      &lt;h2&gt;&#123;&#123; data.title &#125;&#125;&lt;/h2&gt;      &lt;p&gt;&#123;&#123; data.body &#125;&#125;&lt;/p&gt;    &lt;/div&gt;    &lt;p v-else&gt;请选择一篇文章 ID (1-10).&lt;/p&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; ref, computed &#125; from &#x27;vue&#x27;;import &#123; useQuery &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;interface Post &#123;  id: number;  title: string;  body: string;&#125;const selectedPostId = ref&lt;number | null&gt;(1); // 默认选中文章1// 依赖于 selectedPostId 的 Query Keyconst postQueryKey = computed(() =&gt; &#123;  return selectedPostId.value ? [&#x27;post&#x27;, selectedPostId.value] : [] // 当selectedPostId为null时，返回空数组&#125;);// 查询函数，接收 QueryContext 对象，其中包含了 Query Keyconst fetchPostById = async (context: any): Promise&lt;Post&gt; =&gt; &#123;  const [, postId] = context.queryKey; // 从 Query Key 中获取 postId  if (!postId) &#123;    throw new Error(&#x27;No postId provided&#x27;); // 确保有postId  &#125;  const &#123; data &#125; = await axios.get(`https://jsonplaceholder.typicode.com/posts/$&#123;postId&#125;`);  return data;&#125;;const &#123;  data,  isLoading,  isError,  error,&#125; = useQuery(&#123;  queryKey: postQueryKey,  queryFn: fetchPostById,  enabled: computed(() =&gt; !!selectedPostId.value), // 只有当 selectedPostId 有值时才启用查询&#125;);&lt;/script&gt;\n\n解析：\n\nqueryKey: [&#39;post&#39;, selectedPostId.value]：当 selectedPostId.value 改变时，TanStack Query 会识别这是一个新的 Query，并自动触发重新获取数据。\nqueryFn 会接收一个上下文对象，其中包含 queryKey，你可以在查询函数中解构出动态参数。\nenabled: computed(() =&gt; !!selectedPostId.value)：这是一个非常重要的选项。当其值为 false 时，查询将停止自动请求数据（但仍可以手动 refetch）。这对于有条件地启用查询非常有用，例如等待用户输入。\n\n五、使用 useMutation 进行数据变更useMutation 是 TanStack Query 中用于创建、更新或删除服务端数据的 Hook。\n5.1 基本变更示例&lt;!-- components/CreatePost.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;创建新文章&lt;/h1&gt;    &lt;form @submit.prevent=&quot;handleSubmit&quot;&gt;      &lt;input type=&quot;text&quot; v-model=&quot;newPostTitle&quot; placeholder=&quot;文章标题&quot; required /&gt;      &lt;textarea v-model=&quot;newPostBody&quot; placeholder=&quot;文章内容&quot; required&gt;&lt;/textarea&gt;      &lt;button type=&quot;submit&quot; :disabled=&quot;isPending&quot;&gt;        &#123;&#123; isPending ? &#x27;提交中...&#x27; : &#x27;提交&#x27; &#125;&#125;      &lt;/button&gt;    &lt;/form&gt;    &lt;p v-if=&quot;isError&quot;&gt;创建失败: &#123;&#123; error.message &#125;&#125;&lt;/p&gt;    &lt;p v-if=&quot;isSuccess&quot;&gt;创建成功! 文章ID: &#123;&#123; data?.id &#125;&#125;&lt;/p&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; ref &#125; from &#x27;vue&#x27;;import &#123; useMutation, useQueryClient &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;interface NewPost &#123;  title: string;  body: string;  userId: number;&#125;interface CreatedPost extends NewPost &#123;  id: number;&#125;const newPostTitle = ref(&#x27;&#x27;);const newPostBody = ref(&#x27;&#x27;);// 获取 QueryClient 实例，用于手动更新缓存const queryClient = useQueryClient();// 异步创建函数const createPost = async (post: NewPost): Promise&lt;CreatedPost&gt; =&gt; &#123;  const &#123; data &#125; = await axios.post(    &#x27;https://jsonplaceholder.typicode.com/posts&#x27;,    post  );  return data;&#125;;// 使用 useMutation Hookconst &#123;  mutate,     // 触发 Mutation 的函数  data,       // Mutation 成功后的返回数据  isPending,  // Mutation 是否正在进行中  isSuccess,  // Mutation 是否成功  isError,    // Mutation 是否失败  error,      // 错误对象&#125; = useMutation(&#123;  mutationFn: createPost,  onSuccess: () =&gt; &#123;    console.log(&#x27;文章创建成功，正在刷新文章列表缓存...&#x27;);    // Invalidate 和 Refetch：使 &#x27;posts&#x27; 查询的数据失效，并触发后台重新获取    queryClient.invalidateQueries(&#123; queryKey: [&#x27;posts&#x27;] &#125;);     // 或者直接刷新 &#x27;posts&#x27; query, 但 invalidateQueries 带有智能的缓存管理    // queryClient.refetchQueries(&#123; queryKey: [&#x27;posts&#x27;] &#125;);  &#125;,  onError: (err) =&gt; &#123;    console.error(&#x27;创建文章失败:&#x27;, err);  &#125;,&#125;);const handleSubmit = () =&gt; &#123;  mutate(&#123;    title: newPostTitle.value,    body: newPostBody.value,    userId: 1, // 示例  &#125;);  newPostTitle.value = &#x27;&#x27;;  newPostBody.value = &#x27;&#x27;;&#125;;&lt;/script&gt;\n\n解析：\n\nmutationFn: createPost：执行数据变更的异步函数。\nmutate(variables)：这是你调用 Mutation 的函数。variables 是传递给 mutationFn 的参数。\nonSuccess：Mutation 成功后执行的回调。在这里，我们通常会使相关的 Query 失效 (invalidate)，从而触发这些 Query 在后台重新获取最新数据，确保 UI 显示的是最新状态。\nqueryClient.invalidateQueries(&#123; queryKey: [&#39;posts&#39;] &#125;)：告诉 TanStack Query，所有 Query Key 包含 [&#39;posts&#39;] 的 Query 都已过期。下次这些 Query 被渲染时，TanStack Query 会自动在后台重新请求数据。\n\n\n\n5.2 乐观更新 (Optimistic Updates)乐观更新是 useMutation 的一个高级且强大的特性，它能在网络请求还未响应时，就立即更新 UI，给用户流畅的体验。如果请求失败，再回滚 UI。\n&lt;!-- components/ToggleTodo.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h2&gt;待办事项列表&lt;/h2&gt;    &lt;p v-if=&quot;todosQuery.isLoading&quot;&gt;加载中...&lt;/p&gt;    &lt;p v-else-if=&quot;todosQuery.isError&quot;&gt;加载失败: &#123;&#123; todosQuery.error.message &#125;&#125;&lt;/p&gt;    &lt;ul v-else&gt;      &lt;li v-for=&quot;todo in todosQuery.data&quot; :key=&quot;todo.id&quot;&gt;        &lt;label&gt;          &lt;input            type=&quot;checkbox&quot;            :checked=&quot;todo.completed&quot;            @change=&quot;toggleTodoMutation.mutate(&#123; id: todo.id, completed: !todo.completed &#125;)&quot;            :disabled=&quot;toggleTodoMutation.isPending || (toggleTodoMutation.variables?.id === todo.id)&quot;          /&gt;          &lt;span :class=&quot;&#123; &#x27;line-through&#x27;: todo.completed &#125;&quot;&gt;&#123;&#123; todo.title &#125;&#125;&lt;/span&gt;        &lt;/label&gt;        &lt;span v-if=&quot;toggleTodoMutation.variables?.id === todo.id &amp;&amp; toggleTodoMutation.isPending&quot;&gt;          (更新中...)        &lt;/span&gt;      &lt;/li&gt;    &lt;/ul&gt;    &lt;p v-if=&quot;toggleTodoMutation.isError&quot;&gt;更新失败: &#123;&#123; toggleTodoMutation.error?.message &#125;&#125;&lt;/p&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useQuery, useMutation, useQueryClient &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;interface Todo &#123;  id: number;  title: string;  completed: boolean;  userId: number;&#125;// 1. 获取所有待办事项的 Queryconst todosQuery = useQuery(&#123;  queryKey: [&#x27;todos&#x27;],  queryFn: async (): Promise&lt;Todo[]&gt; =&gt; &#123;    const &#123; data &#125; = await axios.get(&#x27;https://jsonplaceholder.typicode.com/todos?_limit=5&#x27;);    return data;  &#125;,&#125;);const queryClient = useQueryClient();// 2. 更新单个待办事项状态的 Mutationconst toggleTodoMutation = useMutation&lt;  Todo, // 返回的数据类型  Error, // 错误类型  &#123; id: number; completed: boolean &#125;, // 传入 mutate 的变量类型  &#123; previousTodos: Todo[] | undefined &#125; // onMutate 返回的上下文类型&gt;(&#123;  mutationFn: async (&#123; id, completed &#125;) =&gt; &#123;    // 模拟网络延迟和可能的失败    if (Math.random() &lt; 0.2) &#123; // 20%的几率失败      await new Promise(resolve =&gt; setTimeout(resolve, 1000));      throw new Error(`模拟网络错误，更新待办事项 $&#123;id&#125; 失败`);    &#125;    const &#123; data &#125; = await axios.put(`https://jsonplaceholder.typicode.com/todos/$&#123;id&#125;`, &#123; completed &#125;);    return data;  &#125;,  // 🎉 onMutate 阶段：在 mutation 发生前触发，用于乐观更新  onMutate: async newTodoStatus =&gt; &#123;    // 1. 取消任何正在进行的 &#x27;todos&#x27; Query，以确保不会覆盖乐观更新    await queryClient.cancelQueries(&#123; queryKey: [&#x27;todos&#x27;] &#125;);    // 2. 获取当前 &#x27;todos&#x27; Query 的缓存快照，用于回滚    const previousTodos = queryClient.getQueryData&lt;Todo[]&gt;([&#x27;todos&#x27;]);    // 3. 乐观更新 &#x27;todos&#x27; 缓存    queryClient.setQueryData&lt;Todo[]&gt;([&#x27;todos&#x27;], oldTodos =&gt; &#123;      return oldTodos        ? oldTodos.map(todo =&gt;            todo.id === newTodoStatus.id              ? &#123; ...todo, completed: newTodoStatus.completed &#125;              : todo          )        : [];    &#125;);    // 返回一个包含旧数据的上下文，供 onError 使用    return &#123; previousTodos &#125;;  &#125;,  // ✅ onSuccess 阶段：mutation 成功后触发  onSuccess: (data) =&gt; &#123;    console.log(&#x27;乐观更新成功，服务器返回:&#x27;, data);    // 可选：成功后也可以使 &#x27;todos&#x27; 失效，确保最终数据一致（虽然乐观更新已经做了）    queryClient.invalidateQueries(&#123; queryKey: [&#x27;todos&#x27;] &#125;);  &#125;,  // ❌ onError 阶段：mutation 失败后触发，用于回滚  onError: (err, newTodoStatus, context) =&gt; &#123;    console.error(&#x27;乐观更新失败，正在回滚:&#x27;, err);    // 回滚到 onMutate 提供的旧数据    if (context?.previousTodos) &#123;      queryClient.setQueryData&lt;Todo[]&gt;([&#x27;todos&#x27;], context.previousTodos);    &#125;  &#125;,  // 🔚 onSettled 阶段：mutation 成功或失败都会触发  onSettled: (data, error, newTodoStatus) =&gt; &#123;    console.log(&#x27;Mutation 完成，无论是成功还是失败。&#x27;);    // 确保 &#x27;todos&#x27; Query 最终被刷新，获取最新数据（清除所有乐观更新可能带来的不一致）    queryClient.invalidateQueries(&#123; queryKey: [&#x27;todos&#x27;] &#125;);  &#125;,&#125;);&lt;/script&gt;&lt;style scoped&gt;.line-through &#123;  text-decoration: line-through;&#125;&lt;/style&gt;\n\n解析：\n\nonMutate：在 mutationFn 实际执行前触发。这是进行乐观更新的最佳时机。\nqueryClient.cancelQueries()：重要！取消任何正在进行的、可能会覆盖你乐观更新的 Query。\nqueryClient.getQueryData()：获取当前 Query 缓存的快照。\nqueryClient.setQueryData()：立即更新缓存中的数据，UI 随即更新。\n返回一个对象作为 context，这个 context 会被传递给 onError 和 onSettled，以便在失败时回滚。\n\n\nonSuccess：请求成功后触发。此时可以 invalidateQueries 再次确认数据新鲜度。\nonError：请求失败后触发。利用 context 中的 previousTodos 回滚 UI 到请求前的状态。\nonSettled：无论成功或失败都会触发。这里通常会 invalidateQueries，确保最终的数据一致性，尤其是在 onMutate 中取消了请求的情况下。\n\n通过乐观更新，用户操作后几乎能立即看到结果，即使网络有延迟，也大大提升了用户体验。\n六、更多高级特性6.1 useQueries：并行查询多个 Query当你有多个独立的 Query 需要在同一组件中发起时，useQueries 允许你并行执行它们，并统一管理它们的状态。\n&lt;!-- components/MultipleDataFetch.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;多数据并行获取&lt;/h1&gt;    &lt;div v-if=&quot;isLoadingAny&quot;&gt;      &lt;p&gt;正在加载所有数据...&lt;/p&gt;    &lt;/div&gt;    &lt;div v-else&gt;      &lt;h2&gt;用户信息&lt;/h2&gt;      &lt;p v-if=&quot;userQuery.isError&quot;&gt;用户加载失败: &#123;&#123; userQuery.error.message &#125;&#125;&lt;/p&gt;      &lt;div v-else-if=&quot;userQuery.data&quot;&gt;        &lt;p&gt;Name: &#123;&#123; userQuery.data.name &#125;&#125;&lt;/p&gt;        &lt;p&gt;Email: &#123;&#123; userQuery.data.email &#125;&#125;&lt;/p&gt;      &lt;/div&gt;      &lt;h2&gt;文章列表&lt;/h2&gt;      &lt;p v-if=&quot;postsQuery.isError&quot;&gt;文章加载失败: &#123;&#123; postsQuery.error.message &#125;&#125;&lt;/p&gt;      &lt;ul v-else-if=&quot;postsQuery.data&quot;&gt;        &lt;li v-for=&quot;post in postsQuery.data&quot; :key=&quot;post.id&quot;&gt;&#123;&#123; post.title &#125;&#125;&lt;/li&gt;      &lt;/ul&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useQueries &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;import &#123; computed &#125; from &#x27;vue&#x27;;interface User &#123;  id: number;  name: string;  email: string;&#125;interface Post &#123;  id: number;  title: string;&#125;const fetchUser = async (): Promise&lt;User&gt; =&gt; &#123;  const &#123; data &#125; = await axios.get(&#x27;https://jsonplaceholder.typicode.com/users/1&#x27;);  return data;&#125;;const fetchPosts = async (): Promise&lt;Post[]&gt; =&gt; &#123;  const &#123; data &#125; = await axios.get(&#x27;https://jsonplaceholder.typicode.com/posts?_limit=3&#x27;);  return data;&#125;;// 使用 useQueries，传入一个 QueryOptions 数组const results = useQueries(&#123;  queries: [    &#123;      queryKey: [&#x27;user&#x27;, 1],      queryFn: fetchUser,      staleTime: 1000 * 60 * 5,    &#125;,    &#123;      queryKey: [&#x27;posts&#x27;],      queryFn: fetchPosts,      staleTime: 1000 * 60 * 1,    &#125;,  ],&#125;);// 计算所有查询的加载状态const isLoadingAny = computed(() =&gt; results.some(q =&gt; q.isLoading.value)); // 注意这里的.value// 解构获取每个查询的结果const userQuery = computed(() =&gt; results[0]);const postsQuery = computed(() =&gt; results[1]);&lt;/script&gt;\n\n解析：\n\nuseQueries 接收一个 queries 数组，每个元素都是一个标准的 QueryOptions 对象。\n它返回一个结果数组，每个元素对应一个 Query 的状态和数据。\n你可以遍历 results 来检查总体状态，或者通过索引访问单个 Query 的详细信息。\n\n6.2 useInfiniteQuery：实现无限滚动与分页useInfiniteQuery 是为了处理“加载更多”或无限滚动（infinite scroll）场景而设计的，它能够管理多个页面（或批次）的数据。\n&lt;!-- components/InfiniteScrollPosts.vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;无限滚动文章&lt;/h1&gt;    &lt;div v-if=&quot;isLoading&quot;&gt;加载中...&lt;/div&gt;    &lt;div v-else-if=&quot;isError&quot;&gt;加载失败: &#123;&#123; error?.message &#125;&#125;&lt;/div&gt;    &lt;ul v-else&gt;      &lt;li v-for=&quot;page in data?.pages&quot; :key=&quot;page.nextCursor&quot;&gt;        &lt;div v-for=&quot;post in page.data&quot; :key=&quot;post.id&quot;&gt;          &lt;h3&gt;&#123;&#123; post.title &#125;&#125;&lt;/h3&gt;          &lt;p&gt;&#123;&#123; post.body &#125;&#125;&lt;/p&gt;          &lt;hr /&gt;        &lt;/div&gt;      &lt;/li&gt;    &lt;/ul&gt;    &lt;button      @click=&quot;fetchNextPage&quot;      :disabled=&quot;!hasNextPage || isFetchingNextPage&quot;      v-if=&quot;hasNextPage&quot;    &gt;      &#123;&#123; isFetchingNextPage ? &#x27;加载更多...&#x27; : &#x27;加载更多&#x27; &#125;&#125;    &lt;/button&gt;    &lt;p v-else-if=&quot;!isLoading&quot;&gt;没有更多文章了。&lt;/p&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useInfiniteQuery &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;interface Post &#123;  id: number;  title: string;  body: string;&#125;interface PostsPage &#123;  data: Post[];  nextCursor?: number; // 下一页的起始ID&#125;// 查询函数，接收 pageParam 作为当前页面的“锚点”const fetchPostsInfinite = async (&#123; pageParam = 1 &#125;): Promise&lt;PostsPage&gt; =&gt; &#123;  const limit = 5;  const start = (pageParam - 1) * limit; // 根据页码计算起始索引  const &#123; data &#125; = await axios.get(    `https://jsonplaceholder.typicode.com/posts?_start=$&#123;start&#125;&amp;_limit=$&#123;limit&#125;`  );  const nextCursor = data.length === limit ? pageParam + 1 : undefined; // 如果返回的数据量等于limit，则可能还有下一页  return &#123;    data,    nextCursor,  &#125;;&#125;;const &#123;  data,          // 包含 pages 数组，每个元素是 fetchPostsInfinite 的返回值  fetchNextPage, // 用于加载下一页的函数  hasNextPage,   // 是否还有下一页  isFetchingNextPage, // 是否正在加载下一页  isLoading,  isError,  error,&#125; = useInfiniteQuery(&#123;  queryKey: [&#x27;infinitePosts&#x27;],  queryFn: fetchPostsInfinite,  initialPageParam: 1, // 初始页码参数  // 获取下一页参数的逻辑  getNextPageParam: (lastPage: PostsPage, allPages: PostsPage[]) =&gt; &#123;    return lastPage.nextCursor; // 使用从服务器返回的nextCursor作为下一页的pageParam  &#125;,  staleTime: 1000 * 60,&#125;);&lt;/script&gt;\n\n解析：\n\nqueryFn 接收一个包含 pageParam 的对象，pageParam 就是你用来请求下一页数据的参数（例如页码、偏移量、ID等）。\ninitialPageParam：设置第一个 pageParam 的值。\ngetNextPageParam：一个函数，接收上一页的数据和所有已加载的页面数据，并返回用于请求下一页的 pageParam。如果返回 undefined 或 null，则 hasNextPage 为 false。\ndata.pages：useInfiniteQuery 返回的数据结构。它是一个数组，每个元素都是 queryFn 返回的一个“页面”数据。在模板中，你需要遍历 data.pages，然后再遍历每个页面中的实际数据。\nfetchNextPage：调用此函数来加载下一页数据。\nhasNextPage：指示是否还有更多页面可以加载。\nisFetchingNextPage：指示是否正在加载下一页数据。\n\n七、与 Nuxt 3 (SSR) 结合使用TanStack Query 对 SSR（Server-Side Rendering，服务器端渲染）友好，特别是在 Nuxt 3 这样的框架中，可以实现数据的预取（Prefetch）和水合（Hydration）。\n7.1 Nuxt 3 配置在你的 Nuxt 3 项目中，创建一个插件文件（例如 plugins/vue-query.ts）：\n// plugins/vue-query.tsimport &#123; VueQueryPlugin, QueryClient, hydrate, dehydrate &#125; from &#x27;@tanstack/vue-query&#x27;import type &#123; DehydratedState &#125; from &#x27;@tanstack/vue-query&#x27;export default defineNuxtPlugin((nuxtApp) =&gt; &#123;  const queryClient = new QueryClient(&#123;    defaultOptions: &#123;      queries: &#123;        // 在 SSR 模式下，第一次请求的数据是预取的（pre-fetched）        // 确保在客户端数据水合后，不会立即后台刷新        staleTime: 1000 * 60, // 数据在 1 分钟内保持 fresh      &#125;,    &#125;,  &#125;)  // 在 Nuxt 服务器端渲染时  nuxtApp.vueApp.use(VueQueryPlugin, &#123; queryClient &#125;)  // Nuxt 3 的 app:rendered 钩子，用于在服务器端渲染完成后脱水（dehydrate）  // 并在客户端水合（hydrate）脱水状态  if (process.server) &#123;    nuxtApp.hook(&#x27;app:rendered&#x27;, () =&gt; &#123;      // 在服务器端渲染完成后，将 QueryClient 的状态脱水      nuxtApp.payload.vueQueryState = dehydrate(queryClient)    &#125;)  &#125;  // 在客户端水合脱水的状态  if (process.client) &#123;    nuxtApp.hook(&#x27;app:created&#x27;, () =&gt; &#123;      // 在客户端创建应用时，用水合（hydrate）服务器端脱水（dehydrate）的状态      hydrate(queryClient, nuxtApp.payload.vueQueryState)    &#125;)  &#125;  return &#123;    provide: &#123;      queryClient, // 可以通过 #useNuxtApp().$queryClient 访问    &#125;,  &#125;&#125;)\n\n7.2 Nuxt 页面中的预取示例在 Nuxt 页面组件中，你可以使用 useAsyncData 或 defineNuxtComponent 结合 TanStack Query 来预取数据。\n&lt;!-- pages/posts/[id].vue --&gt;&lt;template&gt;  &lt;div&gt;    &lt;h1&gt;文章详情 &#123;&#123; $route.params.id &#125;&#125;&lt;/h1&gt;    &lt;div v-if=&quot;isLoading&quot;&gt;Loading Post...&lt;/div&gt;    &lt;div v-else-if=&quot;isError&quot;&gt;Error: &#123;&#123; error?.message &#125;&#125;&lt;/div&gt;    &lt;div v-else-if=&quot;data&quot;&gt;      &lt;h2&gt;&#123;&#123; data.title &#125;&#125;&lt;/h2&gt;      &lt;p&gt;&#123;&#123; data.body &#125;&#125;&lt;/p&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/template&gt;&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useQuery, useQueryClient &#125; from &#x27;@tanstack/vue-query&#x27;;import axios from &#x27;axios&#x27;;interface Post &#123;  id: number;  title: string;  body: string;&#125;const route = useRoute();const postId = computed(() =&gt; Number(route.params.id));const fetchPostById = async (context: any): Promise&lt;Post&gt; =&gt; &#123;  const [, id] = context.queryKey;  if (!id) &#123;    throw new Error(&#x27;Post ID is missing&#x27;);  &#125;  const &#123; data &#125; = await axios.get(`https://jsonplaceholder.typicode.com/posts/$&#123;id&#125;`);  return data;&#125;;// 在 Nuxt 3 中，可以使用 useAsyncData 来预取数据// 但直接使用 useQuery 更符合 TanStack Query 的水合机制const queryClient = useQueryClient(); // 获取 QueryClient 实例// 预热缓存（Prefetch）：在服务器端预先获取数据并填充缓存if (process.server) &#123;  await queryClient.prefetchQuery(&#123;    queryKey: [&#x27;post&#x27;, postId.value],    queryFn: fetchPostById,  &#125;);&#125;const &#123; data, isLoading, isError, error &#125; = useQuery(&#123;  queryKey: [&#x27;post&#x27;, postId], // postId 应该是响应式的ref/computed  queryFn: fetchPostById,  initialData: computed(() =&gt; queryClient.getQueryData([&#x27;post&#x27;, postId.value])), // 从SSR缓存中取初始数据  initialDataUpdatedAt: computed(() =&gt; queryClient.getQueryState([&#x27;post&#x27;, postId.value])?.dataUpdatedAt),  staleTime: 1000 * 60, // 重要：在客户端加载后，这个数据在1分钟内不会被后台刷新  enabled: computed(() =&gt; !!postId.value),&#125;);&lt;/script&gt;\n\n解析：\n\ndefineNuxtPlugin 中配置 VueQueryPlugin，并在服务器端 dehydrate 状态，客户端 hydrate 状态。\n在页面组件中，通过 process.server 判断是否是服务器端，如果是，则使用 queryClient.prefetchQuery 提前加载数据。\ninitialData 和 initialDataUpdatedAt：这两个选项是实现水合的关键。它们告诉 useQuery 从哪里获取初始数据以及这个数据是什么时候生成的。在客户端，TanStack Query 会优先使用这些预取的数据，而不是重新发起请求。\nstaleTime：在 SSR 场景下尤为重要。它定义了数据在客户端加载后，多久之后会变为 stale。设置一个合适的 staleTime 可以避免在客户端立即触发额外的后台刷新，从而提高性能和用户体验。\n\n八、最佳实践与注意事项\n统一 Query Key 命名规范：始终使用数组作为 queryKey，并保持一致的命名模式（例如 [&#39;entityType&#39;, id, &#39;subResource&#39;]）。\nqueryFn 纯净性：queryFn 应该是一个纯函数，只负责数据请求，不应包含副作用。\nstaleTime 与 gcTime：理解并合理配置这两个全局及局部选项。\nstaleTime：数据变为“陈旧”的时间。在此时间内，即使 Query 被重新渲染，也不会触发后台刷新。\ngcTime：“垃圾回收”时间。Query 在变为非活跃（没有组件订阅）后的保留时间。超过此时间会被从缓存中移除。\n\n\n错误处理：全局 QueryClient 可以在 defaultOptions.queries.onError 或 defaultOptions.mutations.onError 中设置统一的错误处理逻辑，如弹出通知。\n懒加载与 enabled 选项：对于依赖参数的 Query，使用 enabled 选项来控制何时发起请求，避免不必要的请求。\nQueryClient 的手动操作：熟练使用 queryClient.invalidateQueries()、queryClient.setQueryData() 等方法进行缓存的精确控制。\n避免在 queryFn 中抛出非 Error 对象：确保 queryFn 在失败时抛出 Error 类的对象，这样 TanStack Query 可以更好地处理它。\nDevtools 辅助调试：充分利用 TanStack Query Devtools 来观察、理解和调试你的数据流。\n\n九、总结TanStack Query 是一个革命性的工具，它极大地改变了前端开发者处理服务器数据的方式。通过自动化缓存、后台刷新、错误重试和乐观更新等复杂逻辑，它让开发者能够将更多精力投入到构建出色的用户界面和业务功能上。\n在 Vue 3 项目中，结合 useQuery、useMutation 及其高级特性，你不仅能够获得更简洁、可维护的代码，还能显著提升应用的用户体验和性能。如果你正在寻求一种更智能、更高效的数据请求和状态管理方案，那么 TanStack Query 绝对值得你深入学习和实践。\n告别手动管理 loading、error、data 状态和繁琐的缓存逻辑吧，拥抱 TanStack Query 带来的便利与强大！\n","categories":["前端技术","Vue"],"tags":["前端技术","2024","Vue","TanStackQuery","Nuxt"]},{"title":"Go Modules(go mod)详解","url":"/2024/2024-10-11_Go%20Modules(go%20mod)%E8%AF%A6%E8%A7%A3/","content":"\nGo Modules 是 Go 语言官方推荐的依赖管理系统，自 Go 1.11 版本引入，并在 Go 1.13 版本中作为默认方案。它旨在解决 Go 语言在依赖管理方面存在的痛点，提供了一种更可靠、可重现且易于使用的模块化方式来组织和管理 Go 项目及其外部依赖。\n\n“Go modules are the future of dependency management in Go.” —— Go 官方博客\n\n\n一、为什么需要 Go Modules？在 Go Modules 之前，Go 语言的依赖管理主要面临以下挑战：\n\nGOPATH 痛点:\n所有项目必须放在 GOPATH 目录下。\n所有项目共享同一份依赖库版本，导致不同项目可能需要不同版本的库，容易冲突。\n对个人开发者而言，项目结构僵硬，跨项目共享代码不便。\n\n\n社区工具碎片化:\n为了解决 GOPATH 问题，社区涌现了 dep、glide、go-vendor 等第三方依赖管理工具，但没有一个成为官方标准。\n这些工具各有优缺点，增加了学习和使用的成本。\n\n\n版本不确定性:\n在没有明确版本控制的情况下，go get 会拉取依赖库的最新版本，可能导致项目在不同时间点构建时使用不同版本的依赖，从而引入不可预测的 bug。\n缺乏锁定依赖版本的机制。\n\n\n\nGo Modules 旨在解决这些问题，提供一个集成在 Go 工具链中的、标准化的、语义化版本控制的（Semantic Versioning, SemVer）依赖管理方案。\n二、Go Modules 核心文件Go Modules 通过以下两个核心文件来管理依赖：\n1. go.mod 文件go.mod 文件定义了模块的路径、所需的依赖及其版本。它是 Go 模块的清单文件。\n主要内容:\n\nmodule &lt;module_path&gt;: 声明当前模块的路径（即模块名），通常是项目在版本控制系统中的路径（例如 github.com/my/project）。\ngo &lt;go_version&gt;: 指定当前模块使用的 Go 语言版本。\nrequire &lt;dependency_path&gt; &lt;version&gt;: 列出直接依赖的模块及其最低版本。\n版本号遵循语义化版本规范 (例如 v1.2.3, v0.0.0-20200810183556-c73c88014e4b)。\n如果版本后缀是 +incompatible，表示该模块在 v2 或更高版本没有采用 Go Modules。\n\n\nexclude &lt;dependency_path&gt; &lt;version&gt;: 排除某个特定版本的依赖，这在使用某些有问题的旧版本时非常有用。\nreplace &lt;old_path&gt; &lt;version&gt; =&gt; &lt;new_path&gt; &lt;new_version&gt;: 替换某个依赖。\n常用于本地开发时，将远程依赖临时替换为本地文件路径 replace example.com/foo =&gt; ../foo。\n也可用于修正模块路径或使用更高版本。\n\n\nretract &lt;version_range&gt;: 撤回一个或多个有问题的版本。go get 将不再解析到这些版本。\n\n示例 go.mod 文件:\nmodule github.com/my/projectgo 1.22require (\tgolang.org/x/text v0.3.8\tgithub.com/gin-gonic/gin v1.9.1\t// indirect 标记表示这是一个间接依赖, 也就是你依赖的库所依赖的库\tgithub.com/ugorji/go/codec v1.2.11 // indirect)\n\n2. go.sum 文件go.sum 文件存储了模块的加密校验和，用于验证下载的依赖文件是否被篡改。\n主要内容:\n\n每一行包含三个字段：模块路径、版本号和哈希值。\n哈希值通常有两个：h1: 值用于校验模块 ZIP 文件的哈希，go.mod 哈希用于校验依赖模块的 go.mod 文件。\ngo.sum 文件是自动生成的，并且你应该将其提交到版本控制系统中。\n\n示例 go.sum 文件:\ngithub.com/bytedance/sonic v1.9.1/go.mod h1:3T2+s2i/2I+n0+Ew3w7R8+e5Xo+h/4m6+W2h9+e5Xo=github.com/chenzhuoyu/base64x v0.0.0-20230717121730-b179ae317e13 h1:X/J4y/6+L9z/0+Ew3w7R8+e5Xo+h/4m6+W2h9+e5Xo=github.com/cpuguy83/go-md2man/v2 v2.0.2/go.mod h1:9Xp9fWJ7Q2Q7Q7y/7+L9z/0+Ew3w7R8+e5Xo+h/4m6+W2h9+e5Xo=...\n\n三、Go Modules 主要命令1. go mod init在一个新项目的根目录下初始化一个 Go 模块。\ncd myprojectgo mod init [module_path]# 示例：# 如果当前目录是 ~/project/webapp# go mod init github.com/youruser/webapp\n\n执行后会在当前目录下生成 go.mod 文件。\n2. go mod tidy清理和同步 go.mod 文件。该命令会：\n\n移除不再使用的依赖块。\n添加新的（直接或间接）依赖块。\n更新 go.sum 文件，添加或移除相应的校验和。\n\ngo mod tidy\n这是一个非常常用的命令，推荐在修改导入路径后、或者在构建项目前经常运行。\n3. go getgo get 命令现在主要用于添加、升级或降级依赖。\n# 添加一个新的依赖或更新到最新版本go get &lt;dependency_path&gt;# 添加指定版本的依赖go get &lt;dependency_path&gt;@&lt;version&gt;# 示例：go get github.com/gin-gonic/gin@v1.9.0# 添加最新版本（或master分支的最新提交）go get &lt;dependency_path&gt;@latest# 删除某个依赖 (go version &gt;= 1.16)go get &lt;dependency_path&gt;@none# 或者手动从 go.mod 中删除 require 语句，然后运行 go mod tidy\n\n4. go mod download下载 go.mod 中列出的所有依赖到本地模块缓存 (GOPATH/pkg/mod)。对于离线构建很有用。\ngo mod download\n\n5. go mod vendor将项目的所有依赖副本复制到项目根目录下的 vendor 目录中。在某些构建环境或私有网络中可能需要。\ngo mod vendor\n使用 vendor 目录后，构建时 Go 工具链会优先从 vendor 目录查找依赖。可以使用 go build -mod=vendor 强制使用 vendor 模式。\n6. go mod verify验证 go.sum 文件中记录的所有模块内容是否与下载的模块哈希值匹配。\ngo mod verify\n\n7. go mod graph打印模块依赖图，显示所有直接和间接依赖。\ngo mod graph\n\n8. go mod edit用于编辑 go.mod 文件，通常是在脚本中使用。\n# 添加一个 require 语句go mod edit -require=example.com/foo@v1.2.3# 添加一个 replace 语句go mod edit -replace=example.com/foo=./foo-local# 查看 go.mod 文件的 JSON 格式go mod edit -json\n\n四、Go Modules 环境配置1. GO111MODULE 环境变量控制 Go Modules 的开关。\n\nGO111MODULE=on: 强制使用 Go Modules。推荐使用。\nGO111MODULE=off: 禁用 Go Modules，回到 GOPATH 模式。\nGO111MODULE=auto: 默认值 (Go 1.11, 1.12)。如果在 GOPATH 之外，并且目录下存在 go.mod 文件，则启用 Go Modules；否则禁用。Go 1.13+ 版本中，如果存在 go.mod 文件，则默认启用 on。\n\n强烈建议将其设置为 on，并在 Go 1.16 以后版本不再需要手动设置，所有项目都默认使用 Go Modules。\n2. GOPROXY 环境变量Go 模块代理，用于加速下载依赖模块，并提高依赖的稳定性。\n\n默认值: https://proxy.golang.org,direct\n可以配置多个代理，用逗号分隔。direct 表示直接从源站下载。\n\n# 设置为阿里云 Go 模块代理export GOPROXY=https://mirrors.aliyun.com/goproxy/,direct# 设置为七牛云 Go 模块代理export GOPROXY=https://goproxy.cn,direct# 也可以设置多个代理export GOPROXY=https://mirrors.aliyun.com/goproxy/,https://goproxy.io,direct\n\n3. GONOPROXY 和 GOSUMDB\nGONOPROXY: 用于指定不应该使用代理下载的模块路径列表，例如私有仓库的模块。这些模块将直接从源站下载。\nGOSUMDB: 用于校验模块哈希值的数据库，防止模块被篡改。默认是 sum.golang.org。如果 GOPROXY 设置为私有代理，可能需要调整此项。\n\n五、Go Modules 的优势\n脱离 GOPATH: 项目可以放置在文件系统的任何位置。\n版本锁定: go.mod 和 go.sum 确保了依赖版本的确定性，使得项目可以被可靠地构建。\n语义化版本控制: 支持 vX.Y.Z 规范，模块升级更可控。\n模块隔离: 不同项目可以依赖同一库的不同版本而不会相互冲突。\n官方支持: 作为 Go 语言的官方解决方案，拥有更好的兼容性和长期维护。\n更清晰的依赖图: go mod graph 等命令提供了对项目依赖的清晰视图。\n\n六、Go Modules 最佳实践\n尽早初始化: 在项目创建之初就运行 go mod init。\n提交 go.mod 和 go.sum: 务必将这两个文件提交到你的版本控制系统（如 Git）。它们定义了项目的可重复构建性。\ngo mod tidy 常用: 在添加、删除或修改导入路径后，或者在解决依赖问题时，经常运行 go mod tidy。\n使用 GOPROXY: 配置一个可靠的 Go 模块代理可以显著提高构建效率和稳定性，尤其是在网络环境不佳时。\n避免手动修改 go.sum: go.sum 文件应由 Go 工具链自动管理。手动修改可能导致校验失败。\nreplace 仅用于开发: 除非有特殊需求，replace 语句通常只用于本地开发或测试时临时替换依赖。在提交代码到共享仓库之前，尽量避免或移除开发用的 replace 语句。\n理解 indirect 依赖: go.mod 文件中带有 // indirect 注释的 require 语句表示这些是间接依赖 (即你的直接依赖所依赖的库)。它们的存在有助于模块图的完整性。\n\n七、总结Go Modules 彻底改变了 Go 语言的依赖管理方式，使其变得更加现代、健壮和用户友好。通过 go.mod 和 go.sum 文件，Go 项目能够准确地定义和锁定其所有依赖，确保了构建的可重复性，并提供了更好的模块隔离和版本控制。掌握 go mod 命令和理解其工作原理，是每个 Go 开发者必备的技能。\n","categories":["Golang","项目构建"],"tags":["2024","项目构建","Golang"]},{"title":"Dockge介绍与部署：下一代 Docker Compose UI","url":"/2024/2024-10-21_Dockge%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%9A%E4%B8%8B%E4%B8%80%E4%BB%A3%20Docker%20Compose%20UI/","content":"\n如果你经常使用 Docker Compose 来管理容器应用，并且厌倦了命令行界面，或者觉得 Portainer 过于庞大复杂，那么 Dockge 可能会成为你的新宠。Dockge 是一个轻量级、直观且专注于 Docker Compose 的 Web UI 工具，它旨在简化 Docker Compose 项目的创建、编辑、部署和管理，让你能够更高效地维护你的容器化服务。\n\n“好的工具让复杂的事情变得简单，Dockge 就是让 Docker Compose 更友好的工具。”\n\n\n一、Dockge 是什么？Dockge 是一个开源的 Docker Compose 管理工具，它提供了一个简洁的 Web 界面，让你可以：\n\n可视化管理 Docker Compose 项目：轻松查看所有 Docker Compose 堆栈（Stack）的状态。\n在线编辑 docker-compose.yml 文件：直接在浏览器中编辑并保存更改，无需 SSH 到服务器。\n一键部署和管理堆栈：启动、停止、重启、删除整个 Docker Compose 堆栈。\n查看容器日志：实时查看容器的输出日志。\n管理容器卷：查看和操作容器创建的卷。\n简单易用：专注于 Docker Compose 核心功能，没有过多的额外负担。\n\n核心特点：\n\n轻量级：安装和运行资源占用极低。\n易上手：界面直观，功能聚焦。\n命令行友好：底层依然是调用 Docker Compose 命令，所有操作都能通过 UI 完成，但也允许你在需要时介入命令行。\n安全：支持多用户管理和权限控制（计划中或高级配置）。\nDocker 原生：直接与 Docker 后台通信。\n\n二、为什么选择 Dockge？\n厌倦了 SSH 和 Vim？：如果你的服务器上没有 VIM 或 Nano 等顺手的编辑器，或者你不喜欢在命令行中编辑 YAML 文件，Dockge 提供了一个方便的浏览器内编辑器。\n追求轻量化：Portainer 固然强大，但对于只关注 Docker Compose 的用户来说，可能显得过于复杂和臃肿。Dockge 更专注于此，提供更精简的体验。\n团队协作：方便团队成员共同管理 Docker Compose 项目，无需每个人都熟悉 SSH 和命令行操作。\n简化自动化：结合 GitHub Actions 或其他 CI&#x2F;CD 工具，可以实现无人值守的部署更新。\n个人服务器管理：对于个人 Homelab 或小型服务器用户，Dockge 是一个极佳的控制面板。\n\n三、部署 DockgeDockge 推荐使用 Docker Compose 自身来部署。\n1. 先决条件\n一台运行 Linux 的服务器（支持 Docker Desktop for Windows&#x2F;macOS，但通常用于服务器）\n已安装 Docker 和 Docker Compose (或 Docker CLI 的 compose 插件)。\n可以通过 docker --version 和 docker compose version (或 docker-compose --version) 检查。\n\n\n\n2. 部署步骤步骤 1：创建 Dockge 的数据目录首先，创建一个目录来存储 Dockge 的配置数据和 docker-compose.yml 文件。这个目录我们将称之为 stacks 目录。\n通过 SSH 连接到你的服务器：\nmkdir -p /opt/stacks\n\n步骤 2：创建 Docker Compose 文件进入刚刚创建的目录，并创建一个 docker-compose.yml 文件来部署 Dockge 本身。\ncd /opt/stacksnano docker-compose.yml # 或者其他你喜欢的编辑器，如 vi\n\n将以下内容粘贴到 docker-compose.yml 文件中：\nversion: &quot;3.8&quot;services:  dockge:    image: louislam/dockge:1 # 使用最新的 Dockge 镜像    container_name: dockge    restart: unless-stopped    ports:      - 5001:5001 # Dockge web UI 默认运行在 5001 端口    volumes:      - /var/run/docker.sock:/var/run/docker.sock # 必须挂载 Docker Vsock, 允许 Dockge 与 Docker Daemon 通信      - ./data:/app/data                          # Dockge 自身的数据存储 (包括登录信息等)      - /opt/stacks:/opt/stacks                   # 你的 Docker Compose 项目（堆栈）存放目录。此目录将被 Dockge 管理。    environment:      # PUID = 1000 and PGID = 100 usually for default user.      # Check your ID (id &lt;your_username&gt;) and modify if necessary.      # - PUID=1000 # 容器内用户ID，通常是 default user，可能需要根据自己系统的用户ID调整      # - PGID=100  # 容器内用户组ID，通常是 users group，可能需要根据自己系统的用户组ID调整      - TZ=Asia/Shanghai # 设置时区      - DOCKGE_STACKS_DIR=/opt/stacks\n\n重要说明：\n\nimage: louislam/dockge:latest：确保你拉取的是最新的 Dockge 镜像。\nports: - 5001:5001：将容器的 5001 端口映射到主机的 5001 端口。你可以根据需要更改主机端口。\nvolumes:\n/var/run/docker.sock:/var/run/docker.sock：这是 Dockge 能够与 Docker Daemon 通信的关键。 这是一个特权挂载，请确保你理解其潜在的安全风险。\n./data:/app/data：这是 Dockge 存储自身配置和持久化数据的地方。./data 会在 /opt/stacks/data 中创建。\n/opt/stacks:/app/stacks：这是 Dockge 管理你的所有 Docker Compose 项目的核心目录。 在此目录下的所有子目录中，如果包含 docker-compose.yml 文件，Dockge 都会将其识别为一个堆栈。\n\n\nPUID 和 PGID：为了确保 Dockge 容器内的进程拥有正确的权限来读写主机的 /opt/stacks 目录。\n你可以通过 SSH 登录服务器后，运行 id your_username 命令来查看你当前用户的 uid (PUID) 和 gid (PGID)。\n对于大多数 Linux 发行版，默认用户的 uid=1000, gid=1000 (或 gid=100 for users group)。请根据实际情况进行调整。\n\n\n\n保存并关闭文件。\n步骤 3：启动 Dockge 容器在 /opt/stacks 目录下，执行以下命令来启动 Dockge：\nsudo docker compose up -d\n\n\ndocker compose up：根据 docker-compose.yml 文件创建并启动服务。（旧版本 Docker 可能需要用 docker-compose 命令）\n-d：表示在后台运行容器。\n\n如果一切顺利，Dockge 容器应该已经启动并运行。\n步骤 4：检查容器状态sudo docker ps -a | grep dockge\n\n你应该看到 dockge 容器的状态是 Up ...。\n步骤 5：访问 Dockge Web UI打开你的浏览器，访问 http://你的服务器IP:5001。\n首次访问时，你需要创建一个管理员用户：\n\n输入用户名。\n输入密码。\n点击 创建。\n\n登录后，你将看到 Dockge 的主界面。由于我们刚刚将 /opt/stacks 目录映射为 Dockge 的 stacks 目录，Dockge 应该会自动检测到在你创建 docker-compose.yml 文件的当前目录下的一个叫做 dockge 的堆栈。\n3. 多堆栈管理示例Dockge 的强大之处在于管理多个 Docker Compose 堆栈。\n假设你现在想在服务器上部署一个 Nginx 服务。\n\n在 /opt/stacks 目录下创建一个新的子目录（例如 nginx）：\nmkdir -p /opt/stacks/nginxcd /opt/stacks/nginx\n创建 docker-compose.yml 文件：\nnano docker-compose.yml\n粘贴如下 Nginx 服务的 Docker Compose 配置：\nversion: &#x27;3.8&#x27;services:  nginx:    image: nginx:latest    container_name: my-nginx    restart: unless-stopped    ports:      - &quot;80:80&quot;        # 映射主机80端口到容器80端口      - &quot;443:443&quot;      # 映射主机443端口到容器443端口    volumes:      - ./nginx.conf:/etc/nginx/nginx.conf:ro # 挂载自定义Nginx配置      - ./html:/usr/share/nginx/html:ro       # 挂载静态网页文件    environment:      - PUID=1000      - PGID=100      - TZ=Asia/Shanghai\n保存并关闭文件。\n\n在 Dockge UI 中刷新：返回 Dockge 的 Web 界面，你会在左侧的导航栏或主界面的堆栈列表中看到多一个名为 nginx 的堆栈。\n\n点击 nginx 堆栈，你可以查看其详情。\n点击绿色的 Up 按钮，Dockge 就会拉取 Nginx 镜像并启动容器。\n\n\n\n通过这种方式，你可以在一个集中的界面管理你的各种服务，每个服务都拥有独立的 docker-compose.yml 文件。\n四、Dockge 界面功能速览\nStacks (堆栈)：列出所有检测到的 Docker Compose 项目。可以一键启动、停止、重启、删除（包括强制删除）。\nEdit (编辑)：直接在浏览器中打开 docker-compose.yml 文件进行编辑，支持语法高亮和基本的错误检查。编辑后会提示你保存并应用更改。\nLogs (日志)：查看堆栈中所有容器的实时日志。\nSettings (设置)：配置 Dockge 本身的一些行为，例如用户管理（未来功能）、主题等。\n更新 Dockge：在 Dockge UI 内部，你通常可以找到一个按钮来更新 Dockge 自身到最新版本。\n\n五、总结与展望Dockge 是一个非常有前景的 Docker Compose Web UI 工具。它专注于核心功能，提供了简洁直观的用户体验，非常适合那些希望通过图形界面来管理 Docker Compose 堆栈的个人开发者或小型团队。\n如果你正在寻找一个轻量级、功能强大且易于使用的 Docker Compose 管理工具，那么 Dockge 绝对值得一试。它能帮你告别繁琐的命令行操作，让 Docker 容器的部署和管理变得更加轻松愉悦。\n开始使用 Dockge 吧，让你的容器管理效率更上一层楼！\n","categories":["Docker"],"tags":["Docker","2024","NAS"]},{"title":"在NAS上部署Jellyfin媒体服务器","url":"/2024/2024-11-01_%E5%9C%A8NAS%E4%B8%8A%E9%83%A8%E7%BD%B2Jellyfin%E5%AA%92%E4%BD%93%E6%9C%8D%E5%8A%A1%E5%99%A8/","content":"\nJellyfin 是一个免费、开源的媒体系统，可以帮助你管理、播放和流式传输你的电影、电视节目、音乐、照片等媒体内容。它是一个强大的替代品，适用于那些希望完全控制自己数据的用户，与 Emby 和 Plex 类似，但完全免费且无任何订阅限制。将 Jellyfin 部署在 NAS 上，可以充分利用 NAS 的存储能力、稳定性和网络共享特性，打造专属的家庭影音中心。\n\n“拥有自己的媒体服务器，意味着你的影音世界，你做主。”\n\n\n一、为什么选择 Jellyfin 和 NAS？为什么是 Jellyfin？\n完全免费且开源：无需任何订阅费用，社区活跃，持续更新。\n私有化部署：所有数据（元数据、观看记录）都存储在你的服务器上，完全掌控。\n跨平台客户端：支持 Web 浏览器、Android、iOS、Apple TV、Roku、Fire TV、Kodi 插件等多种设备。\n硬件加速：支持多种硬件解码&#x2F;编码，提供流畅的转码体验（如果你的 NAS 支持）。\n强大的媒体管理：自动抓取电影、电视节目的元数据、海报、预告片，整理媒体库。\n\n为什么部署在 NAS 上？\n集中存储：NAS 天然就是存储海量媒体文件的最佳场所。\n24&#x2F;7 运行：NAS 通常设计为低功耗、长时间运行，非常适合作为媒体服务器。\n网络共享：方便家庭内网甚至外网访问。\n数据安全：NAS 通常支持 RAID，提供一定的数据冗余和保护。\nDocker 支持：主流 NAS 都支持 Docker，使得 Jellyfin 的部署和管理变得轻而易举。\n\n二、部署前的准备本教程主要以 Docker 部署为例，因为这是最通用、最灵活、最推荐的方式。\n1. NAS 要求\n支持 Docker：确保你的 NAS 型号和操作系统版本支持 Docker。群晖 (Synology) 和威联通 (QNology) 的大部分型号都支持。\n足够的存储空间：存储你的媒体文件。\n足够的内存：建议 4GB 及以上，如果需要进行转码，内存和 CPU 都更重要。\nCPU 性能（可选，但推荐）：如果需要进行实时转码，CPU 性能（尤其是集成核显 Quick Sync 或支持其他转码技术的 CPU）至关重要。\n\n2. 软件准备\nDocker：确保你的 NAS 上已安装 Docker。\nSSH 客户端：如 PuTTY (Windows) 或终端 (macOS&#x2F;Linux)，用于连接 NAS 进行命令行操作。\n文件管理器：用于在 NAS 上创建媒体文件夹。\n\n3. 理解 Docker 部署的好处\n环境隔离：Jellyfin 运行在独立的容器中，不会污染 NAS 系统环境。\n易于部署和管理：使用 Docker Compose 可以一行命令启动整个服务。\n版本控制：方便升级和回滚 Jellyfin 版本。\n可移植性：配置一旦完成，可以轻松迁移到其他支持 Docker 的平台。\n\n三、部署步骤（以 Docker Compose 为例）1. 登录 NAS，启用 SSH大多数 NAS 厂商会提供一个控制面板。请查找并启用 SSH 功能。\n\n群晖 (Synology): 控制面板 -&gt; 终端机和 SNMP -&gt; 启用 SSH 功能\n威联通 (QNOLOGY): 控制台 -&gt; 网络和文件服务 -&gt; Telnet/SSH -&gt; 允许 SSH 连接\n\n记下 NAS 的 IP 地址和 SSH 端口（通常是 22）。\n2. 创建目录结构通过 NAS 的文件管理器或 SSH 命令，创建用于 Jellyfin 存储配置和媒体文件的目录。\n推荐目录结构：\n/volume1/docker/jellyfin/       # Jellyfin 配置目录/volume1/data/media/            # 媒体文件总目录/volume1/data/media/movies/     # 电影/volume1/data/media/tvshows/    # 电视节目/volume1/data/media/music/      # 音乐/volume1/data/media/photos/     # 照片\n\nSSH 命令示例：\n# 登录 NASssh your_nas_username@your_nas_ip# 创建 Docker 配置目录sudo mkdir -p /volume1/docker/jellyfin/configsudo mkdir -p /volume1/docker/jellyfin/cache# 创建媒体文件目录sudo mkdir -p /volume1/data/media/moviessudo mkdir -p /volume1/data/media/tvshowssudo mkdir -p /volume1/data/media/musicsudo mkdir -p /volume1/data/media/photos# 授予 Jellyfin 访问这些媒体目录的权限（重要！）# Jellyfin 容器通常以 UID 1000, GID 100 运行。# 确保 jellyfin 用户或用户组有读写这些目录的权限。# 最简单粗暴的方式是给 777 权限，但生产环境不推荐。# 更好的方式是改变这些目录的所有者或组，使其匹配 Jellyfin 容器内的用户/组。# 例如，如果你的 NAS 上有一个 &#x27;docker&#x27; 用户组，可以将媒体目录的组改为 &#x27;docker&#x27;# 并且确保 jellyfin 容器内的 UID/GID 有权限，或者容器启动时指定 UID/GID。# 这里我们先用最简单的方式测试，后续可以优化权限。sudo chmod -R 777 /volume1/data/media\n\n3. 创建 Docker Compose 文件在 /volume1/docker/jellyfin/ 目录下创建一个名为 docker-compose.yml 的文件。\ncd /volume1/docker/jellyfin/sudo nano docker-compose.yml\n\n将以下内容粘贴到 docker-compose.yml 文件中：\nversion: &quot;3.8&quot;services:  jellyfin:    image: nyanmisaka/jellyfin:latest # 官方镜像为：jellyfin/jellyfin:latest 国内建议使用：nyanmisaka/jellyfin:latest    container_name: jellyfin    network_mode: bridge              # 如果使用 host 网络模式，方便端口映射和硬件加速，无需手动映射端口    ports:      - 8096:8096  # Web UI 端口      # - 8920:8920  # HTTPS 端口 (可选)      # - 1900:1900/udp # DLNA 发现端口      # - 7359:7359/udp # Android TV 发现端口    volumes: # 根据自己的NAS目录调整      - /volume1/docker/jellyfin/config:/config # 配置文件目录      - /volume1/docker/jellyfin/cache:/cache   # 缓存文件目录      # 映射你的媒体文件路径      - /volume1/data/media/movies:/media/movies:ro   # 只读挂载电影      - /volume1/data/media/tvshows:/media/tvshows:ro # 只读挂载电视剧      - /volume1/data/media/music:/media/music:ro     # 只读挂载音乐      - /volume1/data/media/photos:/media/photos:ro   # 只读挂载照片      # 如有需要，可以添加更多媒体文件夹      # **硬件加速配置 (根据你的 NAS 硬件选择)**      # 1. Intel 核显 Quick Sync (群晖大部分 Intel CPU NAS 适用)      #    确保你的NAS系统已安装i915驱动      #    - /dev/dri:/dev/dri      # 2. NVIDIA GPU (如果有支持的独立显卡)      #    确保已安装NVIDIA Docker runtime      #    runtime: nvidia      #    environment:      #      - NVIDIA_VISIBLE_DEVICES=all      #      - NVIDIA_DRIVER_CAPABILITIES=all    environment:      # - PUID=1000 # 容器内用户ID，通常是 default user，可能需要根据自己NAS的用户ID调整      # - PGID=100 # 容器内用户组ID，通常是 users group，可能需要根据自己NAS的用户组ID调整      - TZ=Asia/Shanghai # 设置时区      # - JELLYFIN_FFMPEG=/usr/lib/jellyfin-ffmpeg/ffmpeg # 指定FFmpeg路径（高级用户，通常不需要）      restart: unless-stopped # 容器崩溃或NAS重启后自动重启      # 推荐设定资源限制，防止 Jellyfin 占用过多资源    # deploy:    #   resources:    #     limits:    #       cpus: &#x27;2.0&#x27; # 限制为2个CPU核心    #       memory: 4G  # 限制为4GB内存\n\n关于 PUID 和 PGID：这两个环境变量是为了让 Jellyfin 容器里的进程拥有正确的用户ID和用户组ID，从而能够访问 NAS 文件系统上的媒体文件。\n\n你可以通过 SSH 登录 NAS 后，运行 id your_nas_username 命令来查看你当前用户的 uid (PUID) 和 gid (PGID)。\n通常，uid=1000 (admin 或第一个创建的用户) 和 gid=100 (users 组) 是比较常见的默认值。\n如果 Jellyfin 无法访问媒体文件，这通常是权限问题，检查 PUID 和 PGID 是第一步。\n\n关于硬件加速：\n\nIntel 核显 (/dev/dri): 对于群晖等大部分内置 Intel CPU 带核显的 NAS，挂载 /dev/dri 即可利用 Quick Sync 进行转码。你需要确保 NAS 系统已正确安装驱动。\nNVIDIA GPU: 如果你的 NAS 有独立 NVIDIA 显卡（较少见），你需要安装 NVIDIA Docker Runtime，并配置 runtime 和 environment。\n其他：检查 Jellyfin 官方文档和你的 NAS 硬件手册，了解具体支持的硬件加速方式。如果不需要转码或者 NAS 性能足够，可以不配置。\n\n按 Ctrl + X，然后按 Y 确认保存，再按 Enter 退出 nano 编辑器。\n4. 启动 Jellyfin 容器在 docker-compose.yml 文件所在的目录下，执行以下命令来启动 Jellyfin：\nsudo docker compose up -d\n\n\ndocker compose up：根据 docker-compose.yml 文件创建并启动服务。（旧版本 Docker 可能需要用 docker-compose 命令）\n-d：表示在后台运行容器。\n\n如果一切顺利，Jellyfin 容器应该已经启动并运行。\n5. 检查容器状态sudo docker ps -a | grep jellyfin\n\n你应该看到 jellyfin 容器的状态是 Up ...。\n6. 访问 Jellyfin Web UI 进行初始化打开你的浏览器，访问 http://你的NAS_IP:8096。\n你将看到 Jellyfin 的安装向导：\n\nWelcome: 选择语言。\nCreate your first user: 创建管理员账户。这是 Jellyfin 内部的账户，与 NAS 账户无关。\nAdd Media Library: 添加你的媒体库。\n点击 + 添加媒体库。\n选择 内容类型 (例如：电影、电视节目、音乐)。\n为媒体库起一个名称 (例如：我的电影)。\n选择 文件夹，然后点击 +。你会看到你在 docker-compose.yml 中映射的 /media/movies、/media/tvshows 等目录。选择对应的目录。\n其他选项可以根据需要自行配置（如 下载元数据、抓取图片 等），通常默认即可。\n重复此步骤添加所有媒体库。\n\n\nPreferred Metadata Language: 选择媒体元数据语言。\nConfigure Remote Access: 如果你想从外网访问，这里可以选择允许远程访问。请确保你了解网络安全风险，并配置好路由器端口转发和防火墙。\nDone!: 完成设置。\n\n现在，Jellyfin 会开始扫描你的媒体文件，自动匹配元数据、海报等。你可以在 仪表盘 -&gt; 任务 中查看扫描进度。\n四、高级配置与优化1. 硬件解码&#x2F;编码（Hardware Transcoding）这是提升观看体验的关键，特别是当你需要在低带宽或不支持 Jellyfin 直播的设备上观看高码率视频时。\n\n确认 NAS 支持：检查你的 NAS CPU 是否支持 Intel Quick Sync Video (QSV)、AMD VCE&#x2F;VCN 或 NVIDIA NVENC&#x2F;NVDEC。\nDocker 配置：在 docker-compose.yml 中正确挂载硬件设备（参考前面 volumes 部分的 /dev/dri 或 NVIDIA 配置）。\nJellyfin 设置：\n登录 Jellyfin Web UI。\n点击右上角 管理员仪表盘 (齿轮图标)。\n选择 播放 -&gt; 转码。\n启用 启用硬件加速。\n选择正确的 硬件加速设备 (例如：VAAPI for Intel QSV, NVENC for NVIDIA)。\n保存设置，并尝试播放一个高码率视频，在 仪表盘 的 活动 中，你会看到转码信息，确认是否使用了硬件加速。\n\n\n\n2. 端口转发与外网访问如果你想从家庭网络外部访问 Jellyfin，你需要：\n\nNAS 上固定 IP：为你的 NAS 设置一个静态 IP 地址。\n路由器端口转发 (Port Forwarding)：在你的路由器设置中，将外部端口（例如 8096 或自定义的）转发到 NAS 的内部 IP 地址和 Jellyfin 的 8096 端口。\n域名和 SSL (可选，但非常推荐)：\n注册一个域名。\n使用 DDNS (动态 DNS) 服务，将你的域名解析到你家庭网络的公网 IP。\n通过 Nginx Proxy Manager 或 Caddy 等工具设置反向代理，并配置 SSL 证书（如 Let’s Encrypt），实现 HTTPS 安全访问。\n这会增加复杂度，但能大大提高安全性。\n\n\n\n3. 用户管理在 管理员仪表盘 -&gt; 用户 中，你可以创建新的用户，为他们分配查看不同媒体库的权限，以及设置是否允许转码等。\n4. 优化媒体文件命名Jellyfin 的元数据抓取严重依赖媒体文件的命名规范。遵循 Jellyfin 官方推荐的命名规范可以大大提高元数据匹配的准确性。\n\n电影：电影名称 (年份)/电影名称 (年份).ext (例如: Inception (2010)/Inception (2010).mkv)\n电视节目：节目名称/Season XX/节目名称 - SXXEXX - 剧集标题.ext (例如: Game of Thrones/Season 01/Game of Thrones - S01E01 - Winter Is Coming.mkv)\n\n5. 容器升级当 Jellyfin 有新版本发布时，升级非常简单：\ncd /volume1/docker/jellyfin/ # 进入 docker-compose.yml 所在目录sudo docker compose pull     # 拉取最新镜像sudo docker compose up -d    # 用新镜像重建并启动容器\n\n五、常见问题排查\nJellyfin 无法启动或连接：\n检查 Docker 容器是否正在运行 (sudo docker ps -a | grep jellyfin)。\n检查端口 8096 是否被占用 (sudo netstat -tuln | grep 8096)。\n检查 docker-compose.yml 文件是否有语法错误。\n\n\nJellyfin 无法访问媒体文件：\n最常见的问题是权限不足。 检查 PUID 和 PGID 是否正确对应 NAS 上的用户&#x2F;组 ID。\n检查 NAS 媒体文件夹的权限，确保 jellyfin 容器的用户有读（和部分写，如元数据）权限。可以尝试 sudo chmod -R 777 /volume1/data/media (临时测试用，不推荐长期使用)。\n检查 volumes 映射路径是否正确。\n\n\n媒体文件元数据抓取失败或不准确：\n检查媒体文件命名是否规范。\n在 Jellyfin 媒体库设置中，尝试 刷新元数据。\n检查网络连接，确保 Jellyfin 可以访问外网获取元数据。\n\n\n\n六、总结通过 Docker 在 NAS 上部署 Jellyfin 是一个强大且灵活的私有媒体中心解决方案。它让你能够完全掌控自己的媒体库，并在家庭网络中的各种设备上自由播放。虽然涉及到一些命令行操作和网络配置，但一旦设置完成，你将拥有一个稳定、高效、免费的影音娱乐平台。\n希望本教程能够帮助你成功搭建属于自己的 Jellyfin 媒体服务器！尽情享受你的数字内容吧！\n","categories":["NAS","影音娱乐"],"tags":["Docker","2024","NAS"]},{"title":"js特殊运算符的使用","url":"/2024/2024-11-24_js%E7%89%B9%E6%AE%8A%E8%BF%90%E7%AE%97%E7%AC%A6%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"JavaScript中存在一些特殊的运算符，如 ||=、&amp;&amp;=、??=、?.、??，它们在特定的场景下能够帮助开发者简化代码逻辑或增强代码的健壮性。\n\n\n1. |&#x3D; 逻辑或赋值运算符 (Logical OR assignment)\n\n定义：||&#x3D; 运算符用于指定变量在其值为假（Falsy）时才进行赋值操作。语法：a ||&#x3D; b，意为若 a 为假，则将 b 赋值给 a。使用场景：当需要为一个变量赋值，但仅在其当前值为假时执行赋值操作。\n\nlet x = 10;let y = 0;x ||= 5; // x 仍为 10，因为 10 被视为真值y ||= 5; // y 现在为 5，因为 0 被视为假值\n\n2. &amp;&amp;&#x3D; 逻辑与赋值运算符 (Logical AND assignment)\n\n定义： &amp;&amp;&#x3D; 运算符用于指定变量在其值为真（Truthy）时才进行赋值操作。语法： a &amp;&amp;&#x3D; b，意为若 a 为真，则将 b 赋值给 a。使用场景： 在需要确保变量已经被定义且为真时进行赋值操作。\n\nlet x = 10;let y;x &amp;&amp;= 5; // x 仍为 10，因为 10 被视为真值y &amp;&amp;= 5; // y 仍为 undefined，因为 y 未被定义\n\n3. ??&#x3D; 逻辑空赋值运算符 (Nullish coalescing assignment)\n\n定义：??&#x3D; 运算符用于指定变量在其值为 null 或 undefined 时才进行赋值操作。语法：a ??&#x3D; b，意为若 a 为 null 或 undefined，则将 b 赋值给 a。使用场景：在确保一个变量不存在或其值为 null 时进行赋值操作。\n\nlet x = null;let y = 10;x ??= 5; // x 现在为 5，因为 x 为 nully ??= 5; // y 仍为 10，因为 y 不为 null\n\n4. ?. 可选链运算符 (Optional chaining)\n\n定义：?. 运算符用于在对象链深处避免出现异常，当对象链中的某个属性为 null 或 undefined 时，避免出现错误。语法：obj?.prop，若 obj 存在且有 prop 属性，则返回 prop 属性值，否则返回 undefined。使用场景：在访问深层嵌套的对象属性时，避免因为中间某个属性为 null 或 undefined 导\n\nlet user = &#123;  name: &quot;John&quot;,  address: &#123;    city: &quot;New York&quot;,  &#125;,&#125;;console.log(user.address?.city); // 输出 &quot;New York&quot;console.log(user.address?.zipcode); // 输出 undefinedconsole.log(user.phone?.number); // 输出 undefined\n\n5. ?? 空值合并运算符 (Nullish coalescing operator)\n\n定义：?? 运算符用于在变量为 null 或 undefined 时提供默认值。语法：a ?? b，若 a 为 null 或 undefined，则返回 b，否则返回 a。使用场景：在需要提供默认值的场景下，确保变量不为 null 或 undefined。\n\nlet x = null;let y = 10;console.log(x ?? y); // 输出 10console.log(y ?? x); // 输出 10console.log(x ?? 5); // 输出 5console.log(y ?? 5); // 输出 10\n\n6. 总结\n在实际的开发中，合理使用这些特殊运算符能够提高代码的可读性和健壮性，同时简化复杂的逻辑判断。但是，过度使用这些运算符也会导致代码的可读性降低，因此在使用时需要权衡利弊。\n\n","categories":["前端技术","JavaScript"],"tags":["前端技术","2024","JavaScript"]},{"title":"Go语言排序算法解析","url":"/2024/2024-11-25_Go%E8%AF%AD%E8%A8%80%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/","content":"Go 语言实现经典排序算法解析本文档旨在详细解析在 Go 语言中实现经典的五种排序算法：快速排序、归并排序、插入排序、选择排序和堆排序。我们将深入探讨每种算法的原理、时间复杂度、空间复杂度，并提供清晰的 Go 语言代码实现示例。\n\n\n\n1. 引言排序算法是计算机科学中最基础也是最重要的算法之一。它们用于将一组数据按照特定顺序（升序或降序）进行排列。理解并掌握不同排序算法的原理及实现，对于优化程序性能、解决实际问题至关重要。\n在 Go 语言中，数组和切片 ([]int) 是常见的排序数据结构。我们将在接下来的章节中，针对这五种经典算法，给出易于理解和实践的 Go 语言实现。\n\n2. 快速排序 (Quick Sort)原理快速排序是一种分而治之的排序算法。其基本思想是：\n\n选择基准 (Pivot) 元素： 从数组中选择一个元素作为“基准”（通常是第一个、最后一个或中间元素，或随机选择）。\n分区 (Partition)： 重新排列数组，将所有比基准值小的元素放在基准的左边，所有比基准值大的元素放在基准的右边。基准值现在处于其最终的排序位置。\n递归排序： 递归地对基准值左边和右边的子数组进行快速排序。\n\n这个过程重复进行，直到所有子数组的长度为 0 或 1，此时数组完全有序。\n时间复杂度\n平均情况： O(n log n)\n最坏情况： O(n^2) (当数组已经有序或逆序，且选择的基准不当时发生)\n最好情况： O(n log n)\n\n空间复杂度\n平均情况： O(log n) (由递归栈深度决定)\n最坏情况： O(n) (当每次分区都非常不平衡时)\n\nGo 语言实现package mainimport (\t&quot;fmt&quot;\t&quot;math/rand&quot;\t&quot;time&quot;)// QuickSort 对整数切片进行快速排序func QuickSort(arr []int) &#123;\tquickSortHelper(arr, 0, len(arr)-1)&#125;// quickSortHelper 是快速排序的递归辅助函数func quickSortHelper(arr []int, low, high int) &#123;\tif low &lt; high &#123;\t\t// 分区操作，并返回基准元素的最终位置\t\tpivotIndex := partition(arr, low, high)\t\t// 递归对基准左右两边的子数组进行排序\t\tquickSortHelper(arr, low, pivotIndex-1)\t\tquickSortHelper(arr, pivotIndex+1, high)\t&#125;&#125;// partition 实现了分区操作func partition(arr []int, low, high int) int &#123;\t// 可以选择不同的基准策略，这里简单选择最后一个元素作为基准\t// 也可以随机选择基准，避免最坏情况\trand.Seed(time.Now().UnixNano())\tpivotIndex := low + rand.Intn(high-low+1) // 随机选择基准\tarr[pivotIndex], arr[high] = arr[high], arr[pivotIndex] // 将随机选择的基准与最后一个元素交换\tpivot := arr[high] // 选择最后一个元素作为基准\ti := low - 1       // i 指向小于基准的元素的右边界\tfor j := low; j &lt; high; j++ &#123;\t\t// 如果当前元素小于基准，将其交换到基准的左边\t\tif arr[j] &lt; pivot &#123;\t\t\ti++\t\t\tarr[i], arr[j] = arr[j], arr[i]\t\t&#125;\t&#125;\t// 将基准元素放到正确的位置 (i+1)\tarr[i+1], arr[high] = arr[high], arr[i+1]\treturn i + 1 // 返回基准元素的最终索引&#125;/*func main() &#123;\tarr := []int&#123;10, 7, 8, 9, 1, 5, 23, 12, 11, 2&#125;\tfmt.Println(&quot;原始数组:&quot;, arr)\tQuickSort(arr)\tfmt.Println(&quot;快速排序后:&quot;, arr) // [1 2 5 7 8 9 10 11 12 23]&#125;*/\n\n\n3. 归并排序 (Merge Sort)原理归并排序也是一种分而治之的算法。其基本思想是：\n\n分解 (Divide)： 将待排序的数组分解成两个大致相等的子数组。\n解决 (Conquer)： 递归地对这两个子数组进行归并排序。\n合并 (Combine)： 将两个已经排序的子数组合并成一个最终排序的数组。\n\n这个过程不断分解，直到子数组只包含一个元素（一个元素被认为是自然的有序）。然后从下而上，逐层合并这些有序的子数组。\n时间复杂度\n平均情况： O(n log n)\n最坏情况： O(n log n)\n最好情况： O(n log n)\n\n空间复杂度\nO(n) (合并操作需要一个额外的辅助数组)\n\nGo 语言实现package main// MergeSort 对整数切片进行归并排序func MergeSort(arr []int) []int &#123;\tif len(arr) &lt;= 1 &#123;\t\treturn arr // 递归基：数组长度为0或1时，已经有序\t&#125;\tmid := len(arr) / 2\tleft := MergeSort(arr[:mid])  // 递归排序左半部分\tright := MergeSort(arr[mid:]) // 递归排序右半部分\treturn merge(left, right) // 合并两个有序的子数组&#125;// merge 合并两个已经排序的整数切片func merge(left, right []int) []int &#123;\tresult := make([]int, 0, len(left)+len(right))\ti, j := 0, 0\t// 比较左右两个切片的元素，依次放入结果切片\tfor i &lt; len(left) &amp;&amp; j &lt; len(right) &#123;\t\tif left[i] &lt; right[j] &#123;\t\t\tresult = append(result, left[i])\t\t\ti++\t\t&#125; else &#123;\t\t\tresult = append(result, right[j])\t\t\tj++\t\t&#125;\t&#125;\t// 将剩余的元素添加到结果切片\tresult = append(result, left[i:]...)\tresult = append(result, right[j:]...)\treturn result&#125;/*func main() &#123;\tarr := []int&#123;38, 27, 43, 3, 9, 82, 10&#125;\tfmt.Println(&quot;原始数组:&quot;, arr)\tsortedArr := MergeSort(arr)\tfmt.Println(&quot;归并排序后:&quot;, sortedArr) // [3 9 10 27 38 43 82]&#125;*/\n\n\n4. 插入排序 (Insertion Sort)原理插入排序的工作方式类似于我们整理扑克牌。对于未排序数据中的每一个元素，它都会插入到已排序部分的相应位置上。\n\n假设数组的第一个元素已经排序。\n从第二个元素开始，遍历数组。\n对于当前元素，将其与前面已排序部分的元素逐一比较（从右向左）。\n如果当前元素小于已排序部分的某个元素，则将该已排序元素向右移动一位，为当前元素腾出位置。\n重复步骤 4，直到找到当前元素的正确位置或达到已排序部分的开头。\n将当前元素插入到正确的位置。\n\n时间复杂度\n平均情况： O(n^2)\n最坏情况： O(n^2) (当数组完全逆序时)\n最好情况： O(n) (当数组已经有序时，只需比较一次)\n\n空间复杂度\nO(1) (原地排序，只需要常数额外的空间)\n\nGo 语言实现package main// InsertionSort 对整数切片进行插入排序func InsertionSort(arr []int) &#123;\tn := len(arr)\tfor i := 1; i &lt; n; i++ &#123;\t\tkey := arr[i] // 待插入的元素\t\tj := i - 1    // 已排序部分的最后一个元素的索引\t\t// 将比 key 大的元素向右移动\t\tfor j &gt;= 0 &amp;&amp; arr[j] &gt; key &#123;\t\t\tarr[j+1] = arr[j]\t\t\tj--\t\t&#125;\t\t// 找到正确位置，插入 key\t\tarr[j+1] = key\t&#125;&#125;/*func main() &#123;\tarr := []int&#123;12, 11, 13, 5, 6&#125;\tfmt.Println(&quot;原始数组:&quot;, arr)\tInsertionSort(arr)\tfmt.Println(&quot;插入排序后:&quot;, arr) // [5 6 11 12 13]&#125;*/\n\n\n5. 选择排序 (Selection Sort)原理选择排序是一种简单直观的排序算法。其基本思想是：\n\n查找最小值： 在未排序部分中找到最小（或最大）的元素。\n放置到位： 将找到的最小值与未排序部分的第一个元素进行交换。\n重复： 重复上述步骤，直到整个数组排序完成。\n\n每次遍历都会确定一个元素在最终排序数组中的位置。\n时间复杂度\n平均情况： O(n^2)\n最坏情况： O(n^2)\n最好情况： O(n^2)\n\n空间复杂度\nO(1) (原地排序，只需要常数额外的空间)\n\nGo 语言实现package main// SelectionSort 对整数切片进行选择排序func SelectionSort(arr []int) &#123;\tn := len(arr)\tfor i := 0; i &lt; n-1; i++ &#123; // 外层循环遍历未排序部分的起始位置\t\tminIndex := i // 假设当前元素是最小的\t\t// 内层循环寻找未排序部分中的最小值\t\tfor j := i + 1; j &lt; n; j++ &#123;\t\t\tif arr[j] &lt; arr[minIndex] &#123;\t\t\t\tminIndex = j\t\t\t&#125;\t\t&#125;\t\t// 将找到的最小值与当前未排序部分的第一个元素交换\t\t// 这样，minIndex 处的元素就确定了其最终位置\t\tarr[i], arr[minIndex] = arr[minIndex], arr[i]\t&#125;&#125;/*func main() &#123;\tarr := []int&#123;64, 25, 12, 22, 11&#125;\tfmt.Println(&quot;原始数组:&quot;, arr)\tSelectionSort(arr)\tfmt.Println(&quot;选择排序后:&quot;, arr) // [11 12 22 25 64]&#125;*/\n\n\n6. 堆排序 (Heap Sort)原理堆排序是一种基于比较的排序算法，它利用了二叉堆（Binary Heap）的数据结构特性。二叉堆可以被看作一个完全二叉树，并满足堆的性质：\n\n最大堆（Max Heap）： 任何一个父节点的值都大于或等于其子节点的值。\n最小堆（Min Heap）： 任何一个父节点的值都小于或等于其子节点的值。\n\n堆排序的基本步骤：\n\n构建最大堆： 将待排序数组构建成一个最大堆。这意味着最大的元素总是在根节点（数组的第一个元素）。\n抽取最大值并重建堆：\n将堆顶元素（当前最大值）与堆的最后一个元素交换。\n将数组的有效长度减一，将刚交换到末尾的元素从堆中“删除”。\n对新的堆进行 堆化 (Heapify) 操作，恢复堆的性质（将新的堆顶元素下沉到正确位置）。\n\n\n重复： 重复步骤 2，直到堆中只有一个元素。此时，数组已经有序。\n\n时间复杂度\n平均情况： O(n log n)\n最坏情况： O(n log n)\n最好情况： O(n log n)\n\n空间复杂度\nO(1) (原地排序，只需要常数额外的空间)\n\nGo 语言实现package main// HeapSort 对整数切片进行堆排序func HeapSort(arr []int) &#123;\tn := len(arr)\t// 1. 构建最大堆 (从最后一个非叶子节点开始，向上堆化)\t// 最后一个非叶子节点索引为 n/2 - 1\tfor i := n/2 - 1; i &gt;= 0; i-- &#123;\t\theapify(arr, n, i)\t&#125;\t// 2. 逐个将最大元素 (堆顶) 放到数组末尾，并重新堆化\tfor i := n - 1; i &gt; 0; i-- &#123;\t\t// 将当前堆顶 (最大元素) 与堆的最后一个元素交换\t\tarr[0], arr[i] = arr[i], arr[0]\t\t// 对剩余的 n-1 个元素进行堆化，使其恢复最大堆性质\t\t// 此时，堆的大小为 i\t\theapify(arr, i, 0)\t&#125;&#125;// heapify 维护最大堆的性质，确保以 i 为根的子树是最大堆// n 是堆的有效大小func heapify(arr []int, n, i int) &#123;\tlargest := i       // 假定根节点是最大的\tleft := 2*i + 1    // 左子节点\tright := 2*i + 2   // 右子节点\t// 如果左子节点存在且大于当前 largest\tif left &lt; n &amp;&amp; arr[left] &gt; arr[largest] &#123;\t\tlargest = left\t&#125;\t// 如果右子节点存在且大于当前 largest\tif right &lt; n &amp;&amp; arr[right] &gt; arr[largest] &#123;\t\tlargest = right\t&#125;\t// 如果 largest 不是根节点，说明最大值在子节点中\tif largest != i &#123;\t\tarr[i], arr[largest] = arr[largest], arr[i] // 交换\t\theapify(arr, n, largest)                    // 递归堆化被交换的子树\t&#125;&#125;/*func main() &#123;\tarr := []int&#123;12, 11, 13, 5, 6, 7&#125;\tfmt.Println(&quot;原始数组:&quot;, arr)\tHeapSort(arr)\tfmt.Println(&quot;堆排序后:&quot;, arr) // [5 6 7 11 12 13]&#125;*/\n\n\n7. 总结与比较\n\n\n排序算法\n平均时间复杂度\n最坏时间复杂度\n最好时间复杂度\n空间复杂度\n稳定性\n特点\n\n\n\n快速排序\nO(n log n)\nO(n^2)\nO(n log n)\nO(log n)\n不稳定\n递归、分治、原地排序，平均性能最佳。\n\n\n归并排序\nO(n log n)\nO(n log n)\nO(n log n)\nO(n)\n稳定\n递归、分治，保证 O(n log n) 性能，但需要额外空间。\n\n\n插入排序\nO(n^2)\nO(n^2)\nO(n)\nO(1)\n稳定\n对于部分有序的数组效率高，小规模数据表现良好。\n\n\n选择排序\nO(n^2)\nO(n^2)\nO(n^2)\nO(1)\n不稳定\n简单直观，但性能总是 O(n^2)，交换次数少于冒泡排序。\n\n\n堆排序\nO(n log n)\nO(n log n)\nO(n log n)\nO(1)\n不稳定\n利用堆结构，原地排序，性能稳定。\n\n\n稳定性：指如果数组中有两个相同的元素，排序后它们的相对位置是否保持不变。\n在实际开发中，Go 语言标准库中的 sort 包提供了高效的通用排序函数（例如 sort.Ints, sort.Slice），它们通常基于内省排序 (IntroSort) 或 混合排序（结合了快速排序、堆排序和插入排序的优点），以在各种输入情况下提供最佳性能。了解这些基础算法有助于理解 sort 包的工作原理，并能在特定场景下根据需求定制或优化排序逻辑。\n","categories":["Golang","算法"],"tags":["算法","2024","Golang"]},{"title":"Home Assistant介绍与部署：打造你的智能家居中枢","url":"/2024/2024-12-10_Home%20Assistant%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%9A%E6%89%93%E9%80%A0%E4%BD%A0%E7%9A%84%E6%99%BA%E8%83%BD%E5%AE%B6%E5%B1%85%E4%B8%AD%E6%9E%A2/","content":"\nHome Assistant (HA) 是一个免费开源的智能家居自动化平台，它致力于将你家中所有不同品牌的智能设备连接起来，并提供统一的控制界面，实现设备间的联动自动化。与依赖云端的智能家居平台不同，Home Assistant 强调本地化控制和隐私保护。它是智能家居爱好者的终极控制中心，让你真正掌控自己的智能生活。\n\n“拥有 Home Assistant，意味着拥有一个由你完全掌控的智能家居大脑。”\n\n\n一、Home Assistant 是什么？Home Assistant 是一个用 Python 编写的开源项目，它能让你本地运行智能家居控制中心。它支持超过 2000 个集成（integrations），可以与市面上绝大多数智能设备和服务进行连接，包括但不限于：\n\n各种协议设备：Wi-Fi、Zigbee、Z-Wave、蓝牙、MQTT 等。\n主流品牌设备：飞利浦 Hue、小米、宜家、Sonos、谷歌 Home、亚马逊 Alexa、各种智能插座、传感器等。\n服务集成：天气预报、日历、邮件、通知服务、网络设备（路由器、NAS）监控等。\n\n核心优势：\n\n本地控制，注重隐私：大部分功能可以在本地网络中运行，不依赖云服务，数据不会上传到第三方。你的隐私得到最大程度的保护。\n强大的自动化：Home Assistant 提供了非常灵活和强大的自动化引擎。你可以根据传感器数据、时间、日落日出、人员状态等各种条件触发自动化。\n统一的用户界面 (Lovelace)：将所有智能设备集成到一个直观的 Web 界面，可以高度自定义卡片、仪表盘，满足个性化需求。\n开放性和可扩展性：庞大的社区驱动开发，持续有新的集成和功能加入。你可以通过 YAML 配置深度定制所有功能。\n跨平台：支持多种部署方式（树莓派、Docker、虚拟机、NAS 等）。\n活跃的社区：拥有庞大且热情的全球社区，提供丰富的文档、教程和故障排除支持。\n\n二、为什么选择 Home Assistant？\n打破品牌壁垒：不再受限于单个品牌的生态系统，你可以自由选择不同品牌的智能设备，并通过 Home Assistant 将它们连接起来。\n实现高级自动化：简单的“如果…就…”自动化已经过时。Home Assistant 可以让你构建复杂的自动化逻辑，例如“如果所有家人都离开家，并且窗户已关闭，并且是晚上，则关闭所有灯光并布防安防系统。”\n摆脱云服务依赖：许多智能设备在制造商的云服务器关闭后会变成“砖头”，而 Home Assistant 让你摆脱这种风险，即使互联网中断，你的本地自动化依然可以运行。\n数据安全与隐私：你的所有传感器数据、设备状态等都存储在本地，而非上传到第三方服务器。\nDIY 乐趣：对于喜欢动手、探索和定制的用户，Home Assistant 提供了无与伦比的乐趣和成就感。\n\n三、部署前的准备Home Assistant 有多种安装方式，这里主要介绍两种最常见的部署方式：Home Assistant Operating System (HAOS) 和 Home Assistant Container (Docker)。\n1. 部署方式选择\nHome Assistant Operating System (HAOS)：\n\n优点：最简单、最官方的部署方式。HAOS 是一个基于 Linux 的操作系统，专门为 Home Assistant 优化，集成了 Home Assistant Core、Supervisor、Add-ons (插件商店) 等所有组件。提供了最完整的体验，包括图形化的升级和备份。\n缺点：通常需要一个独立的硬件 (如树莓派、NUC 或虚拟机)。如果你想在现有服务器上同时运行其他服务，HAOS 可能不适合，因为它会接管整个系统。\n适用人群：智能家居入门用户、希望获得最完整和最稳定体验的用户、拥有独立硬件的用户。\n\n\nHome Assistant Container (Docker)：\n\n优点：最灵活的部署方式。你可以在任何支持 Docker 的 Linux、Windows、macOS 系统上运行 Home Assistant，与服务器上的其他 Docker 容器共享资源。适用于 NAS、Mini PC 或现有服务器用户。\n缺点：不包含 Supervisor 和 Add-ons 商店。你无法直接从 Home Assistant 界面安装插件，需要手动部署其他 Docker 容器来替代插件功能（例如 MQTT Broker、文件编辑器等）。\n适用人群：有 Docker 使用经验的用户、希望在现有服务器上共存其他服务的用户、希望最大化资源利用率的用户。\n\n\n\n本教程将主要侧重于 Home Assistant Container (Docker) 的部署，因为这在 NAS 和通用服务器上更为常见和灵活。\n2. 硬件要求\n一台运行 Docker 的服务器（NAS、树莓派 4B&#x2F;5、NUC、Mini PC、旧电脑等）。\nCPU：双核及以上 CPU。对于小型部署，树莓派 4B 足够。随着设备和自动化增多，建议更强的 CPU。\n内存：建议 2GB 及以上，随着集成的设备和自动化增多，建议 4GB 及以上。\n存储：建议使用 SSD 存储，可以显著提升 Home Assistant 的响应速度和数据库操作性能。至少 8GB 空间，建议 32GB 及以上。\n\n\n\n3. 软件要求\nDocker 和 Docker Compose：确保你的服务器已安装。\nSSH 客户端：用于连接服务器进行命令行操作。\n\n四、部署 Home Assistant Container (Docker)这种部署方式是在现有的 Docker 环境中运行 Home Assistant Core。\n1. 创建目录结构通过 SSH 连接到你的服务器，创建一个目录来存储 Home Assistant 的配置数据。\nsudo mkdir -p /opt/homeassistant/configsudo chmod -R 777 /opt/homeassistant/config # 确保容器有读写权限\n注意： /opt/homeassistant 仅为示例路径，你可以根据自己的喜好和 NAS 的卷结构调整。\n2. 创建 Docker Compose 文件进入刚刚创建的目录，并创建一个 docker-compose.yml 文件。\ncd /opt/homeassistantsudo nano docker-compose.yml # 或者其他你喜欢的编辑器，如 vi\n\n将以下内容粘贴到 docker-compose.yml 文件中：\nversion: &#x27;3&#x27;services:  homeassistant:    container_name: homeassistant    image: ghcr.io/home-assistant/home-assistant:stable # 使用稳定版镜像，或者 specific version    # image: homeassistant/home-assistant:latest # 另一个镜像源，但官方推荐 ghcr.io      # 网络模式，通常 host 模式更简单，方便 Home Assistant 发现局域网设备    network_mode: bridge     # 或者用 bridge 模式，需要手动映射端口    ports:      - &quot;8123:8123&quot; # Home Assistant Web UI 默认端口      volumes:      - /config:/config # 映射配置文件目录      - /etc/localtime:/etc/localtime:ro # 同步时区    # 对于需要访问 USB 设备的情况，例如 Zigbee/Z-Wave 棒    # devices:    #   - /dev/ttyUSB0:/dev/ttyUSB0 # Zigbee / Z-Wave USB Stick    #   # 如果有多个USB设备，可以添加更多行，或映射整个/dev/ttyUSB*    #   # 确保 /dev/ttyUSB0 是你的设备路径，可以通过 ls -l /dev/ttyUSB* 查询    environment:      # 在某些系统上，PUID/PGID 可能有助于文件权限      # - PUID=1000      # - PGID=100      - TZ=Asia/Shanghai # 设置时区      restart: unless-stopped # 容器崩溃或服务器重启后自动重启\n\n配置说明：\n\nimage: ghcr.io/home-assistant/home-assistant:stable：使用 Home Assistant 官方容器注册表的稳定版镜像。\nnetwork_mode: host：推荐使用。 让 Home Assistant 容器直接使用宿主机的网络堆栈。这样 Home Assistant 就可以更容易地发现局域网中的各种智能设备（如 Hue Hub、智能电视等），而无需复杂的端口转发或 Bridge 网络配置。缺点是容器不再拥有独立 IP。\nvolumes:\n/opt/homeassistant/config:/config：核心配置数据。 将宿主机的 /opt/homeassistant/config 目录映射到容器内部的 /config。Home Assistant 的所有配置、数据库、日志等都会存储在这里，实现数据持久化。\n/etc/localtime:/etc/localtime:ro：同步容器与宿主机的时区，避免时间显示错误。\n\n\ndevices: (如果需要)\n如果你有连接到服务器的 Zigbee 或 Z-Wave USB 棒，你需要将这些设备映射到容器内部，以便 Home Assistant 能够访问它们。\n你需要通过 ls -l /dev/ttyUSB* 或 ls -l /dev/serial/by-id 命令来确定你的 USB 设备的实际路径。\n\n\nenvironment:\nTZ=Asia/Shanghai：设置容器的时区。\nPUID&#x2F;PGID：在某些文件权限敏感的系统上，设置这些环境变量以确保容器的用户拥有正确的权限。通常对于 Docker 部署，默认用户 root 权限足够。\n\n\n\n保存并关闭文件。\n3. 启动 Home Assistant 容器在 /opt/homeassistant 目录下，执行以下命令来启动 Home Assistant：\nsudo docker compose up -d\n\n\ndocker compose up：根据 docker-compose.yml 文件创建并启动服务。（旧版本 Docker 可能需要用 docker-compose 命令）\n-d：表示在后台运行容器。\n\n如果一切顺利，Home Assistant 容器应该已经启动并运行。\n4. 检查容器状态和日志sudo docker ps -a | grep homeassistantsudo docker logs -f homeassistant # 查看实时日志，检查是否有错误\n在日志中，你可能会看到一些 Home Assistant 启动和设备发现的信息。\n5. 访问 Home Assistant Web UI 进行初始化打开你的浏览器，访问 http://你的服务器IP:8123。\n首次访问时，你需要进行初始化设置：\n\nWelcome: 创建你的第一个管理员账户（用户名和密码）。\nName your Home: 为你的家起一个名字，并设置地理位置、时区和海拔高度。这些信息对于一些基于位置和日照的自动化非常重要（例如，日出时开灯）。\nDiscovered devices: Home Assistant 会自动扫描你的本地网络，并列出它发现的所有兼容设备。你可以选择立即设置它们，或者稍后再添加。\nFinish: 完成设置。\n\n现在，你已经进入了 Home Assistant Lovelace 界面。\n五、首次运行后的配置与使用1. 添加集成 (Integrations)这是 Home Assistant 的核心。通过添加不同的集成，你可以将你的智能设备连接到 Home Assistant。\n\n在 Home Assistant Web UI 中，点击左侧导航栏的 设置。\n点击 设备与服务。\n点击右下角的 添加集成 按钮。\n搜索你家中的智能设备品牌或协议（例如：Philips Hue、Xiaomi Miot、MQTT、Zigbee Home Automation）。\n按照提示完成集成配置。\n\n常见集成示例：\nMQTT：如果你的设备支持 MQTT 协议，你需要先部署一个 MQTT Broker (例如 Eclipse Mosquitto)。然后在 Home Assistant 中配置 MQTT 集成。\nZigbee&#x2F;Z-Wave：如果使用 USB 棒，你需要安装 Zigbee2MQTT 或 ZHA (Zigbee Home Automation) 集成，或 OpenZWave&#x2F;ZwaveJS 集成，并确保 USB 棒已正确映射到容器。\nHomeKit Bridge：Home Assistant 可以模拟 HomeKit Hub，让你通过 Apple Home 应用控制未原生支持 HomeKit 的设备。\n通知服务：设置 Notify 集成，可以通过 Telegram、微信（PushDeer）、邮件等方式发送通知。\n\n2. 创建自动化 (Automations)这是 Home Assistant 的魅力所在。\n\n在 Home Assistant Web UI 中，点击左侧导航栏的 设置。\n点击 自动化与场景。\n点击右下角的 创建自动化。\n你可以使用图形界面来配置“触发器 (Triggers)”、“条件 (Conditions)”和“动作 (Actions)”。\n触发器：什么时候自动化会开始运行？（例如：传感器值变化，时间达到，天黑）\n条件：自动化只有在满足这些条件时才会运行。（例如：只有当所有人都离家时，只有当温度高于25度时）\n动作：自动化启动后会做什么？（例如：打开灯，发送通知，调整空调温度）\n\n\n\n3. 定制仪表盘 (Lovelace UI)Lovelace 是 Home Assistant 的用户界面。你可以高度自定义它，创建多个仪表盘，添加不同类型的卡片来显示设备状态、传感器数据、图表、照片等。\n\n在主界面的右上角，点击三个点 (菜单) -&gt; 编辑仪表盘。\n你可以添加新卡片、调整卡片位置、更改布局。\n对于高级用户，可以通过 YAML 模式进行更深度的自定义。\n\n4. 外部访问 (可选，但推荐)如果想在家庭网络外部访问 Home Assistant，你需要：\n\n固定 IP：为你的服务器或路由器设置静态 IP。\n路由器端口转发 (Port Forwarding)：将外部端口转发到 Home Assistant 的内部 IP 地址和 8123 端口。\n域名和 SSL (非常推荐)：\n注册一个域名并使用 DDNS (动态 DNS) 服务。\n通过 Nginx Proxy Manager 或 Caddy 等工具设置反向代理，并配置 SSL 证书（如 Let’s Encrypt），实现 HTTPS 安全访问。\nHome Assistant Cloud (Nabu Casa)：如果你不想自己配置端口转发和 SSL，可以考虑订阅 Home Assistant Cloud (Nabu Casa)。它能提供简单的外部访问和 Alexa&#x2F;Google Home 语音助手集成，并且费用可以支持 Home Assistant 的开发。\n\n\n\n六、常见问题与注意事项\n文件权限：确保 /opt/homeassistant/config 目录对 Docker 容器的用户有读写权限。如果遇到 Permission Denied 错误，通常是权限问题。\nMQTT Broker：如果你计划使用 MQTT 设备，你需要独立部署一个 MQTT Broker，例如 Eclipse Mosquitto，作为另一个 Docker 容器。\nZigbee&#x2F;Z-Wave 设备：确保 USB 棒已正确映射到容器内部，且服务器的驱动已安装。\n备份：定期备份 /opt/homeassistant/config 目录，这是你 Home Assistant 的所有配置和数据所在。\n性能：随着集成设备和自动化的增多，Home Assistant 可能会消耗更多资源。监控你的服务器资源使用情况，并在需要时升级硬件。\nYAML 配置：虽然 Home Assistant 提供了图形界面来创建自动化，但很多高级功能和调试仍然需要编辑 YAML 文件。学习一些 YAML 基础知识会很有帮助。\n\n七、总结Home Assistant 是一个强大、灵活且高度可定制的智能家居平台。通过本地部署和开放性，它为你提供了前所未有的智能家居控制权和隐私保护。虽然入门可能需要一点学习曲线，但一旦你掌握了它，你将能够打造一个真正属于你自己的、无缝联动的高级智能家居系统。\n开始你的 Home Assistant 之旅吧！一个全新的智能生活正在等待你。\n","categories":["NAS","实用工具"],"tags":["Docker","2024","NAS"]},{"title":"Frigate介绍与部署：基于AI的本地视频监控系统","url":"/2024/2024-12-15_Frigate%E4%BB%8B%E7%BB%8D%E4%B8%8E%E9%83%A8%E7%BD%B2%EF%BC%9A%E5%9F%BA%E4%BA%8EAI%E7%9A%84%E6%9C%AC%E5%9C%B0%E8%A7%86%E9%A2%91%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/","content":"\nFrigate 是一个开源的、高性能的本地视频监控系统，它利用 AI （特别是通过 Google Coral TPU 进行边缘计算）来实现实时目标检测，例如检测人、车辆、宠物等。与传统监控系统只是录像不同，Frigate 能够智能识别画面中的物体，并只在检测到感兴趣的事件时进行录像或发送通知，大大减少了存储空间和误报，同时提高了事件分析的效率。\n\n“让你的摄像头变得更智能，只记录你真正关心的事件。”\n\n\n一、Frigate 是什么？Frigate 是一个基于 FFmpeg 和 TensorFlow 的 AI 目标检测视频监控系统。它的核心思想是利用神经网络在本地对视频流进行实时分析，识别预定义的目标（如人、车），然后根据这些识别结果进行录制、快照捕捉或触发自动化。\n核心优势：\n\n本地处理：所有视频流和 AI 推理都在本地完成，保障隐私，不依赖云服务。\n实时目标检测：利用 Google Coral TPU 等硬件加速，实现毫秒级的实时检测。\n智能录像与快照：只在检测到目标时录制完整的视频片段，并捕捉关键帧快照。\n集成度高：与 Home Assistant 深度集成，可以作为强大的自动化触发器。\n事件分组：自动将连续的检测事件进行分组，方便回溯和管理。\n区域检测：可定义特定区域，只在该区域内进行目标检测。\n丰富的通知：结合 Home Assistant 或其他服务，发送包含快照的通知。\n高度可配置：通过 YAML 文件进行详细配置，满足各种高级需求。\n开源免费：完全开放源代码，社区活跃。\n\n二、为什么选择 Frigate？\n告别误报：传统监控遇到树叶摇曳、光线变化、小动物经过等情况常会误报，Frigate 通过 AI 准确识别“人”或“车”，显著减少误报。\n节省存储：只录制有事件发生的片段，而非持续录像，大大节省 NAS &#x2F; NVR 的存储空间。\n快速查找事件：通过 Frigate 的 Web UI 或 Home Assistant 界面，可以快速浏览所有检测到的“人”或“车”事件，而无需大海捞针般地查看大量录像。\n强大的自动化：结合 Home Assistant，当 Frigate 检测到“人”时，可以自动开灯、发送通知、触发警报等。\n隐私保护：所有处理都在本地完成，无需将视频流上传到第三方云服务。\n\n三、部署前的准备Frigate 推荐使用 Docker 部署。为了发挥其最佳性能，特别是实时 AI 推理，强烈建议使用 Google Coral TPU。\n1. 硬件要求\n服务器：一台能运行 Docker 的 Linux 服务器（如 NAS、NUC、树莓派 4B&#x2F;5、Mini PC、旧电脑等）。\nCPU：建议有一定性能的 CPU，如 Intel Celeron J4125&#x2F;J5005 或更好，用于运行 FFmpeg。\n内存：建议 4GB 及以上。\n\n\n摄像头：支持 RTSP 协议的 IP 摄像头（几乎所有现代网络摄像头都支持）。\nGoogle Coral TPU (强烈推荐)：\nUSB Accelerator：最常见的形式，通过 USB 3.0 接口连接到你的服务器。\nPCIe Accelerator：性能更高，适用于有 PCIe 插槽的服务器。\n为何需要？：没有 Coral TPU，Frigate 也能运行，但 AI 推理将完全依赖 CPU，性能会非常差，可能无法满足多路摄像头的实时检测需求，甚至可能导致 CPU 占用过高。Coral TPU 可以极大地加速 AI 推理，让每秒帧数 (FPS) 大幅提升，从而实现实时分析。\n\n\n\n2. 软件要求\nDocker 和 Docker Compose：确保你的服务器已安装。\nSSH 客户端：用于连接服务器进行命令行操作。\n媒体存储路径：用于保存录像和快照的持久化存储目录。\nPython (如果需要 Coral)：部分系统可能需要安装 Python 和 Coral 驱动。\n\n3. 理解 Docker 部署的好处\n环境隔离：Frigate 运行在独立的容器中，不影响宿主系统。\n易于部署和管理：使用 Docker Compose 一键启动、停止、升级。\n版本控制：方便升级和回滚 Frigate 版本。\nCoral TPU 兼容性：Docker 提供标准化的方式来将 Coral 设备映射到容器内部。\n\n四、部署步骤（以 Docker Compose 为例）1. 挂载 Coral TPU (如果使用)如果你的服务器连接了 Google Coral TPU，需要确保宿主系统能够识别它，并将其映射到 Docker 容器中。\nUSB Coral\n安装 Coral 驱动 (部分系统可能需要)：通常，Linux 系统会自动识别 USB Coral。如果遇到问题，可以参考 Coral 官方文档安装 libedgetpu1-std。\n\n检查设备：连接 Coral TPU 到 USB 3.0 端口，然后运行：\nlsusb # 查看 USB 设备列表，应能看到 Google Inc. 或 Global Unichip Corp.ls -l /dev/bus/usb/ # 查看 USB 设备文件\n你可能会看到一个类似 Bus 001 Device 002: ID 1a6e:089a Global Unichip Corp. 的设备。\n\n\nPCIe Coral\n安装 Coral 驱动：PCIe Coral 需要安装驱动。请参考 Coral 官方文档。\n检查设备：lspci -nn | grep -i coral\n应能看到 Coral PCIe 设备。\n\n2. 创建目录结构通过 SSH 连接到你的服务器，创建用于 Frigate 存储配置、录像、缓存等数据的目录。\n# 创建 Frigate 配置目录sudo mkdir -p /mnt/data/frigate/configsudo chmod -R 777 /mnt/data/frigate/config # 确保容器有读写权限# 创建 Frigate 媒体存储目录sudo mkdir -p /mnt/data/frigate/mediasudo chmod -R 777 /mnt/data/frigate/media # 确保容器有读写权限# 如果你还有额外的用于存储录像的目录，如NAS的共享文件夹，也一并创建并设置权限# sudo mkdir -p /mnt/your_nas_share_for_recordings# sudo chmod -R 777 /mnt/your_nas_share_for_recordings\n注意： /mnt/data/frigate 仅为示例路径，请根据你的存储实际情况调整。\n3. 编写 config.yml (Frigate 核心配置文件)在 /mnt/data/frigate/config 目录下创建一个名为 config.yml 的文件。这是 Frigate 最重要的配置文件，定义了你的摄像头、AI 模型、检测区域等。\nsudo nano /mnt/data/frigate/config/config.yml\n\n一个基本的 config.yml 示例（请根据你的摄像头和需求修改）：\n# Frigate 配置示例# MQTT 配置 (与 Home Assistant 集成需要)# 如果不使用 Home Assistant 或 MQTT，可以禁用或删除此部分mqtt:  host: YOUR_MQTT_BROKER_IP # 例如 Home Assistant 的 IP  # user: mqtt_user         # 如果你的 MQTT 需要认证  # password: mqtt_password # 如果你的 MQTT 需要认证  # port: 1883              # 默认端口# TensorFlow Lite AI 模块配置detectors:  cpu1: # 定义一个 CPU 检测器，如果你没有 Coral TPU，推理会非常慢    type: cpu  # coral_tpu: # 如果你有 Coral TPU，请取消注释并使用此配置  #   type: edgetpu  #   device: usb # 或 &quot;pci&quot; 如果是 PCIe 版本，如果只有一个 TPU 可以不指定 device# Frigate 主配置database:  path: /media/frigate.db # 数据库文件路径 (推荐默认，会自动挂载到持久化目录)# 视频录制和快照等配置record:  enabled: True # 启用录制  events:    pre_capture: 5 # 事件发生前录制 5 秒    post_capture: 5 # 事件发生后录制 5 秒    max_seconds: 300 # 最长录制 300 秒 (5分钟)    # objects:       # 仅录制哪些类型的对象 (默认录制所有配置的检测对象)    #   - person    #   - car  retain:    default: 10 # 默认保存 10 天的录像 (按事件保留)    # days:     # 也可以按天数设置 (默认按事件数量)    #   default: 7snapshots:  enabled: True            # 启用快照  bounding_box: True       # 快照中显示检测框  timestamp: True          # 快照中显示时间戳  retain:    default: 7 # 默认保存 7 天的快照    # days:    #   default: 7object_detection:  enabled: True  # lp_detector:  #   enabled: False # 是否启用车牌检测，需要额外模型ffmpeg:  # 线程数根据你的CPU核心数调整，如果CPU性能不够，可能需要降低  # global_args: -hwaccel vaapi -hwaccel_output_format vaapi # 如果Intel CPU支持VAAPI硬件加速FFmpeg解码  output_args:    detect: -f segment -segment_times 10 -segment_format mp4 -r 10 -c:v libx264 -preset ultrafast -tune zerolatency -crf 23 -bf 0 -g 30 -sc_threshold 0 -pix_fmt yuv420p -movflags +faststart # 检测流的FFmpeg输出参数，推荐    record: -c copy -map 0:v:0 -map 0:a? -f segment -segment_times 10 -segment_format mp4 -reset_timestamps 1 -strftime 1 -ar 44100 # 录制流的FFmpeg输出参数，推荐    rtmp: -c copy -map 0:v:0 -map 0:a? -f flv # RTMP 流输出参数# Web UI 配置web:  port: 5000 # Web UI 端口  # password: your_password # 如果需要为Web UI设置密码# 摄像头配置 (重点配置项)cameras:  front_door: # 摄像头名称，唯一标识符    enabled: True    ffmpeg:      inputs:        - path: rtsp://user:password@192.168.1.100:554/stream1 # 摄像头的 RTSP 地址          roles:            - detect # 用于目标检测的视频流 (通常是低分辨率子码流，节省资源)            - record # 用于录像的视频流 (可以是高分辨率主码流)        # - path: rtsp://user:password@192.168.1.100:554/stream2 # 如果有第二个流用于 Web UI 预览        #   roles:        #     - rtmp # 用于 Web UI 预览的 RTMP 流    detect:      enabled: True      # 减小帧率以降低 CPU 占用，如果你没有 Coral。      # frigate默认会根据FFmpeg output的帧率进行检测。      # 如果你的RTSP流本身是30FPS，并且你想降低检测帧率，则可以在这里指定：      # fps: 5    zones: # 区域检测 (可选)      # 只在特定区域内进行检测，或者在特定区域内不检测      driveway:        coordinates: 0,0,0,1,1,1,1,0 # 定义一个多边形区域，以像素百分比表示 (左上角是0,0，右下角是1,1)                                      # 例如：0,0, 0.5,0, 0.5,0.5, 0,0.5 （上、右、下、左）        objects:          - person # 只在此区域检测人    objects:      track: # 跟踪的对象类型        - person        - car        - dog        - cat      filters: # 过滤条件        person:          min_area: 5000 # 最小检测面积          max_area: 1000000 # 最大检测面积          threshold: 0.7 # 置信度阈值        car:          min_area: 10000    motion:      mask: # 运动检测遮罩区域 (可选)        - 0,0,0,0.5,0.5,0.5,0.5,0 # 示例：遮蔽图像上半部分      threshold: 25 # 运动检测阈值      contour_area: 50 # 轮廓区域阈值  back_yard: # 第二个摄像头，如果需要    enabled: True    ffmpeg:      inputs:        - path: rtsp://user:password@192.168.1.101:554/H264_stream # 另一个摄像头的 RTSP 地址          roles:            - detect            - record    detect:      enabled: True    objects:      track:        - person        - car\n\n保存并关闭文件。\n配置解释：\n\nmqtt：用于与 Home Assistant 集成，将检测事件发布到 MQTT Broker。\ndetectors：定义 AI 推理设备。cpu 是默认的，edgetpu 是为 Coral TPU 准备的。如果你有 Coral，记得取消注释 coral_tpu 部分。\nrecord &#x2F; snapshots：控制录像和快照的行为，保留时间等。\nffmpeg：FFmpeg 的参数配置。inputs 是摄像头的 RTSP 地址，roles 定义了该流的用途 (detect 用于检测，record 用于录像，rtmp 用于 Web UI 预览)。\ncameras：定义你的每个摄像头。\nffmpeg：每个摄像头的 FFmpeg 配置。\ndetect.fps：用于检测的帧率，如果 CPU 性能不足且没有 Coral，可以适当降低此值。\nzones：定义感兴趣的检测区域，可以减少误报。\nobjects.track：指定 Frigate 应该关注哪些类型的对象。\nmotion.mask：定义忽略运动的区域。\n\n\n\n4. 创建 Docker Compose 文件在 /mnt/data/frigate 目录下创建一个名为 docker-compose.yml 的文件。\nsudo nano /mnt/data/frigate/docker-compose.yml\n\n将以下内容粘贴到 docker-compose.yml 文件中：\nversion: &quot;3.8&quot;services:  frigate:    container_name: frigate    image: blakeblackshear/frigate:stable # 推荐使用 stable 标签    # image: blakeblackshear/frigate:0.13.0-beta # 如果你想尝试最新功能，使用特定版本      privileged: true # 必需，允许访问 /dev/dri 或 /dev/bus/usb      # 网络模式，通常 host 模式更简单，避免复杂的端口映射，且方便FFmpeg访问RTSP流    network_mode: host     # 或者用 bridge 模式，需要手动映射端口    # ports:    #   - &quot;5000:5000&quot; # Frigate Web UI    #   - &quot;1935:1935&quot; # RTMP 流      volumes:      - /etc/localtime:/etc/localtime:ro # 同步时区      - /mnt/data/frigate/config:/config:ro # 映射 Frigate 配置文件 (只读，避免容器修改)      - /mnt/data/frigate/media:/media # 映射录像、快照和数据库等数据 (读写)      # 如果使用 Intel 核显进行 FFmpeg 解码/编码硬件加速      # - /dev/dri:/dev/dri      # 如果使用 Coral TPU (USB 版本或 PCIe 版本通用设备映射)      # - /dev/bus/usb:/dev/bus/usb # 挂载整个 USB bus，让容器识别 Coral USB      # 如果你的Coral设备文件是固定的，可以精确映射，例如：      # - /dev/bus/usb/001/002:/dev/bus/usb/001/002 # 精确映射某个USB设备      # 如果使用 NVIDIA GPU (需要安装 NVIDIA Container Toolkit)      # devices:      #   - /dev/nvidia0:/dev/nvidia0      #   - /dev/nvidiactl:/dev/nvidiactl      #   - /dev/nvidia-uvm:/dev/nvidia-uvm      # runtime: nvidia    environment:      # 在某些系统上，PUID/PGID 可能有助于文件权限      # - PUID=1000      # - PGID=100      # 更多环境变量可参考 Frigate 文档，例如：      # - FRIGATE_ENV_VAR=value      # 设定 CPU 核心和内存限制，防止占用过多资源    # deploy:    #   resources:    #     limits:    #       cpus: &#x27;3.0&#x27; # 限制为3个CPU核心    #       memory: 4G  # 限制为4GB内存    restart: unless-stopped # 容器崩溃或服务器重启后自动重启\n\n配置解释：\n\nimage: blakeblackshear/frigate:stable：使用 Frigate 的稳定版 Docker 镜像。\nprivileged: true：必需。允许容器访问宿主机的 /dev 设备，这是为了让容器能够识别和使用 /dev/dri (Intel GPU) 或 /dev/bus/usb (Coral USB)。\nnetwork_mode: host：为了简化，让容器直接使用宿主机的网络堆栈。这样 Frigate 就可以直接访问你的局域网中的摄像头，而无需复杂的端口转发或 Bridge 网络配置。缺点是容器不再拥有独立 IP。如果你需要为 Frigate 分配一个独立的 IP 地址，请使用 bridge 模式并手动映射端口。\nvolumes:\n/etc/localtime:/etc/localtime:ro：同步容器和宿主机的时区。\n/mnt/data/frigate/config:/config:ro：将宿主机的 config 目录映射为容器内部的 /config。ro 表示只读，这意味着 Frigate 无法修改 config.yml。\n/mnt/data/frigate/media:/media：将宿主机的 media 目录映射为容器内部的 /media。Frigate 会将录像、快照、数据库文件等存储在这里。\n/dev/dri:/dev/dri (Intel GPU 解码&#x2F;编码)：如果使用 Intel CPU 的核显进行硬件加速，请取消注释此行。\n/dev/bus/usb:/dev/bus/usb (Coral USB TPU)：如果使用 USB Coral TPU，请取消注释此行。\n\n\nenvironment: 如果宿主机的文件权限与容器内 Jellyfin 的 PUID&#x2F;PGID 不匹配，你可能需要根据实际情况设置这些环境变量，确保容器有权限写入 media 目录。\nrestart: unless-stopped：保证 Frigate 在服务器重启后自动启动。\n\n保存并关闭文件。\n5. 启动 Frigate 容器在 /mnt/data/frigate 目录下，执行以下命令来启动 Frigate：\nsudo docker compose up -d\n\n\ndocker compose up：根据 docker-compose.yml 文件创建并启动服务。\n-d：表示在后台运行容器。\n\n如果一切顺利，Frigate 容器应该已经启动并运行。\n6. 检查容器状态和日志sudo docker ps -a | grep frigatesudo docker logs -f frigate # 查看实时日志，检查是否有错误，特别是关于FFmpeg和Coral TPU的\n在日志中，你应该能看到 FFmpeg 启动、摄像头流接收以及 Coral TPU 初始化成功的消息。\n7. 访问 Frigate Web UI打开你的浏览器，访问 http://你的服务器IP:5000。\n你将看到 Frigate 的 Web UI 界面。\n\n在 Live 页面，你应该能看到你的摄像头实时画面。\n在 Events 页面，当 Frigate 检测到配置的对象时，会生成事件和快照。\n在 Configuration 页面，你可以查看当前的配置（只读）。\n\n五、与 Home Assistant 集成（推荐）Frigate 与 Home Assistant 官方集成，可以极大扩展其功能。\n\n确保你的 Home Assistant 和 Frigate 都在同一个网络中，且 MQTT 服务已运行并配置在 Frigate 的 config.yml 中。\n在 Home Assistant 中，进入 设置 -&gt; 设备与服务 -&gt; 添加集成。\n搜索 Frigate。\nHome Assistant 会尝试自动发现 Frigate。如果发现失败，你可能需要手动输入 Frigate 的 IP 地址。\n通过集成，Home Assistant 会自动创建各种 Frigate 实体，包括：\nbinary_sensor：每次检测到对象时触发。\nmedia_player：用于查看摄像头直播流。\ncamera：用于查看录像、快照。\nsensor：显示当前在线人数、车辆数等。\n\n\n你可以利用这些实体在 Home Assistant 中创建强大的自动化，例如：\n当 Frigate 检测到 person 时，触发智能灯光亮起。\n当 Frigate 检测到 car 且是夜间时，发送包含快照的通知到手机。\n结合门窗传感器，只有在门窗打开时才检测特定区域等。\n\n\n\n六、高级配置与优化1. FFmpeg 硬件加速如果你的服务器 CPU 是 Intel (带核显)，强烈建议开启 FFmpeg 的硬件解码&#x2F;编码，可以大幅降低 CPU 占用。\n\n宿主机驱动：确保你的 Linux 系统已安装 Intel 显卡的 VA-API 驱动。\nDocker Compose：在 volumes 中添加 - /dev/dri:/dev/dri。\nFrigate config.yml：在 ffmpeg 部分的 global_args 中添加：ffmpeg:  global_args: -hwaccel vaapi -hwaccel_output_format vaapi\n并调整 output_args 中的编码器，例如：# ...output_args:  detect: -f segment -segment_times 10 -segment_format mp4 -r 10 -c:v h264_vaapi -preset ultrafast -tune zerolatency -crf 23 -bf 0 -g 30 -sc_threshold 0 -pix_fmt vaapi_vpp -movflags +faststart  record: -c:v h264_vaapi -map 0:v:0 -map 0:a? -f segment -segment_times 10 -segment_format mp4 -reset_timestamps 1 -strftime 1 -ar 44100 # 如果录像也想用VAAPI编码\n具体编码器名称(h264_vaapi) 和像素格式 (vaapi_vpp) 可能因 FFmpeg 版本和驱动而异，请查阅资料匹配。\n\n2. 内存磁盘 (tmpfs)Frigate 会在 cache 中存储一些临时文件。如果你的内存足够大，可以考虑将 /tmp/cache 映射为 tmpfs，以减少磁盘 I&#x2F;O，并提升性能。\n在 docker-compose.yml 中 volumes 部分添加：\nvolumes:  # ... 其他 volumes  - type: tmpfs # 内存磁盘    target: /tmp/cache    tmpfs:      size: 1g # 设定为1GB，根据你的内存和摄像头数量调整\n\n3. 多路 Coral TPU如果你有多个 Coral TPU，可以在 config.yml 的 detectors 部分定义多个 edgetpu 检测器，并为每个摄像头指定使用哪个 TPU。\ndetectors:  coral_tpu_0:    type: edgetpu    device: usb:0 # 引用第一个 USB Coral  coral_tpu_1:    type: edgetpu    device: usb:1 # 引用第二个 USB Coralcameras:  front_door:    detector: coral_tpu_0 # 指定使用哪个检测器    # ...  back_yard:    detector: coral_tpu_1 # 指定使用哪个检测器    # ...\n\n4. 远程存储Frigate 可以配置将录像和快照存储到远程位置（如网络共享、S3 存储桶）。这需要更复杂的配置和额外的工具（如 rclone）。\n5. 自定义 AI 模型Frigate 允许你使用自定义的 TensorFlow Lite 模型，来识别更多类型的物体或优化现有物体识别性能。这需要具备一定的 AI 模型训练和转换知识。\n七、总结Frigate 是一个革命性的本地视频监控解决方案，它将 AI 驱动的目标检测带入了家庭和小型办公室场景。通过智能识别和事件驱动的录制，它解决了传统监控系统在误报、存储空间和事件查找方面的痛点。配合 Google Coral TPU，Frigate 能够提供高性能的实时检测，并与 Home Assistant 无缝集成，开启无限自动化可能。\n虽然部署 Frigate 需要一些 Docker 和 YAML 配置的知识，但一旦配置完成，它将大大提升你的监控体验，让你的智能家居系统真正“智能”起来。强烈推荐给所有希望升级自己视频监控系统的用户！\n","categories":["NAS","实用工具"],"tags":["Docker","2024","NAS"]},{"title":"Go语言embed包详解","url":"/2025/2025-01-12_Go%E8%AF%AD%E8%A8%80embed%E5%8C%85%E8%AF%A6%E8%A7%A3/","content":"\nGo 1.16 版本引入了 embed 包，它提供了一种将静态资源（如HTML、CSS、JavaScript、图片、配置文件等）直接嵌入 (embed) 到 Go 程序二进制文件中的功能。这极大地简化了应用程序的部署流程，尤其是对于需要捆绑前端资源或配置文件的后端服务。\n\n“The embed package provides access to files embedded in the program during compilation.” —— Go embed 官方文档\n\n\n一、为什么需要 embed 包？在 embed 包出现之前，Go 应用程序通常需要通过以下方式处理静态资源：\n\n文件系统访问: 在运行时从文件系统加载资源。这意味着在部署时，除了可执行文件，还需要打包额外的资源文件。\ngo:generate 工具: 使用第三方工具（如 go-bindata、statik 等）将资源文件转换为 Go 源代码文件，然后在运行时加载这些生成的 Go 文件。这种方法引入了额外的构建步骤和依赖。\n\nembed 包的出现，解决了上述痛点：\n\n单一二进制文件: 应用程序和所有静态资源被打包成一个独立的二进制文件，方便部署和分发。\n简化部署: 无需担心资源文件的路径问题或在不同环境中丢失文件。\n原生支持: embed 是 Go 语言的内置功能，无需第三方工具。\n跨平台兼容: 嵌入的资源在所有支持 Go 的平台上都能正常工作。\n\n二、embed 包核心概念embed 包通过特殊的注释指令 (//go:embed) 和三种不同的类型来工作：\n1. //go:embed 注释指令这是 embed 包的核心。它是一个编译器指令，告诉 Go 编译器将指定的文件或目录的内容嵌入到紧随其后的变量中。\n\n位置: //go:embed 指令必须紧跟在变量声明的上方，中间不能有空行或注释。\n作用域: 只能用于包级别变量 (package-level variable) 的声明。这意味着不能用于函数内部的局部变量。\n路径: 支持相对路径和绝对路径（不推荐）。相对路径是相对于包含该 Go 源文件的目录。支持 Unix 风格的路径分隔符 (/)。\n通配符: 支持 * (匹配零个或多个非 / 字符) 和 ** (匹配零个或多个字符，包括 /，但只能作为路径的最后一部分)。\n\n2. 嵌入类型embed 包支持将资源嵌入到三种 Go 类型中：\na. string适用于嵌入单个文本文件，如配置文件、HTML 片段等。\n\n文件内容会被嵌入为 Go 字符串。\n适合小文件或需要直接字符串处理的场景。\n\nb. []byte适用于嵌入单个文件，可以是文本文件或二进制文件，如图片、字体等。\n\n文件内容会被嵌入为字节切片。\n适合所有类型的单个文件。\n\nc. embed.FS这是最强大和最常用的类型，适用于嵌入整个目录或多个文件。\n\nembed.FS 实现了 fs.FS 接口，可以被 Go 标准库中的 io/fs 包兼容的函数使用。\n它创建了一个虚拟的文件系统，你可以在运行时像操作真实文件系统一样访问嵌入的资源。\n非常适合嵌入前端静态资源目录。\n\n三、embed 包使用示例我们将通过实际代码演示如何使用 embed 包嵌入不同类型的资源。\n假设我们有如下项目结构：\nmy-app/├── main.go├── static/│   ├── index.html│   ├── css/│   │   └── style.css│   └── js/│       └── app.js├── config.txt└── image.png\n\n示例 1: 嵌入单个文件到 string 或 []byte我们想嵌入 config.txt 和 image.png。\nconfig.txt 内容:\napp.name=MyEmbeddedAppversion=1.0.0\n\nmain.go:\npackage mainimport (\t_ &quot;embed&quot; // 导入 embed 包，但通常不需要直接使用其内部函数\t&quot;fmt&quot;\t&quot;log&quot;\t&quot;net/http&quot;)//go:embed config.txtvar configContent string // 嵌入文本文件到字符串//go:embed image.pngvar imageBytes []byte // 嵌入二进制文件到字节切片func main() &#123;\tfmt.Println(&quot;--- Embedded string content (config.txt) ---&quot;)\tfmt.Println(configContent)\tfmt.Println(&quot;\\n--- Embedded byte slice content (image.png) ---&quot;)\tfmt.Printf(&quot;Image size: %d bytes\\n&quot;, len(imageBytes))\t// 你可以在这里进一步处理 imageBytes，例如将其写入文件或作为 HTTP 响应\t// 演示如何提供一个嵌入的图片作为 HTTP 响应\thttp.HandleFunc(&quot;/image.png&quot;, func(w http.ResponseWriter, r *http.Request) &#123;\t\tw.Header().Set(&quot;Content-Type&quot;, &quot;image/png&quot;)\t\tw.Write(imageBytes)\t&#125;)\tlog.Println(&quot;Server started on :8080. Access /image.png&quot;)\tlog.Fatal(http.ListenAndServe(&quot;:8080&quot;, nil))&#125;\n\n运行:\ngo run main.go\n访问 http://localhost:8080/image.png 即可看到嵌入的图片。\n示例 2: 嵌入整个目录到 embed.FS我们想嵌入 static 目录下的所有前端资源。\nstatic/index.html:\n&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;    &lt;title&gt;Embedded App&lt;/title&gt;    &lt;link rel=&quot;stylesheet&quot; href=&quot;/static/css/style.css&quot;&gt;&lt;/head&gt;&lt;body&gt;    &lt;h1&gt;Hello from Embedded App!&lt;/h1&gt;    &lt;p&gt;This page is served from an embedded file system.&lt;/p&gt;    &lt;img src=&quot;/image.png&quot; alt=&quot;Embedded Image&quot;&gt;    &lt;script src=&quot;/static/js/app.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;\n\nstatic/css/style.css:\nbody &#123;    font-family: Arial, sans-serif;    background-color: #f0f0f0;    color: #333;    text-align: center;    padding: 20px;&#125;h1 &#123;    color: #007bff;&#125;\n\nstatic/js/app.js:\ndocument.addEventListener(&#x27;DOMContentLoaded&#x27;, () =&gt; &#123;    console.log(&#x27;App.js loaded from embedded resource!&#x27;);&#125;);\n\nmain.go:\npackage mainimport (\t_ &quot;embed&quot;\t&quot;fmt&quot;\t&quot;io/fs&quot;\t&quot;log&quot;\t&quot;net/http&quot;)//go:embed staticvar staticFiles embed.FS // 嵌入整个 static 目录//go:embed image.png // 也可以在同一个文件中嵌入其他单个资源var imageBytes []bytefunc main() &#123;\t// 1. 验证嵌入的 embed.FS\t// 尝试读取 static/index.html\tindexFile, err := staticFiles.ReadFile(&quot;static/index.html&quot;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;Failed to read embedded index.html: %v&quot;, err)\t&#125;\tfmt.Println(&quot;--- Content of static/index.html (first 100 chars) ---&quot;)\tfmt.Println(string(indexFile[:100]) + &quot;...&quot;)\t// 2. 将 embed.FS 用于 HTTP 服务\t// http.FS 接口可以将 fs.FS 转换为 http.FileSystem\t// StripPrefix 是为了移除 URL 中的 /static/ 部分，以便 http.FileServer 能正确查找文件\tstaticHandler := http.StripPrefix(&quot;/static/&quot;, http.FileServer(http.FS(staticFiles)))\thttp.Handle(&quot;/static/&quot;, staticHandler)\t// 3. 服务根路径，重定向或直接提供 index.html\thttp.HandleFunc(&quot;/&quot;, func(w http.ResponseWriter, r *http.Request) &#123;\t\tif r.URL.Path != &quot;/&quot; &amp;&amp; r.URL.Path != &quot;/index.html&quot; &#123;\t\t\thttp.NotFound(w, r)\t\t\treturn\t\t&#125;\t\t// 从 embed.FS 中读取 index.html 并直接提供\t\tindexContent, err := staticFiles.ReadFile(&quot;static/index.html&quot;)\t\tif err != nil &#123;\t\t\thttp.Error(w, &quot;Failed to load index.html&quot;, http.StatusInternalServerError)\t\t\treturn\t\t&#125;\t\tw.Header().Set(&quot;Content-Type&quot;, &quot;text/html; charset=utf-8&quot;)\t\tw.Write(indexContent)\t&#125;)\t// 4. 继续提供嵌入的图片（与示例1相同）\thttp.HandleFunc(&quot;/image.png&quot;, func(w http.ResponseWriter, r *http.Request) &#123;\t\tw.Header().Set(&quot;Content-Type&quot;, &quot;image/png&quot;)\t\tw.Write(imageBytes)\t&#125;)\tlog.Println(&quot;Server started on :8080. Access http://localhost:8080&quot;)\tlog.Fatal(http.ListenAndServe(&quot;:8080&quot;, nil))&#125;\n\n运行:\ngo run main.go\n访问 http://localhost:8080，你将看到一个由一个 Go 二进制文件提供的完整 Web 页面，包括 HTML、CSS、JS 和图片。\n示例 3: 使用通配符 * 和 **//go:embed 支持通配符：\n\n*: 匹配路径段中除 / 以外的零个或多个字符。\n**: 匹配零个或多个字符，包括 /。它只能出现在模式的末尾。\n\npackage mainimport (\t_ &quot;embed&quot;\t&quot;embed&quot;\t&quot;fmt&quot;\t&quot;io/fs&quot;\t&quot;log&quot;)//go:embed *.txt // 嵌入当前目录下所有 .txt 文件var allTxtFiles embed.FS//go:embed static/* // 嵌入 static 目录下所有文件（不包括子目录）var staticRootFiles embed.FS//go:embed static/**/* // 嵌入 static 目录下所有文件和所有子目录中的文件var allStaticAssets embed.FSfunc main() &#123;\t// 打印所有 .txt 文件\tfmt.Println(&quot;--- All .txt files in root ---&quot;)\tfs.WalkDir(allTxtFiles, &quot;.&quot;, func(path string, d fs.DirEntry, err error) error &#123;\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tif !d.IsDir() &#123;\t\t\tfmt.Println(path)\t\t\tcontent, _ := allTxtFiles.ReadFile(path)\t\t\tfmt.Printf(&quot;  Content: %s\\n&quot;, string(content))\t\t&#125;\t\treturn nil\t&#125;)\t// 打印 static 根目录下的文件\tfmt.Println(&quot;\\n--- Files in static/* (no subdirectories) ---&quot;)\tfs.WalkDir(staticRootFiles, &quot;.&quot;, func(path string, d fs.DirEntry, err error) error &#123;\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tif !d.IsDir() &#123;\t\t\tfmt.Println(path)\t\t&#125;\t\treturn nil\t&#125;)\t// 打印 static 目录下所有文件（包括子目录）\tfmt.Println(&quot;\\n--- All files under static/ (including subdirectories) ---&quot;)\tfs.WalkDir(allStaticAssets, &quot;.&quot;, func(path string, d fs.DirEntry, err error) error &#123;\t\tif err != nil &#123;\t\t\treturn err\t\t&#125;\t\tif !d.IsDir() &#123;\t\t\tfmt.Println(path)\t\t&#125;\t\treturn nil\t&#125;)\t\t// 注意：当使用目录模式嵌入时，路径会保留目录名。\t// 例如 `static/**/*` 嵌入后，你可以访问到 `static/css/style.css`。\t// 但如果只希望路径从 `css/style.css` 开始，你需要调整 `//go:embed` 指令。\t// 下一节会介绍如何处理这种情况。&#125;\n\n运行:\n# 确保在 my-app 目录下运行go run main.go\n\n四、embed.FS 路径处理技巧当使用 embed.FS 嵌入整个目录时，被嵌入的路径会包含 //go:embed 指令中指定的目录名。例如，//go:embed static 会导致 static/index.html 在 embed.FS 中依然是 static/index.html。\n如果你希望在 embed.FS 中访问文件时，路径不包含顶层目录名（例如直接通过 index.html 访问），Go 社区通常有两种做法：\n1. 调整 //go:embed 指令将嵌入指令定位到要嵌入目录的内容，而不是目录本身。\npackage mainimport (\t_ &quot;embed&quot;\t&quot;embed&quot;\t&quot;fmt&quot;\t&quot;io/fs&quot;\t&quot;log&quot;\t&quot;net/http&quot;)//go:embed static/* static/**/*var embeddedFS embed.FS // 嵌入 static 目录下的所有文件，但路径将不包含顶层 &#x27;static/&#x27;// 或者如果只嵌入一个顶层目录// //go:embed static/index.html static/css static/js// var embeddedFS embed.FS // 这样 embeddedFS 中会有 index.html, css/, js/ 等func main() &#123;\t// 现在可以直接访问 index.html\tindexFile, err := embeddedFS.ReadFile(&quot;index.html&quot;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;Failed to read embedded index.html: %v&quot;, err)\t&#125;\tfmt.Println(&quot;--- Content of index.html ---&quot;)\tfmt.Println(string(indexFile[:50]) + &quot;...&quot;)\t// http.FileServer 可以直接使用 embeddedFS，而无需 StripPrefix\thttp.Handle(&quot;/&quot;, http.FileServer(http.FS(embeddedFS)))\t// ... 省略其他 HTTP handler\tlog.Println(&quot;Server started on :8080. Access http://localhost:8080&quot;)\tlog.Fatal(http.ListenAndServe(&quot;:8080&quot;, nil))&#125;\n注意: //go:embed static/* static/**/* 这种写法可以满足大多数情况，但具体行为取决于 Go 版本和路径匹配规则。最保险的方法是明确列出所有顶层文件和子目录（虽然可能更繁琐），或者使用 io/fs.Sub。\n2. 使用 io/fs.Subio/fs.Sub 函数可以从一个 fs.FS 中返回一个子文件系统，这样你就可以“剥离”顶层目录。\npackage mainimport (\t_ &quot;embed&quot;\t&quot;embed&quot;\t&quot;fmt&quot;\t&quot;io/fs&quot;\t&quot;log&quot;\t&quot;net/http&quot;)//go:embed static // 嵌入整个 static 目录，包含 &#x27;static/&#x27; 前缀var rawEmbeddedFS embed.FSfunc main() &#123;\t// 创建一个子文件系统，剥离 &#x27;static/&#x27; 前缀\t// 现在 subFS 中访问文件时，可以直接用 &quot;index.html&quot; 而非 &quot;static/index.html&quot;\tsubFS, err := fs.Sub(rawEmbeddedFS, &quot;static&quot;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;Failed to create sub FS: %v&quot;, err)\t&#125;\tindexFile, err := subFS.ReadFile(&quot;index.html&quot;)\tif err != nil &#123;\t\tlog.Fatalf(&quot;Failed to read embedded index.html from subFS: %v&quot;, err)\t&#125;\tfmt.Println(&quot;--- Content of index.html from subFS ---&quot;)\tfmt.Println(string(indexFile[:50]) + &quot;...&quot;)\t// 现在 http.FileServer 就可以直接使用 subFS\thttp.Handle(&quot;/&quot;, http.FileServer(http.FS(subFS)))\tlog.Println(&quot;Server started on :8080. Access http://localhost:8080&quot;)\tlog.Fatal(http.ListenAndServe(&quot;:8080&quot;, nil))&#125;\n这种方法更健壮，且能清晰地表达意图，推荐在需要剥离路径前缀时使用。\n五、embed 包注意事项和限制\n包级别变量: //go:embed 只能用于包级别的变量。\n文件可见性: 只有在构建 Go 二进制文件时可见的文件才会被嵌入。如果你在 go.mod 之外的目录中放置文件，Go 编译器可能无法找到它们。\n不能嵌入符号链接: embed 包不会追踪符号链接。\n文件大小: 嵌入文件会增加最终二进制文件的大小。对于非常大的资源，可能需要权衡利弊。\n不修改元数据: embed 包只嵌入文件内容，不会保留文件的修改时间、权限等元数据。\ngo.mod 模块边界: //go:embed 模式只能匹配当前模块内的文件。例如，你不能嵌入 vendor 目录下的文件，因为它们属于不同的模块。\n//go:embed 必须在变量声明上方紧邻: 中间不能有空行或注释。\n不需要 import embed 即可使用 embed.FS: 虽然类型是 embed.FS，但如果你只使用 //go:embed 指令和其类型，而不需要调用 embed 包内的其他函数，可以省略 import &quot;embed&quot;。然而，为了清晰和防止潜在的编译器警告，通常建议 import _ &quot;embed&quot; 或 import &quot;embed&quot;。\n\n六、embed 包的最佳实践\n有组织的文件结构: 将静态资源放在专门的目录中（例如 static/、public/、assets/），这样 //go:embed 指令更清晰，也方便文件管理。\n\n利用 embed.FS: 对于多个文件或目录，优先使用 embed.FS，并结合 http.FileServer 和 io/fs.Sub 来服务文件。\n\n缓存 HTTP 响应: 对于嵌入的静态资源，可以在 http.Handler 中设置适当的 Cache-Control 头，利用客户端缓存。\n\n调试: 在开发阶段，你可能希望从实际文件系统加载资源，以便进行快速迭代和热重载。在生产环境再切换到 embed 模式。这可以通过判断构建标签 (build tags) 或环境变量来实现。\n// main.go//go:build !dev  package main  import (\t&quot;embed&quot;\t&quot;io/fs&quot;\t&quot;net/http&quot;)  //go:embed staticvar embeddedFS embed.FS  func getFS() fs.FS &#123;\t// 在生产模式下返回嵌入的文件系统\treturn embeddedFS&#125;  // go build -tags dev (在开发模式下使用，从实际文件系统加载)// go build (在生产模式下使用，从嵌入文件系统加载)\n然后创建另一个 main_dev.go 文件 (或同一文件使用 build tag):\n// main_dev.go//go:build dev  package main  import (\t&quot;io/fs&quot;\t&quot;os&quot;)  func getFS() fs.FS &#123;\t// 在开发模式下返回os.DirFS，直接从文件系统加载\treturn os.DirFS(&quot;.&quot;) // 或 os.DirFS(&quot;static&quot;)&#125;\n这样，通过 go build -tags dev 或 go run -tags dev 可以在开发时从磁盘读取文件，而默认构建则使用嵌入资源。\n\n\n七、总结embed 包是 Go 1.16 以来最重要的语言特性之一，它极大地简化了 Go 应用程序中静态资源的管理和部署。通过将所有前端文件、配置文件甚至文档都打包进一个单一的 Go 二进制文件，开发者可以实现无缝分发和更简洁的部署流程。理解其核心概念和使用方法，将使你的 Go 项目更加健壮和易于维护。\n","categories":["Golang","项目构建"],"tags":["前端技术","项目构建","Golang","2025"]},{"title":"Prometheus与Grafana详解：现代监控的黄金组合","url":"/2025/2025-01-27_Prometheus%E4%B8%8EGrafana%E8%AF%A6%E8%A7%A3%EF%BC%9A%E7%8E%B0%E4%BB%A3%E7%9B%91%E6%8E%A7%E7%9A%84%E9%BB%84%E9%87%91%E7%BB%84%E5%90%88/","content":"\n在现代复杂的 IT 基础设施中，如何高效、准确地监控系统和应用的健康状况，并及时发现潜在问题，是运维和开发团队面临的巨大挑战。Prometheus 和 Grafana 正是为此而生的一对黄金搭档。Prometheus 负责数据的收集、存储和查询，而 Grafana 则负责数据的可视化和告警展示。它们共同构建了一个强大的开源监控解决方案，已成为云原生时代监控领域的事实标准。\n\n“没有监控的系统就像在黑暗中航行的船只，随时可能触礁。”\n\n\n一、Prometheus 详解1.1 Prometheus 是什么？Prometheus 是一个开源的时间序列数据库 (TSDB) 和监控系统，由 SoundCloud 公司开发并于 2016 年加入云原生计算基金会 (CNCF)，是其第二个毕业项目。它采用了一种拉取 (Pull) 模型来收集指标数据，并通过强大的多维度数据模型和灵活的查询语言 (PromQL) 来支持复杂的告警和分析。\n1.2 Prometheus 的核心特点与优势\n多维数据模型：所有指标都是以时间戳和键值对（称为标签或 labels）的形式存储的。例如，http_requests_total&#123;method=&quot;post&quot;, handler=&quot;/path&quot;&#125; 表示 http_requests_total 这个指标，但在 method=&quot;post&quot; 和 handler=&quot;/path&quot; 这两个维度上的值。\n灵活的查询语言 (PromQL)：Prometheus Query Language 是一种强大而简洁的查询语言，用于过滤、聚合和转换时间序列数据。它支持各种数学运算、聚合函数和时间范围查询，可以轻松地进行趋势分析、比率计算和更复杂的业务指标分析。\n拉取模式：Prometheus 主动从配置的目标（称为 exporters 或 instrumented applications）拉取指标数据。这种模型易于部署，且在服务发现方面具有优势。\n服务发现：支持多种服务发现机制（如 Kubernetes, Consul, DNS 等），可以动态发现需要监控的目标。\n高效的存储：Prometheus 实现了高效的本地时间序列数据库存储，可以处理大规模的数据，并且易于水平扩展。\n强大的告警：通过 Alertmanager 组件，Prometheus 可以根据 PromQL 查询结果触发告警，并通过多种渠道（如邮件、Slack、Webhook 等）发送通知。\n云原生集成：与 Docker、Kubernetes 等云原生技术栈深度融合，拥有丰富的 exporters 和集成方案。\n\n1.3 Prometheus 的架构组件一个典型的 Prometheus 监控系统包含以下核心组件：\n\nPrometheus Server：\nRetrieval (抓取)：通过 HTTP 协议从目标端点拉取指标数据。\nStorage (存储)：将抓取到的数据以时间序列的形式存储在本地磁盘中。\nQuery Engine (查询引擎)： PromQL 查询语言的解析器和执行器。\n\n\nExporters &#x2F; Instrumented Applications：\nExporters：是一种小型助手服务，它将现有系统的指标（例如操作系统、数据库、消息队列等）转换为 Prometheus 兼容的格式暴露出来。常见的有 Node Exporter (用于主机指标)、cAdvisor (用于容器指标)、&#96;&#96;MySQL Exporter&#96; 等。\nInstrumented Applications：应用程序本身嵌入了 Prometheus 客户端库，直接以 Prometheus 格式暴露自己的内部指标。\n\n\nPushgateway (可选)：用于那些无法被 Prometheus 直接抓取（如短生命周期作业或批量任务）的指标。它允许这些作业将指标推送到 Pushgateway，然后 Prometheus 从 Pushgateway 拉取。\nAlertmanager：独立于 Prometheus Server 运行，接收 Prometheus 发送的告警通知，进行分组、去重、静默、并将告警路由到不同的通知接收器（邮件、Slack、Webhook 等）。\nGrafana (或其它可视化工具)：用于查询 Prometheus 数据并以图表、仪表盘的形式进行可视化展示。\n\n1.4 Prometheus 部署示例 (使用 Docker Compose)这里我们将部署一个 Prometheus Server 和一个 Node Exporter 来监控宿主机。\n1. 创建 Prometheus 目录结构sudo mkdir -p /opt/prometheus/configsudo mkdir -p /opt/prometheus/data # 用于存储 Prometheus 数据sudo chmod -R 777 /opt/prometheus # 确保权限cd /opt/prometheus\n\n2. 创建 Prometheus 配置文件 (prometheus.yml)在 /opt/prometheus/config 目录下创建 prometheus.yml。\nsudo nano config/prometheus.yml\n\nglobal:  scrape_interval: 15s # 默认每 15 秒抓取一次  evaluation_interval: 15s # 评估规则的频率scrape_configs:  - job_name: &#x27;prometheus&#x27; # 监控 Prometheus 自身    static_configs:      - targets: [&#x27;localhost:9090&#x27;] # Prometheus 默认运行在 9090 端口  - job_name: &#x27;node_exporter&#x27; # 监控宿主机    static_configs:      - targets: [&#x27;localhost:9100&#x27;] # Node Exporter 默认运行在 9100 端口\n\n保存并关闭文件。\n3. 创建 Docker Compose 文件 (docker-compose.yml)在 /opt/prometheus 目录下创建 docker-compose.yml。\nsudo nano docker-compose.yml\n\nversion: &#x27;3.8&#x27;services:  prometheus:    image: prom/prometheus:latest # Prometheus 镜像    container_name: prometheus    restart: unless-stopped    ports:      - &quot;9090:9090&quot; # 映射 Prometheus Web UI 端口    volumes:      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml:ro # 挂载配置文件      - ./data:/prometheus # 挂载数据目录，实现持久化    command:      - &#x27;--config.file=/etc/prometheus/prometheus.yml&#x27;      - &#x27;--storage.tsdb.path=/prometheus&#x27;      - &#x27;--web.console.libraries=/usr/share/prometheus/console_libraries&#x27;      - &#x27;--web.console.templates=/usr/share/prometheus/consoles&#x27;  node_exporter:    image: prom/node-exporter:latest # Node Exporter 镜像    container_name: node_exporter    command:      - &#x27;--path.procfs=/host/proc&#x27;      - &#x27;--path.sysfs=/host/sys&#x27;      - &#x27;--path.rootfs=/host/root&#x27;      - &#x27;--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/run/docker.sock)($|/)&#x27;    restart: unless-stopped    network_mode: host # 使用 host 模式，Node Exporter 才能监控宿主机    # ports: # host 模式下无需映射端口    #   - &quot;9100:9100&quot;    volumes:      - /proc:/host/proc:ro      - /sys:/host/sys:ro      - /:/host/root:ro,rslave # 用于监控文件系统\n\n保存并关闭文件。\n4. 启动 Prometheus 和 Node Exporter在 /opt/prometheus 目录下执行：\nsudo docker compose up -d\n\n5. 访问 Prometheus Web UI打开浏览器访问 http://你的服务器IP:9090。你可以在 Status -&gt; Targets 页面看到 prometheus 和 node_exporter 两个目标都处于 UP 状态。在 Graph 页面输入 PromQL 查询语句（例如 node_cpu_seconds_total），即可查看数据。\n二、Grafana 详解2.1 Grafana 是什么？Grafana 是一个开源的度量分析和可视化工具。它允许你查询、可视化、告警和探索存储在各种数据源中的指标数据。Grafana 不存储数据，它只是作为数据的前端展示层。\n2.2 Grafana 的核心特点与优势\n多数据源支持：可以连接多种数据源，包括 Prometheus、InfluxDB、Loki、Elasticsearch、MySQL、PostgreSQL、CloudWatch 等等。\n丰富的可视化选项：提供多种面板类型（图表、折线图、柱状图、饼图、仪表盘、状态时间线、地理地图等）来展示数据。\n灵活的仪表盘：通过拖放和配置面板，可以构建高度自定义的仪表盘，满足各种监控需求。\n强大的告警功能：基于查询结果设置告警规则，并通过多种渠道（邮件、Slack、Webhook 等）发送通知。\n变量与模板：使用变量可以将查询变为动态，方便在不同维度上切换视图，例如切换不同的服务器或容器。\n查询编辑器：直观的查询编辑器，可以轻松构建复杂的查询语句。\n用户与权限管理：支持多用户，并提供灵活的权限控制。\n插件生态系统：拥有丰富的社区插件，可以扩展功能和支持新的数据源&#x2F;面板。\n\n2.3 Grafana 部署示例 (使用 Docker Compose)我们将在 Prometheus 部署的基础上，额外部署 Grafana，并将其连接到 Prometheus 作为数据源。\n1. 创建 Grafana 目录结构sudo mkdir -p /opt/grafana/data # 用于存储 Grafana 配置和数据库sudo chmod -R 777 /opt/grafana # 确保权限cd /opt/grafana\n\n2. 创建 Docker Compose 文件 (docker-compose.yml)我们可以在 /opt/prometheus 目录下增加 Grafana 的服务到原有的 docker-compose.yml 中，或者在 /opt/grafana 目录下创建新的 docker-compose.yml 来单独部署。为了保持模块独立，我们选择在 /opt/grafana 目录下创建新的 docker-compose.yml。\nsudo nano docker-compose.yml\n\nversion: &#x27;3.8&#x27;services:  grafana:    image: grafana/grafana:latest # Grafana 镜像    container_name: grafana    restart: unless-stopped    ports:      - &quot;3000:3000&quot; # 映射 Grafana Web UI 端口    volumes:      - ./data:/var/lib/grafana # 挂载数据目录，实现持久化    environment:      - GF_SECURITY_ADMIN_USER=admin     # Grafana 管理员用户名      - GF_SECURITY_ADMIN_PASSWORD=admin # Grafana 管理员密码 (请务必修改为强密码!)      - GF_SERVER_ROOT_URL=http://localhost:3000 # 根据实际情况调整域名或IP    depends_on:      - prometheus # 确保 Prometheus 启动后再启动 Grafana (如果在一个 compose 文件中)    # 如果 Prometheus 是在另一个 compose 文件中独立部署的，则需要确保其网络可达\n\n注意：\n\nGF_SECURITY_ADMIN_PASSWORD=admin 请务必将其修改为一个强密码！\ndepends_on：如果 Prometheus 和 Grafana 在同一个 docker-compose.yml 文件中，可以添加此项。如果它们是独立部署的（如本例），则可以删除，但要确保 Grafana 能够通过网络访问到 Prometheus。\n\n保存并关闭文件。\n3. 启动 Grafana在 /opt/grafana 目录下执行：\nsudo docker compose up -d\n\n4. 访问 Grafana Web UI打开浏览器访问 http://你的服务器IP:3000。\n使用你之前设置的用户名 (admin) 和密码 (admin - 请务必修改后登录!) 登录。\n5. 配置 Prometheus 数据源登录 Grafana 后，你需要添加 Prometheus 作为数据源：\n\n在左侧导航栏中，点击齿轮图标 (⚙️) -&gt; Data sources。\n点击 Add data source。\n搜索并选择 Prometheus。\n在 HTTP -&gt; URL 字段中输入 Prometheus Server 的地址。\n如果 Prometheus 和 Grafana 在同一个 Docker Compose 文件中，且 network_mode 不是 host，则可以是 http://prometheus:9090 (这里的 prometheus 是 docker-compose.yml 中 Prometheus 服务的名称)。\n如果 Prometheus 和 Grafana 分开部署，且 Prometheus 使用 network_mode: host，则输入 http://你的服务器IP:9090。\n如果 Prometheus 使用 network_mode: bridge，则需要输入 Prometheus 容器的 IP 地址或为其设置 DNS。\n\n\n点击 Save &amp; Test。如果成功，页面会显示 Data source is working。\n\n6. 创建第一个仪表盘现在你可以创建一个仪表盘来展示 Prometheus 的数据了：\n\n在左侧导航栏中，点击加号图标 (+) -&gt; Dashboard -&gt; New dashboard。\n点击 Add new panel。\n在查询编辑器中，选择你的 Prometheus 数据源。\n在 PromQL 查询框中输入你的查询语句，例如 node_cpu_seconds_total。\n选择合适的 Visualization 类型（如 Graph）。\n调整面板标题、图例、轴标签等。\n点击 保存。\n\n2.4 Grafana 高级用法\n导入预设仪表盘：Grafana 社区有大量的共享仪表盘模板（例如用于 Node Exporter、Prometheus 自身等）。你可以在 Grafana Labs 找到并导入它们。\n在左侧导航栏中，点击加号图标 (+) -&gt; Dashboard -&gt; Import。\n输入仪表盘 ID 或粘贴 JSON 模型。\n\n\n告警：在任何面板中，你都可以点击 Alert 选项卡来创建告警规则，并配置告警通知渠道 (Notification channels)。\n变量：在仪表盘设置中创建变量，可以在仪表盘顶部添加下拉菜单，动态改变查询的范围，例如切换 instance (服务器实例)。\nLoki &#x2F; Tempo &#x2F; Mimir：Grafana Labs 不仅有 Grafana，还推出了 Loki (日志聚合)、Tempo (分布式链路追踪) 和 Mimir (可扩展的 Prometheus 存储)，它们与 Grafana 完美集成，共同构建了完整的可观测性解决方案。\n\n三、总结：监控的黄金组合Prometheus 和 Grafana 的组合是现代 IT 监控领域的强大基石。Prometheus 提供了强大的多维数据模型、灵活的查询语言和高效的存储能力，而 Grafana 则将这些数据以直观、美观、可定制的方式呈现在你面前，并辅以强大的告警功能。\n无论是监控你的个人服务器，还是复杂的云原生应用集群，Prometheus 和 Grafana 都能提供卓越的性能和功能。它们的开源特性和活跃社区也保证了持续的创新和支持。通过本指南，你应该已经成功部署并开始探索这两个工具的强大功能，为你的系统和应用保驾护航。\n","categories":["开发工具","数据监控"],"tags":["Docker","2025","Prometheus","Grafana","数据监控"]},{"title":"哈希表(Hash Table)原理详解","url":"/2025/2025-02-19_%E5%93%88%E5%B8%8C%E8%A1%A8(Hash%20Table)%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/","content":"\n哈希表（Hash Table），又称散列表，是一种根据键（Key）直接访问存储位置的数据结构。它通过哈希函数将键映射到表中的一个位置来访问记录，从而实现平均 O(1) 时间复杂度的查找、插入和删除操作。哈希表是计算机科学中最重要的数据结构之一，广泛应用于数据库索引、缓存、符号表、唯一性检查等多种场景。\n\n“A hash table is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index, also called a hash code or hash value, into an array of buckets or slots, from which the desired value can be found.” —— Wikipedia\n\n\n一、哈希表的基本概念哈希表的核心思想是键值映射。它将用户提供的键（key）通过一个特定的函数（哈希函数）转换成一个整数，这个整数就是数据在底层数组中的索引（下标）。\n\n键 (Key): 唯一的标识符，用于查找、插入和删除数据。\n值 (Value): 与键关联的数据。\n哈希函数 (Hash Function): 将键映射到数组索引的函数。\n哈希值 (Hash Value 或 Hash Code): 哈希函数计算出的整数值。\n桶&#x2F;槽 (Bucket&#x2F;Slot): 底层数组中的一个位置，用于存储键值对。\n\n示意图：哈希表基本概念\n                       +--------------------+                       |    哈希表 (Hash Table)    |                       +--------------------+                              |                              |   Key (键)                              |                          +-----------------+                          |  哈希函数 (Hash)   |                          +-----------------+                              |                              |   Hash Code (哈希值)                              |   -&gt; (进一步处理，如取模)                              |   -&gt; Array Index (数组索引)                              V              +------------------------------------------------+              | 数组/桶 (Buckets/Slots)                         |              +------------------------------------------------+索引 0 -----&gt; | [Empty or Linked List/Entry 1]                 |  (可能存储 Key: &quot;grape&quot;, Value: &quot;葡萄&quot;)              +------------------------------------------------+索引 1 -----&gt; | [Entry 2]                                      |  (可能存储 Key: &quot;apple&quot;, Value: &quot;苹果&quot;)              +------------------------------------------------+索引 2 -----&gt; | [Empty or Linked List/Entry 3]                 |  (可能存储 Key: &quot;orange&quot;, Value: &quot;橘子&quot;)              +------------------------------------------------+索引 3 -----&gt; | [Entry 4]  --&gt;  [Entry 5]                      |  (这是链地址法解决冲突的例子)              |            ↑         ↑                        |  Key: &quot;banana&quot;, Value: &quot;香蕉&quot;              |            |         |                        |  Key: &quot;band&quot;, Value: &quot;乐队&quot; (哈希冲突，都映射到索引3)              +------------------------------------------------+索引 4 -----&gt; | [Entry 6]                                      |  (可能存储 Key: &quot;cat&quot;, Value: &quot;猫&quot;)              +------------------------------------------------+索引 5 -----&gt; | [Empty]                                        |              +------------------------------------------------+索引 ... -----&gt; | ...                                            |              +------------------------------------------------+\n\n二、哈希函数 (Hash Function)哈希函数是哈希表的“心脏”，它的质量直接决定了哈希表的性能。一个好的哈希函数应该满足以下条件：\n\n确定性: 对于相同的输入键，哈希函数必须总是产生相同的哈希值。\n快速计算: 哈希函数计算哈希值的速度要快，否则会抵消哈希表带来的性能优势。\n均匀分布: 尽可能地将不同的键均匀地分布到哈希表的各个桶中，减少冲突。\n一致性: 数据结构中的等价键应该有相同的哈希值。\n\n常见的哈希函数构造方法\n直接定址法: H(key) = key 或 H(key) = a * key + b\n适用于键的范围不大且分布均匀的情况。\n例：学号为 1-100，则直接用学号作为索引。\n\n\n除留余数法 (Division Method): H(key) = key % m\n最常用的方法。m 是哈希表的桶数量 (通常选择一个质数可以减少冲突)。\n例：H(key) = key % 7。键 12 的哈希值是 12 % 7 = 5。\n\n\n乘法哈希法 (Multiplication Method): H(key) = floor(m * (key * A mod 1))\nA 是一个常数，通常选择 0 &lt; A &lt; 1。\n特点是 m 的选择不那么严格，可以是 2 的幂次。\n\n\n折叠法 (Folding Method): 将键分成几部分，然后把这些部分相加或进行位运算，取结果的最后几位作为哈希值。\n适用于键很长的情况。\n\n\n数字分析法 (Digit Analysis Method): 分析键的分布情况，选取键中分布比较均匀的位作为哈希值。\n字符串哈希: 对于字符串键，常会用到各种变种的哈希，如 BKDR Hash、DJB Hash、AP Hash、SDBM Hash、RS Hash、JS Hash、ELF Hash 等。它们通过迭代地结合字符的 ASCII 值和乘法&#x2F;位移运算来生成哈希值。\n例 (简单的字符串哈希): hash = 0; for char in key: hash = (hash * P + char_value) % M (其中 P 是一个质数，M 是桶的数量)\n\n\n\n三、哈希冲突 (Hash Collision)无论哈希函数设计得多么优秀，由于键空间通常远大于表空间，不同的键被映射到同一个哈希值是不可避免的，这就称为哈希冲突。哈希表设计中一个重要部分就是如何解决哈希冲突。\n常见的冲突解决策略1. 链地址法 (Separate Chaining)\n原理: 每个桶不再直接存储一个元素，而是存储一个链表（或红黑树、数组等数据结构）。当多个键被哈希到同一个桶时，这些键值对都会被存储到该桶对应的链表中。\n操作:\n插入: 计算哈希值得到桶索引，将新元素插入到该桶对应的链表中。\n查找&#x2F;删除: 计算哈希值得到桶索引，然后遍历该桶对应的链表查找&#x2F;删除目标元素。\n\n\n优点:\n实现简单。\n对负载因子不敏感，即使负载因子大于 1 也能很好工作。\n删除操作相对简单。\n\n\n缺点:\n需要额外的空间存储链表节点（指针）。\n当链表过长时，查找效率会下降（最坏 O(n)）。\n\n\n示例: Java 的 HashMap 在冲突较少时使用链表，当链表长度超过一定阈值 (如 8) 时，会将链表转换为红黑树以提高查找效率 (最坏 O(log N))。\n\n示意图：链地址法\n+-------------------------------------------------------------+|              哈希表 (Hash Table) - 链地址法                  |+-------------------------------------------------------------+| 索引 | 桶 (Bucket)                                           |+------+-------------------------------------------------------+|  0   |  NULL / 空链表                                        |+------+-------------------------------------------------------+|  1   |  NULL / 空链表                                        |+------+-------------------------------------------------------+|  2   |  NULL / 空链表                                        |+------+-------------------------------------------------------+|  3   |  [ (3, &quot;B&quot;) ] --&gt; [ (10, &quot;C&quot;) ] --&gt; [ (24, &quot;D&quot;) ] --&gt; [ (17, &quot;E&quot;) ] --&gt; [ (31, &quot;F&quot;) ]  ||      |  (这是一个链表，存储所有哈希到索引 3 的键值对)       |+------+-------------------------------------------------------+|  4   |  NULL / 空链表                                        |+------+-------------------------------------------------------+|  5   |  NULL / 空链表                                        |+------+-------------------------------------------------------+|  6   |  [ (20, &quot;A&quot;) ]                                        |+------+-------------------------------------------------------+\n\n2. 开放寻址法 (Open Addressing)\n原理: 当发生冲突时，不把元素放在另一个数据结构中，而是探测（Probe）哈希表的其他空闲位置来存储冲突的元素。所有元素都存储在哈希表的底层数组中。\n探测方法:\n线性探测 (Linear Probing): 发生冲突时，探查下一个连续的桶，H(key, i) = (H(key) + i) % m。\n缺点: 容易形成聚集 (Clustering)，即冲突的元素聚集在一起，导致后续查找时间增加。\n\n\n二次探测 (Quadratic Probing): 发生冲突时，以二次方的方式偏移，H(key, i) = (H(key) + c1*i + c2*i^2) % m。\n可以缓解线性探测的聚集问题，但可能形成二次聚集。\n\n\n双重哈希 (Double Hashing): 使用两个哈希函数 H1(key) 和 H2(key)。当 H1(key) 发生冲突时，使用 H2(key) 的结果作为步长进行探测：H(key, i) = (H1(key) + i * H2(key)) % m。\n能有效消除聚集问题，减少冲突。\n\n\n\n\n操作:\n插入: 计算哈希值得到初始桶索引，如果该位置已被占用，根据探测方法找到下一个空闲位置。\n查找: 计算哈希值得到初始桶索引，如果该位置不是目标元素且不是空，根据探测方法继续查找，直到找到目标元素或遇到空桶（表示元素不存在）。\n删除: 比较复杂。简单地删除会破坏后续查找，通常采用惰性删除 (Lazy Deletion)，即将被删除的位置标记为“已删除”，后续插入可以覆盖，查找时跳过。\n\n\n优点:\n不需要额外的指针空间。\n缓存友好（元素存储在连续内存区域）。\n\n\n缺点:\n对负载因子非常敏感，负载因子不能超过 1，且接近 1 时性能急剧下降。\n删除操作复杂。\n可能存在聚集问题。\n\n\n示例: Python 的 dict 实现了开放寻址法。\n\n示意图：开放寻址法\n+-------------------------------------------------------------+|              哈希表 (Hash Table) - 开放寻址法                |+-------------------------------------------------------------+| 索引 | 桶 (Bucket)                                           |+------+-------------------------------------------------------+|  0   |  [ (17, &quot;E&quot;) ]                                        |+------+-------------------------------------------------------+|  1   |  [ 空 ]                                               |+------+-------------------------------------------------------+|  2   |  [ 空 ]                                               |+------+-------------------------------------------------------+|  3   |  [ (3, &quot;B&quot;) ]                                         |+------+-------------------------------------------------------+|  4   |  [ (10, &quot;C&quot;) ]                                        |+------+-------------------------------------------------------+|  5   |  [ (24, &quot;D&quot;) ]                                        |+------+-------------------------------------------------------+|  6   |  [ (20, &quot;A&quot;) ]                                        |+------+-------------------------------------------------------+\n\n四、负载因子 (Load Factor) 与扩容 (Resizing)负载因子 (Load Factor) 是衡量哈希表满载程度的指标：\n$$\\text{Load Factor} &#x3D; \\frac{\\text{当前元素数量 (n)}}{\\text{桶的总数量 (m)}}$$\n\n负载因子过低: 内存浪费，但冲突少，性能好。\n负载因子过高: 内存利用率高，但冲突多，性能差。\n\n当负载因子达到某个预设的阈值时，哈希表会进行扩容 (Resizing&#x2F;Rehashing)。\n扩容过程:\n\n创建一个新的、更大的底层数组（通常容量翻倍）。\n遍历旧哈希表中的所有键值对。\n对每个键值对重新计算哈希值（因为桶的数量 m 变了），将其插入到新数组的正确位置。\n释放旧数组。\n\n扩容开销: 扩容是一个 O(n) 的操作，但由于它的发生频率逐渐降低，平均每次插入的开销（摊销分析）仍然是 O(1)。\n常见阈值: Java HashMap 的默认负载因子阈值是 0.75。对于开放寻址法，阈值通常更低，例如 0.5 或 0.67。\n五、哈希表的性能分析\n平均时间复杂度:\n查找、插入、删除: O(1)\n前提是哈希函数设计良好，哈希值分布均匀，且负载因子在合理范围内。\n\n\n\n\n最坏时间复杂度:\n查找、插入、删除: O(n)\n发生在所有键都哈希到同一个桶（哈希函数设计极差），导致哈希表退化为链表。\n\n\n\n\n\n六、实际应用中的哈希表\nJava: HashMap, HashTable, ConcurrentHashMap\nHashMap 使用链地址法，并在链表过长时转换为红黑树。\nConcurrentHashMap 是线程安全的 HashMap 变体。\n\n\nPython: dict\n使用开放寻址法。\n\n\nC++: std::unordered_map, std::unordered_set\n通常使用链地址法。\n\n\nGo: map\n内部实现是类似链地址法的结构，但每个桶不是简单的链表，而是一个存有多个键值对的小数组（bmap），当小数组满时，会溢出到链表。\n\n\n\n七、总结哈希表是一种非常强大的数据结构，通过哈希函数将键映射到内存地址，允许我们以接近常数时间复杂度进行数据操作。它的核心在于：\n\n优秀的哈希函数: 尽可能均匀地分散键。\n有效的冲突解决策略: 优雅地处理多个键映射到同一地址的情况（链地址法或开放寻址法）。\n动态扩容机制: 在保证性能的同时，适应数据量的增长。\n\n理解这些原理对于高效地使用哈希表和解决相关问题至关重要。\n","categories":["数据结构"],"tags":["数据结构","2025","哈希表"]},{"title":"PromQL详解：深入理解Prometheus查询语言","url":"/2025/2025-02-04_PromQL%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Prometheus%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80/","content":"\nPromQL (Prometheus Query Language) 是 Prometheus 监控系统中用于查询、聚合和分析时间序列数据的一种功能强大的查询语言。它是 Prometheus 核心价值的体现之一。无论你是要构建仪表盘、创建告警规则，还是进行故障排查，PromQL 都是你与 Prometheus 数据进行交互的唯一途径。掌握 PromQL 是有效利用 Prometheus 的关键。\n\n“PromQL 让你能够将原始指标数据转化为有意义的洞察和可操作的智能信息。”\n\n\n一、Prometheus 指标类型回顾在深入 PromQL 之前，我们先快速回顾一下 Prometheus 的四种核心指标类型，因为 PromQL 的查询行为会根据指标类型有所不同：\n\nCounter (计数器)：一种累计型指标，只增不减（重置除外）。通常用于统计请求总数、错误总数等。\n例子：http_requests_total\n适用 PromQL 函数：rate()、irate()、increase()\n\n\nGauge (测量仪)：一种可任意上下变动的指标，反映当前状态。通常用于表示内存使用量、CPU 温度、并发连接数等。\n例子：node_memory_MemFree_bytes\n适用 PromQL 函数：直接查询、delta()、deriv()\n\n\nHistogram (直方图)：用于对采样值（如请求持续时间、响应大小）进行聚合统计，提供分布情况。它暴露 _bucket (区间内样本数)、_sum (所有样本值之和)、_count (样本总数) 三个指标。\n例子：http_request_duration_seconds_bucket\n适用 PromQL 函数：histogram_quantile()\n\n\nSummary (摘要)：与 Histogram 类似，但它在客户端计算分位数，如 0.5、0.99，也提供 _sum 和 _count。\n例子：http_request_duration_seconds_count (同 Summary 的 _count)\n\n\n\n二、PromQL 基础概念2.1 指标名称 (Metric Name)PromQL 查询的基础是指标名称。指标名称通常描述了被测量事物的通用特征。\n\n例子：http_requests_total （记录 HTTP 请求总数）\n\n2.2 标签 (Labels)标签是 Prometheus 最强大的特性之一。它们是键值对，用于标识指标的各个维度。通过标签，我们可以精确地过滤和聚合数据。\n\n例子：http_requests_total&#123;method=&quot;post&quot;, path=&quot;/api/v1&quot;&#125;\n\n2.3 查询结果类型PromQL 查询可以返回四种类型的结果：\n\n瞬时向量 (Instant vector)：由一组时间序列组成，每个时间序列只有一个样本值，且所有样本值都对应于查询的“瞬时时间”。这是最常用的返回类型。\n例子：http_requests_total\n\n\n区间向量 (Range vector)：由一组时间序列组成，每个时间序列包含在给定时间范围内的多个样本值。主要用于函数操作。\n例子：http_requests_total[5m] （过去 5 分钟内的 http_requests_total 值）\n\n\n标量 (Scalar)：一个简单的浮点数值（不带时间戳和标签）。\n例子：count(http_requests_total)\n\n\n字符串 (String)：目前未使用。\n\n三、PromQL 查询语法3.1 表达式语言元素PromQL 表达式包括：\n\n字面量：布尔值 (true&#x2F;false) 和数字。\n字符串：双引号或单引号包围的文本。\n变量：自定义的动态值（通常在 Grafana 中使用）。\n向量选择器：用于选择瞬时向量或区间向量。\n函数：对向量执行操作（如 rate()、sum()）。\n操作符：数学运算 (+, -, *, /, %, ^)，比较运算 (==, !=, &gt;, &lt;, &gt;=, &lt;=)，逻辑运算 (and, or, unless)，聚合运算 (sum, avg, min, max, count)。\n\n3.2 瞬时向量选择器用于选择在给定时间戳上的所有匹配标签的时间序列的最新样本。\n\n选择所有 http_requests_total 指标：http_requests_total\n通过标签过滤：\n精确匹配：&#123;&lt;labelname&gt;=&quot;&lt;labelvalue&gt;&quot;&#125;http_requests_total&#123;method=&quot;post&quot;, status=&quot;200&quot;&#125;\n不等于：&#123;&lt;labelname&gt;!=&quot;&lt;labelvalue&gt;&quot;&#125;http_requests_total&#123;instance!=&quot;localhost:8080&quot;&#125;\n正则表达式匹配：&#123;&lt;labelname&gt;=~&quot;&lt;regex&gt;&quot;&#125;http_requests_total&#123;job=~&quot;api-server|my-app&quot;&#125;\n正则表达式不匹配：&#123;&lt;labelname&gt;!~&quot;&lt;regex&gt;&quot;&#125;http_requests_total&#123;path!~&quot;/admin/.*&quot;&#125;\n\n\n\n3.3 区间向量选择器通过在瞬时向量选择器后添加 [&lt;duration&gt;] 来获取一个时间范围内的样本。持续时间用数字加单位表示，单位包括 s (秒), m (分钟), h (小时), d (天), w (周), y (年)。\n\n例子：http_requests_total[5m] # 过去 5 分钟内 http_requests_total 的所有样本node_cpu_seconds_total[1h] # 过去 1 小时内 CPU 使用的累积秒数\n\n3.4 偏移量 (Offset)通过 offset &lt;duration&gt; 可以在查询中将表达式的时间点向过去偏移。\n\n例子：http_requests_total offset 5m # 5 分钟前的 http_requests_total 值http_requests_total[1h] offset 1d # 昨天同一时间段的 1 小时内的总请求\n\n3.5 操作符 (Operators)3.5.1 数学运算符+, -, *, /, %, ^ (幂)。可以用于标量和瞬时向量之间，或两个瞬时向量之间。\n\n例子：node_memory_MemFree_bytes / node_memory_MemTotal_bytes * 100 # 计算内存空闲百分比\n\n3.5.2 比较运算符==, !=, &gt;, &lt;, &gt;=, &lt;=。返回结果只有在比较条件为真时才会保留。\n\n例子：node_cpu_usage &gt; 0.8 # 返回 CPU 使用率大于 0.8 的时间序列\n\n3.5.3 逻辑&#x2F;集合运算符and (交集), or (并集), unless (差集)。\n\n例子：# 返回 status=&quot;200&quot; 和 method=&quot;post&quot; 的请求交集http_requests_total&#123;status=&quot;200&quot;&#125; and http_requests_total&#123;method=&quot;post&quot;&#125;\n\n3.5.4 向量匹配 (Vector Matching)当两个瞬时向量操作时，Prometheus 会尝试匹配它们的标签集。\n\n一对一匹配 (One-to-one matching)：操作符两侧的向量元素具有完全相同的标签集。\n\n多对一 &#x2F; 一对多匹配 (Many-to-one &#x2F; One-to-many matching)：一侧的向量元素可以与多侧的多个元素匹配。需要使用 on() 或 ignoring() 来指定匹配标签。\n\non(&lt;label list&gt;)：仅在指定的标签上匹配。\nignoring(&lt;label list&gt;)：忽略指定的标签进行匹配。\n\n\n例子：\n# 计算每个 job 的请求成功率(http_requests_total&#123;status=&quot;200&quot;&#125; / http_requests_total) by (job)# 假设一个服务有 error 和 total 两个计数器，通过实例匹配sum by (instance) (service_errors_total) / sum by (instance) (service_requests_total)\n\n3.6 聚合函数 (Aggregation Operators)用于将多个时间序列聚合为一个或多个时间序列。语法：&lt;agg-op&gt;([parameter,] &lt;vector expression&gt;) [by / without &lt;label list&gt;]\n\n&lt;agg-op&gt;：sum, avg, min, max, count, stddev, stdvar, group, topk, bottomk, quantile。\n\nby (&lt;label list&gt;)：对指定的标签进行分组聚合，保留这些标签。\n\nwithout (&lt;label list&gt;)：对除了指定的标签以外的所有标签进行分组聚合，丢弃这些标签。\n\n例子：\n# 所有 Prometheus 抓取目标的活跃连接总数sum(up)# 每个 job 的 HTTP 请求总数sum(http_requests_total) by (job)# 排除 method 和 status 标签后，聚合 HTTP 请求的总数sum(http_requests_total) without (method, status)\n\n四、PromQL 函数 (Functions)PromQL 提供了丰富的内置函数来处理和分析时间序列数据。\n4.1 计数器相关函数 (Counters)\nrate(v range-vector)：计算区间向量 v 中时间序列每秒的平均增长率。这对于 Counter 类型指标是计算每秒平均增量的主要方法。rate(http_requests_total[5m]) # 每 5 分钟的平均每秒请求数\nirate(v range-vector)：计算区间向量 v 中时间序列最近两个样本的每秒瞬时增长率。对频繁变化的 Counter 指标更敏感。irate(node_network_transmit_bytes_total[1m]) # 1 分钟内的瞬时网络发送速率\nincrease(v range-vector)：计算区间向量 v 中时间序列总的增量。适用于 Counter 指标，会处理计数器重置。increase(http_requests_total[1h]) # 过去 1 小时内 HTTP 请求的总数\n\n4.2 Gauge 相关函数 (Gauges)\ndelta(v range-vector)：计算区间向量 v 中时间序列的样本值变化量。delta(node_temp_celsius[1h]) # 1 小时内温度的变化量\nderiv(v range-vector)：计算区间向量 v 中时间序列的一阶导数。deriv(node_fans_speed_rpm[5m]) # 风扇转速的瞬时变化率\npredict_linear(v range-vector, t scalar)：基于区间向量 v 中时间序列的线性回归，预测 t 秒后的值。predict_linear(node_disk_free_bytes[1h], 4 * 3600) # 预测 4 小时后磁盘剩余空间\n\n4.3 直方图相关函数 (Histograms)\nhistogram_quantile(quantile scalar, bucket_le_series range-vector)：计算 Histogram 类型指标的分位数。它将 _bucket 指标作为输入。histogram_quantile(0.99, http_request_duration_seconds_bucket[5m]) # 过去 5 分钟内 HTTP 请求耗时的 99% 分位数\n\n4.4 其他常用函数\nsum_over_time(v range-vector)：返回区间向量 v 中每个时间序列所有样本值的和。\navg_over_time(v range-vector)：返回区间向量 v 中每个时间序列所有样本值的平均值。\ncount_over_time(v range-vector)：返回区间向量 v 中每个时间序列的样本数量。\nabsent(v instant-vector)：如果查询结果为空，则返回 1；否则返回 0。常用于告警，检测服务是否停止上报指标。absent(up&#123;job=&quot;my-app&quot;&#125;) # 如果 my-app 停止上报，则触发告警\nclamp_max(v instant-vector, max scalar)：将瞬时向量 v 中的值限制在 max 以下。\nclamp_min(v instant-vector, min scalar)：将瞬时向量 v 中的值限制在 min 以上。\n\n五、PromQL 告警规则示例Prometheus 的告警规则也是用 PromQL 编写的。规则存储在 .yml 文件中，并通过 rules 配置加载。\n# rules.ymlgroups:  - name: server_alerts    rules:      - alert: HostHighCPUUsage # 告警名称        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) * 100) &gt; 80        # 表达式：当前 CPU 利用率在过去 5 分钟的平均值超过 80%        for: 5m # 持续 5 分钟后触发告警        labels:          severity: critical # 告警级别        annotations:          summary: &quot;主机 &#123;&#123; $labels.instance &#125;&#125; CPU 使用率过高&quot;          description: &quot;主机 &#123;&#123; $labels.instance &#125;&#125; CPU 使用率已达到 &#123;&#123; $value &#125;&#125;%，持续超过 5 分钟。&quot;      - alert: ServiceDown        expr: absent(up&#123;job=&quot;my_service&quot;&#125;)        for: 1m        labels:          severity: major        annotations:          summary: &quot;服务 &#123;&#123; $labels.job &#125;&#125; 已停止上报指标&quot;          description: &quot;服务 &#123;&#123; $labels.job &#125;&#125; 在过去 1 分钟内未上报任何指标，可能已停止运行。&quot;\n\n告警规则解析：\n\nalert：告警名称。\nexpr：用于判断是否触发告警的 PromQL 表达式。\nfor：如果 expr 持续多长时间为真，才触发告警。用于减少瞬时波动的误报。\nlabels：附加到告警上的静态标签。\nannotations：提供更详细信息的文本字段，支持 Go 模板语法 (&#123;&#123; $labels.label_name &#125;&#125; 和 &#123;&#123; $value &#125;&#125;)。\n\n六、实际案例分析6.1 计算 HTTP 总请求量sum(http_requests_total) # 所有 HTTP 请求的总数sum(http_requests_total) by (job, instance) # 按 job 和 instance 分组的 HTTP 请求总数\n\n6.2 计算每秒请求数 (QPS)rate(http_requests_total[1m]) # 过去 1 分钟的平均每秒请求数\n\n6.3 计算 CPU 利用率100 - (avg by (instance) (rate(node_cpu_seconds_total&#123;mode=&quot;idle&quot;&#125;[5m])) * 100)# 首先计算 idle 模式下的 CPU 在 5 分钟内的平均每秒增量（即空闲时间占比）# 然后 `1 - 空闲时间占比` 得到忙碌时间占比# 最后乘以 100 得到百分比\n\n6.4 计算网络带宽使用率# 传入带宽rate(node_network_receive_bytes_total&#123;device=&quot;eth0&quot;&#125;[5m]) # eth0 网卡每秒接收字节数# 传出带宽rate(node_network_transmit_bytes_total&#123;device=&quot;eth0&quot;&#125;[5m]) # eth0 网卡每秒发送字节数\n\n6.5 检测磁盘空间不足 (少于 20%)(node_filesystem_avail_bytes&#123;device=&quot;/dev/sda1&quot;&#125; / node_filesystem_size_bytes&#123;device=&quot;/dev/sda1&quot;&#125;) * 100 &lt; 20\n\n6.6 应用程序错误率假设有 app_requests_total 和 app_errors_total 两个 Counter：\n# 计算过去 5 分钟内的错误率rate(app_errors_total[5m]) / rate(app_requests_total[5m])\n\n七、学习资源与进阶\nPrometheus 官方文档：https://prometheus.io/docs/prometheus/latest/querying/basics/\nPromQL Cheat Sheet：网上有很多 PromQL 速查卡片，是很好的参考。\nPromQL Playground：在 Prometheus Web UI 的 Graph 页面或 PromLens (一个强大的 PromQL 调试工具) 中进行实验和练习。\nGrafana：通过实践创建仪表盘来巩固 PromQL 知识。\n\n八、总结PromQL 是 Prometheus 监控系统的心脏，理解和熟练运用它是发挥 Prometheus 强大功能的基础。它通过多维数据模型、灵活的标签匹配、丰富的操作符和函数，使得从海量时间序列数据中抽取有价值的信息成为可能。从简单的指标查询到复杂的告警规则和趋势预测，PromQL 授予你对数据的高度掌控力，是构建高效、智能监控系统的必备技能。不断实践和探索，你将发现 PromQL 的无限潜力。\n","categories":["开发工具","数据监控"],"tags":["2025","Prometheus","数据监控"]},{"title":"哈希表负载因子详解(Load Factor)","url":"/2025/2025-03-01_%E5%93%88%E5%B8%8C%E8%A1%A8%E8%B4%9F%E8%BD%BD%E5%9B%A0%E5%AD%90%E8%AF%A6%E8%A7%A3(Load%20Factor)/","content":"\n哈希表（Hash Table） 是一种非常高效的数据结构，它通过哈希函数将键（key）映射到数组的索引位置，从而实现常数时间复杂度 O(1) 的平均查找、插入和删除操作。然而，哈希表的性能并非总是 O(1)，它严重依赖于哈希函数、冲突解决策略以及一个关键的指标——负载因子（Load Factor）。\n\n“The load factor of a hash table is a measure of how full the hash table is during its operation.” —— Wikipedia\n\n\n一、什么是负载因子？负载因子 (Load Factor) 是衡量哈希表满载程度的一个指标。它定义为：\n$$\\text{Load Factor} &#x3D; \\frac{\\text{Number of elements in the hash table (n)}}{\\text{Total number of buckets (m)}}$$\n或者：\n$$\\alpha &#x3D; \\frac{n}{m}$$\n其中：\n\nn (也可表示为 size) 是当前哈希表中存储的键值对（或元素）的数量。\nm (也可表示为 capacity 或 buckets) 是哈希表中桶（bucket）的总数量，也就是底层数组的大小。\n\n负载因子是一个浮点数，它表示了平均每个桶中存储了多少个元素。\n二、负载因子对哈希表性能的影响负载因子直接影响哈希表的性能和内存使用效率。理解其影响至关重要。\n1. 负载因子过小 ($\\alpha \\ll 1$)\n优点:\n更低的冲突率: 每个桶中平均存储的元素更少，导致哈希冲突的概率降低。\n更快的查找、插入、删除性能: 由于冲突少，解决冲突所需的链表遍历（开放寻址中的探测）次数减少，使得操作更接近 O(1) 的理想状态。\n\n\n缺点:\n内存浪费: 表中会有大量的空桶，浪费了内存空间。\n\n\n\n2. 负载因子过大 ($\\alpha \\gg 1$)\n优点:\n更少的内存占用 (相对): 在给定元素数量的情况下，桶的数量较少，内存使用更紧凑。\n\n\n缺点:\n更高的冲突率: 更多的元素挤在有限的桶中，导致哈希冲突的概率急剧增加。\n更慢的查找、插入、删除性能:\n链地址法: 链表会变得很长，遍历链表的时间复杂度增加，平均操作性能可能退化到 O(n)。\n开放寻址法: 探查序列会变长，可能导致聚集（clustering）问题，性能显著下降。\n\n\n哈希表退化: 极端情况下，所有元素都冲突并聚集在一个桶中，哈希表退化为链表，操作性能降至 O(n)。\n\n\n\n3. 恰当的负载因子选择一个恰当的负载因子是平衡时间和空间的关键。\n\n过低：浪费内存，但性能好。\n过高：节省内存，但性能差。\n\n大多数哈希表的实现会根据负载因子动态地调整其底层数组的大小——这个过程称为扩容 (Resizing&#x2F;Rehashing)。\n三、哈希表的扩容 (Resizing&#x2F;Rehashing)当哈希表的负载因子达到某个预设的**阈值 (Threshold)**时，哈希表通常会进行扩容。\n扩容过程:\n\n创建一个新的、更大的底层数组（桶的数量通常是原来的两倍）。\n遍历旧哈希表中的所有元素。\n对每个元素重新计算哈希值，并将其插入到新数组的正确位置。这是因为桶的数量变化了，哈希函数对模数 m 的操作也会变化，所以元素需要重新映射到新位置。\n释放旧的数组。\n\n扩容的代价:\n扩容是一个耗时的操作，因为它需要重新哈希并移动所有现有元素。这个操作的复杂度是 O(n)，其中 n 是当前哈希表中的元素数量。由于扩容的发生，单次插入操作在最坏情况下可能不是 O(1)。\n为了摊销扩容的开销，哈希表通常会进行两倍扩容。这样，即使某个插入操作触发了扩容，但在一系列操作中，平均每次操作的复杂度仍然接近于 O(1)（摊销分析）。\n四、常见的负载因子阈值不同的哈希表实现会采用不同的负载因子阈值，以在内存和性能之间达到最佳平衡。\n\nJava HashMap: 默认负载因子阈值为 0.75。\n当元素数量达到 容量 * 0.75 时，HashMap 会进行扩容。\n0.75 是一个经验值，被认为在时间和空间之间提供了良好的折衷。\n\n\nPython dict: 负载因子阈值更复杂，但通常在 2/3 到 3/4 之间。Python 的 dict 采用开放寻址法，对负载因子更为敏感。\nC++ std::unordered_map: 没有强制的固定阈值。它通常允许用户在构造时指定 max_load_factor，默认值通常是 1.0。\n对于链地址法，负载因子可以大于 1。例如，负载因子为 2.0 意味着平均每个桶有两个元素。\n对于开放寻址法，负载因子一般不能超过 1.0，因为每个桶最多只能存储一个元素。实际上，为了避免效率急剧下降，通常会远小于 1.0。\n\n\n\n为什么 0.75 是一个常见的选择？\n0.75 是一个折中：\n\n它足够大，可以避免过度内存浪费。\n它足够小，可以避免太多的哈希冲突，保证平均 O(1) 的性能。\n当负载因子为 0.75 时，采用链地址法的哈希表，链表长度通常不会太长。这可以保证大部分操作保持高效。\n\n五、负载因子与冲突解决策略的关系负载因子对于不同的冲突解决策略有不同的敏感度。\n1. 链地址法 (Separate Chaining)\n每个桶存储一个链表（或红黑树等），当发生冲突时，新元素添加到链尾。\n负载因子 可以大于 1。例如，负载因子为 2 意味着平均每个桶的链表长度为 2。\n即使负载因子较大，性能也不会急剧下降，但链表会变长，导致查找时间增加。\n扩容阈值通常在 0.75 到 1.0 之间。\n\n2. 开放寻址法 (Open Addressing)\n所有元素直接存储在哈希表的底层数组中。当发生冲突时，通过探测序列（线性探测、二次探测、双重哈希）找到下一个空闲位置。\n负载因子 不允许大于 1。因为每个桶只能存储一个元素。\n对负载因子非常敏感。当负载因子接近 1 时，哈希表会变得非常稠密，导致长的探测序列和严重的聚集问题，性能急剧下降。\n扩容阈值通常远低于 1.0，例如 0.5 到 0.7。\n\n六、自定义负载因子（何时以及如何）在某些情况下，你可能需要根据具体应用场景调整哈希表的负载因子：\n\n读多写少，对查找速度要求极高: 可以设置更低的负载因子（例如 0.5），以减少冲突，即使这意味着更多的内存消耗。\n内存极其受限，对性能要求不是极致: 可以设置更高的负载因子（例如 0.9），以节省内存，但要接受潜在的性能下降。\n特定数据分布: 如果你的数据哈希分布很差，可能会导致即使负载因子很低也冲突严重。这种情况下，需要优化哈希函数或选择适应性更强的冲突解决策略（如 Java 8 HashMap 从链表到红黑树的转换）。\n\n如何自定义:\n大多数语言的标准库哈希表实现都允许在构造时或通过方法设置 initial_capacity 和 max_load_factor。\n\n初始容量 (Initial Capacity): 在创建哈希表时预估可能存储的元素数量，设置一个合适的初始容量可以减少扩容的次数，避免性能抖动。\n最大负载因子 (Max Load Factor): 调整扩容的阈值。\n\n示例 (伪代码):\n// 初始化一个HashMap，指定初始容量和最大负载因子HashMap&lt;KeyType, ValueType&gt; myMap = new HashMap&lt;&gt;(initialCapacity, maxLoadFactor);// 例如：HashMap&lt;String, Integer&gt; scores = new HashMap&lt;&gt;(100, 0.6); // 初始容量100，负载因子0.6\n\n七、总结负载因子是哈希表性能和内存效率之间权衡的关键指标。\n\n较低的负载因子：更高的空间效率（更多空桶），但更少的冲突，因此性能更好。\n较高的负载因子：更高的空间利用率（更少空桶），但更多的冲突，因此性能可能下降。\n\n理解并合理设置负载因子（或让标准库使用其明智的默认值），对于设计高效、稳定的哈希表应用程序至关重要。在实际应用中，通常建议使用标准库提供的哈希表，它们已经针对各种场景进行了优化，并采用了经验证的负载因子阈值。只有在对性能或内存有极特殊要求时，才考虑自定义哈希表的行为。\n","categories":["数据结构"],"tags":["数据结构","2025","哈希表"]},{"title":"浏览器指纹 (Browser Fingerprinting) 详解","url":"/2025/2025-03-15_%E6%B5%8F%E8%A7%88%E5%99%A8%E6%8C%87%E7%BA%B9%20(Browser%20Fingerprinting)%20%E8%AF%A6%E8%A7%A3/","content":"\n浏览器指纹 (Browser Fingerprinting) 是一种用于识别或追踪用户在线行为的技术，即使在用户清除了 cookies、使用无痕模式甚至更换 IP 地址之后，它也能尝试标识出唯一的用户或设备。与 cookies 不同，浏览器指纹不是存储在用户设备上的数据，而是通过收集用户浏览器的各种配置和设置信息来生成的。\n\n“你的浏览器就像你的手纹一样，看似普通，却独一无二。”\n\n\n一、什么是浏览器指纹？浏览器指纹是指网站或在线服务通过收集用户浏览器和设备的大量可公开信息（如操作系统、浏览器类型和版本、屏幕分辨率、字体、插件、MIME 类型、时区、语言设置、GPU 信息、Canvas 渲染结果、AudioContext 信息等），并将这些信息综合起来生成一个近似唯一的“指纹”，从而在一定概率上识别单个用户或设备的技术。\n这个“指纹”的强大之处在于其持久性和隐蔽性，用户很难通过常规手段进行清除或规避。\n二、浏览器指纹的工作原理网站通过 JavaScript 或其他客户端脚本，在用户访问时执行一系列操作来获取其浏览器和设备特征。这些特征包括：\n1. HTTP 请求头信息 (HTTP Headers)这是最基础的指纹信息，每次 HTTP 请求都会携带：\n\nUser-Agent: 浏览器、操作系统和设备类型。\nAccept-Language: 浏览器接受的语言设置。\nAccept-Encoding: 浏览器接受的编码方式。\n\n2. 屏幕和显示器信息 (Screen &amp; Display)通过 window.screen 和 window.innerWidth&#x2F;innerHeight 等 API 获取：\n\n屏幕分辨率 (e.g., 1920x1080)。\n颜色深度 (e.g., 24-bit)。\n操作系统界面缩放比例 (DPI)。\n\n3. 插件和扩展信息 (Plugins &amp; Extensions)过去常通过 navigator.plugins 和 navigator.mimeTypes 获取 Flash, Java 等插件信息。现在随着 Flash 等插件的淘汰，这个方法的重要性下降，但浏览器扩展依然可以被检测到。\n4. 字体信息 (Fonts)通过 JavaScript 检测系统上安装的字体列表。即使只是几款独特字体，也能显著增加指纹的独特性。\n\n原理: 创建一个隐藏的 DOM 元素，设置待检测字体，然后测量该元素的宽度和高度。如果尺寸与默认字体不同，则说明该字体已安装。\n\n5. Canvas 指纹 (Canvas Fingerprinting)这是目前最强大、最普遍的指纹技术之一。\n\n原理: 浏览器使用 Canvas API 绘制（渲染）一段文本或图形。由于不同设备、操作系统、浏览器、GPU、字体渲染引擎甚至硬件驱动之间存在的微小差异，即使是完全相同的指令，渲染出的像素数据也会有微小的不同。\n过程:\n网站在 Canvas 上绘制一些文本（通常带一些渐变、阴影等效果）和图形。\n将 Canvas 内容导出为图片数据（例如 toDataURL() 或 getImageData()）。\n对图像数据进行哈希运算，生成一个唯一的字符串作为指纹。\n\n\n独特性: 即使肉眼无法察觉的像素差异，也会导致哈希值不同。\n\n6. AudioContext 指纹 (AudioContext Fingerprinting)与 Canvas 指纹类似，它利用 Web Audio API。\n\n原理: 通过 JavaScript 创建一个 AudioContext，生成特定的音频波形，然后通过读取音频数据的特性（如音量、相位等）来生成哈希值。不同设备上的音频硬件、驱动、操作系统和软件库在处理音频时产生的微小差异，会导致相同音频指令的输出结果不一致。\n过程:\n使用 AudioContext 构造一个独特的音频信号图。\n处理该信号（例如，进行压缩、混响等操作）。\n将处理后的信号数据转换为哈希值。\n\n\n独特性: 同样具有高度的唯一识别能力。\n\n7. WebGL 指纹 (WebGL Fingerprinting)利用 WebGL API 访问 GPU 信息。\n\n原理: 通过 WebGL 绘制 3D 图形，获取 GPU 的渲染细节和能力。不同显卡型号、驱动版本、操作系统对 WebGL 的实现差异会产生独特的渲染结果。\n过程: 获取 renderer 字符串、纹理单元数量、最大视口尺寸等，并结合渲染结果进行哈希。\n\n8. WebRTC 和系统信息\n本地 IP 地址: WebRTC 可以获取用户设备的本地 IP 地址，即使使用了 VPN。但这通常需要用户授权。\n操作系统和硬件: 通过 navigator.platform, navigator.hardwareConcurrency (CPU 核心数), navigator.deviceMemory (内存) 等获取。\n\n9. 时区和语言设置通过 Intl.DateTimeFormat().resolvedOptions().timeZone 和 navigator.language&#x2F;languages 获取。\n10. 其他细微差异\n电池状态 API: navigator.getBattery() (现在通常被限制使用)。\n摄像头&#x2F;麦克风设备 ID: 在某些情况下可能获取。\n浏览器对特定 CSS 属性、JS API 的实现差异或 BUG。\n\n三、浏览器指纹的挑战和影响1. 隐私问题\n持久性追踪: 即使清除 cookies 或使用隐私模式，用户也可能被持续追踪，这破坏了用户的匿名性期望。\n数据聚合: 跨网站的数据聚合变得更加容易，用户在不同网站上的行为可能被关联起来，形成更完整的用户画像。\n个性化广告: 广告商可以更精准地投放广告，甚至基于用户的“隐形”数据进行定向。\n\n2. 安全问题\n身份伪造: 恶意攻击者如果能获取到你的浏览器指纹，可能尝试伪造你的设备身份，绕过一些简单的设备验证。\n账户接管: 与其他信息结合，可以增加账户被接管的风险。\n\n3. 法规和伦理争议\n许多隐私法规（如 GDPR、CCPA）要求网站在收集用户数据前获得明确同意。浏览器指纹的隐蔽性使其难以符合这些规定。\n关于这种“隐形追踪”是否符合伦理道德，一直存在争议。\n\n四、如何对抗浏览器指纹？对抗浏览器指纹是一个复杂且持续发展的猫鼠游戏，没有一劳永逸的解决方案，但以下方法可以增加识别难度：\n1. 使用隐私浏览器\nTor 浏览器 (Tor Browser): 被认为是目前对抗浏览器指纹最有效的工具之一。它通过标准化所有用户的指纹，使得所有 Tor 用户的浏览器看起来都一样，从而提高匿名性。\nBrave 浏览器: 内置了指纹保护功能，可以随机化或限制指纹信息的暴露。\nFirefox 的增强型跟踪保护: 提供“严格”模式，一定程度上减轻指纹追踪。\n\n2. 浏览器扩展&#x2F;插件安装专门对抗指纹的扩展，例如：\n\nCanvasBlocker: 阻止或欺骗 Canvas API。\nTrace: 尝试伪造或随机化多种指纹信息。\nPrivacy Badger: 识别并阻止隐藏的追踪器。\n\n3. 通用设置调整\n禁用 JavaScript (慎重): 禁用 JavaScript 会阻止绝大多数指纹收集，但也会导致绝大多数网站无法正常工作。\n频繁更换浏览器和设备: 实际操作性较差。\n使用虚拟机或沙箱环境: 每次启动都提供一个“全新”的浏览器环境，可以有效对抗指纹，但操作麻烦。\n\n4. 随机化指纹信息 (SPOOFING)某些工具或浏览器，通过每次访问时随机化部分指纹信息（例如 User-Agent, Canvas 渲染结果的微小噪声），使得每次生成的指纹都略有不同，从而避免被关联。\n5. 注意浏览习惯\n尽量避免登录或使用不同身份访问同一网站。\n定期审查和调整浏览器的隐私设置。\n\n五、浏览器指纹的积极用途 (双刃剑)尽管主要被用于追踪和广告，浏览器指纹在某些情况下也有积极作用：\n\n欺诈检测和预防: 银行、电商网站等可以使用指纹来检测可疑登录和欺诈交易，例如，如果用户突然从一个过去从未见过的指纹设备（即使 IP 地址在正常范围内）登录，可能会触发额外的安全验证。\n账户安全: 作为辅助验证手段，帮助识别用户设备，增强账户安全性。\n防止机器人和爬虫: 识别非人类访问，保护网站资源。\n提供更好的用户体验: 识别设备特性，为用户提供更匹配其设备性能的网页版本。\n\n六、总结浏览器指纹是数字时代隐私与便利之争的一个缩影。它揭示了我们在线行为的透明性远超我们想象。作为用户，了解其工作原理有助于我们更好地采取措施保护自己的隐私。作为开发者，我们需要在利用这些技术提供更好服务的同时，认真考虑其中的隐私风险和伦理界限，并遵守相关的政策法规。隐私保护是一个持续的挑战，需要技术、法律和用户意识的共同努力。\n","categories":["网络安全"],"tags":["前端技术","网络安全","JavaScript","2025"]},{"title":"Netlify介绍","url":"/2025/2025-03-17_Netlify%E4%BB%8B%E7%BB%8D/","content":"\nNetlify 是一个领先的自动化平台，专为 Jamstack 架构的现代化 Web 项目提供构建、部署和托管服务。它通过将 Git 工作流、全球 CDN、自动化 CI&#x2F;CD 和 Serverless 功能整合在一起，极大地简化了网站和 Web 应用的开发和部署流程，让开发者能够专注于代码，而不是基础设施。\n\n“Netlify is the de-facto platform for Jamstack sites, providing an all-in-one workflow for building, deploying, and scaling modern web applications.” —— Netlify Official\n\n\n一、Netlify 核心理念与 Jamstack1. 什么是 Jamstack？Netlify 最初是为了推广和优化 Jamstack 架构而诞生的平台。Jamstack 代表 JavaScript, APIs, Markup。其核心理念是：\n\n预构建: 网站内容在构建时生成静态文件 (HTML, CSS, JS)。\n客户端 JavaScript: 通过 JavaScript 处理动态交互和数据获取。\n可复用 API: 业务逻辑和数据通过可复用的 API (SaaS, Serverless Functions, GraphQL 等) 提供。\n\nJamstack 的优势在于更高的性能、更高的安全性、更低的成本和更简单的扩展性。\n2. Netlify 的定位Netlify 旨在成为 Jamstack 应用的“一体化平台”，提供将 Git 仓库中的代码转化为全球可用的高性能 Web 应用所需的一切，包括：\n\n持续部署 (CI&#x2F;CD): 每次 Git 提交都会自动构建和部署。\n全球 CDN: 网站内容在全球范围内快速分发。\nServerless Functions: 无需运维的后端功能。\n表单处理: 内置的表单提交和管理。\n身份验证: Netlify Identity 提供了用户管理和认证服务。\n\n3. Netlify 的优势\n极致的开发者体验: 从 Git 克隆到生产部署只需几分钟。直观的 UI 和强大的 CLI。\n高性能: 预构建的静态内容通过全球 CDN 分发，结合边缘计算，实现极快的加载速度。\n简化运维: 无需管理服务器、负载均衡、SSL 证书等。\n弹性伸缩: 自动处理流量峰值。\n降低成本: 按需付费，通常比传统服务器托管更经济。\n安全性: 静态内容减少了服务器端的攻击面。\n\n二、Netlify 的主要功能和特性1. 自动 CI&#x2F;CD 与部署\nGit 集成: 与 GitHub, GitLab, Bitbucket 深度集成。当您连接仓库时，Netlify 会自动检测您的网站构建工具（如 Next.js, Gatsby, VuePress, Hugo 等）。\n持续部署: 每次向 Git 仓库提交代码时，Netlify 都会自动触发构建、部署和缓存失效。\n即时部署 (Atomic Deploys): 新旧版本之间无缝切换，保证零停机时间。\n预览部署 (Deploy Previews): 对于 Pull Request (PR) 或非生产分支的每次提交，Netlify 都会生成一个唯一的预览 URL。这对于团队协作、设计评审和功能测试非常有用。\n回滚: 轻松一键回滚到任何之前的部署版本。\n版本控制: 部署日志和历史永久保留。\n\n2. 全球 CDN (内容分发网络)\n边缘网络: Netlify 的全球边缘网络将您的静态资产缓存到离用户最近的节点，大大减少了页面加载时间。\n自动 SSL: 免费提供 Let’s Encrypt SSL 证书，并自动续订。\n自定义域名: 轻松连接自己的域名。\n\n3. Netlify Functions (Serverless Functions)\n基于 AWS Lambda: Netlify Functions 是基于 AWS Lambda 构建的无服务器函数，但 Netlify 提供了更友好的开发和部署体验。\n文件系统约定: 通常将函数代码放在 netlify/functions 目录下，Netlify 会自动检测并部署它们。\nAPI 后端: 用于处理动态数据、集成第三方 API、执行身份验证、处理表单提交等后端逻辑。\n支持语言: Node.js, Go, Python, Ruby。\n边缘函数 (Edge Functions): 类似于 Vercel 的 Edge Functions，允许在 CDN 边缘节点执行 JavaScript&#x2F;TypeScript 函数，提供更低延迟的动态内容。\n\n4. Netlify Forms (表单处理)\n无需后端: 自动检测 HTML 表单，并将其提交数据收集到 Netlify 后台，无需编写任何后端代码。\n垃圾邮件过滤: 内置了 Akismet 和 reCAPTCHA 过滤。\nWebhook 集成: 可以将表单提交数据发送到其他服务。\n\n5. Netlify Identity (身份认证)\nGoTrue 开源认证: 提供了一个简单的、基于 JWT（JSON Web Token）的身份验证和用户管理服务。\n第三方登录: 支持 Google, GitHub 等 OAuth 提供者。\n电子邮件验证: 自动处理用户注册、登录、密码重置等流程。\n\n6. Netlify CMS (内容管理系统)\nGit-based CMS: 一个开源的、基于 Git 的内容管理系统，允许非技术用户通过友好的界面编辑网站内容，并将变更直接提交到 Git 仓库。\nMarkdown 支持: 非常适合博客、文档站点。\n无服务器部署: 作为一个单页应用 (SPA) 部署在 Netlify 上。\n\n7. Netlify Redirects &amp; Rewrites (_redirects 文件或 netlify.toml)\n强大的路由规则: 使用项目根目录下的 _redirects 文件或 netlify.toml 定义重定向和重写规则，支持通配符、HTTP 状态码、代理等。\n示例 _redirects:/old-path /new-path 301/blog/* /posts/:splat 200/api/* /.netlify/functions/api/:splat 200\n\n\n\n8. Netlify CLI (命令行工具)\n强大的命令行工具，用于本地开发、部署、管理网站和 Netlify Functions。\n\n三、如何使用 Netlify\n连接 Git 仓库: 登录 Netlify 账户，点击 “New site from Git”，选择您要部署的 Git 仓库（GitHub, GitLab, Bitbucket）。\n选择项目与配置: 选择仓库、分支。Netlify 会自动检测您的项目类型，并建议构建命令和发布目录。\nBuild command: 项目的构建命令 (例如 npm run build, gatsby build)。\nPublish directory: 构建后静态文件输出的目录 (例如 public, dist, .next)。\n\n\n部署: 点击 “Deploy site” 按钮。Netlify 会自动拉取代码、执行构建命令、然后将构建产物部署到全球 CDN。\n自定义域名: 部署完成后，您可以为您的网站配置自定义域名。\n\n四、Netlify 的计费模式Netlify 提供免费层级 (Starter 计划) 和付费计划 (Pro, Business, Enterprise)。\n\nStarter 计划: 适用于个人项目、开源项目。提供慷慨的构建时间、带宽、函数执行时间、Forms 提交限制等免费额度。\nPro &#x2F; Business &#x2F; Enterprise 计划: 提供更高的额度，更多的团队功能、高级支持、更长的函数执行时间、SLA 等。\n\n其按使用量和功能付费的模式，使得 Netlify 对于初创公司和个人开发者而言，是非常经济高效且功能强大的选择。\n五、总结与展望Netlify 凭借其易用性、集成性、高性能和对 Jamstack 架构的深度支持，已经成为现代 Web 开发部署的标志性平台。它不仅简化了从代码到全球可访问网站的整个链路，还通过一系列附加功能（如 Serverless Functions, Forms, Identity）赋能开发者构建全功能的 Web 应用，而无需担心底层基础设施。\n对于前端开发者，尤其是那些利用 React, Vue, Svelte 等框架结合静态站点生成器或单页应用的项目，Netlify 提供了一个几乎完美的部署体验。随着 Web 技术对性能、安全和开发者体验要求的不断提高，Netlify 的“Frontend Cloud”模式将继续在行业中扮演重要角色。\n","categories":["开发工具","云服务"],"tags":["Serverless","云服务","CI/CD","2025","Netlify"]},{"title":"Node.js 本地静态服务详解：http-server 与 live-server","url":"/2025/2025-04-24_Node.js%20%E6%9C%AC%E5%9C%B0%E9%9D%99%E6%80%81%E6%9C%8D%E5%8A%A1%E8%AF%A6%E8%A7%A3%EF%BC%9Ahttp-server%20%E4%B8%8E%20live-server/","content":"\n在前端开发过程中，我们经常需要一个本地服务器来预览 HTML、CSS、JavaScript 等静态文件。虽然许多现代前端框架（如 React, Vue, Angular）都自带了开发服务器，但对于一些简单的项目、纯静态网站或快速原型开发，使用 Node.js 提供轻量级的本地静态服务器会更加方便快捷。本文将详细介绍两个广受欢迎的 Node.js 静态服务器工具：http-server 和 live-server。\n\n“好的本地开发服务器，让你的前端工作流如丝般顺滑。”\n\n\n一、为什么需要本地静态服务？在浏览器中直接打开本地的 HTML 文件（file:/// 协议）通常会有一些限制和问题：\n\nAJAX&#x2F;Fetch 请求受限：浏览器出于安全考虑（同源策略），不允许 file:/// 协议下的页面进行跨域 AJAX 请求，甚至无法加载本地其他文件的 AJAX 请求。\n动态加载问题：某些 JavaScript 模块加载器（如 ES Module import 语句）在 file:/// 协议下可能无法正常工作。\n开发工具功能不全：一些浏览器扩展或开发工具可能依赖于 HTTP&#x2F;HTTPS 协议才能正常工作。\n实时预览：没有热重载或自动刷新功能，每次修改代码都需要手动刷新浏览器。\n\n一个本地的 HTTP 服务器可以解决以上所有问题，提供一个更接近生产环境的开发预览环境。\n二、http-server：轻量级静态文件服务器http-server 是一个简单、零配置的命令行 HTTP 服务器。它能够一键启动一个本地服务器，并将当前目录下的文件作为静态资源提供访问。\n1. 安装http-server 通常作为全局工具安装，这样你可以在任何目录下直接使用。\nnpm install -g http-server# 或者使用 yarnyarn global add http-server\n\n2. 基本使用在需要提供静态服务的目录下，打开命令行工具，运行：\nhttp-server\n\n示例输出：\nStarting up http-server, serving ./Available on:  http://192.168.1.100:8080  http://127.0.0.1:8080 (lo)Hit CTRL-C to stop the server\n这表示服务器已经在 http://127.0.0.1:8080 (以及你的局域网 IP) 启动，端口号为 8080。在浏览器中访问这个地址，就会看到当前目录下的文件列表或 index.html 文件。\n3. 常用选项http-server 提供了许多命令行选项来定制其行为：\n\n-p &lt;port&gt; 或 --port &lt;port&gt;: 指定服务器端口。默认为 8080。http-server -p 3000\n-a &lt;address&gt; 或 --address &lt;address&gt;: 指定服务器监听的 IP 地址。默认为 0.0.0.0 (监听所有可用 IP)。http-server -a 127.0.0.1 # 只允许本地访问\n-s 或 --silent: 静默模式，不输出任何日志到控制台。\n-d &lt;seconds&gt; 或 --delay &lt;seconds&gt;: 访问文件时，人为延迟响应时间，用于模拟慢速网络。\n-i 或 --no-indexes: 禁用目录索引。如果目录下没有 index.html，将会返回 404 错误而不是文件列表。\n-c &lt;seconds&gt; 或 --cache &lt;seconds&gt;: 设置最长缓存时间（Cache-Control 头）。默认 3600 秒 (1小时)。设为 -1 禁用缓存。http-server -c -1 # 禁用缓存\n-o 或 --open: 服务器启动后自动在浏览器中打开指定的 URL。http-server -o /my-page.html # 默认打开 index.html\n-S 或 --ssl: 启用 HTTPS。需要提供 --cert 和 --key 选项。http-server -S --cert cert.pem --key key.pem\n-C &lt;file&gt; 或 --cors &lt;file&gt;: 启用 CORS。\n-P 或 --proxy: 代理模式，将所有未匹配到的请求代理到指定的 URL。http-server -P http://localhost:8081\n\n示例：在 3000 端口启动，并自动在浏览器中打开\nhttp-server -p 3000 -o\n\n4. 路由和 SPA 支持http-server 本身不提供复杂的路由功能，它是一个纯粹的静态文件服务器。对于单页应用 (SPA)，如果刷新页面或直接访问深层路由（如 /users/123），服务器会尝试查找对应的物理文件，导致 404 错误。\n为了支持 SPA，你可以使用其 --entry-file 选项，它会将所有未找到的请求重定向到你指定的入口文件（通常是 index.html）。\nhttp-server --entry-file index.html\n这样，不管访问 /users/123 还是 /about，都会返回 index.html，然后由前端路由库进行客户端路由。\n三、live-server：带实时重载的静态文件服务器live-server 是在 http-server 的基础上增加了实时重载 (Live Reload) 功能。每当你修改并保存文件时，浏览器会自动刷新，这极大提高了开发效率。\n1. 安装live-server 也通常作为全局工具安装。\nnpm install -g live-server# 或者使用 yarnyarn global add live-server\n\n2. 基本使用在需要提供静态服务的目录下，打开命令行工具，运行：\nlive-server\n\n示例输出：\nServing &#x27;.&#x27; at http://127.0.0.1:8080Opening &#x27;index.html&#x27;Hit CTRL-C to stop the server\n与 http-server 类似，这会在 8080 端口启动服务器并自动打开浏览器。不同的是，当你修改并保存项目中的 HTML、CSS 或 JS 文件时，浏览器会自动检测到变化并刷新页面。\n3. 常用选项live-server 的选项与 http-server 类似，并增加了一些实时重载相关的选项：\n\n--port=&lt;port&gt;: 指定服务器端口。默认为 8080。\n--host=&lt;address&gt;: 指定服务器监听的 IP 地址。默认为 0.0.0.0。live-server --port=3000 --host=127.0.0.1\n--open=&lt;path&gt;: 服务器启动后自动在浏览器中打开指定的 URL。默认为 index.html。live-server --open=/my-page.html\n--no-browser: 不自动打开浏览器。\n--no-css-inject: 禁用 CSS 注入（实时更新 CSS 而不刷新整个页面）。默认启用。\n--quiet: 不输出任何日志。\n--wait=&lt;seconds&gt;: 每当文件更改时，在刷新浏览器前等待指定秒数。\n--mount=&lt;route&gt;:&lt;path&gt;: 将某个路径挂载到指定的路由上。live-server --mount=/api:./data # 访问 /api 会去 ./data 目录找文件\n--entry-file=&lt;file&gt;: 指定入口文件。类似于 http-server 的 SPA 支持。live-server --entry-file=index.html\n--proxy=&lt;source&gt;:&lt;target&gt;: 代理请求。live-server --proxy=/api:http://localhost:8081\n--htpasswd=&lt;file&gt;: 使用 htpasswd 文件进行密码保护。\n\n示例：在 3000 端口启动，禁用 CSS 热更新，并指定 SPA 入口文件\nlive-server --port=3000 --no-css-inject --entry-file=index.html\n\n4. 监听文件变化live-server 默认会监听当前目录下的所有文件变化。你可以使用 .gitignore 文件来忽略某些文件或目录，使其不触发实时重载。\n四、选择 http-server 还是 live-server？\nhttp-server:\n优点: 纯粹、简单、无额外功能，启动速度可能略快一点点。\n适用场景: 只需要一个基本的 HTTP 服务器，F5 刷新不是问题，或者在 CI&#x2F;CD 环境中提供静态文件。\n\n\nlive-server:\n优点: 内置实时重载，极大提升开发效率。CSS 默认支持 HMR (热模块替换)，即只更新样式而不刷新页面。\n适用场景: 前端日常开发中的首选，无论是纯静态网站、学习示例，还是为复杂的框架应用提供辅助的静态文件服务。\n\n\n\n对于大多数前端开发者而言，live-server 由于其自动刷新的功能，通常是更优的选择，因为它能够显著提升你的开发体验。\n五、在项目中配置脚本虽然全局安装很方便，但在团队协作或项目依赖管理中，更推荐将这些工具作为项目的开发依赖安装，并配置到 package.json 的 scripts 中。\n\n安装为开发依赖:npm install http-server --save-devnpm install live-server --save-dev\n配置 package.json:&#123;  &quot;name&quot;: &quot;my-static-project&quot;,  &quot;version&quot;: &quot;1.0.0&quot;,  &quot;description&quot;: &quot;A simple static website&quot;,  &quot;scripts&quot;: &#123;    &quot;start&quot;: &quot;live-server --port=8080 --open=/index.html&quot;,    &quot;serve:basic&quot;: &quot;http-server --port=8000&quot;  &#125;,  &quot;keywords&quot;: [],  &quot;author&quot;: &quot;&quot;,  &quot;license&quot;: &quot;MIT&quot;,  &quot;devDependencies&quot;: &#123;    &quot;http-server&quot;: &quot;^14.1.1&quot;,    &quot;live-server&quot;: &quot;^1.2.2&quot;  &#125;&#125;\n运行:npm start         # 启动 live-servernpm run serve:basic # 启动 http-server\n这样，其他团队成员在克隆项目后，只需运行 npm install 即可拥有所有必要的开发工具，而无需全局安装。\n\n六、总结http-server 和 live-server 都是 Node.js 生态中优秀且常用的本地静态服务器工具。\n\nhttp-server 提供了一个纯粹、高效的 HTTP 服务，适合简单的文件共享和不需要实时刷新的场景。\nlive-server 则在 http-server 的基础上增加了革命性的实时重载功能，是前端开发工作流中不可或缺的利器。\n\n两者的使用都极其简单，通过几行命令即可启动，并提供了丰富的选项来满足不同的开发需求。掌握它们，无疑会大大提升你的前端开发效率和体验。\n","categories":["开发工具","Server"],"tags":["开发工具","2025","Node.js","Server"]},{"title":"Python 打包工具 uv 详解：下一代包管理器与构建器","url":"/2025/2025-05-12_Python%20%E6%89%93%E5%8C%85%E5%B7%A5%E5%85%B7%20uv%20%E8%AF%A6%E8%A7%A3%EF%BC%9A%E4%B8%8B%E4%B8%80%E4%BB%A3%E5%8C%85%E7%AE%A1%E7%90%86%E5%99%A8%E4%B8%8E%E6%9E%84%E5%BB%BA%E5%99%A8/","content":"\nuv (通常读作 “yoo-vee”) 是由 Astral (Rye, Ruff, Linter 等工具的创造者) 开源的一个超快的 Python 包管理器和包安装器，旨在成为 pip 和 venv 的直接替代品。它使用 Rust 编写，专注于速度、可靠性和稳定性，正在迅速改变 Python 包管理的格局。\n\n传统的 Python 包管理工具如 pip 和 venv 虽然功能完善，但在大规模项目或频繁操作时，其性能瓶颈日益凸显。例如，复杂的依赖解析可能耗时很久，创建虚拟环境也并非瞬间完成。uv 的出现正是为了解决这些痛点，它将速度提升了几个数量级，并且提供了更加一致和可靠的语义。\n\n\n一、uv 简介与核心优势uv 的诞生是为了提供一个现代、高效的 Python 包管理解决方案，它集成了 包安装器、解析器和虚拟环境管理器 的功能。\nuv 的核心优势：\n\n极速性能：这是 uv 最突出的特点。由于使用 Rust 编写，并采用了先进的图算法进行依赖解析，uv 在安装、更新、解析依赖等操作上比 pip 和 venv 快 10-100 倍。\n单一工具链：uv 不仅是一个安装器，还集成了虚拟环境创建（替代 python -m venv）和 requirements.txt &#x2F; pyproject.toml 等文件的解析与管理。未来甚至有望整合版本管理功能。\n兼容性强：uv 旨在与现有的 pip 生态系统完全兼容，支持 requirements.txt、pyproject.toml (PyPA Standards) 和 setup.py 等标准。\n离线安装：第一次安装后，uv 会缓存包，后续操作可以在离线状态下进行。\n可靠性高：uv 内部的依赖解析器能够更好地处理复杂的依赖图，减少因依赖冲突导致的安装失败。\n更强的安全性：采用 trusty 的离线模式（未来功能），并且 Rust 的内存安全特性也能减少潜在的 bug。\n\n二、安装 uvuv 的安装非常简单。\n1. 使用 pip (推荐)最常见的方式是通过 pipx (如果已安装) 或 pip 将其安装为全局工具。\n# 推荐使用 pipx，将 uv 安装到独立环境，不污染主 Python 环境pipx install uv# 如果没有 pipx，也可以直接用 pippip install uv\n\n2. 通过 brew (macOS)brew install uv\n\n3. 下载预编译二进制文件访问 uv 的 GitHub Release 页面 (https://github.com/astral-sh/uv/releases) 下载对应操作系统的预编译二进制文件，并将其添加到系统 PATH。\n4. 验证安装安装完成后，运行以下命令验证：\nuv --version\n\n三、uv 的基本用法与功能uv 的命令设计旨在模仿 pip 和 venv 的核心功能，并提供更简洁的接口。\n3.1 虚拟环境管理 (替代 python -m venv)uv 可以直接创建和激活虚拟环境。\n1. 创建虚拟环境uv venv # 在当前目录创建 .venvuv venv my_env # 在当前目录创建名为 my_env 的虚拟环境\n这会创建一个新的 Python 虚拟环境，并默认安装 pip 和 setuptools。\n2. 激活虚拟环境创建成功后，会提示你如何激活。\n# macOS/Linuxsource .venv/bin/activate# Windows (Command Prompt).venv\\Scripts\\activate.bat# Windows (PowerShell).venv\\Scripts\\Activate.ps1\n\n3. 指定 Python 解释器版本你可以指定用于创建虚拟环境的 Python 解释器。\nuv venv --python python3.10 # 使用系统中的 python3.10uv venv --python /usr/bin/python3.11 # 使用指定路径的解释器\n\n3.2 包安装与管理 (替代 pip install)uv 的安装命令与 pip 非常相似，但速度快得多。\n1. 安装单个包uv pip install requests\n\n2. 安装多个包uv pip install numpy pandas matplotlib\n\n3. 安装指定版本包uv pip install &quot;requests==2.28.1&quot;\n\n4. 从 requirements.txt 文件安装uv pip install -r requirements.txt\n\n5. 更新包uv pip install --upgrade requests # 更新 requestsuv pip install --upgrade -r requirements.txt # 更新 requirements.txt 中的所有包uv pip install --upgrade-all # 更新所有已安装的包\n\n6. 卸载包uv pip uninstall requestsuv pip uninstall -r requirements.txt\n\n7. 查看已安装的包uv pip list\n\n8. 移除所有包 (清空虚拟环境)uv pip uninstall --all\n\n3.3 发布依赖文件 (替代 pip freeze)uv 可以生成 requirements.txt 文件，但它还提供了更强大的 uv pip freeze 和 uv pip compile 命令。\n1. uv pip freeze (生成当前环境的包列表)uv pip freeze &gt; requirements.txt\n这与 pip freeze 类似，列出当前虚拟环境中所有已安装的包及其精确版本。\n2. uv pip compile (生成锁文件，替代 pip-tools)uv pip compile 是 uv 中一个非常强大的功能，它类似于 pip-compile (来自 pip-tools 项目)。它可以根据你的高级依赖 (pyproject.toml 或 requirements.in) 生成一个包含所有确切依赖及版本的锁文件 (通常是 requirements.txt 或 requirements.lock)。\n示例：pyproject.toml\n# pyproject.toml[project]name = &quot;my-project&quot;version = &quot;0.1.0&quot;dependencies = [    &quot;requests&quot;,    &quot;fastapi&quot;,][project.optional-dependencies]dev = [    &quot;pytest&quot;,    &quot;uvicorn&quot;,]\n\n编译生成锁文件：\nuv pip compile pyproject.toml -o requirements.txtuv pip compile pyproject.toml --extras dev -o requirements-dev.txt\n这会解析所有依赖，包括传递性依赖，并生成一个包含精确版本号的 requirements.txt 文件。然后，你可以使用 uv pip install -r requirements.txt 来安装这些锁定的依赖，确保所有环境的依赖版本一致。\n3.4 缓存管理uv 有强大的本地缓存，使得离线安装和重复安装变得极快。\n1. 查看缓存信息uv cache dir # 查看缓存目录uv cache clean # 清理缓存\n\n四、与 pip 和 venv 的对比\n\n\n特性\npip &#x2F; venv 组合\nuv\n\n\n\n速度\n慢，特别是复杂的依赖解析\n极速，10-100倍提升\n\n\n语言\nPython\nRust\n\n\n依赖解析\n较慢，可能遇到循环依赖等问题\n使用先进算法，快速可靠，降低冲突\n\n\n虚拟环境\n需 python -m venv 命令，再用 pip 安装\nuv venv 一步到位，更简洁\n\n\n锁文件\n需 pip-tools 等额外工具\nuv pip compile 内置提供，功能强大\n\n\n缓存\n缓存一般，不强求离线工作\n强大的离线缓存能力\n\n\n生态兼容\nPython 包管理标准\n旨在完全兼容 pip 生态，支持所有标准格式\n\n\n安装方式\nPython 包\n预编译二进制，或 pip &#x2F; pipx\n\n\n未来方向\n稳定，功能迭代缓慢\n快速发展中，有望整合更多工具链功能\n\n\n五、为什么 uv 如此之快？\nRust 语言的性能：Rust 提供了接近 C&#x2F;C++ 的运行时性能，同时兼顾内存安全，这为 uv 的高速执行奠定了基础。\n并发处理：uv 充分利用现代 CPU 的多核优势，进行并发的 HTTP 请求下载包，大大缩短了网络等待时间。\n先进的依赖解析算法：uv 借鉴了 Rye 和 Cargo (Rust 的包管理器) 的经验，采用了更高效的依赖解析算法 (SAT Solver)，能够更快地处理复杂的依赖图。\n高效的网络和文件 I&#x2F;O：Rust 的异步 I&#x2F;O 库能够更高效地处理文件读写和网络请求。\n不需 C 编译：Python 包的安装有时需要 C 编译器来编译某些依赖项。uv 在解析和下载阶段并不会触发 C 编译，仅在包最终安装时才会用到，这使得下载和缓存阶段更快。\n\n六、未来展望与生态影响uv 仍在快速发展中，它被视为下一代 Python 包管理器。它与 Astral 的其他工具（如 Ruff）共同构成了一个高效、现代的 Python 工具链愿景。\n潜在影响：\n\n提升开发者效率：极大地缩短了依赖安装和环境设置的时间，从而让开发者更专注于代码本身。\n降低 CI&#x2F;CD 成本：在持续集成&#x2F;持续部署 (CI&#x2F;CD) 环境中，uv 可以显著减少构建时间，从而节省时间和资源。\n推动 Python 生态发展：通过提供更快的工具，鼓励开发者使用更规范的依赖管理方式 (如 pyproject.toml 和锁文件)。\n挑战主流工具地位：uv 有潜力成为 pip 和 venv 的事实标准替代品，甚至可能影响 Poetry 和 Rye 等更高级包管理器的定位。\n\n七、总结uv 是 Python 包管理领域的一个革命性工具，它以惊人的速度、强大的功能和对现有生态的兼容性，为 Python 开发者带来了前所未有的体验。\n如果你受够了 pip 漫长的等待，或者希望在 CI&#x2F;CD 流水线中显著提升效率，那么 uv 绝对值得你立即尝试。它简洁的命令、无缝的集成以及强大的性能，正在重新定义 Python 包管理的标准。\n拥抱 uv，体验飞一般的 Python 开发流程吧！\n","categories":["Python"],"tags":["Python","项目构建","2025","包管理"]},{"title":"LazyGit使用解析：你的Git命令行效率神器","url":"/2025/2025-06-01_LazyGit%E4%BD%BF%E7%94%A8%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BD%A0%E7%9A%84Git%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%95%88%E7%8E%87%E7%A5%9E%E5%99%A8/","content":"\n本文将带你深入了解 LazyGit，一个简单直观的终端 UI Git 客户端。如果你厌倦了反复输入 Git 命令，又觉得 GUI 客户端不够灵活，那么 LazyGit 可能会成为你的新宠。它将终端的强大与 GUI 的便捷完美结合，让你的 Git 工作流变得前所未有的高效和愉悦。\n\n对于开发者而言，Git 无疑是日常工作中不可或缺的工具。然而，即使是最熟练的 Git 用户，也可能被一些重复、繁琐的命令行操作所困扰，例如 git add ., git status, git commit -m &quot;...&quot;, git log --oneline 等等。虽然有各种图形化 Git 客户端，但它们往往意味着脱离终端环境，或多或少牺牲了速度和灵活性。LazyGit 正是为了解决这一痛点而生的——它提供了一个文本用户界面 (TUI)，让你在终端中就能以图形化的方式快速、直观地执行 Git 操作，大幅提升工作效率。\n\n\n一、为什么选择 LazyGit？LazyGit 并不是简单的 Git 命令别名集合，它提供了一个交互式的视图，将 git status, git branch, git log, git diff 等信息在一个屏幕上统一展示，并允许你用最少的按键进行操作。它的核心吸引力在于：\n\n统一视图：在一个终端屏幕上同时查看工作区文件、暂存区、提交历史、分支列表等信息，无需频繁切换命令。\n效率极高：大量操作通过单键或组合键完成，减少了命令输入和上下文切换。例如，按 s 键即可暂存当前文件，按 c 键即可提交。\n直观操作：分支切换、rebase、cherry-pick 等复杂操作通过光标移动和确认即可完成，减少了出错的可能。\n不脱离终端：保持在终端环境中，与你的编辑器、其他 CLI 工具无缝衔接。\nGit 功能完善：覆盖了日常 Git 工作流的绝大部分功能，包括 diff、commit、checkout、branch、merge、rebase、stash、push&#x2F;pull 等。\n可定制性：支持自定义快捷键和主题。\n\n如果你追求命令行效率，但又希望拥有图形化工具的直观性，LazyGit 绝对值得一试。\n二、安装 LazyGitLazyGit 支持 macOS, Linux, Windows 等多个平台。以下是常用平台的安装方式：\n2.1 macOS (使用 Homebrew)brew install lazygit\n\n2.2 Linux (各种包管理器或手动安装)使用 Go (推荐):\ngo install github.com/jesseduffield/lazygit@latest\n请确保你的 GOPATH/bin 路径已添加到 $PATH 环境变量中。\n使用 apt (Debian&#x2F;Ubuntu):\nsudo add-apt-repository ppa:lazygit-team/releasesudo apt-get updatesudo apt-get install lazygit\n\n使用 snap:\nsudo snap install lazygit\n\n2.3 Windows (使用 Scoop 或 Chocolatey)使用 Scoop:\nscoop install lazygit\n\n使用 Chocolatey:\nchoco install lazygit\n\n安装完成后，在任意 Git 仓库目录下，只需在终端输入 lazygit 即可启动。\n三、LazyGit 界面概览启动 LazyGit 后，你将看到一个分为多个面板的交互式界面：\n+-----------+-----------+---------+---------+|   Files   |   Commits |  Branches | Remote  |+-----------+-----------+---------+---------+|           |           |         |         | (主面板/内容面板)|           |           |         |         ||           |           |         |         |+-----------+-----------+----------+--------+|    Status Message &amp; Help Tips             | (底部状态栏/快捷键提示)+-------------------------------------------+\n\n核心面板：\n\nFiles (文件)：显示工作区中所有已修改、已暂存、未跟踪的文件。\nCommits (提交)：显示当前分支的提交历史。\nBranches (分支)：显示本地和远程分支列表。\nRemote (远程)：显示远程仓库信息。\n\n最底部是状态栏，会显示当前操作的上下文信息和快捷键提示。按 ? 键可以随时打开完整的帮助菜单，查看所有快捷键。\n四、常用操作详解以下是 LazyGit 中最常用的一些 Git 操作及其快捷键。\n4.1 通用操作\nq：退出 LazyGit。\n?：打开帮助菜单 (查看所有快捷键)。\n鼠标左键点击：激活面板并选择项。\nTab &#x2F; Shift+Tab：切换面板。\n↑ &#x2F; ↓：在当前面板中上下移动光标。\nspace：在文件面板中，暂存&#x2F;取消暂存文件或 Hunk。\nd：删除 (文件、分支、提交等，会提示确认)。\n\n4.2 文件面板 (Files)此面板用于管理工作区和暂存区文件。\n\na：暂存所有文件。\nu：取消暂存所有文件。\nspace (选择文件后)：暂存&#x2F;取消暂存单个文件或 Hunk。\ns (选择文件后)：暂存文件。\nr (选择文件后)：撤销文件更改 (discard changes)。\nc：提交暂存区文件。(会打开编辑器让你输入提交信息)\nC：修改最后一次提交 (amend previous commit)。\nm (选择文件后)：移动&#x2F;重命名文件。\nv (选择文件后)：创建新的文件 Hunk (选择部分内容进行暂存)。\n\nHunk 操作 (文件 diff 视图中):\n当你在文件面板选择一个已修改的文件并按 enter 键，会进入文件内容的 diff 视图。\n\nspace：暂存&#x2F;取消暂存当前的 Hunk。\ns：暂存当前的 Hunk。\nd：撤销当前的 Hunk。\n&lt; &#x2F; &gt;：在 Hunk 之间切换。\ne：在你的默认编辑器中打开文件。\n\n4.3 提交面板 (Commits)此面板用于查看和操作提交历史。\n\nc：新的提交 (如果暂存区有文件，会打开编辑器输入信息)。\nC：修改上一个提交 (amend previous commit)。\ne (选择提交后)：编辑提交信息 (reword)。\ns (选择提交后)：压缩提交 (squash - 将当前提交与上一个提交合并)。\nr (选择提交后)：重命名提交 (reword - 与 e 相同)。\np (选择提交后)：挑选提交 (cherry-pick - 将当前提交应用到 HEAD)。\ng (选择提交后)：Reset HEAD 到此提交 (Hard&#x2F;Mixed&#x2F;Soft reset)。\nb (选择提交后)：从该提交创建新分支。\nf (选择提交后)：快速前进到此提交 (fast-forward)。\nShift+R (选择提交后)：交互式 Rebase (interactive rebase) - 这是一个非常强大的功能，可以对多个提交进行批量操作 (reword, squash, edit, fixup, drop)。\n\n4.4 分支面板 (Branches)此面板用于管理本地和远程分支。\n\nn：创建新分支。\nspace (选择分支后)：Checkout (切换) 到此分支。\nm (选择分支后)：合并当前分支到 HEAD。\nd (选择分支后)：删除分支 (会提示确认，可选择强制删除)。\nR (选择分支后)：重命名分支。\n&lt; &#x2F; &gt;：切换到上一个&#x2F;下一个分支。\np (选择分支后)：推送到远程 (push - 如果远程没有此分支，会提示创建上游分支)。\nP (选择远程分支后)：拉取远程分支 (pull - 与 git pull 相似)。\nf (选择远程分支后)：Rebase 当前分支到此远程分支。\n\n4.5 远程面板 (Remotes)此面板用于管理远程仓库。\n\nn：添加新的远程仓库。\np (选择远程仓库后)：推送到此远程 (如果未设置上游，会询问分支)。\nf (选择远程仓库后)：拉取此远程。\n\n五、Git Flow 与 LazyGitLazyGit 极其适合遵循 Git Flow 或 Trunk-Based Development 等开发流程。例如：\n\n创建 Feature 分支：在 Branches 面板按 n。\n开发与提交：在 Files 面板 space 暂存文件，c 提交。\nRebase 远程主干：在 Branches 面板选择 develop 或 main 分支，按 p (pull)，然后切换回你的 feature 分支，在 Commits 面板选择 develop 或 main 最新的提交，按 Shift+R，进入交互式 Rebase 模式。\n合并 PR 前 Squash 提交：在 Commits 面板选择需要合并的提交，按 s (squash) 合并为一个整洁的提交。\nCherry-Pick：在 Commits 面板选择一个提交，按 p 即可将其应用到当前分支。\n\n所有这些复杂操作，在 LazyGit 中都以直观的界面和少量按键即可完成，大大降低了学习成本和操作心智负担。\n六、高级功能与定制化6.1 交互式 Rebase在 Commits 面板选择一个提交，按 Shift+R 即可进入交互式 Rebase 模式。这会打开一个新窗口，列出从该提交到 HEAD 的所有提交。你可以通过快捷键对这些提交进行：\n\np：pick (使用该提交)。\nr：reword (修改提交信息)。\ne：edit (停止在当前提交，允许修改文件后 git add 和 git commit --amend)。\ns：squash (将当前提交与上一个提交合并)。\nf：fixup (将当前提交与上一个提交合并，并废弃当前提交的信息)。\nd：drop (删除当前提交)。\n\n完成操作后按 q 退出 Rebase 界面，然后按 m 确认 Rebase。\n6.2 Stash (储藏)在 Files 面板按 w 可以将当前工作区的未暂存和已暂存的修改储藏起来。\n\ng：显示 Stash 列表。\n在 Stash 列表中：\nspace：应用 Stash。\nd：删除 Stash。\nP：弹出 Stash (应用并删除)。\n\n\n\n6.3 自定义快捷键和主题LazyGit 的配置文件通常位于 ~/.config/lazygit/config.yml (Linux&#x2F;macOS) 或 %APPDATA%\\lazygit\\config.yml (Windows)。\n你可以编辑此文件来自定义快捷键、颜色主题、面板布局等。\n示例 (config.yml):\n# ~/.config/lazygit/config.ymlgui:  theme:    activeBorderColor:      - green      - bold    selectedLineBgColor:      - blue  # 更多主题配置...keybindings:  files:    # 例如：将暂存单个文件从 &#x27;s&#x27; 改为 &#x27;S&#x27;    StageFile: &#x27;S&#x27;   commits:    # 例如：将开始交互式Rebase从 &#x27;R&#x27; 改为 &#x27;i&#x27;    InteractiveRebase: &#x27;i&#x27;   # 更多快捷键配置...# 其他配置：例如外部编辑器os:  edit: &#x27;code -w &#123;&#123;filename&#125;&#125;&#x27; # 使用 VS Code 作为默认编辑器\n修改后，保存文件并重启 LazyGit 即可生效。\n七、与 Neovim &#x2F; VS Code 等编辑器的集成LazyGit 的强大在于它让你可以停留在一个终端会话中。许多用户会将其与终端编辑器（如 Vim&#x2F;Neovim、Emacs）结合使用。\n\n你可以在 LazyGit 中选择文件并按 e 键，它将会在你配置的默认编辑器中打开文件。\n例如，在 config.yml 中设置 os.edit: &#39;nvim &#123;&#123;filename&#125;&#125;&#39;（使用 Neovim）。\n\n\n在 Neovim 中，可以安装插件包装 LazyGit，例如 nvim-lazygit.lua，让你可以在 Neovim 内部直接调用 LazyGit。\n对于 VS Code 用户，虽然是 GUI，但一些终端插件或配置也能让你快速启动 LazyGit。\n\n八、总结LazyGit 是一款独特且极其高效的 Git 客户端。它通过创新的 TUI 模式，在保留命令行速度和灵活性的同时，提供了媲美甚至超越许多 GUI 客户端的直观性和易用性。无论你是 Git 新手还是经验丰富的老兵，LazyGit 都能显著提升你的 Git 工作流体验。告别繁琐的 git status、git add -p 和复杂的 rebase 命令，只需几个按键，就能掌控你的代码仓库。\n如果你还没有尝试过 LazyGit，现在就是时候了！投入几分钟学习它的基本快捷键，你将收获长期的效率提升。它将成为你终端中不可或缺的 Git 伴侣。\n","categories":["开发工具","Git"],"tags":["开发工具","Git","2025","LazyGit"]},{"title":"GitHub Actions 详解：自动化你的开发工作流","url":"/2025/2025-07-25_GitHub%20Actions%20%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%87%AA%E5%8A%A8%E5%8C%96%E4%BD%A0%E7%9A%84%E5%BC%80%E5%8F%91%E5%B7%A5%E4%BD%9C%E6%B5%81/","content":"\nGitHub Actions 是 GitHub 提供的持续集成 (CI) 和持续部署 (CD) 服务，它可以帮助开发者自动化软件开发生命周期中的各种任务，例如代码构建、测试、部署，甚至代码审查和发布管理。通过 GitHub Actions，你可以在代码仓库中定义一系列自动化工作流，让你的开发过程更加高效、可靠。\n\n“好的工具能让开发者专注于创造，而不是重复劳动。GitHub Actions 就是这样的工具。”\n\n\n一、什么是 GitHub Actions？GitHub Actions 是一种事件驱动的自动化平台。这意味着当 GitHub 仓库中发生特定事件（例如 push 代码、pull_request 创建、issue 开启等）时，它可以自动触发预定义的工作流（Workflow）执行。\n核心优势：\n\n与 GitHub 深度集成：直接在 GitHub 仓库中管理 CI&#x2F;CD，无需外部工具。\n事件驱动：灵活配置触发事件，覆盖开发流程的各个环节。\n丰富生态：拥有庞大的 Actions 市场，提供各种预构建的自动化任务块。\n云原生：在云端虚拟机上运行，无需维护自己的 CI 服务器。\n免费额度：为开源项目和个人用户提供免费的构建时间。\n\n二、核心概念在深入使用 GitHub Actions 之前，理解以下核心概念至关重要：\n\nWorkflow (工作流)\n\n一个工作流是一个可配置的自动化过程。它由一个或多个作业（Job）组成。\n工作流使用 YAML 文件定义，存储在 .github/workflows/ 目录下。\n每个工作流文件代表一个独立的自动化流程，例如一个用于测试，一个用于部署。\n\n\nEvent (事件)\n\n触发工作流运行的特定活动。\n常见的事件包括 push（代码推送到仓库）、pull_request（PR 被创建、打开、同步等）、schedule（定时任务）、workflow_dispatch（手动触发）、issue_comment 等。\n你可以在工作流文件中指定一个或多个事件来触发它。\n\n\nJob (作业)\n\n一个作业是在一个新的虚拟机环境中执行的一系列步骤（Step）。\n一个工作流可以包含多个作业。这些作业可以并行运行，也可以按顺序依赖关系运行。\n每个作业都独立运行，拥有自己的虚拟机环境。\n\n\nStep (步骤)\n\n作业中的单个任务单元。\n一个步骤可以是一个 run 命令（执行 shell 脚本），也可以是一个 uses 操作（使用一个预定义的 Action）。\n步骤的执行是顺序的。\n\n\nAction (操作)\n\nGitHub Actions 平台中可重用的代码单元，是实现特定任务的基础组件。\n一个 Action 可以是一个 Shell 脚本、一个 Docker 容器，或者一个 JavaScript 程序。\nAction 通常由社区或 GitHub 官方提供，可以在 GitHub Marketplace 中找到。\n例如：actions/checkout@v4 用于拉取仓库代码，actions/setup-node@v4 用于设置 Node.js 环境。\n\n\nRunner (运行器)\n\n执行工作流的服务器。\nGitHub 提供 GitHub-hosted runners (托管运行器)，支持 Linux、Windows、macOS 等操作系统环境。\n你也可以搭建 Self-hosted runners (自托管运行器)，在自己的服务器上运行工作流，适用于特殊环境或私有网络需求。\n\n\n\n三、工作流文件 (.yml) 结构详解工作流文件是 GitHub Actions 的核心配置文件，采用 YAML 格式。\n# .github/workflows/ci.yml# 1. workflow 名称name: CI Build and Test# 2. 触发事件on:  # 在 push 到 main 分支时触发  push:    branches:      - main  # 在 pull request 目标为 main 分支时触发  pull_request:    branches:      - main  # 允许手动触发  workflow_dispatch:# 3. 定义一个或多个作业 (Jobs)jobs:  # 第一个作业：build  build:    # 运行此作业的操作系统环境    runs-on: ubuntu-latest      # 步骤 (Steps) 列表    steps:      # 步骤 1: 打印一条消息      - name: Say Hi        run: echo &quot;Hello, GitHub Actions!&quot;      # 步骤 2: 拉取代码 (使用官方 action)      - name: Checkout Code        uses: actions/checkout@v4 # 使用 actions/checkout@v4 这个 Action      # 步骤 3: 设置 Node.js 环境 (使用官方 action)      - name: Setup Node.js        uses: actions/setup-node@v4        with:          node-version: &#x27;20&#x27; # 指定 Node.js 版本      # 步骤 4: 安装依赖      - name: Install dependencies        run: npm install      # 步骤 5: 运行构建      - name: Run build        run: npm run build  # 第二个作业：test  test:    # 这个作业依赖于 build 作业，只有 build 成功后才运行    needs: build    runs-on: ubuntu-latest      steps:      - name: Checkout Code        uses: actions/checkout@v4      - name: Setup Node.js        uses: actions/setup-node@v4        with:          node-version: &#x27;20&#x27;      - name: Install dependencies        run: npm install      # 运行测试      - name: Run tests        run: npm test  # 第三个作业：deploy (仅在 push 到 main 分支时，且 test 成功后才运行)  deploy:    if: github.event_name == &#x27;push&#x27; &amp;&amp; github.ref == &#x27;refs/heads/main&#x27;    needs: test # 依赖 test 作业    runs-on: ubuntu-latest      steps:      - name: Checkout Code        uses: actions/checkout@v4      # ... 部署相关的步骤，例如登录云平台、上传文件等      - name: Deploy to Production        run: echo &quot;Deploying to production...&quot;\n\n关键配置项详解：\nname：工作流的名称，显示在 GitHub UI 中。\non：定义触发工作流的事件。\npush: 当代码 push 到指定分支时触发。\nbranches: 指定分支列表。\npaths: 指定文件路径，只有这些文件发生变化才触发。\ntags: 指定触发的 Git 标签。\n\n\npull_request: 当 PR 发生变化时触发。\nschedule: 使用 cron 语法定义定时触发。\nworkflow_dispatch: 允许从 GitHub UI 手动触发。\nrepository_dispatch: 允许从外部 webhook 触发。\n\n\njobs：工作流中的一系列作业。\njob_id：每个作业的唯一标识符（如 build, test, deploy）。\nruns-on：指定运行作业的执行环境，例如 ubuntu-latest, windows-latest, macos-latest 或自定义的 self-hosted 标签。\nsteps：作业中的一系列步骤，按顺序执行。\nname：步骤的名称。\nrun：执行 shell 命令或脚本。\nuses：使用一个 Action。格式通常是 owner/repo@ref (如 actions/checkout@v4)。你可以传递 with 参数给 Action。\nenv：在当前步骤中设置环境变量。\nwith：向 Action 或 run 命令传递输入参数。\nif：条件表达式，用于决定是否执行该步骤。\n\n\nneeds：指定当前作业依赖的其他作业的 job_id。依赖的作业会先运行，并且成功后才会运行当前作业。\ntimeout-minutes: 作业超时时间，单位分钟。\nstrategy: 定义矩阵策略，用于并行运行多个变体配置的作业（如多个 Node 版本或操作系统）。\nenv: 在整个作业范围内设置环境变量。\n\n\nenv：在整个工作流范围内设置环境变量。\ndefaults: 为工作流或作业中的所有 run 命令设置默认的 shell 或工作目录。\n\n四、事件类型与表达式1. 常见事件\npush: 代码推送到仓库。\npull_request: PR 的各种活动（opened, synchronize, closed, reopened）。\nschedule: 定时任务，使用 cron 语法（0 0 * * * 表示每天午夜）。\nworkflow_dispatch: 手动触发，可以在 UI 界面输入参数。\nissue_comment: 当 issue 收到评论时触发。\nrelease: 发布新的 release 时触发。\n\n2. 条件表达式 (if)if 关键字允许你基于特定条件来决定是否执行某个 Job 或 Step。它可以使用 GitHub Contexts 来获取工作流运行时的各种信息。\njobs:  conditional_job:    runs-on: ubuntu-latest    if: github.event_name == &#x27;push&#x27; &amp;&amp; github.ref == &#x27;refs/heads/main&#x27; # 只有 push 到 main 分支时才运行    steps:      - run: echo &quot;This runs only on main branch pushes.&quot;  another_conditional_job:    runs-on: ubuntu-latest    steps:      - name: Conditional Step        if: success() # 只有前一个步骤成功才运行        run: echo &quot;Previous step was successful.&quot;\n\n常用的上下文 (Contexts)：\n\ngithub: 包含仓库信息、触发事件、提交信息等。\ngithub.event_name, github.ref, github.sha, github.actor\n\n\nenv: 环境变量。\njob: 当前作业的信息。\nsteps: 步骤的输出信息。\nrunner: 运行器信息。\nsecrets: 存储的敏感信息。\n\n五、Actions 市场与自定义 Actions1. Actions 市场 (GitHub Marketplace)GitHub Actions 市场是一个巨大的宝库，你可以在其中找到各种预构建的 Action，用于：\n\n代码仓库操作 (checkout, upload artifact)\n环境设置 (setup-node, setup-python, setup-go, setup-java)\n构建工具 (npm, yarn, gradle, maven)\n测试工具 (jest, cypress)\n通知 (slack, teams)\n部署 (to AWS, Azure, GCP, Heroku, Netlify)\n代码扫描、安全检查等\n\n使用 Action 非常简单，只需在 uses 关键字后指定其路径和版本。\n- name: Upload coverage reports to Codecov  uses: codecov/codecov-action@v4  with:    token: $&#123;&#123; secrets.CODECOV_TOKEN &#125;&#125; # 使用 Secrets 传递敏感信息    flags: unittest # optional\n\n2. 自定义 Actions如果你在市场上找不到满足需求的 Action，或者想要封装自己的逻辑，可以编写自定义 Actions。自定义 Actions 可以是：\n\nJavaScript Actions：用 JavaScript 编写，推荐用于复杂逻辑。\nDocker Container Actions：用 Docker 容器封装环境和逻辑。\nComposite Actions: 将多个 run 步骤和 uses 步骤组合成一个可复用的 Action。\n\n六、Secrets (秘密)在 CI&#x2F;CD 流程中，经常需要使用敏感信息，如 API 密钥、数据库凭证等。GitHub Actions 提供了 Secrets 机制来安全地存储和使用这些信息。\n\n存储位置：在 GitHub 仓库的 Settings -&gt; Secrets and variables -&gt; Actions 中配置。\n使用方式：通过 $&#123;&#123; secrets.SECRET_NAME &#125;&#125; 表达式在工作流中引用。\n安全性：Secrets 在日志中会被自动遮盖，不会明文显示。\n\n- name: Deploy to AWS  env:    AWS_ACCESS_KEY_ID: $&#123;&#123; secrets.AWS_ACCESS_KEY_ID &#125;&#125;    AWS_SECRET_ACCESS_KEY: $&#123;&#123; secrets.AWS_SECRET_ACCESS_KEY &#125;&#125;  run: |    aws s3 sync ./build s3://$&#123;&#123; secrets.S3_BUCKET_NAME &#125;&#125;\n\n七、神器：Artifacts (构件)Artifacts 允许你在不同的 Job 之间共享数据，例如：\n\n构建产物：在一个 Job 中构建的二进制文件、打包文件可以作为 Artifact 上传。\n测试报告：测试结果报告可以作为 Artifact 上传。\n\njobs:  build:    runs-on: ubuntu-latest    steps:      # ... 构建代码      - name: Upload build artifact        uses: actions/upload-artifact@v4        with:          name: my-app-bundle          path: ./dist # 将 dist 目录作为构件上传  deploy:    runs-on: ubuntu-latest    needs: build    steps:      - name: Download build artifact        uses: actions/download-artifact@v4        with:          name: my-app-bundle # 下载名为 my-app-bundle 的构件          path: ./deploy_tmp # 下载到 deploy_tmp 目录      - name: Deploy        run: ls -l ./deploy_tmp &amp;&amp; echo &quot;Now deploying...&quot;\n\n八、实践场景举例GitHub Actions 可以应用于广泛的开发场景：\n\n代码质量检查：每次 Push 代码时，自动运行 ESLint、Prettier、单元测试。\n自动化测试：PR 被创建或更新时，自动运行单元测试、集成测试、端到端测试。\n构建与打包：每次 Push 到 main 分支时，自动构建 Docker 镜像、打包前端应用。\n持续部署 (CD)：代码合并到 main 分支并通过所有测试后，自动部署到开发、测试或生产环境。\n发布管理：当创建新的 Git Tag 时，自动生成发布日志、创建 GitHub Release、发布到 NPM 或 Docker Hub。\n任务自动化：定时清理不活跃的 Issues、自动回复 PR 评论等。\n\n九、总结与展望GitHub Actions 提供了一个强大、灵活且与 GitHub 平台深度集成的自动化解决方案。通过 YAML 文件配置工作流，你可以轻松地将各种自动化任务集成到你的开发流程中。\n掌握 GitHub Actions 不仅能提升你的个人开发效率，也能帮助团队构建更健壮、更高效的 CI&#x2F;CD 管道。随着云原生技术和 DevOps 理念的普及，自动化工具的重要性日益增加，GitHub Actions 无疑是这个领域中的一颗璀璨明星。\n开始尝试编写你的第一个 .github/workflows/*.yml 文件吧，将你的重复性任务交给自动化，专注于更有创造性的编码工作！\n","categories":["开发工具","GitHub"],"tags":["CI/CD","2025","GitHub"]},{"title":"告别 goroutine 等待烦恼：Go 语言四种高效同步方法详解","url":"/2025/2025-08-11_Go%20%E8%AF%AD%E8%A8%80%E5%9B%9B%E7%A7%8D%E9%AB%98%E6%95%88%E5%90%8C%E6%AD%A5%E6%96%B9%E6%B3%95/","content":"\n本文由 简悦 SimpRead 转码， 原文地址 mp.weixin.qq.com\n\n大家好！在 Go 语言的世界里，goroutine 是并发编程的核心，但主 goroutine 常常需要等待其他 goroutine 完成任务后才能继续执行或退出程序。这是并发同步的常见需求。今天，我将为大家介绍 4 种在 Go 中等待多个 goroutine 的核心方法，从基础到高级，帮助你在不同场景下都能优雅地处理并发任务等待问题。\n\n\n一、sync.WaitGroup：最常用的并发任务协调员1.1 基础概念与工作原理sync.WaitGroup 是 Go 语言中最常用的并发同步工具，专为等待一组 goroutine 完成任务而设计。它通过一个计数器机制工作，特别适合主 goroutine 需要等待多个子 goroutine 的场景。\n想象一下，你是一个老师，需要等待所有学生完成作业才能放学。sync.WaitGroup 就像是一个点名器，记录需要等待的学生数量，每个学生完成作业后就会报告一声，直到所有学生都报告完毕，老师才能放学。\n1.2 代码示例与执行流程让我们通过一个简单的例子来理解它的工作原理：\npackage mainimport (    &quot;fmt&quot;    &quot;sync&quot;)func main() &#123;    var wg sync.WaitGroup    // 启动3个goroutine    for i := 1; i &lt;= 3; i++ &#123;        wg.Add(1) // 增加计数器，表示有一个goroutine需要等待        gofunc(id int) &#123;            defer wg.Done() // 任务完成后，计数器减1            fmt.Printf(&quot;Goroutine %d is running\\n&quot;, id)        &#125;(i)    &#125;    wg.Wait() // 主goroutine等待所有goroutine完成    fmt.Println(&quot;All goroutines finished&quot;)&#125;\n\n可能的输出（顺序可能不同）：\nGoroutine 1 is runningGoroutine 2 is runningGoroutine 3 is runningAll goroutines finished\n\nsync.WaitGroup 的工作原理：\n\nwg.Add(n)：增加计数器，表示有 n 个 goroutine 需要等待\n\nwg.Done()：通常在 defer 中调用，任务完成后计数器减 1\n\nwg.Wait()：阻塞主 goroutine，直到计数器变为 0\n\n\n1.3 使用优势与局限性优势：\n\n简单易用，适合固定数量的 goroutine\n\n不需要额外的 channel，性能开销低\n\n是 Go 社区中最常用的并发同步工具\n\n\n局限性：\n\n不支持错误处理\n\n不支持任务取消\n\n无法动态调整等待的 goroutine 数量\n\n\n二、Channel：灵活的信号传递机制\n2.1 基本概念与实现思路当需要更灵活的控制，或者需要传递任务结果时，使用 channel 来等待多个 goroutine 是一个不错的选择。通过 channel 传递信号，主 goroutine 可以等待所有其他 goroutine 发送完成信号。\n想象一下，每个 goroutine 完成任务后会向一个 “完成队列” 发送一个信号，主 goroutine 则从这个队列中收集所有信号，直到收到足够数量的信号才继续执行。\n2.2 代码示例与执行流程让我们看看如何用 channel 实现等待多个 goroutine：\npackage mainimport&quot;fmt&quot;func main() &#123; done := make(chanstruct&#123;&#125;) // 创建一个无缓冲channel，用于发送完成信号 numGoroutines := 3for i := 1; i &lt;= numGoroutines; i++ &#123;gofunc(id int) &#123;   fmt.Printf(&quot;Goroutine %d is running\\n&quot;, id)   done &lt;- struct&#123;&#125;&#123;&#125; // 任务完成后发送一个信号  &#125;(i) &#125;// 等待所有goroutine完成for i := 0; i &lt; numGoroutines; i++ &#123;  &lt;-done // 接收完成信号 &#125; fmt.Println(&quot;All goroutines finished&quot;)&#125;\n\n可能的输出（顺序可能不同）：\nGoroutine 1 is runningGoroutine 2 is runningGoroutine 3 is runningAll goroutines finished\n\nchannel 方法的工作原理：\n\n每个 goroutine 完成任务后，向 done channel 发送一个信号\n\n主 goroutine 通过循环接收 numGoroutines 次信号，确认所有任务完成\n\n使用 struct {} 作为 channel 元素类型，因为不需要传递实际数据，只需要信号\n\n\n2.3 使用优势与局限性优势：\n\n高度灵活，可以携带数据（如任务结果）\n\n适合动态数量的 goroutine\n\n可以与 select 语句结合使用，实现更复杂的同步逻辑\n\n\n局限性：\n\n需要手动管理接收次数，代码可能略显繁琐\n\n不直接支持错误处理\n\n容易导致 goroutine 泄漏，如果没有正确发送或接收信号\n\n\n三、context：优雅的任务取消与超时控制\n3.1 基本概念与适用场景当需要更复杂的控制，如任务取消或超时机制时，context 包提供了强大的解决方案。通过 context.Context，主 goroutine 可以优雅地控制 goroutine 的退出，并等待所有任务完成。\n想象一下，context 就像是一个远程控制，可以随时 “关闭” 所有相关的 goroutine，同时确保主 goroutine 等待它们完成清理工作后再继续执行。\n3.2 代码示例与执行流程让我们看看如何结合 context 和 WaitGroup 来等待 goroutine：\npackage mainimport (&quot;context&quot;&quot;fmt&quot;&quot;sync&quot;)func main() &#123; ctx, cancel := context.WithCancel(context.Background())var wg sync.WaitGroupfor i := 1; i &lt;= 3; i++ &#123;  wg.Add(1)gofunc(id int) &#123;   defer wg.Done()   select &#123;   case &lt;-ctx.Done():    fmt.Printf(&quot;Goroutine %d cancelled\\n&quot;, id)    return   default:    fmt.Printf(&quot;Goroutine %d is running\\n&quot;, id)   &#125;  &#125;(i) &#125;// 模拟任务完成，发送取消信号 cancel()// 等待所有goroutine退出 wg.Wait() fmt.Println(&quot;All goroutines finished&quot;)&#125;\n\n可能的输出（取决于取消信号何时到达）：\nGoroutine 1 is runningGoroutine 2 cancelledGoroutine 3 is runningAll goroutines finished\n\ncontext 方法的工作原理：\n\n使用 context.WithCancel 创建可取消的上下文\n\n每个 goroutine 在执行前检查是否收到取消信号\n\ncancel () 函数发送取消信号\n\nWaitGroup 确保主 goroutine 等待所有 goroutine 完成清理工作\n\n\n3.3 使用优势与局限性优势：\n\n支持任务取消和超时控制\n\n可以传递截止时间或超时时间\n\n适合复杂的并发场景，如网络请求处理\n\n\n局限性：\n\n代码复杂度略有增加\n\n需要与其他同步机制（如 WaitGroup）结合使用\n\n错误处理需要额外实现\n\n\n四、errgroup：现代 Go 应用的最佳选择\n4.1 基本概念与功能特点errgroup 是 Go 语言中一个高级并发工具，它结合了 WaitGroup 的功能和错误处理能力，特别适合需要等待多个任务完成并处理可能出现的错误的场景。\n想象一下，errgroup 就像是一个智能的任务管理器，它不仅能等待所有任务完成，还能处理任务中出现的错误，并且可以在任何一个任务出错时立即取消其他任务。\n4.2 代码示例与执行流程让我们看看如何使用 errgroup 来等待多个 goroutine：\npackage mainimport (&quot;fmt&quot;&quot;golang.org/x/sync/errgroup&quot;)func main() &#123;var g errgroup.Groupfor i := 1; i &lt;= 3; i++ &#123;  id := i  g.Go(func() error &#123;   fmt.Printf(&quot;Goroutine %d is running\\n&quot;, id)   returnnil// 返回nil表示任务成功  &#125;) &#125;// 等待所有goroutine完成，并获取可能的错误if err := g.Wait(); err != nil &#123;  fmt.Println(&quot;Error:&quot;, err) &#125; else &#123;  fmt.Println(&quot;All goroutines finished successfully&quot;) &#125;&#125;\n\n输出（顺序可能不同）：\nGoroutine 1 is runningGoroutine 2 is runningGoroutine 3 is runningAll goroutines finished successfully\n\nerrgroup 方法的工作原理：\n\n使用 errgroup.Group 来管理一组 goroutine\n\ng.Go () 方法启动一个 goroutine，并自动管理计数器\n\ng.Wait () 等待所有 goroutine 完成，并返回第一个非 nil 错误\n\n所有 goroutine 在接收到错误信号后会立即停止\n\n\n4.3 使用优势与局限性优势：\n\n内置错误处理机制，非常适合处理多个可能出错的任务\n\n支持上下文取消（可以使用 errgroup.WithContext）\n\n代码简洁优雅，现代 Go 项目推荐使用\n\n自动处理 goroutine 泄漏\n\n\n局限性：\n\n需要导入额外的包：golang.org&#x2F;x&#x2F;sync&#x2F;errgroup\n\n错误处理方式较为特殊，需要适应\n\n不熟悉的开发者可能需要一些时间学习\n\n\n五、如何选择适合的方法？根据不同的应用场景，我们应该如何选择合适的等待 goroutine 的方法呢？下面是一个简单的决策指南：\n方法适用场景主要优势主要劣势sync.WaitGroup简单任务，固定数量 goroutine简单高效，标准库内置不支持错误处理和取消Channel动态任务数量或需要传递结果高度灵活，可传递数据手动管理较为复杂context需要取消或超时控制的复杂场景支持取消和超时代码复杂度增加errgroup需要错误处理的现代应用强大的错误处理能力，优雅的 API需要额外依赖\n\n5.1 实际应用建议\n简单场景：如果你只需要等待固定数量的 goroutine 完成，并且不需要处理错误或取消，使用 sync.WaitGroup 是最佳选择。\n\n动态任务场景：当 goroutine 数量在运行时确定，或者需要收集任务结果时，考虑使用 channel 方法。\n\n复杂服务场景：在需要处理取消、超时或清理资源的服务器环境中，结合 context 和 WaitGroup 是一个好的选择。\n\n现代 Go 应用：对于新开发的 Go 应用，尤其是需要处理多个可能出错的任务时，推荐使用 errgroup，它提供了简洁而强大的解决方案。\n\n\n5.2 为什么不直接让主 goroutine 休眠？你可能会想：”为什么不直接使用 time.Sleep 来等待 goroutine 完成呢？”\n答案是：time.Sleep 只引入一个固定的延迟，并不能准确等待任务完成。这可能导致程序过早退出或不必要的长时间等待。使用专用的同步工具（如 WaitGroup 或 channel）可以确保程序正确性，避免资源泄漏和逻辑错误。\n总结在 Go 语言中，主 goroutine 等待其他 goroutine 完成任务是并发编程的基础需求。本文介绍了四种常用的方法：\n\nsync.WaitGroup：最常用的方法，简单高效，适合固定数量的 goroutine。\n\nChannel：高度灵活，适合动态任务或需要传递结果的场景。\n\ncontext：支持取消和超时控制，适合复杂的服务端应用。\n\nerrgroup：现代 Go 应用推荐使用，结合了错误处理和等待功能。\n\n\n根据你的具体需求选择合适的工具，可以确保程序逻辑清晰，避免资源泄漏，提高代码的健壮性。\n记住，没有放之四海而皆准的解决方案，根据实际需求选择合适的工具才是王道。希望本文的介绍能帮助你在 Go 并发编程的道路上更进一步！\n","categories":["Golang","goroutine"],"tags":["Golang","2025","goroutine","转载"]},{"title":"Go 语言 Array 与 Slice 深度解析：核心区别、实战指南与高效运用","url":"/2025/2025-09-04_Go%20%E8%AF%AD%E8%A8%80%20Array%20%E5%92%8C%20Slice%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","content":"🚀 一文掌握 Go 语言最核心的数据结构！掌握 Slice 助你高效开发，理解 Array 精髓更能体现你的 Go 语言功力。\n\n\n在 Go 语言的世界里，数组 (Array) 和切片 (Slice) 是我们日常编程中接触最频繁的两种数据结构。它们虽然在表面上有些相似，但骨子里却有着根本性的区别，深刻理解这些差异是写出高效、可靠 Go 代码的关键。本文将带你深入剖析 Array 和 Slice 的核心原理、实战中的使用场景、常见陷阱，以及如何做出最明智的选择。\n1. 基础定义：Array vs Slice\n1.1 数组 (Array)：编译时确定的固定长度序列数组是一种固定长度的、连续存储的相同类型元素序列。它的长度在声明时就已确定，并且是其类型的一部分。这意味着 [3]int 和 [4]int 是两种完全不同的类型。\n// 声明数组的几种常用方式var arr1 [3]int        // 声明一个长度为3的int数组，元素默认值 [0, 0, 0]arr2 := [3]int&#123;1, 2&#125;   // 长度为3，初始化前两个元素，[1, 2, 0]（未赋值元素取零值）arr3 := [...]int&#123;1, 2, 3&#125; // 编译器自动推断长度，类型为 [3]int\n\n数组是值类型。当将一个数组赋值给另一个变量，或将其作为参数传递给函数时，会进行整个数组数据的完整复制。\n1.2 切片 (Slice)：运行时动态大小的底层数组视图切片是对底层数组的一个动态窗口（或称作引用类型）。它由三个组成部分构成：\n\n指向底层数组的指针 (Pointer): 指向切片所关联的底层数组的起始位置。\n当前长度 (Length): 切片当前包含的元素数量。\n容量 (Capacity): 从切片指针位置到其底层数组末尾的元素数量。\n\n// 创建切片的几种常见方式// 方式1：从现有数组创建切片 (注意：此时切片与数组共享底层存储)arr := [5]int&#123;0, 1, 2, 3, 4&#125;s1 := arr[1:4] // 创建一个切片 [1, 2, 3]，此时 len=3, cap=4 (从索引1到数组末尾)// 方式2：直接声明并初始化一个切片 (Go 会自动创建并关联一个底层数组)s2 := []int&#123;1, 2, 3&#125; // 创建一个切片，len=3, cap=3// 方式3：使用 make 函数创建切片 (推荐明确指定长度和容量)s3 := make([]int, 3, 5) // 创建一个类型为 []int 的切片，初始 len=3，cap=5s4 := make([]int, 3)    // 创建一个切片，初始 len=3，cap=3 (容量默认为长度)\n\n切片是引用类型。当赋值或传参时，只会复制切片头（即指针、长度和容量这三个属性），它们共享同一个底层数组。\n2. 核心区别：Array 与 Slice 对比\n为了让您更直观地理解两者区别，下表总结了数组和切片在关键特性上的对比：\n\n\n\n特性\n数组 (Array)\n切片 (Slice)\n\n\n\n长度\n固定（是类型的一部分）\n动态可变（len() 获取）\n\n\n内存分配\n直接存储数据（通常栈上）\n存储 Header (指针&#x2F;长度&#x2F;容量) + 底层数组 (堆上)\n\n\n传递行为\n值拷贝（完整复制）\n引用传递（Header 拷贝，共享底层）\n\n\n类型\n值类型\n引用类型\n\n\n容量\n无 (固定等于长度)\n有（cap() 获取，可扩容）\n\n\n声明方式\n[N]T\n[]T\n\n\n零值\n元素全为零值\nnil (表示未初始化)\n\n\nJSON 序列化\n正常 JSON 数组\n正常 JSON 数组 &#x2F; null\n\n\n3. 切片动态特性深度剖析\n3.1 自动扩容机制：Append 的魔力当使用 append() 函数向切片中添加元素，并且切片的当前长度超出其容量时，Go 运行时会自动执行扩容。具体机制如下：\n\n分配新底层数组：通常会分配一个比原容量大两倍（当原容量小于1024时）或按一定比例（大于1024时）的新底层数组。\n数据拷贝：将原底层数组的所有元素复制到新底层数组中。\n更新切片头：新切片将指向新的底层数组，并更新其长度和容量。\n\ns := []int&#123;1, 2&#125;fmt.Println(&quot;初始切片:&quot;, s, &quot;len:&quot;, len(s), &quot;cap:&quot;, cap(s)) // 初始切片: [1 2] len: 2 cap: 2s = append(s, 3) // 此时 len=2 == cap=2，需要扩容                 // 分配一个新数组，通常是原容量的两倍，即 cap=4fmt.Println(&quot;扩容后切片:&quot;, s, &quot;len:&quot;, len(s), &quot;cap:&quot;, cap(s)) // 扩容后切片: [1 2 3] len: 3 cap: 4s = append(s, 4, 5, 6) // 继续添加，可能再次触发扩容fmt.Println(&quot;再次扩容后切片:&quot;, s, &quot;len:&quot;, len(s), &quot;cap:&quot;, cap(s)) // 再次扩容后切片: [1 2 3 4 5 6] len: 6 cap: 8 (原cap=4，再次翻倍)\n\n注意： 频繁扩容会涉及内存分配和数据拷贝，可能带来性能开销。\n3.2 切片截取操作与底层数组共享切片截取（s[i:j]）并不会创建新的底层数组，而是创建一个新的切片头，指向原底层数组的同一部分。这意味着，修改子切片的元素会直接影响原始切片（及其所有关联切片）。\norig := []int&#123;0, 1, 2, 3, 4&#125;fmt.Println(&quot;原始切片:&quot;, orig, &quot;len:&quot;, len(orig), &quot;cap:&quot;, cap(orig)) // 原始切片: [0 1 2 3 4] len: 5 cap: 5sub := orig[1:3] // 截取 [1,2,3] 中的索引 1 到 2 (不包含索引3)fmt.Println(&quot;子切片 (orig[1:3]):&quot;, sub, &quot;len:&quot;, len(sub), &quot;cap:&quot;, cap(sub)) // 子切片 (orig[1:3]): [1 2] len: 2 cap: 4 (从原数组索引1到末尾)// 修改子切片的一个元素sub[0] = 99fmt.Println(&quot;修改子切片后:&quot;)fmt.Println(&quot;子切片:&quot;, sub)       // 子切片: [99 2]fmt.Println(&quot;原始切片:&quot;, orig)     // 原始切片: [0 99 2 3 4] (原切片受到影响)\n\n3.3 使用 copy 创建独立副本：深拷贝若要避免上述共享底层数组的副作用，确保切片操作互不影响，应使用 copy 函数进行深拷贝：\ns1 := []int&#123;1, 2, 3&#125;s2 := make([]int, len(s1)) // 注意：目标切片 s2 必须有足够的容量copy(s2, s1)               // 将 s1 的元素复制到 s2s2[0] = 99                 // 修改 s2 不会影响 s1fmt.Println(&quot;s1:&quot;, s1)     // s1: [1 2 3]fmt.Println(&quot;s2:&quot;, s2)     // s2: [99 2 3]\n\n4. 函数参数传递行为差异：至关重要\n这是理解数组和切片最关键的差异之一，直接决定了函数操作是否会影响调用者的数据：\n// 接收一个固定长度为3的int数组func modifyArray(arr [3]int) &#123;    arr[0] = 100 // 这里的修改只会作用于传入数组的副本    fmt.Println(&quot;函数内数组:&quot;, arr) // 函数内数组: [100 2 3]&#125;// 接收一个int切片func modifySlice(s []int) &#123;    s[0] = 100 // 这里的修改会作用于切片指向的底层数组，影响外部的切片    fmt.Println(&quot;函数内切片:&quot;, s) // 函数内切片: [100 2 3]&#125;func main() &#123;    // ---- 数组作为参数 ----    arr := [3]int&#123;1, 2, 3&#125;    fmt.Println(&quot;调用前数组:&quot;, arr) // 调用前数组: [1 2 3]    modifyArray(arr)    fmt.Println(&quot;调用后数组:&quot;, arr) // 调用后数组: [1 2 3] (原数组未被修改)    fmt.Println(&quot;----&quot;)    // ---- 切片作为参数 ----    slice := []int&#123;1, 2, 3&#125;    fmt.Println(&quot;调用前切片:&quot;, slice) // 调用前切片: [1 2 3]    modifySlice(slice)    fmt.Println(&quot;调用后切片:&quot;, slice) // 调用后切片: [100 2 3] (原切片被修改)&#125;\n\n核心总结：\n\n数组作为参数是值传递（复制整个数组），函数内部的修改不会影响外部数组。\n切片作为参数是引用传递（复制切片头），函数内部对切片元素的修改会影响外部切片所指向的底层数组。\n\n5. 常见 “陷阱” 与解决方案\n5.1 陷阱 1：意外的数据修改（切片共享底层数组）前文已提及，切片的截取和赋值都可能指向同一底层数组，导致意外的修改：\noriginal := []int&#123;1, 2, 3, 4, 5&#125;subSlice := original[1:3] // [2,3]subSlice[0] = 99          // 修改子切片会影响原切片fmt.Println(original)     // 输出: [1 99 3 4 5]\n\n解决方案：需要独立副本时，使用 copy 函数。\noriginal := []int&#123;1, 2, 3, 4, 5&#125;subSlice := make([]int, 2) // 创建一个新切片用于接收副本copy(subSlice, original[1:3])subSlice[0] = 99 // 不影响 originalfmt.Println(original) // 输出: [1 2 3 4 5]fmt.Println(subSlice) // 输出: [99 3]\n\n5.2 陷阱 2：扩容导致的地址变化与分离当一个切片扩容后，它可能会获得一个新的底层数组。如果之前有其他切片与旧底层数组共享，那么扩容后的切片将与那些旧切片“分离”，不再共享同一底层数据。\ns1 := []int&#123;1, 2, 3&#125;s2 := s1[:2] // s2 是 [1, 2]，与 s1 共享底层数组             // 此时 s1: [1 2 3], len=3, cap=3             // 此时 s2: [1 2], len=2, cap=2 (从 s1[0] 到 s1 数组末尾)s1 = append(s1, 4) // s1 长度正好等于容量，触发扩容                  // s1 会分配一个新底层数组 (如容量变为6)，并复制旧数据s1[0] = 100       // s1 修改的是新底层数组的第一个元素fmt.Println(&quot;s1:&quot;, s1) // s1: [100 2 3 4]fmt.Println(&quot;s2:&quot;, s2) // s2: [1 2] (s2 仍指向旧底层数组的 [1, 2]，未受影响)\n\n解决方案：如果需要所有引用都保持一致，应避免在共享切片的情况下进行可能触发扩容的操作。或者，在创建切片时就预分配足够的容量以减少扩容的发生。\n// 预分配足够容量，尽量避免扩容导致分离s1 := make([]int, 3, 5) // len=3, cap=5s1[0], s1[1], s1[2] = 1, 2, 3s2 := s1[:2] // s2 是 [1, 2]，与 s1 共享底层数组             // 此时 s1: [1 2 3], len=3, cap=5             // 此时 s2: [1 2], len=2, cap=4 (从 s1[0] 到 s1 数组末尾)s1 = append(s1, 4) // s1 容量足够 (cap=5)，不会触发扩容，直接在原底层数组添加s1[0] = 100fmt.Println(&quot;s1:&quot;, s1) // s1: [100 2 3 4]fmt.Println(&quot;s2:&quot;, s2) // s2: [100 2] (s2 仍共享，且被 s1 的修改影响)\n\n5.3 陷阱 3：空切片 []int&#123;&#125; vs nil 切片 var []int两者在 len 和 cap 上都返回 0，但在一些操作和语义上存在差异。\nimport &quot;encoding/json&quot;import &quot;fmt&quot;var nilSlice []int      // nil 切片，其值为 nilemptySlice := []int&#123;&#125;   // 空切片，非 nil，指向一个长度为0的底层数组fmt.Println(&quot;nilSlice == nil:&quot;, nilSlice == nil)        // truefmt.Println(&quot;emptySlice == nil:&quot;, emptySlice == nil)    // falsefmt.Println(&quot;len(nilSlice):&quot;, len(nilSlice), &quot;cap(nilSlice):&quot;, cap(nilSlice)) // len: 0 cap: 0fmt.Println(&quot;len(emptySlice):&quot;, len(emptySlice), &quot;cap(emptySlice):&quot;, cap(emptySlice)) // len: 0 cap: 0// JSON 序列化差异（常见于 API 返回）nilJSON, _ := json.Marshal(nilSlice)emptyJSON, _ := json.Marshal(emptySlice)fmt.Println(&quot;nilSlice JSON:&quot;, string(nilJSON))      // &quot;null&quot;fmt.Println(&quot;emptySlice JSON:&quot;, string(emptyJSON))  // &quot;[]&quot;\n\n最佳实践：\n\n当函数返回值表示“没有数据”或“错误”时，返回 nil 切片。\n当函数返回值表示“一个空的集合”时，返回 []T&#123;&#125; 或 make([]T, 0)。例如，json.Marshal(nil) 会输出 null，而 json.Marshal([]) 会输出 []。在设计 RESTful API 接口时，这两种情况的语义是不同的。\n\n6. 性能对比与使用场景推荐\n6.1 性能特点\n数组 (Array):\n访问速度快：内存连续且固定，编译器在编译时能做更多优化（如边界检查）。\n无额外开销：不涉及指针、长度、容量等额外元数据。\n局部变量可以栈上分配：减少 GC 压力 (如果数组不是太大)。\n零内存管理开销：长度固定，无需考虑扩容。\n\n\n切片 (Slice):\n动态灵活：无需预先知道确切大小，可以动态增删改查。\n扩容开销：当容量不足时，需要分配新底层数组并拷贝数据，可能影响性能。\nGC 压力：底层数组通常在堆上分配，会增加 GC 负担。\n引用开销：每次操作都需要通过切片头来间接访问底层数组。\n\n\n\n6.2 使用场景推荐6.2.1 适合使用数组 (Array) 的场景\n集合大小在编译时完全确定：例如，表示 RGB 颜色 var color [3]byte，或者一周的固定天数。\n需要精确的内存控制：例如，嵌入式系统编程、需要将数据直接映射到硬件寄存器。\n高性能的循环处理：当需要极致性能，且数据量固定不大时。\n固定大小的数据结构：如密码哈希算法中的固定大小哈希值（[32]byte）、或表示固定长度的 IPv6 地址 [16]byte。\n作为函数参数时，确保传入数据不被修改：尤其在传递较大的数据结构时，数组值拷贝可以起到保护作用。\n\n6.2.1 适合使用切片 (Slice) 的场景\n动态大小集合：绝大多数日常编程场景，需要处理数量可变的数据，如用户输入、数据库查询结果、文件读取等。\n函数参数传递：作为函数参数，可以避免大数组的拷贝开销，并允许函数修改其底层数据。\n各种标准库和框架：Go 的标准库几乎都是围绕切片设计的，例如 io.Reader 接口接收 []byte。\n作为可扩展的缓冲：使用 make([]byte, 0, initialCap) 来创建可增长的缓冲区。\n\n7. 实战选择指南\n这是一个经验法则：当不确定大小时或需要高度灵活性时，总是优先使用切片。只有在有明确、特殊需求时，才考虑数组。\n以下是一些具体的实用建议：\n\n默认选择切片：在 Go 语言开发中，你可能 90% 的时间都在使用切片。它是处理集合数据的首选，因为它自动化了内存管理、扩容等复杂问题。\n\n何时考虑数组：当你需要一个严格规定长度，且其长度是类型定义的一部分的集合时。例如，实现一些底层协议、加密算法中的固定长度字段，或者当你非常关注内存布局和零GC开销时。\n\n传递大块数据且不希望被修改：可以考虑将指向数组的指针作为函数参数 *[N]T，这避免了整个数组的复制，同时通过指针的只读访问来避免意外修改。\nfunc processFixedSizeBuffer(buf *[512]byte) &#123;    // 可以读取 buf 的内容，但修改会直接影响原始数组    // 如果想避免修改，在函数内再次 copy&#125;\n关注性能时，预先分配容量：如果你知道切片最终会达到某个大致的长度，可以使用 make([]T, 0, n) 来预分配足量容量，从而减少 append 时的扩容次数，提高性能。\n\n返回空集合的最佳实践：\n\nnil 切片 (var s []T) 通常用于表示“不存在”或“尚未初始化”的情况，它在 JSON 中序列化为 null。\n空切片 ([]T&#123;&#125; 或 make([]T, 0)) 表示“一个空的集合”，它在 JSON 中序列化为 []。根据 API 语义选择。\n\n\n\n8. 总结\nGo 语言的 Array 和 Slice，这对看似孪生的数据结构，实则在底层机制和行为上有着天壤之别：\n\n数组 (Array)：固定长度、值类型、完整复制，适用于编译时确定大小、对内存和性能有极致要求的场景。\n切片 (Slice)：可变长度、引用类型、动态扩容，是 Go 语言中处理可变大小数据的主力容器，灵活高效，但需注意其共享底层数组及扩容带来的影响。\n\n理解它们的底层原理、核心区别及其在函数参数传递时的行为，是写出高效、可靠且符合 Go 语言惯用法的关键。在日常开发中，应熟练运用切片的强大，同时在特定情境下，也能清晰地识别并利用数组的独特优势。\n希望这篇文章能帮助你彻底理解 Go 语言中数组和切片的差异，让你的代码更加高效和可靠！\n","categories":["数据结构"],"tags":["数据结构","Golang","2025"]},{"title":"Go 语言协程设计与调度原理","url":"/2025/2025-09-05_Go%E8%AF%AD%E8%A8%80%E5%8D%8F%E7%A8%8B%E8%AE%BE%E8%AE%A1%E4%B8%8E%E8%B0%83%E5%BA%A6%E5%8E%9F%E7%90%86/","content":"\n本文由 简悦 SimpRead 转码， 原文地址 mp.weixin.qq.com\n\n协程设计 - GMP 模型线程是操作系统调度到 CPU 中执行的基本单位，多线程总是交替式地抢占 CPU 的时间片，线程在上下文的切换过程中需要经过操作系统用户态与内核态的切换。\ngolang 的协程 (G) 依然运行在工作线程 (M) 之上，但是借助语言的调度器，协程只需要在用户态即可完成切换，工作线程是感受不到协程存在的。\ngolang 在设计上通过逻辑处理器 (P) 建立起了工作线程与协程之间的联系。最简单的 GMP 关系模型为(图是静态的，在程序运行的过程中，GMP 三者之间的绑定关系都是不固定的):\n\n工作线程 M工作线程是最终运行协程的实体。操作系统中的线程与在运行时代表线程的 m 结构体进行了绑定：\n// go/src/runtime/runtime2.gotype m struct &#123;    g0      *g     // goroutine with scheduling stack    tls           [tlsSlots]uintptr // thread-local storage (for x86 extern register)    curg          *g       // current running goroutine    p             puintptr // attached p for executing go code (nil if not executing go code)    nextp         puintptr    oldp          puintptr // the p that was attached before executing a syscall    park          note  ...&#125;\n\n为了执行 go 代码，每一个工作线程 m 都与一个逻辑处理器 p 进行绑定，同时记录了线程当前正在运行的用户协程 curg。\n每一个工作线程中都有一个特殊的协程 g0，称为调度协程，其主要作用是执行协程调度。而普通的协程 g 无差别地用于执行用户代码。\n当用户协程 g 主动让渡、退出或者是被抢占时，m 内部就需要重新执行协程调度，这时需要从用户协程 g 切换到调度协程 g0，g0 调度一个普通协程 g 来执行用户代码，便从 g0 又切换回普通协程 g。每个工作线程内部都在完成 g-&gt;g0-&gt;g 这样的调度循环。\n操作系统的线程与 m 结构体是通过线程本地存储 (thread-local storage) 进行绑定的。普通的全局变量对进程中的所有线程可见，而线程本地存储 (tls) 中的变量只对当前线程可见。系统线程通过 m.tls 即可在任意时刻获取到当前线程上的正在运行的协程 g、逻辑处理器 p、特殊协程 g0、线程结构体 m 等信息。\n想学编程的同学，可以关注一下这个网站，上面的内容很全哦~\n网站地址：https://www.j301.cn\n逻辑处理器 p系统线程 m 想要运行用户协程 g，必须先绑定逻辑处理器 p。在代码中可以通过 runtime.GOMAXPROCS() 具体指定程序运行需要使用多少个逻辑处理器 p。通常指定多少个逻辑处理器 p 最多就可以同时使用到多少个 CPU 核心数。\n逻辑处理器 p 通过结构体 p 进行定义：\ntype p struct &#123;    id          int32    status      uint32 // one of pidle/prunning/...  schedtick   uint32     // incremented on every scheduler call    syscalltick uint32     // incremented on every system call    m           muintptr   // back-link to associated m (nil if idle)    // Queue of runnable goroutines. Accessed without lock.    runqhead uint32    runqtail uint32    runq     [256]guintptr    runnext guintptr  ... &#125;\n\n在 p 中，通过字段 m 维护了与工作线程 m 的绑定关系。每一个逻辑处理器 p 都具有唯一的 id，以及当前的状态 status。如果 p 的状态为正在运行中，则必然绑定到了一个工作线程 m 上，当逻辑处理完成后，解绑工作线程 (m&#x3D;&#x3D;nil)，p 的状态便是空闲的。\n需要注意的是，m 与 p 的数量没有绝对关系，当 m 阻塞时，p 就会切换到一个空闲的 m，当不存在空闲的 m 时，便会创建一个 m。所以即使 p 的数量是 1，也有可能会创建很多个 m 出来。\n程序中往往有成千上万的协程存在，不可能同时被执行。协程需要进行调度执行，而那些等待被调度执行的协程存储在运行队列中。go 语言调度器将运行队列分为全局运行队列与局部运行队列。逻辑处理器 p 中维护了局部运行队列 runq。\n局部运行队列是每个 p 特有的长度为 256 的数组。该数组模拟了一个循环队列，p.runqhead 为队头，p.runqtail 为队尾，协程 g 都从队尾入队，从队头获取。而全局运行队列维护在 schedt.runq 中 (见后文)。\np 中还有一个特殊的 runnext 字段，用于标识下一个要执行的协程 g，如果 p.runnext 不为空，则会直接执行 runnext 指向的协程，而不会再去 p.runq 数组中寻找。\n协程 g协程通常分为特殊的调度协程 g0 以及执行用户代码的普通协程 g。无论 g0 还是 g，都通过结构体 g 进行定义：\n// go/src/runtime/runtime2.gotype g struct &#123;    stack       stack   // offset known to runtime/cgo    m         *m      // current m; offset known to arm liblink    sched     gobuf  ...&#125;// Stack describes a Go execution stack.type stack struct &#123;    lo uintptr    hi uintptr&#125;type gobuf struct &#123;    sp   uintptr    pc   uintptr    g    guintptr    ctxt unsafe.Pointer    ret  uintptr    lr   uintptr    bp   uintptr // for framepointer-enabled architectures&#125;\n\n协程 g 中包含了协程的执行栈空间 (stack)，执行当前协程的工作线程 m 以及执行现场 sched。协程 g 执行上下文切换时需要保存当前的执行现场，以便在切回协程 g 时能够继续正常执行。协程 g 中的执行现场由结构体 gobuf 定义，其保存了 CPU 中几个重要的寄存器值，以及执行现场信息属于哪个协程 g。\n全局调度信息 schedtgolang 协程设计中，除了工作线程 m、逻辑处理器 p、协程 g 以外，还存在一个存储全局调度信息的结构体 schedt：\n// go/src/runtime/runtime2.gotype schedt struct &#123;    lock mutex    midle        muintptr // idle m&#x27;s waiting for work    nmidle       int32    // number of idle m&#x27;s waiting for work    nmidlelocked int32    // number of locked m&#x27;s waiting for work    mnext        int64    // number of m&#x27;s that have been created and next M ID    maxmcount    int32    // maximum number of m&#x27;s allowed (or die)    nmsys        int32    // number of system m&#x27;s not counted for deadlock    nmfreed      int64    // cumulative number of freed m&#x27;s    ngsys uint32 // number of system goroutines; updated atomically    pidle      puintptr // idle p&#x27;s    npidle     uint32    nmspinning uint32 // See &quot;Worker thread parking/unparking&quot; comment in proc.go.    // Global runnable queue.    runq     gQueue    runqsize int32  // Global cache of dead G&#x27;s.    gFree struct &#123;        lock    mutex        stack   gList // Gs with stacks        noStack gList // Gs without stacks        n       int32    &#125;    // freem is the list of m&#x27;s waiting to be freed when their    // m.exited is set. Linked through m.freelink.    freem *m    ...&#125;\n\nschedt 中维护了空闲的工作线程 midle、空闲工作线程的数量 nmidle、等待被释放的线程列表 freem、系统协程 g 的数量 ngsys、空闲逻辑处理器 pidle、空闲逻辑处理器的数量 npidle、以及全局运行队列 runq 及全局运行队列的大小 runqsize、处于新建或者被销毁状态的协程 g 列表 gFree 等信息。\nschedt 中的信息是全局共享的，例如全局运行队列 runq 被所有 p 共享，所以 schedt 中也持有一个锁 lock 以保证原子性访问。\nGMP 详细示图通过上述说明，我们可以进一步细化 GMP 模型示图为:\n\n协程调度\n已经知道，每个工作线程 m 中都有一个调度协程 g0，专门执行协程的调度循环 (g-&gt;g0-&gt;g-&gt;g0-g)。在调度循环中，协程 g 具体是如何被调度的呢？go 语言调度器实现了自己的调度策略。\n调度策略工作线程 m 需要通过协程调度获得具体可运行的某一协程 g。获取协程 g 的一般策略主要包含三大步:\n\n查找 p 本地的局部运行队列\n\n查找 schedt 中的全局运行队列\n\n窃取其他 p 中的局部运行队列\n\n\n在运行时通过 findRunnable() 函数获取可运行的协程 g:\n// go/src/runtime/proc.go// Finds a runnable goroutine to execute.func findRunnable() (gp *g, inheritTime, tryWakeP bool) &#123;  ...  // Check the global runnable queue once in a while to ensure fairness.    // Otherwise two goroutines can completely occupy the local runqueue    // by constantly respawning each other.    if _p_.schedtick%61 == 0 &amp;&amp; sched.runqsize &gt; 0 &#123;        lock(&amp;sched.lock)        gp = globrunqget(_p_, 1)        unlock(&amp;sched.lock)        if gp != nil &#123;            return gp, false, false        &#125;    &#125;  ...  // local runq    if gp, inheritTime := runqget(_p_); gp != nil &#123;        return gp, inheritTime, false    &#125;    // global runq    if sched.runqsize != 0 &#123;        lock(&amp;sched.lock)        gp := globrunqget(_p_, 0)        unlock(&amp;sched.lock)        if gp != nil &#123;            return gp, false, false        &#125;    &#125;  ...    // Spinning Ms: steal work from other Ps.    //    // Limit the number of spinning Ms to half the number of busy Ps.    // This is necessary to prevent excessive CPU consumption when    // GOMAXPROCS&gt;&gt;1 but the program parallelism is low.    procs := uint32(gomaxprocs)    if _g_.m.spinning || 2*atomic.Load(&amp;sched.nmspinning) &lt; procs-atomic.Load(&amp;sched.npidle) &#123;        if !_g_.m.spinning &#123;            _g_.m.spinning = true            atomic.Xadd(&amp;sched.nmspinning, 1)        &#125;        gp, inheritTime, tnow, w, newWork := stealWork(now)        now = tnow        if gp != nil &#123;            // Successfully stole.            return gp, inheritTime, false        &#125;    ...    &#125;&#125;\n\n获取本地运行队列在查找可运行的协程 g 时，首先通过函数 runqget() 从 p 本地的运行队列中获取:\n首先尝试从 runnext 中获取下一个执行的 g。当 runnext 不为空时则返回对应的协程 g，如果为空则继续从局部运行队列 runq 中查找。\n当循环队列的队头 runqhead 和队尾 runqtail 相同时，说明循环队列中没有任何可运行的协程，否则从队列头部获取一个协程返回。\n由于可能存在其他逻辑处理器 p 来窃取协程，从而造成当前 p 与其他 p 同时访问局部队列的情况，因此在此处需要加锁访问，访问结束后释放锁。\n// go/src/runtime/proc.gofunc runqget(_p_ *p) (gp *g, inheritTime bool) &#123;    // If there&#x27;s a runnext, it&#x27;s the next G to run.    next := _p_.runnext    // If the runnext is non-0 and the CAS fails, it could only have been stolen by another P,    // because other Ps can race to set runnext to 0, but only the current P can set it to non-0.    // Hence, there&#x27;s no need to retry this CAS if it falls.    if next != 0 &amp;&amp; _p_.runnext.cas(next, 0) &#123;        return next.ptr(), true    &#125;    for &#123;        h := atomic.LoadAcq(&amp;_p_.runqhead) // load-acquire, synchronize with other consumers        t := _p_.runqtail        if t == h &#123;            return nil, false        &#125;        gp := _p_.runq[h%uint32(len(_p_.runq))].ptr()        if atomic.CasRel(&amp;_p_.runqhead, h, h+1) &#123; // cas-release, commits consume            return gp, false        &#125;    &#125;&#125;\n\n协程调度时由于总是优先查找局部运行队列中的协程 g，如果只是循环往复的地执行局部队列中的 g，那么全局队列中的 g 可能一个都不会被调度到。因此，为了保证调度的公平性，p 中每执行 61 次调度，就会优先从全局队列中获取一个 g 到当前 p 中执行:\n// go/src/runtime/proc.gofunc findRunnable() (gp *g, inheritTime, tryWakeP bool) &#123;  ...    if _p_.schedtick%61 == 0 &amp;&amp; sched.runqsize &gt; 0 &#123;        lock(&amp;sched.lock)        gp = globrunqget(_p_, 1)        unlock(&amp;sched.lock)        if gp != nil &#123;            return gp, false, false        &#125;    &#125;  ...&#125;\n\n获取全局运行队列当 p 每执行 61 次调度，或者 p 本地运行队列不存在可运行的协程时，需要从全局运行队列中获取一批协程分配给本地运行队列。由于每个 p 共享了全局运行队列，因此为了保证公平，需要将全局运行队列中的 g 按照 p 的数量进行平分，平分后数量也不能超过局部运行队列容量的一半 (即 128&#x3D;256&#x2F;2)。最后通过循环调用 runqput 将全局队列中的 g 放入到 p 的局部运行队列中。\n\n// go/src/runtime/proc.go// Try get a batch of G&#x27;s from the global runnable queue.// sched.lock must be held.func globrunqget(_p_ *p, max int32) *g &#123;    assertLockHeld(&amp;sched.lock)    if sched.runqsize == 0 &#123;        return nil    &#125;    n := sched.runqsize/gomaxprocs + 1    if n &gt; sched.runqsize &#123;        n = sched.runqsize    &#125;    if max &gt; 0 &amp;&amp; n &gt; max &#123;        n = max    &#125;    if n &gt; int32(len(_p_.runq))/2 &#123;        n = int32(len(_p_.runq)) / 2    &#125;    sched.runqsize -= n    gp := sched.runq.pop()    n--    for ; n &gt; 0; n-- &#123;        gp1 := sched.runq.pop()        runqput(_p_, gp1, false)    &#125;    return gp&#125;\n\n协程窃取当 p 在局部运行队列、全局运行队列中都找不到可运行的协程时，就需要从其他 p 的本地运行队列中窃取一批可用的协程。所有的 p 都存储在全局的 allp []*p 变量中, 调度器随机在其中选择一个 p 来进行协程窃取工作。窃取工作总共会执行不超过 4 次，当窃取成功时即返回。\n// go/src/runtime/proc.go// stealWork attempts to steal a runnable goroutine or timer from any P.func stealWork(now int64) (gp *g, inheritTime bool, rnow, pollUntil int64, newWork bool) &#123;    pp := getg().m.p.ptr()    ranTimer := false    const stealTries = 4    for i := 0; i &lt; stealTries; i++ &#123;        stealTimersOrRunNextG := i == stealTries-1        for enum := stealOrder.start(fastrand()); !enum.done(); enum.next() &#123;            if sched.gcwaiting != 0 &#123;                // GC work may be available.                return nil, false, now, pollUntil, true            &#125;            p2 := allp[enum.position()]            if pp == p2 &#123;                continue            &#125;            ...            // Don&#x27;t bother to attempt to steal if p2 is idle.            if !idlepMask.read(enum.position()) &#123;                if gp := runqsteal(pp, p2, stealTimersOrRunNextG); gp != nil &#123;                    return gp, false, now, pollUntil, ranTimer                &#125;            &#125;        &#125;    &#125;  ...&#125;\n\n协程窃取的主要执行逻辑通过 runqsteal 以及 runqgrab 函数实现，窃取的核心逻辑是：将要窃取的 p 本地运行队列中 g 个数的一半放入到自己的运行队列中。\n\n// Steal half of elements from local runnable queue of p2// and put onto local runnable queue of p.// Returns one of the stolen elements (or nil if failed).func runqsteal(_p_, p2 *p, stealRunNextG bool) *g &#123;    t := _p_.runqtail    n := runqgrab(p2, &amp;_p_.runq, t, stealRunNextG)    if n == 0 &#123;        return nil    &#125;    n--    gp := _p_.runq[(t+n)%uint32(len(_p_.runq))].ptr()    if n == 0 &#123;        return gp    &#125;    h := atomic.LoadAcq(&amp;_p_.runqhead) // load-acquire, synchronize with consumers    if t-h+n &gt;= uint32(len(_p_.runq)) &#123;        throw(&quot;runqsteal: runq overflow&quot;)    &#125;    atomic.StoreRel(&amp;_p_.runqtail, t+n) // store-release, makes the item available for consumption    return gp&#125;// Grabs a batch of goroutines from _p_&#x27;s runnable queue into batch.func runqgrab(_p_ *p, batch *[256]guintptr, batchHead uint32, stealRunNextG bool) uint32 &#123;    for &#123;        h := atomic.LoadAcq(&amp;_p_.runqhead) // load-acquire, synchronize with other consumers        t := atomic.LoadAcq(&amp;_p_.runqtail) // load-acquire, synchronize with the producer        n := t - h        n = n - n/2        ...        for i := uint32(0); i &lt; n; i++ &#123;            g := _p_.runq[(h+i)%uint32(len(_p_.runq))]            batch[(batchHead+i)%uint32(len(batch))] = g        &#125;        if atomic.CasRel(&amp;_p_.runqhead, h, h+n) &#123; // cas-release, commits consume            return n        &#125;    &#125;&#125;\n\n调度时机调度策略让我们知道了协程是如何调度的，下面继续说明什么时候会发生协程调度。\n主动调度协程可以选择主动让渡自己的执行权，这主要通过在代码中主动执行 runtime.Gosched() 函数实现。\n\n主动调度会从当前协程 g 切换到 g0 并更新协程状态由运行中_Grunning 变为可运行_Grunnable；\n\n然后通过 dropg() 取消 g 与 m 的绑定关系；\n\n接着通过 globrunqput() 将 g 放入到全局运行队列中；\n\n最后调用 schedule() 函数开启新一轮的调度循环。\n\n\n// go/src/runtime/proc.go// Gosched yields the processor, allowing other goroutines to run. It does not// suspend the current goroutine, so execution resumes automatically.func Gosched() &#123;    checkTimeouts()    mcall(gosched_m) //&#125;// Gosched continuation on g0.func gosched_m(gp *g) &#123;    ...    goschedImpl(gp) //&#125;func goschedImpl(gp *g) &#123;    ...    casgstatus(gp, _Grunning, _Grunnable)    dropg() //    lock(&amp;sched.lock)    globrunqput(gp)    unlock(&amp;sched.lock)    schedule()&#125;// dropg removes the association between m and the current goroutine m-&gt;curg (gp for short).func dropg() &#123;    _g_ := getg()    setMNoWB(&amp;_g_.m.curg.m, nil)    setGNoWB(&amp;_g_.m.curg, nil)&#125;\n\n被动调度当协程休眠、通道堵塞、网络堵塞、垃圾回收导致暂停时，协程会被动让渡出执行的权利给其他可运行的协程继续执行。调度器通过 gopark() 函数执行被动调度逻辑。gopark() 函数最终调用 park_m() 函数来完成调度逻辑。\n\n首先会从当前协程 g 切换到 g0 并更新协程状态由运行中_Grunning 变为等待中_Gwaiting；\n\n然后通过 dropg() 取消 g 与 m 的绑定关系；\n\n接着执行 waitunlockf 函数，如果该函数返回 false, 则协程 g 立即恢复执行，否则等待唤醒；\n\n最后调用 schedule() 函数开启新一轮的调度循环。\n\n\n// go/src/runtime/proc.go// Puts the current goroutine into a waiting state and calls unlockf on the// system stack.func gopark(unlockf func(*g, unsafe.Pointer) bool, lock unsafe.Pointer, reason waitReason, traceEv byte, traceskip int) &#123;    ...    mcall(park_m)&#125;// park continuation on g0.func park_m(gp *g) &#123;    ...    casgstatus(gp, _Grunning, _Gwaiting)    dropg()    if fn := _g_.m.waitunlockf; fn != nil &#123;        ok := fn(gp, _g_.m.waitlock)        _g_.m.waitunlockf = nil        _g_.m.waitlock = nil        if !ok &#123;            ...            casgstatus(gp, _Gwaiting, _Grunnable)            execute(gp, true) // Schedule it back, never returns.        &#125;    &#125;    schedule()&#125;\n\n与主动调度不同的是，被动调度的协程 g 不会放入到全局队列中进行调度。而是一直处于等待中_Gwaiting 状态等待被唤醒。当等待中的协程被唤醒时，协程的状态由_Gwaiting 变为可运行_Grunnable 状态，然后被添加到当前 p 的局部运行队列中。唤醒逻辑通过函数 goready() 调用 ready() 实现：\n// go/src/runtime/proc.gofunc goready(gp *g, traceskip int) &#123;    systemstack(func() &#123;        ready(gp, traceskip, true)    &#125;)&#125;// Mark gp ready to run.func ready(gp *g, traceskip int, next bool) &#123;    ...    // status is Gwaiting or Gscanwaiting, make Grunnable and put on runq    casgstatus(gp, _Gwaiting, _Grunnable)    runqput(_g_.m.p.ptr(), gp, next)    wakep()    ...&#125;\n抢占调度go 应用程序在启动时会开启一个特殊的线程来执行系统监控任务，系统监控运行在一个独立的工作线程 m 上，该线程不用绑定逻辑处理器 p。系统监控每隔 10ms 会检测是否有准备就绪的网络协程，并放置到全局队列中。\n为了保证每个协程都有执行的机会，系统监控服务会对执行时间过长 (大于 10ms) 的协程、或者处于系统调用 (大于 20 微秒) 的协程进行抢占。抢占的核心逻辑通过 retake()函数实现:\n// go/src/runtime/proc.go// forcePreemptNS is the time slice given to a G before it is// preempted.const forcePreemptNS = 10 * 1000 * 1000 // 10msfunc retake(now int64) uint32 &#123;    n := 0    lock(&amp;allpLock)    for i := 0; i &lt; len(allp); i++ &#123;        _p_ := allp[i]        if _p_ == nil &#123;            continue        &#125;        pd := &amp;_p_.sysmontick        s := _p_.status        sysretake := false        if s == _Prunning || s == _Psyscall &#123;            // Preempt G if it&#x27;s running for too long.            t := int64(_p_.schedtick)            if int64(pd.schedtick) != t &#123;                pd.schedtick = uint32(t)                pd.schedwhen = now            &#125; else if pd.schedwhen+forcePreemptNS &lt;= now &#123;                preemptone(_p_)                // In case of syscall, preemptone() doesn&#x27;t                // work, because there is no M wired to P.                sysretake = true            &#125;        &#125;        if s == _Psyscall &#123;            // Retake P from syscall if it&#x27;s there for more than 1 sysmon tick (at least 20us).      t := int64(_p_.syscalltick)            if !sysretake &amp;&amp; int64(pd.syscalltick) != t &#123;                pd.syscalltick = uint32(t)                pd.syscallwhen = now                continue            &#125;            if runqempty(_p_) &amp;&amp; atomic.Load(&amp;sched.nmspinning)+atomic.Load(&amp;sched.npidle) &gt; 0 &amp;&amp; pd.syscallwhen+10*1000*1000 &gt; now &#123;                continue            &#125;      ...    &#125;    unlock(&amp;allpLock)    return uint32(n)&#125;","categories":["Golang","goroutine"],"tags":["Golang","2025","goroutine","转载"]},{"title":"Redis持久化深度解析：RDB与AOF的终极对决与实战优化","url":"/2025/2025-09-14_Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9ARDB%E4%B8%8EAOF%E7%9A%84%E7%BB%88%E6%9E%81%E5%AF%B9%E5%86%B3%E4%B8%8E%E5%AE%9E%E6%88%98%E4%BC%98%E5%8C%96/","content":"\n本文由 简悦 SimpRead 转码， 原文地址 mp.weixin.qq.com\n\nRedis 持久化不仅仅是简单的数据备份，更是保障系统高可用的关键防线。\n\n\n一、为什么 Redis 持久化如此重要？1.1 Redis 的 “阿喀琉斯之踵”Redis 以其极致的性能著称，但内存存储的特性也带来了致命弱点：\n\n• 断电即失：服务器宕机、进程崩溃都会导致数据永久丢失\n\n• 成本压力：纯内存方案成本高昂，1TB 内存服务器月租可达数万元\n\n• 合规要求：金融、电商等行业对数据持久性有严格的监管要求\n\n\n1.2 持久化带来的价值通过合理的持久化策略，我们可以：\n\n• 实现秒级 RTO（恢复时间目标），将故障恢复时间从小时级降至分钟级\n\n• 支持跨机房容灾，构建异地多活架构\n\n• 满足数据审计需求，实现关键操作的追溯回放\n\n\n二、RDB：简单粗暴的快照机制2.1 RDB 的工作原理RDB（Redis Database）采用定期快照的方式，将某一时刻的内存数据完整地持久化到磁盘。想象一下，这就像给 Redis 的内存状态拍了一张 “全家福”。\n# redis.conf 中的 RDB 配置示例save 900 1      # 900秒内至少1个key变化则触发save 300 10     # 300秒内至少10个key变化则触发  save 60 10000   # 60秒内至少10000个key变化则触发dbfilename dump.rdb           # RDB文件名dir /var/lib/redis            # RDB文件存储路径rdbcompression yes            # 开启压缩（LZF算法）rdbchecksum yes              # 开启CRC64校验stop-writes-on-bgsave-error yes  # 后台保存出错时停止写入\n\n2.2 触发机制详解RDB 持久化有多种触发方式，每种都有其适用场景：\n# Python示例：监控RDB触发情况import redisimport timer = redis.Redis(host=&#x27;localhost&#x27;, port=6379)# 手动触发 BGSAVEdefmanual_backup():    result = r.bgsave()    print(f&quot;后台保存已触发: &#123;result&#125;&quot;)        # 监控保存进度    whileTrue:        info = r.info(&#x27;persistence&#x27;)        if info[&#x27;rdb_bgsave_in_progress&#x27;] == 0:            print(f&quot;RDB保存完成，耗时: &#123;info[&#x27;rdb_last_bgsave_time_sec&#x27;]&#125;秒&quot;)            break        time.sleep(1)        print(f&quot;保存中...当前进度: &#123;info[&#x27;rdb_current_bgsave_time_sec&#x27;]&#125;秒&quot;)# 获取RDB统计信息defget_rdb_stats():    info = r.info(&#x27;persistence&#x27;)    stats = &#123;        &#x27;最后保存时间&#x27;: time.strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;,                                  time.localtime(info[&#x27;rdb_last_save_time&#x27;])),        &#x27;最后保存状态&#x27;: &#x27;ok&#x27;if info[&#x27;rdb_last_bgsave_status&#x27;] == &#x27;ok&#x27;else&#x27;failed&#x27;,        &#x27;当前保存进行中&#x27;: info[&#x27;rdb_bgsave_in_progress&#x27;] == 1,        &#x27;fork耗时(ms)&#x27;: info[&#x27;latest_fork_usec&#x27;] / 1000    &#125;    return stats\n\n2.3 RDB 的优势与劣势优势：\n\n• 恢复速度快：加载 RDB 文件比重放 AOF 日志快 10 倍以上\n\n• 存储效率高：二进制格式 + 压缩，文件体积小\n\n• 性能影响小：fork 子进程异步执行，主进程无阻塞\n\n\n劣势：\n\n• 数据丢失风险：最多丢失一个快照周期的数据\n\n• fork 开销大：大内存实例 fork 可能导致毫秒级阻塞\n\n\n2.4 实战优化技巧# 1. 避免频繁全量备份导致的IO压力# 错误示例：生产环境不要这样配置！save 10 1  # 每10秒只要有1个key变化就备份# 2. 合理设置备份策略# 推荐配置：根据业务特点调整save 3600 1        # 1小时内至少1次变更save 300 100       # 5分钟内至少100次变更save 60 10000      # 1分钟内至少10000次变更# 3. 利用主从复制减少主库压力# 在从库上执行RDB备份redis-cli -h slave_host CONFIG SET save &quot;900 1&quot;\n\n三、AOF：精确到每一条命令的日志3.1 AOF 的核心机制AOF（Append Only File）通过记录每一条写命令来实现持久化，类似 MySQL 的 binlog。这种方式可以最大程度地减少数据丢失。\n# AOF 核心配置appendonly yes                    # 开启AOFappendfilename &quot;appendonly.aof&quot;   # AOF文件名appendfsync everysec              # 每秒同步一次（推荐）# appendfsync always              # 每次写入都同步（最安全但最慢）# appendfsync no                  # 由操作系统决定（最快但最不安全）no-appendfsync-on-rewrite no      # 重写时是否暂停同步auto-aof-rewrite-percentage 100   # 文件增长100%时触发重写auto-aof-rewrite-min-size 64mb    # AOF文件最小重写大小\n\n3.2 AOF 重写机制深度剖析AOF 文件会不断增长，重写机制通过生成等效的最小命令集来压缩文件：\n# 模拟AOF重写过程classAOFRewriter:    def__init__(self):        self.commands = []        self.data = &#123;&#125;        defrecord_command(self, cmd):        &quot;&quot;&quot;记录原始命令&quot;&quot;&quot;        self.commands.append(cmd)        # 模拟执行命令        if cmd.startswith(&quot;SET&quot;):            parts = cmd.split()            self.data[parts[1]] = parts[2]        elif cmd.startswith(&quot;INCR&quot;):            key = cmd.split()[1]            self.data[key] = str(int(self.data.get(key, 0)) + 1)        defrewrite(self):        &quot;&quot;&quot;生成优化后的命令集&quot;&quot;&quot;        optimized = []        for key, value inself.data.items():            optimized.append(f&quot;SET &#123;key&#125; &#123;value&#125;&quot;)        return optimized    # 示例：优化前后对比rewriter = AOFRewriter()original_commands = [    &quot;SET counter 0&quot;,    &quot;INCR counter&quot;,    &quot;INCR counter&quot;,    &quot;INCR counter&quot;,    &quot;SET name redis&quot;,    &quot;SET name Redis6.0&quot;]for cmd in original_commands:    rewriter.record_command(cmd)print(f&quot;原始命令数: &#123;len(original_commands)&#125;&quot;)print(f&quot;优化后命令数: &#123;len(rewriter.rewrite())&#125;&quot;)print(f&quot;压缩率: &#123;(1 - len(rewriter.rewrite())/len(original_commands))*100:.1f&#125;%&quot;)\n\n3.3 AOF 的三种同步策略对比#!/bin/bash# 性能测试脚本：对比不同fsync策略echo&quot;测试环境准备...&quot;redis-cli FLUSHDB &gt; /dev/nullstrategies=(&quot;always&quot;&quot;everysec&quot;&quot;no&quot;)for strategy in&quot;$&#123;strategies[@]&#125;&quot;; do    echo&quot;测试 appendfsync = $strategy&quot;    redis-cli CONFIG SET appendfsync $strategy &gt; /dev/null        # 使用redis-benchmark测试    result=$(redis-benchmark -t set -n 100000 -q)    echo&quot;$result&quot; | grep &quot;SET&quot;        # 检查实际持久化情况    sync_count=$(grep -c &quot;sync&quot; /var/log/redis/redis.log | tail -1)    echo&quot;同步次数: $sync_count&quot;    echo&quot;---&quot;done\n\n3.4 AOF 优化实践-- Lua脚本：批量操作优化AOF记录-- 将多个命令合并为一个原子操作，减少AOF条目local prefix = KEYS[1]local count = tonumber(ARGV[1])local value = ARGV[2]local results = &#123;&#125;for i = 1, count do    local key = prefix .. &#x27;:&#x27; .. i    redis.call(&#x27;SET&#x27;, key, value)    table.insert(results, key)endreturn results\n\n四、RDB vs AOF：如何选择？4.1 核心指标对比指标RDBAOF数据安全性较低（可能丢失分钟级数据）高（最多丢失 1 秒数据）恢复速度快（直接加载二进制）慢（需要重放所有命令）文件体积小（压缩后的二进制）大（文本格式命令日志）性能影响周期性 fork 开销持续的磁盘 IO适用场景数据分析、缓存消息队列、计数器\n\n4.2 混合持久化：鱼和熊掌兼得Redis 4.0 引入的混合持久化结合了两者优势：\n# 开启混合持久化aof-use-rdb-preamble yes# 工作原理：# 1. AOF重写时，先生成RDB格式的基础数据# 2. 后续增量命令以AOF格式追加# 3. 恢复时先加载RDB部分，再重放AOF增量\n\n4.3 实战选型决策树def choose_persistence_strategy(requirements):    &quot;&quot;&quot;根据业务需求推荐持久化策略&quot;&quot;&quot;        if requirements[&#x27;data_loss_tolerance&#x27;] &lt;= 1:  # 秒级        if requirements[&#x27;recovery_time&#x27;] &lt;= 60:    # 1分钟内恢复            return&quot;混合持久化 (RDB+AOF)&quot;        else:            return&quot;AOF everysec&quot;        elif requirements[&#x27;data_loss_tolerance&#x27;] &lt;= 300:  # 5分钟        if requirements[&#x27;memory_size&#x27;] &gt;= 32:  # GB            return&quot;RDB + 从库AOF&quot;        else:            return&quot;RDB (save 300 10)&quot;        else:  # 可容忍较大数据丢失        return&quot;RDB (save 3600 1)&quot;# 示例：电商订单缓存order_cache_req = &#123;    &#x27;data_loss_tolerance&#x27;: 60,  # 可容忍60秒数据丢失    &#x27;recovery_time&#x27;: 30,        # 要求30秒内恢复    &#x27;memory_size&#x27;: 16           # 16GB内存&#125;print(f&quot;推荐方案: &#123;choose_persistence_strategy(order_cache_req)&#125;&quot;)\n\n五、生产环境最佳实践5.1 监控告警体系# 持久化监控指标采集import redisimport timefrom datetime import datetimeclassPersistenceMonitor:    def__init__(self, redis_client):        self.redis = redis_client        self.alert_thresholds = &#123;            &#x27;rdb_last_save_delay&#x27;: 3600,     # RDB超过1小时未保存            &#x27;aof_rewrite_delay&#x27;: 7200,       # AOF超过2小时未重写            &#x27;aof_size_mb&#x27;: 1024,             # AOF文件超过1GB            &#x27;fork_time_ms&#x27;: 1000             # fork时间超过1秒        &#125;        defcheck_health(self):        &quot;&quot;&quot;健康检查并返回告警&quot;&quot;&quot;        alerts = []        info = self.redis.info(&#x27;persistence&#x27;)                # 检查RDB状态        last_save_delay = time.time() - info[&#x27;rdb_last_save_time&#x27;]        if last_save_delay &gt; self.alert_thresholds[&#x27;rdb_last_save_delay&#x27;]:            alerts.append(&#123;                &#x27;level&#x27;: &#x27;WARNING&#x27;,                &#x27;message&#x27;: f&#x27;RDB已&#123;last_save_delay/3600:.1f&#125;小时未保存&#x27;            &#125;)                # 检查AOF大小        if info.get(&#x27;aof_enabled&#x27;):            aof_size_mb = info[&#x27;aof_current_size&#x27;] / 1024 / 1024            if aof_size_mb &gt; self.alert_thresholds[&#x27;aof_size_mb&#x27;]:                alerts.append(&#123;                    &#x27;level&#x27;: &#x27;WARNING&#x27;,                     &#x27;message&#x27;: f&#x27;AOF文件过大: &#123;aof_size_mb:.1f&#125;MB&#x27;                &#125;)                return alerts# 使用示例monitor = PersistenceMonitor(redis.Redis())alerts = monitor.check_health()for alert in alerts:    print(f&quot;[&#123;alert[&#x27;level&#x27;]&#125;] &#123;alert[&#x27;message&#x27;]&#125;&quot;)\n\n5.2 备份恢复演练#!/bin/bash# 自动化备份恢复测试脚本REDIS_HOST=&quot;localhost&quot;REDIS_PORT=&quot;6379&quot;BACKUP_DIR=&quot;/data/redis-backup&quot;TEST_KEY=&quot;backup:test:$(date +%s)&quot;# 1. 写入测试数据echo&quot;写入测试数据...&quot;redis-cli SET $TEST_KEY&quot;test_value&quot; EX 3600# 2. 执行备份echo&quot;执行BGSAVE...&quot;redis-cli BGSAVEsleep 5# 3. 备份文件cp /var/lib/redis/dump.rdb $BACKUP_DIR/dump_$(date +%Y%m%d_%H%M%S).rdb# 4. 模拟数据丢失redis-cli DEL $TEST_KEY# 5. 恢复数据echo&quot;停止Redis...&quot;systemctl stop redisecho&quot;恢复备份...&quot;cp$BACKUP_DIR/dump_*.rdb /var/lib/redis/dump.rdbecho&quot;启动Redis...&quot;systemctl start redis# 6. 验证恢复if redis-cli GET $TEST_KEY | grep -q &quot;test_value&quot;; then    echo&quot;✓ 备份恢复成功&quot;else    echo&quot;✗ 备份恢复失败&quot;    exit 1fi\n\n5.3 容量规划与优化# 持久化容量评估工具classPersistenceCapacityPlanner:    def__init__(self, daily_writes, avg_key_size, avg_value_size):        self.daily_writes = daily_writes        self.avg_key_size = avg_key_size        self.avg_value_size = avg_value_size        defestimate_aof_growth(self, days=30):        &quot;&quot;&quot;估算AOF文件增长&quot;&quot;&quot;        # 每条命令约占用: SET key value\\r\\n        cmd_size = 6 + self.avg_key_size + self.avg_value_size        daily_growth_mb = (self.daily_writes * cmd_size) / 1024 / 1024                # 考虑重写压缩率约60%        after_rewrite = daily_growth_mb * 0.4                return &#123;            &#x27;daily_growth_mb&#x27;: daily_growth_mb,            &#x27;monthly_size_mb&#x27;: after_rewrite * days,            &#x27;recommended_rewrite_size_mb&#x27;: daily_growth_mb * 2        &#125;        defestimate_rdb_size(self, total_keys):        &quot;&quot;&quot;估算RDB文件大小&quot;&quot;&quot;        # RDB压缩率通常在30-50%        raw_size = total_keys * (self.avg_key_size + self.avg_value_size)        compressed_size_mb = (raw_size * 0.4) / 1024 / 1024                return &#123;            &#x27;estimated_size_mb&#x27;: compressed_size_mb,            &#x27;backup_time_estimate_sec&#x27;: compressed_size_mb / 100# 假设100MB/s        &#125;# 使用示例planner = PersistenceCapacityPlanner(    daily_writes=10_000_000,  # 日写入1000万次    avg_key_size=20,    avg_value_size=100)aof_estimate = planner.estimate_aof_growth()print(f&quot;AOF日增长: &#123;aof_estimate[&#x27;daily_growth_mb&#x27;]:.1f&#125;MB&quot;)print(f&quot;建议重写阈值: &#123;aof_estimate[&#x27;recommended_rewrite_size_mb&#x27;]:.1f&#125;MB&quot;)\n\n六、踩坑经验与故障案例6.1 案例一：fork 阻塞导致的雪崩问题描述：32GB 内存的 Redis 实例，执行 BGSAVE 时主线程阻塞 3 秒，导致大量请求超时。\n根因分析：\n\n• Linux 的 fork 采用 COW（写时复制）机制\n\n• 需要复制页表，32GB 约需要 64MB 页表\n\n• 在内存压力大时，分配页表内存耗时增加\n\n\n解决方案：\n# 1. 开启大页内存，减少页表项echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled# 2. 调整内核参数sysctl -w vm.overcommit_memory=1# 3. 错峰执行持久化redis-cli CONFIG SET save &quot;&quot;  # 禁用自动RDB# 通过crontab在业务低峰期手动触发0 3 * * * redis-cli BGSAVE\n\n6.2 案例二：AOF 重写死循环问题描述：AOF 文件达到 5GB 后触发重写，但重写期间新增数据量大于重写压缩量，导致重写永远无法完成。\n解决方案：\n-- 限流脚本：重写期间降低写入速度local current = redis.call(&#x27;INFO&#x27;, &#x27;persistence&#x27;)if string.match(current, &#x27;aof_rewrite_in_progress:1&#x27;) then    -- AOF重写中，限制写入    local key = KEYS[1]    local limit = tonumber(ARGV[1])    local current_qps = redis.call(&#x27;INCR&#x27;, &#x27;qps_counter&#x27;)        if current_qps &gt; limit then        return &#123;err = &#x27;系统繁忙，请稍后重试&#x27;&#125;    endend-- 正常执行业务逻辑return redis.call(&#x27;SET&#x27;, KEYS[1], ARGV[2])\n\n6.3 案例三：混合持久化的版本兼容问题问题描述：从 Redis 5.0 降级到 4.0 时，无法识别混合格式的 AOF 文件。\n预防措施：\n# 版本兼容性检查工具import structdefcheck_aof_format(filepath):    &quot;&quot;&quot;检查AOF文件格式&quot;&quot;&quot;    withopen(filepath, &#x27;rb&#x27;) as f:        header = f.read(9)                if header.startswith(b&#x27;REDIS&#x27;):            # RDB格式头部            version = struct.unpack(&#x27;bbbbbbbb&#x27;, header[5:])            returnf&quot;混合格式 (RDB v&#123;version&#125;)&quot;        elif header.startswith(b&#x27;*&#x27;):            # 纯AOF格式            return&quot;纯AOF格式&quot;        else:            return&quot;未知格式&quot;# 迁移前检查aof_format = check_aof_format(&#x27;/var/lib/redis/appendonly.aof&#x27;)print(f&quot;当前AOF格式: &#123;aof_format&#125;&quot;)if&quot;混合&quot;in aof_format:    print(&quot;警告: 目标版本可能不支持混合格式，建议先执行BGREWRITEAOF&quot;)\n\n七、性能调优实战7.1 基准测试与调优#!/bin/bash# 持久化性能基准测试echo&quot;=== 持久化性能基准测试 ===&quot;# 测试1: 无持久化redis-cli CONFIG SET save &quot;&quot;redis-cli CONFIG SET appendonly noecho&quot;场景1: 无持久化&quot;redis-benchmark -t set,get -n 1000000 -q# 测试2: 仅RDBredis-cli CONFIG SET save &quot;60 1000&quot;redis-cli CONFIG SET appendonly noecho&quot;场景2: 仅RDB&quot;redis-benchmark -t set,get -n 1000000 -q# 测试3: 仅AOF (everysec)redis-cli CONFIG SET save &quot;&quot;redis-cli CONFIG SET appendonly yesredis-cli CONFIG SET appendfsync everysececho&quot;场景3: AOF everysec&quot;redis-benchmark -t set,get -n 1000000 -q# 测试4: RDB+AOFredis-cli CONFIG SET save &quot;60 1000&quot;redis-cli CONFIG SET appendonly yesecho&quot;场景4: RDB+AOF&quot;redis-benchmark -t set,get -n 1000000 -q\n\n7.2 持久化与内存优化# 内存碎片与持久化关系分析defanalyze_memory_fragmentation(redis_client):    &quot;&quot;&quot;分析内存碎片对持久化的影响&quot;&quot;&quot;    info = redis_client.info(&#x27;memory&#x27;)        fragmentation_ratio = info[&#x27;mem_fragmentation_ratio&#x27;]    used_memory_gb = info[&#x27;used_memory&#x27;] / 1024 / 1024 / 1024        recommendations = []        if fragmentation_ratio &gt; 1.5:        recommendations.append(&#123;            &#x27;issue&#x27;: &#x27;内存碎片率过高&#x27;,            &#x27;impact&#x27;: f&#x27;RDB文件可能增大&#123;(fragmentation_ratio-1)*100:.1f&#125;%&#x27;,            &#x27;solution&#x27;: &#x27;考虑执行内存整理: MEMORY PURGE&#x27;        &#125;)        if used_memory_gb &gt; 16and fragmentation_ratio &gt; 1.2:        fork_time_estimate = used_memory_gb * 100# ms        recommendations.append(&#123;            &#x27;issue&#x27;: &#x27;大内存+高碎片&#x27;,            &#x27;impact&#x27;: f&#x27;fork预计阻塞&#123;fork_time_estimate:.0f&#125;ms&#x27;,            &#x27;solution&#x27;: &#x27;建议使用主从架构，在从节点执行持久化&#x27;        &#125;)        return recommendations\n\n八、未来展望与新特性8.1 Redis 7.0 的持久化改进Redis 7.0 带来了多项持久化优化：\n\n1. 增量 RDB 快照：只保存变更的数据页，大幅减少 IO\n\n2. AOF 时间戳记录：支持按时间点恢复 (PITR)\n\n3. 多线程持久化：利用多核 CPU 加速 RDB 生成\n\n\n8.2 云原生时代的持久化策略在 Kubernetes 环境下，持久化策略需要重新思考：\n# Redis StatefulSet with 持久化配置apiVersion:apps/v1kind:StatefulSetmetadata:name:redis-clusterspec:volumeClaimTemplates:-metadata:      name:redis-data    spec:      accessModes: [&quot;ReadWriteOnce&quot;]      storageClassName:&quot;fast-ssd&quot;      resources:        requests:          storage:100Gitemplate:    spec:      containers:      -name:redis        image:redis:7.0        volumeMounts:        -name:redis-data          mountPath:/data        command:        -redis-server        ---save9001        ---appendonlyyes        ---appendfsync everysec\n\n结语：持久化的平衡艺术Redis 持久化不是非黑即白的选择题，而是需要根据业务特点精心权衡的平衡艺术。记住这几个核心原则：\n\n1. 没有银弹：RDB 快但可能丢数据，AOF 安全但恢复慢\n\n2. 监控先行：建立完善的监控体系，及时发现问题\n\n3. 演练常态化：定期进行故障演练，验证恢复流程\n\n4. 与时俱进：关注 Redis 新版本特性，适时升级优化\n\n\n最后，回到文章开头的生产事故，我们最终采用了混合持久化 + 主从架构的方案，将 RTO 从 4 小时缩短到 5 分钟，RPO 从 6 小时缩短到 1 秒。技术选型没有对错，只有适合与否。\n","categories":["中间件","Redis"],"tags":["Redis","2025"]},{"title":"GoLang Wails 框架详解：用 Web 技术构建桌面应用","url":"/2025/2025-09-18_GoLang%20Wails%20%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3%EF%BC%9A%E7%94%A8%20Web%20%E6%8A%80%E6%9C%AF%E6%9E%84%E5%BB%BA%E6%A1%8C%E9%9D%A2%E5%BA%94%E7%94%A8/","content":"\nWails 是一个允许您使用 Go 和 Web 技术构建桌面应用程序的框架。它结合了 Go 语言的强大后端能力与现代 Web 界面的灵活性，帮助开发者快捷地创建轻量级、原生感强的跨平台桌面应用。\n\n传统的桌面应用开发通常需要学习特定的 GUI 框架（如 Qt, Electron, WPF&#x2F;WinForms 等），这对于 Web 开发者来说学习曲线陡峭。Electron 虽然解决了 Web 技术栈的问题，但其应用体积庞大、内存占用高，且集成了 Node.js 运行时，额外增加了依赖。Wails 则提供了一种优雅的解决方案：它使用原生 WebView 渲染界面，后端逻辑全部由 Go 语言编写，实现了轻量级、高性能和原生体验的桌面应用。\n\n\n一、Wails 简介与核心优势Wails 的核心理念是：用 Go 语言编写应用后端（业务逻辑），用 Web 前端技术（HTML, CSS, JavaScript）构建应用界面（UI）。它将 Go 程序和基于 Webview 的前端巧妙地结合在一起，实现两者之间的双向通信。\nWails 的核心优势：\n\n原生 Webview 渲染：不捆绑 Chromium 运行时（像 Electron 那样），而是利用操作系统提供的原生 Webview 控件（如 Windows 上的 WebView2&#x2F;EdgeHTML, macOS 上的 WebKit, Linux 上的 WebKitGTK&#x2F;WebView2 ）。\n体积小巧：最终应用程序包大小显著小于 Electron 应用。\n内存占用低：原生 Webview 通常比嵌入式 Chromium 更节省内存。\n原生体验：UI 渲染性能接近原生，集成了系统级功能。\n\n\n高性能 Go 后端：所有业务逻辑都在 Go 运行时中执行，充分利用 Go 语言的并发优势和高性能特性。\n双向通信：Go 后端可以直接调用前端 JavaScript 函数，前端 JavaScript 也可以直接调用 Go 后端方法，实现无缝交互。\n跨平台：一次编写，多处运行，支持 Windows、macOS 和 Linux。\n易于集成前端框架：支持 Vue, React, Angular, Svelte 等任何前端框架。\n编译为单个可执行文件：部署简单，无需额外依赖 (除了原生 Webview，通常系统自带或易于安装)。\n\n二、Wails 工作原理Wails 的工作原理可以概括为以下几点：\n\nWebview 嵌入：Wails 创建一个 Go 语言进程，并在该进程中启动一个原生 Webview 控件。这个 Webview 控件负责渲染你的前端 Web 代码（HTML, CSS, JavaScript）。\n文件服务：在应用程序启动时，Wails 会将你编译后的前端项目打包或作为静态资源嵌入到 Go 可执行文件中。Go 后端会运行一个小型文件服务器，将这些前端资源提供给 Webview 控件。\nJavaScript 绑定：Wails 在 Webview 的 JavaScript 全局对象上注入了一个 window.wails 对象（或其他名称），该对象包含了与 Go 后端通信的方法。\nGo 方法注册：Go 后端通过 Wails SDK 注册需要暴露给前端调用的 Go 方法。\n通信桥接：\nJS 调用 Go：当前端 JavaScript 调用 window.wails.Call(&quot;YourGoMethod&quot;, ...args) 时，Wails 会将该调用请求序列化，通过内部的通信桥接（通常是基于 Webview 的原生通信机制，如 dom.bind 等）传递给 Go 后端。Go 后端解析请求，执行对应的 Go 方法，并将结果返回给前端 JS。\nGo 调用 JS：Go 后端可以通过 Wails 的运行时 API runtime.EventsEmit 或 runtime.Callback 直接向前端发送事件或调用 JS 函数。\n\n\n最小化依赖：Go 应用编译成单一可执行文件，减少了外部依赖。唯一需要的系统依赖是对应平台的 WebView 运行时。\n\n三、开发环境准备3.1 安装 Go 语言确保你的系统已安装 Go 1.18 或更高版本。\ngo version\n\n3.2 安装 Wails CLIWails 提供了命令行工具 wails 来创建、运行和构建项目。\ngo install github.com/wailsapp/wails/v2/cmd/wails@latest\n\n安装完成后，验证是否成功：\nwails doctor\nwails doctor 会检查你的系统环境是否满足 Wails 的开发和构建要求，并提示缺少哪些依赖。根据提示安装缺少的依赖（例如在 Windows 上安装 WebView2 Runtime 和 C++ Build Tools，在 Linux 上安装 WebKitGTK 及其开发库等）。\n3.3 Node.js &#x2F; NPM (可选，取决于你的前端技术栈)如果你使用 Vue, React 等现代前端框架，可能需要安装 Node.js 和 npm&#x2F;yarn 来管理和构建前端项目。\n四、创建你的第一个 Wails 项目使用 wails init 命令创建新项目：\nwails init -n MyWailsApp -t vanilla\n\n\n-n MyWailsApp：指定项目名称为 MyWailsApp。\n-t vanilla：指定前端模板为 vanilla (原生 JS&#x2F;HTML&#x2F;CSS)。Wails 也支持 vue, react, svelte, angular 等模板。\n\n这会在当前目录创建一个名为 MyWailsApp 的文件夹，包含 Wails 项目的基本结构。\n项目结构概览MyWailsApp/├── wails.json              # Wails 项目配置文件├── main.go                 # Go 后端主入口文件├── go.mod                  # Go 模块文件├── frontend/               # 前端项目目录│   ├── src/                # 前端源码│   │   ├── main.js│   │   └── style.css│   │   └── index.html│   └── package.json        # 前端依赖管理 (如果使用 npm/yarn)│   └── ...                 # 其他前端文件├── build/                  # 构建目录 (Wails 自动生成)│   ├── appicon.png│   └── ...└── app.go                  # Go 应用逻辑文件 (Wails 自动生成)\n\n五、开发流程5.1 Go 后端逻辑 (app.go)app.go 文件包含了你的 Go 应用程序的核心逻辑，它会作为前端可调用的方法被 Wails 自动绑定。\npackage mainimport (\t&quot;context&quot;\t&quot;fmt&quot;)// App structtype App struct &#123;\tctx context.Context&#125;// NewApp creates a new App application structfunc NewApp() *App &#123;\treturn &amp;App&#123;&#125;&#125;// Startup is called when the app starts. The context is saved// so we can call the runtime methodsfunc (a *App) Startup(ctx context.Context) &#123;\ta.ctx = ctx&#125;// Greet returns a greeting for the given namefunc (a *App) Greet(name string) string &#123;\treturn fmt.Sprintf(&quot;Hello %s, Go is awesome!&quot;, name)&#125;// SumNumbers sums two numbersfunc (a *App) SumNumbers(a, b int) int &#123;\treturn a + b&#125;\n\n\nApp 结构体：定义了你的应用对象。\nStartup(ctx context.Context)：当应用启动时被调用，你可以保存 context 以便后续使用 Wails runtime 方法（如事件发送）。\nGreet(name string) string 和 SumNumbers(a, b int) int：这些都是暴露给前端的 Go 方法。Wails 会自动将它们注册到前端 window.wails 对象上。注意：方法名首字母需大写才能被前端调用。\n\n5.2 前端界面 (frontend/src/main.js 和 frontend/src/index.html)前端的 main.js 文件将通过 window.go.main.App.Greet 等方式调用 Go 方法。\n&lt;!-- frontend/src/index.html --&gt;&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;    &lt;meta charset=&quot;UTF-8&quot;&gt;    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;    &lt;title&gt;My Wails App&lt;/title&gt;    &lt;link rel=&quot;stylesheet&quot; href=&quot;./style.css&quot;&gt;&lt;/head&gt;&lt;body&gt;    &lt;div id=&quot;app&quot;&gt;        &lt;h1&gt;Welcome to Wails!&lt;/h1&gt;        &lt;input id=&quot;nameInput&quot; type=&quot;text&quot; placeholder=&quot;Enter your name...&quot;&gt;        &lt;button onclick=&quot;greet()&quot;&gt;Greet&lt;/button&gt;        &lt;p id=&quot;greetingOutput&quot;&gt;&lt;/p&gt;        &lt;h2&gt;Sum two numbers&lt;/h2&gt;        &lt;input id=&quot;num1Input&quot; type=&quot;number&quot; value=&quot;10&quot;&gt;        &lt;input id=&quot;num2Input&quot; type=&quot;number&quot; value=&quot;20&quot;&gt;        &lt;button onclick=&quot;sum()&quot;&gt;Sum&lt;/button&gt;        &lt;p id=&quot;sumOutput&quot;&gt;&lt;/p&gt;    &lt;/div&gt;    &lt;script src=&quot;./main.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;\n\n// frontend/src/main.jsimport &#123; main &#125; from &quot;../wailsjs/go/models&quot;; // 导入Go的模型（类型定义）import &#123; App &#125; from &quot;../wailsjs/go/main&quot;; // 导入Go后端方法document.addEventListener(&#x27;DOMContentLoaded&#x27;, () =&gt; &#123;    // 监听 Go 方法的调用    window.onload = function() &#123;        console.log(&quot;Wails has loaded!&quot;);    &#125;;&#125;);async function greet() &#123;    const nameInput = document.getElementById(&#x27;nameInput&#x27;);    const greetingOutput = document.getElementById(&#x27;greetingOutput&#x27;);    const name = nameInput.value;    if (name) &#123;        // 调用 Go 后端的 App 结构体中的 Greet 方法        const result = await App.Greet(name);        greetingOutput.textContent = result;    &#125; else &#123;        greetingOutput.textContent = &quot;Please enter a name.&quot;;    &#125;&#125;async function sum() &#123;    const num1Input = document.getElementById(&#x27;num1Input&#x27;);    const num2Input = document.getElementById(&#x27;num2Input&#x27;);    const sumOutput = document.getElementById(&#x27;sumOutput&#x27;);    const num1 = parseInt(num1Input.value);    const num2 = parseInt(num2Input.value);    // 调用 Go 后端的 App 结构体中的 SumNumbers 方法    const result = await App.SumNumbers(num1, num2);    sumOutput.textContent = `Sum: $&#123;result&#125;`;&#125;// 暴露出函数以便在 HTML 中通过 onclick 调用window.greet = greet;window.sum = sum;\n注意：\n\n../wailsjs/go/main 和 ../wailsjs/go/models 是 Wails 自动生成的 Go 后端方法和类型定义的 JavaScript 绑定文件。这些文件在 wails dev 或 wails build 时会自动生成&#x2F;更新。\n你需要将函数暴露出到 window 对象，才能在 index.html 的 onclick 中直接引用。或者使用更现代的前端框架来管理事件。\n\n5.3 运行应用程序在项目根目录执行：\nwails dev\nwails dev 会启动一个开发服务器，自动编译 Go 代码，并在一个新窗口中打开你的应用。每次保存 Go 代码或前端代码时，它都会自动热重载，方便调试。\n六、Wails 双向通信机制详解Wails 提供强大的双向通信能力，是其核心亮点之一。\n6.1 前端调用 Go (JS -&gt; Go)这是最常见的模式，前端通过 JavaScript 调用 Go 后端的逻辑。\n\n调用方式：通过 Wails 自动生成的 window.go.&lt;packageName&gt;.&lt;StructName&gt;.&lt;MethodName&gt;(...args)\n例子：window.go.main.App.Greet(&quot;World&quot;) (如果你的 App 结构体在 main 包中)\n推荐方式 (JS Module)：如上例，先 import &#123; App &#125; from &quot;../wailsjs/go/main&quot;;，然后 App.Greet(&quot;World&quot;)。\n\n\n参数类型：Go 方法可以接受基本类型、结构体、切片、Map 等作为参数。Wails 会自动进行 JSON 序列化&#x2F;反序列化。\n返回值：Go 方法可以返回任何可序列化的 Go 类型。\n\n6.2 Go 调用前端 (Go -&gt; JS)Go 后端可以通过 Wails Runtime API 向前端发送事件或执行 JS 代码。\n6.2.1 发送事件 (推荐)Go 后端向前端广播事件，前端监听事件并触发响应。这是更解耦、优雅的通信方式。\nGo 代码 (app.go):\npackage mainimport (\t&quot;context&quot;\t&quot;fmt&quot;\t&quot;time&quot;\t&quot;github.com/wailsapp/wails/v2/pkg/runtime&quot;)type App struct &#123;\tctx context.Context&#125;// ... Startup 方法省略 ...// SendMessageToFrontend sends a message to the frontend every secondfunc (a *App) StartSendingMessages() &#123;\tgo func() &#123;\t\tfor i := 0; i &lt; 5; i++ &#123;\t\t\tmsg := fmt.Sprintf(&quot;Message from Go: %d&quot;, i)\t\t\t// 发布事件\t\t\truntime.EventsEmit(a.ctx, &quot;myMessage&quot;, msg) // &quot;myMessage&quot; 是事件名, msg 是数据\t\t\ttime.Sleep(time.Second)\t\t&#125;\t\truntime.EventsEmit(a.ctx, &quot;myMessage&quot;, &quot;Go has finished sending messages!&quot;)\t&#125;()&#125;\n\n前端 JS (main.js):\n// ... (之前的代码)document.addEventListener(&#x27;DOMContentLoaded&#x27;, () =&gt; &#123;    // ... (之前的代码)    // 监听 Go 后端发送的事件    window.runtime.EventsOn(&quot;myMessage&quot;, (message) =&gt; &#123;        console.log(&quot;Received from Go:&quot;, message);        const eventOutput = document.createElement(&#x27;p&#x27;);        eventOutput.textContent = `Event from Go: $&#123;message&#125;`;        document.getElementById(&#x27;app&#x27;).appendChild(eventOutput);    &#125;);    // 启动 Go 后端发送消息的函数    App.StartSendingMessages();&#125;);\n\nruntime.EventsEmit(ctx, eventName, data)：在 Go 后端发送事件。\nwindow.runtime.EventsOn(eventName, callback)：在前端 JS 监听事件。\n\n6.2.2 执行 JavaScript (慎用)Go 后端可以执行任意的 JavaScript 代码。\nGo 代码 (某个 Go 方法中):\nruntime.ExecJS(a.ctx, &quot;alert(&#x27;Hello from Go backend in JavaScript!&#x27;);&quot;)\n\n前端 JS: 无需额外代码，直接执行。\n考量：\n\n优点：直接、灵活。\n缺点：耦合度高，不易维护，可能导致安全问题 (应避免执行不可信的 JS)。\n推荐：除非特定场景，尽量使用事件通信。\n\n七、构建与部署当你的应用开发完成后，可以使用 wails build 命令进行构建。\nwails build\n\n这会在 build/bin 目录下生成一个独立的、特定于当前操作系统的可执行文件。\n常用构建选项：\n\nwails build -r：构建 release 版本（优化、减小体积），默认包含调试信息。\nwails build --clean：在构建前清理缓存。\nwails build --upx：使用 UPX 压缩可执行文件（需要先安装 UPX）。\nwails build --platform windows/amd64：交叉编译到指定平台。\nwails build --platform windows/amd64,linux/amd64：交叉编译到多个平台。\n\n注意事项：\n\nWindows：确保系统中安装了 WebView2 Runtime (Edge Chromium)。Windows 10&#x2F;11 通常预装；旧版本可能需要手动安装。\nmacOS：通常无需额外依赖。\nLinux：依赖 WebKitGTK 或 WebView2。你需要确保目标系统安装了 webkit2gtk 或类似的包。例如在 Ubuntu&#x2F;Debian 上：sudo apt install webkit2gtk-4.0。\n\n八、Wails 配置文件 (wails.json)wails.json 文件是 Wails 项目的配置中心，你可以自定义应用名称、图标、窗口大小、Frontend 命令等。\n&#123;  &quot;$schema&quot;: &quot;https://wails.io/schemas/wails.json&quot;,  &quot;name&quot;: &quot;MyWailsApp&quot;,  &quot;outputfilename&quot;: &quot;mywailsapp&quot;,  &quot;frontend:install&quot;: &quot;npm install&quot;,  &quot;frontend:build&quot;: &quot;npm run build&quot;,  &quot;frontend:dev&quot;: &quot;npm run dev&quot;,  &quot;frontend:dir&quot;: &quot;frontend&quot;,  &quot;wailsjsdir&quot;: &quot;./frontend/wailsjs&quot;,  &quot;author&quot;: &#123;    &quot;name&quot;: &quot;Your Name&quot;,    &quot;email&quot;: &quot;you@example.com&quot;  &#125;,  &quot;info&quot;: &#123;    &quot;productName&quot;: &quot;My Awesome Wails App&quot;  &#125;,  &quot;options&quot;: &#123;    &quot;bindings&quot;: &#123;      &quot;css&quot;: &#123;        &quot;output&quot;: &quot;&quot;      &#125;,      &quot;typescript&quot;: &#123;          &quot;output&quot;: &quot;&quot;      &#125;    &#125;,    &quot;appicon&quot;: &quot;build/appicon.png&quot;,    &quot;devtools&quot;: &#123;      &quot;enabled&quot;: true    &#125;,    &quot;window&quot;: &#123;      &quot;width&quot;: 1024,      &quot;height&quot;: 768,      &quot;resizable&quot;: true,      &quot;frameless&quot;: false,      &quot;sizefixed&quot;: false,      &quot;fullscreen&quot;: false,      &quot;alwaysOnTop&quot;: false,      &quot;backgroundType&quot;: &quot;opaque&quot;,      &quot;minimisable&quot;: true,      &quot;maximisable&quot;: true    &#125;  &#125;&#125;\n\nfrontend:install, frontend:build, frontend:dev：自定义前端项目的安装、构建和开发命令。如果你使用 npm, yarn, pnpm 或其他构建工具，可以在这里配置。\nfrontend:dir：前端项目源代码的目录。\nwailsjsdir：Wails 自动生成的 JS 绑定文件的输出目录。\n\n九、其他实用特性\n上下文菜单： Wails 允许你自定义右键上下文菜单。\n通知：支持系统级的通知。\nDialogs：文件选择、消息提示等系统原生对话框。\nDark Mode (深色模式)：Wails 可以感知系统深色模式设置，方便前端适配。\n应用图标和构建设置：通过 wails.json 和 build/ 目录进行配置。\n\n十、总结Wails 框架为 Go 开发者提供了一个强大而新颖的桌面应用开发体验。它巧妙地结合了 Go 的后端性能与 Web 的前端灵活性，同时避免了 Electron 的体积和内存开销。如果你是 Go 开发者，又希望利用现代 Web 技术构建跨平台的桌面应用，Wails 绝对是一个值得你投入学习和使用的优秀选择。\n通过简洁的 API、高效的双向通信和轻量级的原生 Webview，Wails 使得创建美观、高性能的桌面应用变得前所未有的简单。开始你的 Wails 之旅，用 Go 语言和 Web 技术，探索桌面应用的无限可能吧！\n","categories":["桌面开发"],"tags":["前端技术","Golang","2025","Wails","桌面开发"]},{"title":"WSL2详解：在Windows运行Linux的新标准","url":"/2025/2025-09-22_WSL2%E8%AF%A6%E8%A7%A3%EF%BC%9A%E5%9C%A8Windows%E8%BF%90%E8%A1%8CLinux%E7%9A%84%E6%96%B0%E6%A0%87%E5%87%86/","content":"\nWSL 2 (Windows Subsystem for Linux 2) 是微软对 WSL 架构的重大革新，它提供了一个运行在轻量级虚拟机中的完整 Linux 内核。相较于其前身 WSL 1，WSL 2 实现了更强的 Linux 系统调用兼容性、显著提升的文件系统性能，并为 Docker Desktop 等需要原生 Linux 内核的工具提供了无缝集成。WSL 2 已经成为在 Windows 上进行 Linux 开发体验的新标准。\n\n“WSL 2 从根本上改变了 Windows 上的 Linux 体验，它提供了一个真正的 Linux 内核，这意味着你可以在 Windows 上运行更多原生的 Linux 应用和工具。”\n\n\n一、WSL 2 的核心：轻量级虚拟机与真实 Linux 内核1.1 与 WSL 1 的根本区别WSL 2 的核心在于采用了轻量级虚拟机 (VM) 的架构，而不是像 WSL 1 那样通过系统调用翻译层。\n\n\n\n特性\nWSL 1\nWSL 2\n\n\n\n底层架构\n系统调用翻译层（无虚拟机）\n基于 Hyper-V 的轻量级虚拟机，运行真实 Linux 内核\n\n\nLinux 内核\n无，Windows NT 内核模拟\n有，微软定制的 Linux 4.19 (或更高)\n\n\n系统调用兼容性\n中等，部分应用（如 Docker）无法运行\n极高，几乎 100% 兼容，可运行 Docker、Fuse 等\n\n\nLinux 文件系统性能\n较差（在 /home 等 Linux 内部路径）\n极佳（在 /home 等 Linux 内部路径，与原生 Linux 相当）\n\n\nWindows 文件系统性能\n极佳（在 /mnt/c 等 Windows 挂载点）\n略逊于 WSL 1，但在 \\\\wsl$\\... 路径下性能良好\n\n\n内存管理\n共享 Windows 内存，占用低\n动态分配，启动时占用低，可按需增长，并在不使用时自动释放回 Windows（自 Win 10 2004 版本）\n\n\n网络模式\n共享主机 IP\n独立的虚拟 IP 地址，默认 NAT 模式\n\n\n适用场景\n轻量级脚本、简单命令行工具\n所有 Linux 开发场景，包括 Docker、Kubernetes、Web&#x2F;AI&#x2F;ML 开发等\n\n\n1.2 工作原理概览\nHyper-V 平台：WSL 2 利用 Windows 内置的 Hyper-V 虚拟化技术，但其管理方式远比传统的 Hyper-V VM 更轻量和自动化。\n精简 Linux 内核：微软维护并分发一个优化的 Linux 内核（通常基于最新稳定版），专门用于 WSL 2。这个内核被放置在一个 VHD (Virtual Hard Disk) 文件中，并由 Hyper-V VM 运行。\nVHD 文件：每个 WSL 2 发行版都有一个独立的 VHD 文件（通常位于 C:\\Users\\&lt;YourUser&gt;\\AppData\\Local\\Packages\\&lt;DistroName&gt;\\LocalState），其中包含其文件系统。\n动态资源分配：WSL 2 虚拟机不会占用固定的大量 RAM。它会根据需要动态分配内存和 CPU 资源，并在你关闭所有 WSL 实例后自动释放大部分资源。\n\n二、WSL 2 的安装与基本操作 (快速指南)2.1 安装要求\nWindows 10 版本 2004 (Build 19041) 或更高版本，或 Windows 11。\n主板 BIOS&#x2F;UEFI 中启用虚拟化技术（如 Intel VT-x &#x2F; AMD-V）。\n\n2.2 推荐安装方式 (Windows 11 或较新 Win 10)只需一条命令（以管理员身份运行 PowerShell 或 CMD）：\nwsl --install\n这条命令将自动：\n\n安装 WSL 所需的 Windows 可选组件。\n下载并安装最新的 WSL 2 Linux 内核。\n默认安装 Ubuntu 发行版。\n设置 WSL 2 为默认版本。\n首次启动 Ubuntu 并提示创建用户。\n\n2.3 手动安装或升级现有发行版到 WSL 2如果已安装 WSL 1 或需要特定步骤，可以：\n\n确保已启用“适用于 Linux 的 Windows 子系统”和“虚拟机平台”：\ndism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestartdism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n重启计算机。\n\n下载并安装 WSL 2 内核更新包：前往 微软官方文档 下载并运行 wsl_update_x64.msi。\n\n将 WSL 2 设置为默认版本：\nwsl --set-default-version 2\n\n将现有发行版转换为 WSL 2：\nwsl --set-version &lt;DistroName&gt; 2\n例如：wsl --set-version Ubuntu-22.04 2。此过程可能需要几分钟。\n\n\n2.4 WSL 常用管理命令\nwsl -l -v：列出所有已安装的发行版、其状态和 WSL 版本。\nwsl --shutdown：停止所有运行中的发行版虚拟机。\nwsl --terminate &lt;DistroName&gt;：停止指定发行版。\nwsl --unregister &lt;DistroName&gt;：卸载并删除指定发行版的所有数据。\n\n三、WSL 2 的核心优势与应用场景3.1 极高的 Linux 系统调用兼容性这是 WSL 2 最重要的优势。由于运行的是真实 Linux 内核，WSL 2 支持所有 Linux 内核功能，这意味着你可以运行此前在 WSL 1 中无法工作的应用程序：\n\nDocker Desktop：完美集成，无需 Hyper-V VM，直接在 WSL 2 后端运行 Linux 容器。\nKubernetes：通过 Docker Desktop 的 Kubernetes 集成，或直接在 WSL 2 中安装 K3s&#x2F;Minikube 等轻量级 K8s 发行版。\nFUSE 文件系统：如 SSHFS, FUSE 驱动的文件系统。\n低级网络工具：如 tcpdump, wireshark。\n更多 Linux 发行版：可以运行更多依赖特定内核特性的 Linux 发行版。\n安全性：某些安全工具或渗透测试工具需要更完整的 Linux 内核特性。\n\n3.2 卓越的 Linux 文件系统性能如果你经常在 WSL 内部进行编译、Git 操作、大型项目文件处理，WSL 2 在其 Linux 文件系统 (Ext4) 内部的性能几乎与原生 Linux 持平。\n\n最佳实践：将你的开发项目克隆到 WSL 内部（例如 /home/user/projects），而不是通过 /mnt/c/ 访问 Windows 目录。在 WSL 内部对这些文件进行操作将获得最佳性能。\n\n3.3 无缝的图形化应用程序支持 (WSLg)自 Windows 11 开始，WSLg (WSL Graphical Architecture) 成为了 WSL 2 的内置功能，极大地提升了 WSL 的可用性。\n\n工作原理：WSLg 包含了一个轻量级的 Wayland&#x2F;X Server、PulseAudio Server 和必要的驱动，通过 RemoteFX 技术在 Windows 桌面无缝运行 Linux GUI 应用。\n使用方式：在 WSL 命令行中直接运行你安装的 Linux GUI 应用（例如 firefox、gimp、code、pycharm），它们会像原生 Windows 应用一样以独立的窗口启动。\n优势：\n可以在 Windows 上使用 Linux 专属的 IDE、开发工具、浏览器、图形设计软件等。\n在开发环境下进行更真实的测试，无需额外的虚拟机或双启动。\n\n\n\n3.4 与 Windows 工具链的深度集成WSL 2 除了提供独立的 Linux 环境，还保持了与 Windows 的良好互操作性。\n\nVS Code Remote Development：最佳开发体验。在 Windows 上运行 VS Code，但其所有开发工作都在 WSL 2 内部进行。\n命令行互操作：\n从 Windows CMD&#x2F;PowerShell 运行 Linux 命令：wsl &lt;command&gt;。\n从 Linux Bash 运行 Windows 命令：explorer.exe .（在当前 Linux 路径打开 Windows 文件管理器），cmd.exe，notepad.exe 等。\n\n\n网络访问：\n通过 localhost 访问 WSL 内部运行的服务（Windows 自动进行端口转发）。\n从 WSL 访问 Windows 的服务（例如 --host 192.168.X.X 指向 Windows 主机 IP）。\n从外部访问 WSL 服务通常需要手动进行端口转发 (netsh interface portproxy ...)。\n\n\n\n四、WSL 2 开发工作流示例4.1 全栈 Web 开发 (React, Node.js, Python, Go 等)\n安装 WSL 2 (Ubuntu 22.04 LTS)。\n在 WSL 内部安装 Node.js&#x2F;NVM, Python&#x2F;Pyenv, GoLang, Git 等开发工具链。\n在 WSL 内部克隆你的项目到 /home/user/my-project。\n在 VS Code 中安装 Remote - WSL 扩展。\n在 WSL 终端中进入项目目录，运行 code .，VS Code 会自动连接并打开项目。\n在 VS Code 终端中运行 npm install 或 pip install，然后 npm start 或 python app.py 启动开发服务器。\n在 Windows 浏览器中访问 http://localhost:&lt;port&gt;。\n\n4.2 Docker&#x2F;Kubernetes 开发\n安装 Docker Desktop for Windows，并确保其配置为使用 WSL 2 后端。\n在 WSL 内部，你可以像在原生 Linux 中一样使用 docker 和 docker-compose 命令。Docker Desktop 会自动将这些命令代理到 WSL 2 宿主机。\n构建、运行、管理容器，甚至部署本地 Kubernetes 集群 (minikube 或 Docker Desktop 内置的 K8s)。\n\n五、高级配置与优化5.1 .wslconfig 文件这是一个全局配置文件，位于 C:\\Users\\&lt;你的用户名&gt;\\.wslconfig。可以用来限制 WSL 2 虚拟机的资源。\n[wsl2]memory=4GB         # 限制 WSL 2 虚拟机的总内存为 4GB。默认是 Windows 主机内存的 50%。processors=2       # 限制 WSL 2 虚拟机使用的 CPU 核心数为 2。默认是所有核心。swap=2GB           # 设置虚拟机的交换空间大小。默认是内存的 25% 或 16GB。localhostForwarding=true # 允许 localhost 转发，默认开启。\n保存后，需要运行 wsl --shutdown 然后重新启动 WSL 发行版才能生效。\n5.2 磁盘空间管理\nWSL 2 的 VHD 文件会动态增长。\n压缩 VHD 文件：当 WSL 发行版占用磁盘空间过大时，可以对 VHD 文件进行压缩。\n停止所有 WSL 实例：wsl --shutdown。\n打开 PowerShell (管理员身份)\n运行 diskpart。\n在 DISKPART&gt; 提示符下：\nselect vdisk file=&quot;&lt;PathToVHDFile&gt;&quot; （路径在 wsl -l -v 的 Location 字段中）\ncompact vdisk\nexit\n\n\n\n\n\n5.3 网络配置与端口转发由于 WSL 2 的默认 NAT 网络模式，从 Windows 外部访问 WSL 内部服务需要端口转发。\n\n永久端口转发 (PowerShell 管理员)：# 获取 WSL 2 默认网关 IP (通常是 172.xx.xx.1)$wsl_gateway = (Get-NetIPAddress -AddressFamily IPv4 -PrefixLength 20 | Where-Object &#123; $_.InterfaceAlias -like &quot;vEthernet (WSL)*&quot; &#125;).IPAddress.ToString()# 获取你的 WSL 2 实例 IP$wsl_ip = (wsl -d Ubuntu-22.04 hostname -I).Trim() # 替换为你的发行版名称# 添加端口转发规则 (例如将 Windows 的 8000 转发到 WSL 的 8000)netsh interface portproxy add v4tov4 listenport=8000 listenaddress=0.0.0.0 connectaddress=$wsl_ip connectport=8000\n防火墙规则：确保 Windows 防火墙允许入站连接到你转发的端口。\n\n5.4 Dotfiles 管理使用 Git 来管理 .bashrc, .zshrc, .gitconfig 等配置文件，方便在不同 WSL 实例或机器上同步你的 Linux 环境。\n六、总结WSL 2 彻底改变了 Windows 上的 Linux 开发范式，它不再是一个简单的兼容层，而是一个全功能的、高度集成的轻量级 Linux 虚拟机。其卓越的系统调用兼容性、文件系统性能、原生 Docker 支持以及突破性的 WSLg 功能，使其成为现代 Windows 开发者不可或缺的利器。通过理解其底层工作原理和掌握高级配置技巧，你可以充分发挥 WSL 2 的潜力，构建一个高效、灵活且强大的开发环境，真正实现 Windows 和 Linux 的优势互补。\n","categories":["开发工具","虚拟机"],"tags":["2025","WSL2","Linux","虚拟机"]}]