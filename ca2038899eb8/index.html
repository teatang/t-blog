<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Scrapy (Python Web 爬虫框架) 深度解析 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Scrapy 是一个用 Python 编写的开源且功能强大的 Web 爬虫框架，它被设计用于快速、高效地从网站上提取结构化数据。Scrapy 不仅提供了完整的爬虫生命周期管理，包括请求调度、并发控制、数据解析和持久化，还通过其高度模块化的架构，允许开发者轻松扩展和定制爬虫行为。  核心思想：将 Web 爬取视为一个事件驱动的流程，通过异步 I&#x2F;O (基于 Twisted) 实现高并发，">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy (Python Web 爬虫框架) 深度解析">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/ca2038899eb8/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="Scrapy 是一个用 Python 编写的开源且功能强大的 Web 爬虫框架，它被设计用于快速、高效地从网站上提取结构化数据。Scrapy 不仅提供了完整的爬虫生命周期管理，包括请求调度、并发控制、数据解析和持久化，还通过其高度模块化的架构，允许开发者轻松扩展和定制爬虫行为。  核心思想：将 Web 爬取视为一个事件驱动的流程，通过异步 I&#x2F;O (基于 Twisted) 实现高并发，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-25.jpg">
<meta property="article:published_time" content="2025-03-26T22:24:00.000Z">
<meta property="article:modified_time" content="2025-11-28T08:07:24.407Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-25.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Scrapy (Python Web 爬虫框架) 深度解析",
  "url": "https://blog.tbf1211.xx.kg/ca2038899eb8/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-25.jpg",
  "datePublished": "2025-03-26T22:24:00.000Z",
  "dateModified": "2025-11-28T08:07:24.407Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/ca2038899eb8/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Scrapy (Python Web 爬虫框架) 深度解析',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">327</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">202</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">71</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-25.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">Scrapy (Python Web 爬虫框架) 深度解析</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy (Python Web 爬虫框架) 深度解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-03-26T22:24:00.000Z" title="发表于 2025-03-27 06:24:00">2025-03-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/">Python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Python/%E5%BA%93/">库</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/ca2038899eb8/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>Scrapy</strong> 是一个用 Python 编写的开源且功能强大的 Web 爬虫框架，它被设计用于快速、高效地从网站上提取结构化数据。Scrapy 不仅提供了完整的爬虫生命周期管理，包括请求调度、并发控制、数据解析和持久化，还通过其高度模块化的架构，允许开发者轻松扩展和定制爬虫行为。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将 Web 爬取视为一个事件驱动的流程，通过异步 I&#x2F;O (基于 Twisted) 实现高并发，并提供一套可插拔的组件，以便开发者专注于数据提取逻辑。</strong></p>
</div>
<hr>
<h2 id="一、为什么需要-Scrapy？"><a href="#一、为什么需要-Scrapy？" class="headerlink" title="一、为什么需要 Scrapy？"></a>一、为什么需要 Scrapy？</h2><p>在数据驱动的时代，从 Web 获取大量结构化信息的需求日益增长。虽然我们可以使用 <code>requests</code> 库发送 HTTP 请求并结合 <code>BeautifulSoup</code> 或 <code>lxml</code> 等库解析 HTML，但当面临以下挑战时，手动编写爬虫会变得复杂且低效：</p>
<ol>
<li><strong>并发与效率</strong>：需要同时发送大量请求以提高爬取速度，手动管理并发、线程或协程将非常繁琐。</li>
<li><strong>请求调度与去重</strong>：爬虫需要跟踪哪些 URL 已访问、哪些待访问，并避免重复请求，这需要复杂的调度逻辑。</li>
<li><strong>中间件处理</strong>：处理 User-Agent 轮换、代理 IP、Cookie 管理、重试机制、请求限速等功能时，需要一个统一的机制。</li>
<li><strong>数据清洗与持久化</strong>：将提取到的数据进行清洗、验证，并存储到数据库、文件或其他存储介质时，需要结构化的处理流程。</li>
<li><strong>可维护性与扩展性</strong>：随着爬虫规则的增加或网站结构的变化，代码需要易于修改和扩展。</li>
</ol>
<p>Scrapy 旨在解决这些问题，提供一个<strong>端到端</strong>的爬虫解决方案，让开发者能够专注于<strong>如何提取数据</strong>，而不是<strong>如何实现爬虫底层机制</strong>。</p>
<h2 id="二、Scrapy-架构与核心组件"><a href="#二、Scrapy-架构与核心组件" class="headerlink" title="二、Scrapy 架构与核心组件"></a>二、Scrapy 架构与核心组件</h2><p>Scrapy 的架构是高度模块化的，基于一个事件驱动的异步 I&#x2F;O 引擎 (Scrapy Engine)，其核心组件协同工作，构成一个完整的爬虫生命周期。</p>
<h3 id="2-1-架构概览"><a href="#2-1-架构概览" class="headerlink" title="2.1 架构概览"></a>2.1 架构概览</h3><div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    subgraph &quot;Scrapy Engine (核心)&quot;
        Engine
    end

    subgraph 请求&#x2F;响应处理
        Downloader[下载器]
        DownloaderMiddleware[下载器中间件]
        SpiderMiddleware[爬虫中间件]
    end

    subgraph 数据处理
        Spider[爬虫]
        Item[数据模型]
        ItemPipeline[项目管道]
    end

    subgraph 请求调度
        Scheduler[调度器]
    end

    User(用户&#x2F;Start URLs) --&gt; Engine
    Engine -- 请求 (Request) --&gt; Scheduler
    Scheduler -- 待爬取请求 --&gt; DownloaderMiddleware
    DownloaderMiddleware -- 修改请求 --&gt; Downloader
    Downloader -- 下载页面 (返回 Response) --&gt; DownloaderMiddleware
    DownloaderMiddleware -- 修改响应 --&gt; Engine
    Engine -- 响应 (Response) --&gt; SpiderMiddleware
    SpiderMiddleware -- 修改响应&#x2F;生成新请求 --&gt; Spider
    Spider -- 解析数据 (生成 Item) &#x2F; 生成新请求 --&gt; Engine

    Spider -- 生成 Item --&gt; ItemPipeline
    ItemPipeline -- 处理&#x2F;存储 Item --&gt; Storage(数据库&#x2F;文件等)

    Spider -- 生成 Request --&gt; Engine
  </pre></div>

<h3 id="2-2-核心组件定义"><a href="#2-2-核心组件定义" class="headerlink" title="2.2 核心组件定义"></a>2.2 核心组件定义</h3><ol>
<li><p><strong>Scrapy Engine (Scrapy 引擎)</strong>：</p>
<ul>
<li><strong>定义</strong>：Scrapy 的核心，负责控制所有组件之间的数据流，并根据事件驱动的机制触发组件的动作。它就像爬虫的“大脑”，协调各个部分的工作。</li>
<li><strong>职责</strong>：处理请求与响应，将任务分发给调度器、下载器和爬虫，以及将 Item 传递给 Item Pipeline。</li>
</ul>
</li>
<li><p><strong>Scheduler (调度器)</strong>：</p>
<ul>
<li><strong>定义</strong>：负责接收 Scrapy Engine 发送的请求，并将其放入队列，等待 Downloader 获取。它也负责请求的去重。</li>
<li><strong>职责</strong>：存储和管理待抓取的 URL 队列，确保请求按优先级顺序发送，并过滤掉重复的请求。</li>
</ul>
</li>
<li><p><strong>Downloader (下载器)</strong>：</p>
<ul>
<li><strong>定义</strong>：负责执行所有网络请求，获取网页内容并返回给 Scrapy Engine。</li>
<li><strong>职责</strong>：通过 HTTP(S) 协议下载页面，处理重定向、代理、User-Agent 等。</li>
</ul>
</li>
<li><p><strong>Spiders (爬虫)</strong>：</p>
<ul>
<li><strong>定义</strong>：开发者自定义的类，负责定义如何抓取特定网站，包括初始请求、如何跟随链接以及如何从页面中提取结构化数据 (Items)。</li>
<li><strong>职责</strong>：生成初始请求，接收 Downloader 返回的响应，并解析响应以提取数据或生成新的请求。</li>
</ul>
</li>
<li><p><strong>Item (数据模型)</strong>：</p>
<ul>
<li><strong>定义</strong>：用于存储从网页中提取的数据的简单容器。它类似于 Python 字典，但提供了额外的基于声明的字段定义。</li>
<li><strong>职责</strong>：定义数据的结构，使得数据在爬虫、中间件和管道之间以统一的格式传递。</li>
</ul>
</li>
<li><p><strong>Item Pipelines (项目管道)</strong>：</p>
<ul>
<li><strong>定义</strong>：用于处理 Spider 提取的 Item。它们是一系列独立的组件，按顺序处理 Item。</li>
<li><strong>职责</strong>：清洗、验证、持久化 (存储到数据库、文件等) Item 数据。</li>
</ul>
</li>
<li><p><strong>Downloader Middleware (下载器中间件)</strong>：</p>
<ul>
<li><strong>定义</strong>：介于 Scrapy Engine 和 Downloader 之间的钩子框架。</li>
<li><strong>职责</strong>：在请求发送给 Downloader 之前或响应返回给 Scrapy Engine 之前，修改请求或响应。例如，设置代理、User-Agent 轮换、处理 Cookie、实现重试机制等。</li>
</ul>
</li>
<li><p><strong>Spider Middleware (爬虫中间件)</strong>：</p>
<ul>
<li><strong>定义</strong>：介于 Scrapy Engine 和 Spiders 之间的钩子框架。</li>
<li><strong>职责</strong>：在响应发送给 Spider 之前或 Spider 处理请求返回结果之后，修改请求或 Item。例如，处理 Spider 的输入&#x2F;输出、过滤重复数据等。</li>
</ul>
</li>
</ol>
<h2 id="三、Scrapy-项目结构与核心概念实战"><a href="#三、Scrapy-项目结构与核心概念实战" class="headerlink" title="三、Scrapy 项目结构与核心概念实战"></a>三、Scrapy 项目结构与核心概念实战</h2><h3 id="3-1-创建-Scrapy-项目"><a href="#3-1-创建-Scrapy-项目" class="headerlink" title="3.1 创建 Scrapy 项目"></a>3.1 创建 Scrapy 项目</h3><p>首先，安装 Scrapy：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy</span><br></pre></td></tr></table></figure>

<p>然后，创建一个新的 Scrapy 项目：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br></pre></td></tr></table></figure>
<p>这会生成以下目录结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">myproject/</span><br><span class="line">├── scrapy.cfg          # 项目配置文件</span><br><span class="line">├── myproject/          # 项目的 Python 模块</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py        # Item 定义文件</span><br><span class="line">│   ├── middlewares.py  # Spider 和 Downloader 中间件定义文件</span><br><span class="line">│   ├── pipelines.py    # Item Pipeline 定义文件</span><br><span class="line">│   ├── settings.py     # 项目设置文件</span><br><span class="line">│   └── spiders/        # 存放爬虫的目录</span><br><span class="line">│       └── __init__.py</span><br></pre></td></tr></table></figure>

<h3 id="3-2-Item-数据模型"><a href="#3-2-Item-数据模型" class="headerlink" title="3.2 Item (数据模型)"></a>3.2 Item (数据模型)</h3><p>在 <code>myproject/items.py</code> 中定义你想要提取的数据结构。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myproject/items.py</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Product</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># 定义字段名称</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    price = scrapy.Field()</span><br><span class="line">    category = scrapy.Field()</span><br><span class="line">    description = scrapy.Field()</span><br><span class="line">    <span class="comment"># 可以定义更多的字段，例如 URL、图片等</span></span><br><span class="line">    url = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h3 id="3-3-Spider-爬虫"><a href="#3-3-Spider-爬虫" class="headerlink" title="3.3 Spider (爬虫)"></a>3.3 Spider (爬虫)</h3><p>在 <code>myproject/spiders/</code> 目录下创建一个新的爬虫文件，例如 <code>quotes_spider.py</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myproject/spiders/quotes_spider.py</span></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product <span class="comment"># 导入之前定义的 Item</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">QuotesSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;quotes&quot;</span> <span class="comment"># 爬虫的唯一名称</span></span><br><span class="line">    start_urls = [  <span class="comment"># 爬虫开始抓取的 URL 列表</span></span><br><span class="line">        <span class="string">&quot;https://quotes.toscrape.com/page/1/&quot;</span>,</span><br><span class="line">        <span class="string">&quot;https://quotes.toscrape.com/page/2/&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># 这是一个默认的回调函数，用于处理从 start_urls 下载的响应</span></span><br><span class="line">        <span class="comment"># response 对象包含了下载的页面内容</span></span><br><span class="line">        <span class="comment"># 使用 CSS 选择器或 XPath 提取数据</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 示例：提取每条引言的文本、作者和标签</span></span><br><span class="line">        <span class="keyword">for</span> quote <span class="keyword">in</span> response.css(<span class="string">&#x27;div.quote&#x27;</span>):</span><br><span class="line">            item = Product() <span class="comment"># 实例化 Item</span></span><br><span class="line">            item[<span class="string">&#x27;name&#x27;</span>] = quote.css(<span class="string">&#x27;span.text::text&#x27;</span>).get() <span class="comment"># 用 name 字段来存储引言文本</span></span><br><span class="line">            item[<span class="string">&#x27;price&#x27;</span>] = quote.css(<span class="string">&#x27;small.author::text&#x27;</span>).get() <span class="comment"># 用 price 字段来存储作者名</span></span><br><span class="line">            item[<span class="string">&#x27;category&#x27;</span>] = quote.css(<span class="string">&#x27;div.tags a.tag::text&#x27;</span>).getall() <span class="comment"># 用 category 字段存储所有标签</span></span><br><span class="line">            item[<span class="string">&#x27;url&#x27;</span>] = response.url <span class="comment"># 存储当前页面的 URL</span></span><br><span class="line">            <span class="keyword">yield</span> item <span class="comment"># 将提取到的 Item 传递给 Item Pipeline</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查找下一页的链接，并生成新的请求</span></span><br><span class="line">        next_page = response.css(<span class="string">&#x27;li.next a::attr(href)&#x27;</span>).get()</span><br><span class="line">        <span class="keyword">if</span> next_page <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 使用 response.follow() 自动生成完整 URL</span></span><br><span class="line">            <span class="keyword">yield</span> response.follow(next_page, callback=<span class="variable language_">self</span>.parse)</span><br></pre></td></tr></table></figure>

<h3 id="3-4-Item-Pipeline-项目管道"><a href="#3-4-Item-Pipeline-项目管道" class="headerlink" title="3.4 Item Pipeline (项目管道)"></a>3.4 Item Pipeline (项目管道)</h3><p>在 <code>myproject/pipelines.py</code> 中定义 Item Pipeline。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myproject/pipelines.py</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyprojectPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># 在爬虫启动时打开文件</span></span><br><span class="line">        <span class="variable language_">self</span>.file = <span class="built_in">open</span>(<span class="string">&#x27;quotes.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.file.write(<span class="string">&quot;[\n&quot;</span>) <span class="comment"># 写入 JSON 数组的开始</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        <span class="comment"># 在爬虫关闭时关闭文件</span></span><br><span class="line">        <span class="variable language_">self</span>.file.write(<span class="string">&quot;\n]&quot;</span>) <span class="comment"># 写入 JSON 数组的结束</span></span><br><span class="line">        <span class="variable language_">self</span>.file.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        <span class="comment"># 处理每个 Item</span></span><br><span class="line">        line = json.dumps(<span class="built_in">dict</span>(item), ensure_ascii=<span class="literal">False</span>) + <span class="string">&quot;,\n&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.file.write(line)</span><br><span class="line">        <span class="keyword">return</span> item <span class="comment"># 确保 Item 被返回，以便其他管道继续处理 (如果有的话)</span></span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：为了使 Item Pipeline 生效，需要在 <code>myproject/settings.py</code> 中启用它。</p>
<h3 id="3-5-Settings-项目设置"><a href="#3-5-Settings-项目设置" class="headerlink" title="3.5 Settings (项目设置)"></a>3.5 Settings (项目设置)</h3><p>在 <code>myproject/settings.py</code> 中配置爬虫的各种参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myproject/settings.py</span></span><br><span class="line"><span class="comment"># Scrapy settings for myproject project</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">&#x27;myproject&#x27;</span></span><br><span class="line">SPIDER_MODULES = [<span class="string">&#x27;myproject.spiders&#x27;</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">&#x27;myproject.spiders&#x27;</span></span><br><span class="line"></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">True</span> <span class="comment"># 是否遵守 robots.txt 协议，生产环境通常设置为 True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置并发请求数</span></span><br><span class="line">CONCURRENT_REQUESTS = <span class="number">16</span> <span class="comment"># 同时处理的最大请求数 (默认16)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置下载延迟，防止被网站封禁</span></span><br><span class="line">DOWNLOAD_DELAY = <span class="number">1</span> <span class="comment"># 每次请求的最小下载延迟秒数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User-Agent 配置 (可以轮换)</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;myproject (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用 Item Pipeline</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">&#x27;myproject.pipelines.MyprojectPipeline&#x27;</span>: <span class="number">300</span>, <span class="comment"># 数字代表优先级，越小优先级越高</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用下载器中间件 (例如，如果启用了代理或自定义 User-Agent)</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="comment"># &#x27;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#x27;: None, # 禁用内置的 User-Agent</span></span><br><span class="line">    <span class="comment"># &#x27;myproject.middlewares.RandomUserAgentMiddleware&#x27;: 400, # 启用自定义的 User-Agent 中间件</span></span><br><span class="line">    <span class="comment"># &#x27;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#x27;: 1, # 启用代理中间件</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># LOG_LEVEL = &#x27;INFO&#x27; # 设置日志级别，可选 DEBUG, INFO, WARNING, ERROR, CRITICAL</span></span><br><span class="line"><span class="comment"># FEED_FORMAT = &#x27;json&#x27; # 也可以直接通过 FEED_EXPORT_ITEM_FIELDS 控制输出字段</span></span><br><span class="line"><span class="comment"># FEED_URI = &#x27;quotes.json&#x27; # 更简单的输出方式，但控制力不如 Item Pipeline</span></span><br></pre></td></tr></table></figure>

<h3 id="3-6-运行爬虫"><a href="#3-6-运行爬虫" class="headerlink" title="3.6 运行爬虫"></a>3.6 运行爬虫</h3><p>在项目根目录下 (与 <code>scrapy.cfg</code> 同级)，运行：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>
<p>这将启动 <code>quotes</code> 爬虫，并在完成后生成 <code>quotes.json</code> 文件。</p>
<h2 id="四、高级特性与最佳实践"><a href="#四、高级特性与最佳实践" class="headerlink" title="四、高级特性与最佳实践"></a>四、高级特性与最佳实践</h2><h3 id="4-1-中间件-Middleware"><a href="#4-1-中间件-Middleware" class="headerlink" title="4.1 中间件 (Middleware)"></a>4.1 中间件 (Middleware)</h3><p>中间件是 Scrapy 强大的扩展点，用于在请求&#x2F;响应处理流程中插入自定义逻辑。</p>
<p><strong>自定义 Downloader Middleware 示例 (User-Agent 轮换)</strong>：<br>在 <code>myproject/middlewares.py</code> 中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># myproject/middlewares.py</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> signals</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomUserAgentMiddleware</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, user_agents</span>):</span><br><span class="line">        <span class="variable language_">self</span>.user_agents = user_agents</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">from_crawler</span>(<span class="params">cls, crawler</span>):</span><br><span class="line">        <span class="keyword">return</span> cls(crawler.settings.getlist(<span class="string">&#x27;USER_AGENTS&#x27;</span>)) <span class="comment"># 从 settings 获取 User-Agent 列表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):</span><br><span class="line">        user_agent = random.choice(<span class="variable language_">self</span>.user_agents)</span><br><span class="line">        request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = user_agent</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在 settings.py 中配置 USER_AGENTS 和启用此中间件</span></span><br><span class="line"><span class="comment"># USER_AGENTS = [</span></span><br><span class="line"><span class="comment">#     &#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment">#     &#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36&#x27;,</span></span><br><span class="line"><span class="comment">#     # ... 更多 User-Agent</span></span><br><span class="line"><span class="comment"># ]</span></span><br><span class="line"><span class="comment"># DOWNLOADER_MIDDLEWARES = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#x27;: None, # 禁用 Scrapy 内置 User-Agent</span></span><br><span class="line"><span class="comment">#     &#x27;myproject.middlewares.RandomUserAgentMiddleware&#x27;: 400,</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="4-2-避免被封禁的策略"><a href="#4-2-避免被封禁的策略" class="headerlink" title="4.2 避免被封禁的策略"></a>4.2 避免被封禁的策略</h3><ul>
<li><strong><code>DOWNLOAD_DELAY</code></strong>：设置请求之间的延迟。</li>
<li><strong><code>AUTOTHROTTLE_ENABLED = True</code></strong>：自动调节下载延迟，根据网站响应速度智能调整。</li>
<li><strong><code>USER_AGENTS</code> 轮换</strong>：使用 Downloader Middleware 实现，模拟不同浏览器。</li>
<li><strong>代理 IP 池</strong>：使用 Downloader Middleware 实现，通过代理发送请求。</li>
<li><strong><code>CONCURRENT_REQUESTS_PER_DOMAIN</code></strong>：限制单个域名的并发请求数。</li>
<li><strong><code>CONCURRENT_REQUESTS_PER_IP</code></strong>：限制单个 IP 的并发请求数。</li>
<li><strong>处理 Cookie</strong>：Downloader 会自动处理 Cookie，但有时需要手动清理或管理。</li>
</ul>
<h3 id="4-3-错误处理与日志"><a href="#4-3-错误处理与日志" class="headerlink" title="4.3 错误处理与日志"></a>4.3 错误处理与日志</h3><ul>
<li><strong><code>LOG_LEVEL</code></strong>：在 <code>settings.py</code> 中设置日志级别 (DEBUG, INFO, WARNING, ERROR, CRITICAL)。</li>
<li><strong><code>LOG_FILE</code></strong>：将日志输出到文件。</li>
<li><strong>Request 的 <code>errback</code></strong>：在请求失败时（例如 HTTP 错误、DNS 错误），可以指定一个错误回调函数来处理。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在 Spider 中</span></span><br><span class="line"><span class="keyword">yield</span> scrapy.Request(url, callback=<span class="variable language_">self</span>.parse, errback=<span class="variable language_">self</span>.handle_error)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">handle_error</span>(<span class="params">self, failure</span>):</span><br><span class="line">    <span class="comment"># 记录错误或进行重试</span></span><br><span class="line">    <span class="variable language_">self</span>.logger.error(<span class="string">f&quot;Request failed for <span class="subst">&#123;failure.request.url&#125;</span>: <span class="subst">&#123;failure.value&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="4-4-其他实用命令"><a href="#4-4-其他实用命令" class="headerlink" title="4.4 其他实用命令"></a>4.4 其他实用命令</h3><ul>
<li><code>scrapy genspider example example.com</code>：快速生成一个爬虫模板。</li>
<li><code>scrapy shell &lt;url&gt;</code>：启动交互式 Scrapy Shell，用于测试选择器和调试爬虫。</li>
<li><code>scrapy crawl &lt;spider_name&gt; -o output.json</code>：将爬取结果直接输出到文件，无需 Item Pipeline。</li>
</ul>
<h2 id="五、Scrapy-的优缺点"><a href="#五、Scrapy-的优缺点" class="headerlink" title="五、Scrapy 的优缺点"></a>五、Scrapy 的优缺点</h2><h3 id="5-1-优点"><a href="#5-1-优点" class="headerlink" title="5.1 优点"></a>5.1 优点</h3><ol>
<li><strong>高并发与效率</strong>：基于 Twisted 异步网络库，实现高效的并发爬取，吞吐量大。</li>
<li><strong>模块化与可扩展性</strong>：清晰的架构和丰富的扩展点 (中间件、管道、自定义调度器等)，方便定制和维护。</li>
<li><strong>功能丰富</strong>：内置请求调度、去重、Cookie 处理、会话管理、DNS 缓存、自动限速等功能。</li>
<li><strong>易于使用</strong>：提供了强大的选择器 (CSS&#x2F;XPath) 进行数据提取，且有完善的文档和社区支持。</li>
<li><strong>适用性广</strong>：适合爬取大型网站、复杂数据结构和需要长期运行的爬虫项目。</li>
<li><strong>Shell 调试</strong>：<code>scrapy shell</code> 提供了强大的交互式调试环境。</li>
</ol>
<h3 id="5-2-缺点"><a href="#5-2-缺点" class="headerlink" title="5.2 缺点"></a>5.2 缺点</h3><ol>
<li><strong>学习曲线</strong>：对于初学者，其架构和异步机制可能比 <code>requests</code> + <code>BeautifulSoup</code> 更复杂，学习成本略高。</li>
<li><strong>Twisted 依赖</strong>：基于 Twisted，可能在某些环境中安装和调试相对复杂。</li>
<li><strong>不适合简单任务</strong>：对于只需爬取少数几个页面的简单任务，引入 Scrapy 框架可能显得过于“重”。</li>
<li><strong>JavaScript 渲染</strong>：Scrapy 自身不直接支持 JavaScript 渲染。对于大量依赖 JS 动态加载内容的网站，需要集成其他工具 (如 Splash 或 Selenium&#x2F;Playwright)。</li>
</ol>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>Scrapy 是 Python 社区中最成熟、最受欢迎的 Web 爬虫框架之一。它通过其强大的异步架构和高度可插拔的组件，为开发者提供了一套全面的解决方案，以应对现代 Web 爬取所面临的各种挑战。无论您是需要构建一个小型的数据采集脚本，还是一个大规模的分布式爬虫系统，Scrapy 都能提供强大的支持。理解其核心架构、组件职责和灵活的扩展机制，将使您能够高效地构建出稳定、可扩展的 Web 爬虫。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/ca2038899eb8/">https://blog.tbf1211.xx.kg/ca2038899eb8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-25.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/51be17189d42/" title="Selenium (浏览器自动化工具) 深度解析"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Selenium (浏览器自动化工具) 深度解析</div></div><div class="info-2"><div class="info-item-1"> Selenium 是一个功能强大的开源工具集，最初设计用于 Web 应用程序的自动化测试，但其能力远不止于此。它允许开发者像真实用户一样，直接控制浏览器执行各种操作，如点击按钮、填写表单、导航页面等。通过模拟用户与网页的交互，Selenium 成为了处理动态加载内容 (JavaScript 渲染)、实现 Web UI 自动化测试和进行高级网络爬取的关键工具。  核心思想：Selenium 通过 WebDriver API 直接与浏览器进行通信，发送指令并接收浏览器执行结果，从而实现对浏览器的完全控制。 这使得它能够处理任何人类用户可以做到的网页交互。   一、为什么需要 Selenium？传统爬虫的局限性传统的网页爬取工具（如 Python 的 requests + BeautifulSoup 或 Scrapy 框架）非常高效，适用于抓取静态 HTML 页面或 API 返回的结构化数据。然而，面对现代 Web 应用的复杂性时，它们会遇到显著的局限性：  JavaScript 渲染内容：许多网站使用 JavaScript 动态加载内容（AJAX 请求、SPA - Single P...</div></div></div></a><a class="pagination-related" href="/737a469a0dcc/" title="Golang Testify (Go 测试库) 深度解析"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Golang Testify (Go 测试库) 深度解析</div></div><div class="info-2"><div class="info-item-1"> Go Testify (github.com/stretchr/testify) 是 Go 语言中一个功能强大且广泛使用的测试工具集。它在 Go 标准库 testing 的基础上，提供了更富有表现力的断言、灵活的 Mock 框架和便捷的测试套件管理功能，旨在简化 Go 程序的测试编写过程，提高测试代码的可读性和可维护性。  核心思想：将 Go 标准测试包的低级别错误检查提升为高级、语义化的断言，并提供解耦的 Mock 和 Suite 管理机制。 这使得测试代码更清晰、更易于编写和理解。   一、为什么需要 Testify？标准库 testing 的局限性Go 语言标准库的 testing 包提供了基础的测试框架，包括测试运行器、t.Error &#x2F; t.Fail &#x2F; t.Fatalf 等错误报告方法。然而，在实际项目中，纯粹使用 testing 包编写测试可能会遇到一些局限性：  断言冗长：标准库没有内置的断言函数。开发者通常需要手动编写大量的 if/else 语句来比较预期值和实际值，并手动报告错误。例如：123if actual != expected &...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/51be17189d42/" title="Selenium (浏览器自动化工具) 深度解析"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-29</div><div class="info-item-2">Selenium (浏览器自动化工具) 深度解析</div></div><div class="info-2"><div class="info-item-1"> Selenium 是一个功能强大的开源工具集，最初设计用于 Web 应用程序的自动化测试，但其能力远不止于此。它允许开发者像真实用户一样，直接控制浏览器执行各种操作，如点击按钮、填写表单、导航页面等。通过模拟用户与网页的交互，Selenium 成为了处理动态加载内容 (JavaScript 渲染)、实现 Web UI 自动化测试和进行高级网络爬取的关键工具。  核心思想：Selenium 通过 WebDriver API 直接与浏览器进行通信，发送指令并接收浏览器执行结果，从而实现对浏览器的完全控制。 这使得它能够处理任何人类用户可以做到的网页交互。   一、为什么需要 Selenium？传统爬虫的局限性传统的网页爬取工具（如 Python 的 requests + BeautifulSoup 或 Scrapy 框架）非常高效，适用于抓取静态 HTML 页面或 API 返回的结构化数据。然而，面对现代 Web 应用的复杂性时，它们会遇到显著的局限性：  JavaScript 渲染内容：许多网站使用 JavaScript 动态加载内容（AJAX 请求、SPA - Single P...</div></div></div></a><a class="pagination-related" href="/13596be529bf/" title="Python神库Pydantic深度解析：数据验证与设置管理的利器"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-10</div><div class="info-item-2">Python神库Pydantic深度解析：数据验证与设置管理的利器</div></div><div class="info-2"><div class="info-item-1"> Pydantic 是一个 Python 库，用于数据验证和设置管理，它利用 Python 的类型提示 (type hints) 来定义数据模式。Pydantic 在运行时强制执行类型提示，并为您的数据提供友好的错误信息，使得数据模型更加健壮、可维护和自文档化。它广泛应用于 Web API (如 FastAPI)、数据科学、配置管理等领域。  核心思想：将 Python 的类型提示转化为强大的运行时数据验证和序列化工具，从而提高代码的健壮性和开发效率。   一、为什么需要 Pydantic？在现代 Python 应用开发中，数据从外部来源（如 JSON API、数据库、配置文件、用户输入）进入系统是常态。这些外部数据往往不可信，结构复杂且容易出错。传统的 Python 处理方式存在一些问题：  缺乏数据验证：直接使用字典或弱类型对象，无法保证数据的结构和类型正确性，容易导致运行时错误。 手动验证繁琐：编写大量的 if/else 语句进行数据类型检查和值验证，导致代码冗长、难以维护。 序列化&#x2F;反序列化复杂：将 Python 对象转换为 JSON&#x2F;XML 或反之，...</div></div></div></a><a class="pagination-related" href="/cac9fddb0534/" title="Python 3 各版本新特性详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-20.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-19</div><div class="info-item-2">Python 3 各版本新特性详解</div></div><div class="info-2"><div class="info-item-1"> Python 3.x 系列 自 2008 年首次发布以来，一直在持续发展和完善。每个小版本（如 3.6, 3.7, 3.8 等）都会引入一系列新的语言特性、标准库改进、性能优化以及重要的 bug 修复。理解这些新特性对于 Python 开发者来说至关重要，它能帮助我们编写更高效、更简洁、更现代的代码。  核心思想： Python 3 的版本迭代聚焦于提升开发效率、代码可读性、执行性能以及引入现代编程范式，同时保持语言的易用性。   一、Python 3.0 - 3.3：从 2.x 到 3.x 的演变Python 3.0 是一个里程碑式的版本，它引入了许多不兼容的改变，旨在解决 Python 2.x 的设计缺陷并为未来发展铺平道路。 1.1 Python 3.0 (2008-12-03) 字符串和字节分离：str 类型现在是 Unicode 字符串，bytes 类型是原始字节序列。这是最重要的改变，解决了 Python 2.x 中 Unicode 处理的混乱。 print 成为函数：print 语句被 print() 函数取代。 Python 2.x: print &quot;H...</div></div></div></a><a class="pagination-related" href="/92d90a6caba1/" title="Python 项目管理工具 Poetry 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-10</div><div class="info-item-2">Python 项目管理工具 Poetry 详解</div></div><div class="info-2"><div class="info-item-1"> Poetry 是一款现代化的 Python 项目管理和打包工具。它将依赖管理、虚拟环境管理、打包和发布功能集成在一个直观的命令行界面中。Poetry 的核心理念是提供一个统一的、声明式的项目配置方式，以 pyproject.toml 文件 (遵循 PEP 518 和 PEP 621) 作为所有项目元数据和依赖的唯一真实来源。  核心思想：Poetry 旨在通过一个工具，简化 Python 项目从创建到发布的全生命周期管理，确保环境隔离、依赖可重现性和便捷的打包发布流程。   一、为什么需要 Poetry？传统的 Python 项目管理方式通常涉及多个工具和手动步骤，带来了诸多痛点：  pip 和 requirements.txt 的局限性： requirements.txt 仅记录直接依赖，不处理传递性依赖，容易导致环境不一致。 缺乏强大的依赖解析能力，解决包版本冲突困难。 没有统一的元数据管理，项目信息分散在 setup.py、README.md 等文件中。   虚拟环境管理不便： 需要手动创建 venv 或 virtualenv，并手动激活、切换。 项目与虚拟环境的关联不够...</div></div></div></a><a class="pagination-related" href="/62acbead38d8/" title="Python 打包工具 uv 详解：下一代包管理器与构建器"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-12</div><div class="info-item-2">Python 打包工具 uv 详解：下一代包管理器与构建器</div></div><div class="info-2"><div class="info-item-1"> UV 是由 Astral (Ruff 的创建者) 公司开发的一款极速的 Python 包安装器、解析器和虚拟环境管理器。它被设计为 pip、pip-tools 和 virtualenv 的直接替代品，旨在解决 Python 现有包管理工具的速度瓶颈和用户体验问题。UV 完全用 Rust 编写，专注于提供闪电般的性能、一致的行为和可靠的依赖解析，同时兼容 pip 和 virtualenv 的现有工作流。  核心思想：UV 利用 Rust 的高性能和并发能力，提供超快的依赖解析、虚拟环境创建和包安装体验，旨在通过一个统一的、命令与 pip 相似的工具，成为 Python 包管理的新一代标准。   一、为什么需要 UV？Python 的包管理生态系统虽然成熟，但在速度和用户体验方面一直存在痛点：  pip 的速度瓶颈： pip 在解析大型或复杂项目的依赖图时可能会非常缓慢，尤其是在 requirements.txt 中包含大量次级依赖时。 每次安装都需要重新下载已有的包，缺乏高效的缓存机制。   virtualenv &#x2F; venv 的性能： 创建和管理虚拟环境的速度通常不够...</div></div></div></a><a class="pagination-related" href="/53e63dc49a04/" title="PyInstaller 深度解析与指令详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-25</div><div class="info-item-2">PyInstaller 深度解析与指令详解</div></div><div class="info-2"><div class="info-item-1"> PyInstaller 是一个将 Python 应用程序及其所有依赖项（包括 Python 解释器本身、所有第三方库、数据文件等）打包成一个独立的、可执行的二进制文件的工具。其核心目标是简化 Python 应用程序的分发，使得最终用户无需安装 Python 环境或任何依赖即可直接运行程序。  核心思想：将 Python 应用程序及其所有运行时依赖“冻结”为一个独立的软件包，通常是一个可执行文件（.exe、可执行二进制文件等）或一个包含可执行文件和相关资源的目录。   一、为什么需要 PyInstaller？Python 应用程序的部署和分发常常面临以下挑战：  用户环境依赖：最终用户需要安装正确版本的 Python 解释器，并手动安装所有项目所需的第三方库。这对于非技术用户而言门槛较高。 环境差异性：不同操作系统、不同 Python 版本或不同库版本之间的兼容性问题可能导致应用程序在某些环境中无法正常运行。 依赖管理复杂性：应用程序依赖的库可能有很多，手动追踪和安装这些依赖既繁琐又容易出错。 源代码暴露：直接分发 Python 脚本会暴露源代码，这对于商业应用或知识产权保护而言...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">327</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">202</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">71</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-Scrapy%EF%BC%9F"><span class="toc-text">一、为什么需要 Scrapy？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Scrapy-%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-text">二、Scrapy 架构与核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88"><span class="toc-text">2.1 架构概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%AE%9A%E4%B9%89"><span class="toc-text">2.2 核心组件定义</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81Scrapy-%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E5%AE%9E%E6%88%98"><span class="toc-text">三、Scrapy 项目结构与核心概念实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%88%9B%E5%BB%BA-Scrapy-%E9%A1%B9%E7%9B%AE"><span class="toc-text">3.1 创建 Scrapy 项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Item-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 Item (数据模型)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Spider-%E7%88%AC%E8%99%AB"><span class="toc-text">3.3 Spider (爬虫)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Item-Pipeline-%E9%A1%B9%E7%9B%AE%E7%AE%A1%E9%81%93"><span class="toc-text">3.4 Item Pipeline (项目管道)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-Settings-%E9%A1%B9%E7%9B%AE%E8%AE%BE%E7%BD%AE"><span class="toc-text">3.5 Settings (项目设置)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="toc-text">3.6 运行爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-text">四、高级特性与最佳实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%B8%AD%E9%97%B4%E4%BB%B6-Middleware"><span class="toc-text">4.1 中间件 (Middleware)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E9%81%BF%E5%85%8D%E8%A2%AB%E5%B0%81%E7%A6%81%E7%9A%84%E7%AD%96%E7%95%A5"><span class="toc-text">4.2 避免被封禁的策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E9%94%99%E8%AF%AF%E5%A4%84%E7%90%86%E4%B8%8E%E6%97%A5%E5%BF%97"><span class="toc-text">4.3 错误处理与日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%85%B6%E4%BB%96%E5%AE%9E%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-text">4.4 其他实用命令</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Scrapy-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">五、Scrapy 的优缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E4%BC%98%E7%82%B9"><span class="toc-text">5.1 优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E7%BC%BA%E7%82%B9"><span class="toc-text">5.2 缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">六、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/be24ef88e59a/" title="WebRTC 技术详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebRTC 技术详解"/></a><div class="content"><a class="title" href="/be24ef88e59a/" title="WebRTC 技术详解">WebRTC 技术详解</a><time datetime="2025-11-27T22:24:00.000Z" title="发表于 2025-11-28 06:24:00">2025-11-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/53e63dc49a04/" title="PyInstaller 深度解析与指令详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyInstaller 深度解析与指令详解"/></a><div class="content"><a class="title" href="/53e63dc49a04/" title="PyInstaller 深度解析与指令详解">PyInstaller 深度解析与指令详解</a><time datetime="2025-11-24T22:24:00.000Z" title="发表于 2025-11-25 06:24:00">2025-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/61fddda6a7a8/" title="Go 语言 GC (Garbage Collection) 机制详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go 语言 GC (Garbage Collection) 机制详解"/></a><div class="content"><a class="title" href="/61fddda6a7a8/" title="Go 语言 GC (Garbage Collection) 机制详解">Go 语言 GC (Garbage Collection) 机制详解</a><time datetime="2025-11-23T22:24:00.000Z" title="发表于 2025-11-24 06:24:00">2025-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/fae19f12de23/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"/></a><div class="content"><a class="title" href="/fae19f12de23/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析">压缩字典树 (Radix Trie/Patricia Trie) 深度解析</a><time datetime="2025-11-17T22:24:00.000Z" title="发表于 2025-11-18 06:24:00">2025-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/c28be5a597e1/" title="Golang 内存对齐详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang 内存对齐详解"/></a><div class="content"><a class="title" href="/c28be5a597e1/" title="Golang 内存对齐详解">Golang 内存对齐详解</a><time datetime="2025-11-12T22:24:00.000Z" title="发表于 2025-11-13 06:24:00">2025-11-13</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-25.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            pagePV.textContent = data.pageviews.value
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors && typeof data.visitors.value !== 'undefined') {
            siteUV.textContent = data.visitors.value
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            sitePV.textContent = data.pageviews.value
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>