<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLM中相似性与相关性：概念、度量与应用详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="在大型语言模型 (LLM) 和更广泛的自然语言处理 (NLP) 领域中，相似性 (Similarity) 和 相关性 (Relevance) 是两个经常被提及但又有所区别的核心概念。它们都量化了两个文本片段之间的某种关联程度，但在具体含义、度量方法和应用场景上存在微妙但重要的差异。理解这两者的区别与联系，对于构建和优化基于 LLM 的智能系统至关重要。  核心思想：相似性通常指文本内容在语义或结">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM中相似性与相关性：概念、度量与应用详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/d382adface17/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="在大型语言模型 (LLM) 和更广泛的自然语言处理 (NLP) 领域中，相似性 (Similarity) 和 相关性 (Relevance) 是两个经常被提及但又有所区别的核心概念。它们都量化了两个文本片段之间的某种关联程度，但在具体含义、度量方法和应用场景上存在微妙但重要的差异。理解这两者的区别与联系，对于构建和优化基于 LLM 的智能系统至关重要。  核心思想：相似性通常指文本内容在语义或结">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg">
<meta property="article:published_time" content="2025-06-02T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-27T11:54:24.693Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM中相似性与相关性：概念、度量与应用详解",
  "url": "https://blog.tbf1211.xx.kg/d382adface17/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-04.jpg",
  "datePublished": "2025-06-02T22:24:00.000Z",
  "dateModified": "2025-12-27T11:54:24.693Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/d382adface17/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM中相似性与相关性：概念、度量与应用详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">438</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">224</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-04.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">LLM中相似性与相关性：概念、度量与应用详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LLM中相似性与相关性：概念、度量与应用详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-06-02T22:24:00.000Z" title="发表于 2025-06-03 06:24:00">2025-06-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>14分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>在大型语言模型 (LLM) 和更广泛的自然语言处理 (NLP) 领域中，<strong>相似性 (Similarity)</strong> 和 <strong>相关性 (Relevance)</strong> 是两个经常被提及但又有所区别的核心概念。它们都量化了两个文本片段之间的某种关联程度，但在具体含义、度量方法和应用场景上存在微妙但重要的差异。理解这两者的区别与联系，对于构建和优化基于 LLM 的智能系统至关重要。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>相似性通常指文本内容在语义或结构上的“形似”或“意近”，强调固有属性的匹配；而相关性则指文本内容与特定“查询”、“任务”或“上下文”之间的“关联程度”或“有用性”，强调功能性匹配。</strong></p>
</div>
<hr>
<h2 id="一、为什么相似性与相关性在-LLM-中如此重要？"><a href="#一、为什么相似性与相关性在-LLM-中如此重要？" class="headerlink" title="一、为什么相似性与相关性在 LLM 中如此重要？"></a>一、为什么相似性与相关性在 LLM 中如此重要？</h2><p>LLM 通过将文本数据转换为高维向量空间中的数值向量（即<strong>嵌入</strong>），从而能够捕捉词语和文本的复杂语义。这种表示方法使得计算机可以进行超越简单关键词匹配的语义理解。而相似性和相关性正是这种语义理解的两个重要视角：</p>
<ul>
<li><strong>语义理解的基石</strong>：它们让 LLM 能够理解文本的实际含义，而不仅仅是表面文字。</li>
<li><strong>信息检索的核心</strong>：无论是搜索、问答还是推荐，核心都是找出“最相似”或“最相关”的信息。</li>
<li><strong>生成质量的衡量</strong>：评估 LLM 生成的文本与用户意图或参考答案的匹配程度。</li>
<li><strong>知识整合</strong>：在检索增强生成 (RAG) 等应用中，它们指导模型如何有效地从外部知识库中提取有用信息。</li>
</ul>
<h2 id="二、核心概念：嵌入与向量空间"><a href="#二、核心概念：嵌入与向量空间" class="headerlink" title="二、核心概念：嵌入与向量空间"></a>二、核心概念：嵌入与向量空间</h2><p>在探讨相似性和相关性之前，我们必须理解 LLM 如何表示文本。</p>
<h3 id="2-1-嵌入-Embeddings"><a href="#2-1-嵌入-Embeddings" class="headerlink" title="2.1 嵌入 (Embeddings)"></a>2.1 嵌入 (Embeddings)</h3><ul>
<li><strong>定义</strong>: <strong>嵌入</strong>是将高维、离散的文本数据（如词、短语、句子或文档）转换成低维、连续的实数向量的过程。这些向量捕捉了原始文本的语义和句法信息。在向量空间中，语义相似的文本片段，其对应的向量在空间中的距离会更近，而语义不相似的文本则距离较远。</li>
<li><strong>生成方式</strong>: LLM 通过各种神经网络架构（如 Transformer 编码器部分）学习生成嵌入。例如，BERT、Word2Vec、GloVe 等模型都能生成不同粒度的嵌入。</li>
<li><strong>特性</strong>:<ul>
<li><strong>语义丰富性</strong>: 向量的每个维度都捕获了文本在某种抽象特征上的信息。</li>
<li><strong>维度连续性</strong>: 相较于独热编码等稀疏表示，嵌入是密集且连续的。</li>
<li><strong>降维</strong>: 将高维文本特征映射到相对低维的向量空间，便于计算和处理。</li>
</ul>
</li>
</ul>
<h3 id="2-2-向量空间-Vector-Space"><a href="#2-2-向量空间-Vector-Space" class="headerlink" title="2.2 向量空间 (Vector Space)"></a>2.2 向量空间 (Vector Space)</h3><ul>
<li><strong>定义</strong>: 嵌入将文本映射到的数学空间称为<strong>向量空间</strong>。在这个空间中，每个文本片段都被表示为一个点或一个从原点出发的向量。</li>
<li><strong>几何直观</strong>: 我们可以将向量看作带有方向和大小的箭头。相似性或相关性度量就是计算这些箭头之间的几何关系（如夹角或距离）。</li>
</ul>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[文本输入] --&gt; B[LLM Embedding Model];
    B --&gt; C[&quot;高维向量 (Embedding)&quot;];
    C --&gt; D[向量空间中的点&#x2F;向量];

    subgraph 向量空间中的关联性
        D -- 计算几何关系 --&gt; E[相似性&#x2F;相关性分数];
    end
  </pre></div>

<h2 id="三、相似性-Similarity-详解"><a href="#三、相似性-Similarity-详解" class="headerlink" title="三、相似性 (Similarity) 详解"></a>三、相似性 (Similarity) 详解</h2><h3 id="3-1-定义"><a href="#3-1-定义" class="headerlink" title="3.1 定义"></a>3.1 定义</h3><p><strong>相似性</strong>通常指两个文本片段之间在固有语义内容、词汇选择或句法结构上的**“接近程度”**。它回答的是“这两个文本自身有多像？”的问题。</p>
<h3 id="3-2-主要度量方法"><a href="#3-2-主要度量方法" class="headerlink" title="3.2 主要度量方法"></a>3.2 主要度量方法</h3><p>在向量空间中，相似性主要通过计算两个向量之间的几何距离或夹角来度量。</p>
<h4 id="3-2-1-余弦相似度-Cosine-Similarity"><a href="#3-2-1-余弦相似度-Cosine-Similarity" class="headerlink" title="3.2.1 余弦相似度 (Cosine Similarity)"></a>3.2.1 余弦相似度 (Cosine Similarity)</h4><ul>
<li><strong>定义</strong>: 余弦相似度衡量了两个非零向量之间夹角的余弦值。它的值域在 -1 到 1 之间。<ul>
<li>1 表示两个向量方向完全相同（极度相似）。</li>
<li>0 表示两个向量相互正交（不相关）。</li>
<li>-1 表示两个向量方向完全相反（极度不相似）。</li>
</ul>
</li>
<li><strong>计算公式</strong>:<br>对于两个向量 <code>A</code> 和 <code>B</code>，它们的余弦相似度计算如下：<br>$$<br>\text{similarity} &#x3D; \cos(\theta) &#x3D; \frac{A \cdot B}{||A|| \cdot ||B||} &#x3D; \frac{\sum_{i&#x3D;1}^{n} A_i B_i}{\sqrt{\sum_{i&#x3D;1}^{n} A_i^2} \sqrt{\sum_{i&#x3D;1}^{n} B_i^2}}<br>$$</li>
<li><strong>优点</strong>: 与向量长度无关，只关注方向，非常适合文本嵌入，因为文本长度可能不同但含义相似。</li>
<li><strong>Go 语言代码示例</strong>:</li>
</ul>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;math&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// cosineSimilarity calculates the cosine similarity between two vectors.</span></span><br><span class="line"><span class="comment">// Returns -2.0 for error conditions (e.g., vectors of different lengths or zero vectors where similarity is ambiguous).</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">cosineSimilarity</span><span class="params">(vec1, vec2 []<span class="type">float64</span>)</span></span> <span class="type">float64</span> &#123;</span><br><span class="line">	<span class="keyword">if</span> <span class="built_in">len</span>(vec1) != <span class="built_in">len</span>(vec2) &#123;</span><br><span class="line">		<span class="comment">// fmt.Println(&quot;Error: Vectors must have the same dimension.&quot;)</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">-2.0</span> <span class="comment">// Sentinel for error</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> dotProduct <span class="type">float64</span></span><br><span class="line">	<span class="keyword">var</span> normVec1Sq <span class="type">float64</span> <span class="comment">// Squared norm for optimization</span></span><br><span class="line">	<span class="keyword">var</span> normVec2Sq <span class="type">float64</span> <span class="comment">// Squared norm for optimization</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="built_in">len</span>(vec1); i++ &#123;</span><br><span class="line">		dotProduct += vec1[i] * vec2[i]</span><br><span class="line">		normVec1Sq += vec1[i] * vec1[i]</span><br><span class="line">		normVec2Sq += vec2[i] * vec2[i]</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	normVec1 := math.Sqrt(normVec1Sq)</span><br><span class="line">	normVec2 := math.Sqrt(normVec2Sq)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> normVec1 == <span class="number">0</span> || normVec2 == <span class="number">0</span> &#123;</span><br><span class="line">		<span class="comment">// handle zero vectors:</span></span><br><span class="line">		<span class="comment">// if both are zero, they are considered perfectly similar (1.0)</span></span><br><span class="line">		<span class="comment">// if one is zero, typically undefined, we return -2.0 as an error.</span></span><br><span class="line">		<span class="keyword">if</span> normVec1 == <span class="number">0</span> &amp;&amp; normVec2 == <span class="number">0</span> &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="number">1.0</span> <span class="comment">// Or 0.0, depending on convention. Here, same &#x27;direction&#x27; conceptually.</span></span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// fmt.Println(&quot;Error: One or both vectors are zero vectors, similarity undefined.&quot;)</span></span><br><span class="line">		<span class="keyword">return</span> <span class="number">-2.0</span> <span class="comment">// Sentinel for error</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> dotProduct / (normVec1 * normVec2)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	fmt.Println(<span class="string">&quot;--- Cosine Similarity Examples ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Example 1: Highly similar (semantic synonyms)</span></span><br><span class="line">	<span class="comment">// Embeddings of &quot;cat&quot; and &quot;kitten&quot; might be very close.</span></span><br><span class="line">	vecCat := []<span class="type">float64</span>&#123;<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.1</span>&#125;</span><br><span class="line">	vecKitten := []<span class="type">float64</span>&#123;<span class="number">0.7</span>, <span class="number">0.55</span>, <span class="number">0.15</span>&#125;</span><br><span class="line">	simCK := cosineSimilarity(vecCat, vecKitten)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Similarity(&#x27;cat&#x27;, &#x27;kitten&#x27;): %.4f (Expected: High)\n&quot;</span>, simCK)</span><br><span class="line">	<span class="comment">// Example 2: Less similar (related but distinct)</span></span><br><span class="line">	<span class="comment">// Embeddings of &quot;cat&quot; and &quot;dog&quot; might be somewhat close but not identical.</span></span><br><span class="line">	vecDog := []<span class="type">float64</span>&#123;<span class="number">0.6</span>, <span class="number">0.7</span>, <span class="number">0.2</span>&#125;</span><br><span class="line">	simCD := cosineSimilarity(vecCat, vecDog)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Similarity(&#x27;cat&#x27;, &#x27;dog&#x27;): %.4f (Expected: Moderate)\n&quot;</span>, simCD)</span><br><span class="line">	<span class="comment">// Example 3: Dissimilar</span></span><br><span class="line">	<span class="comment">// Embeddings of &quot;cat&quot; and &quot;car&quot; should be quite far apart.</span></span><br><span class="line">	vecCar := []<span class="type">float64</span>&#123;<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.9</span>&#125;</span><br><span class="line">	simCCar := cosineSimilarity(vecCat, vecCar)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Similarity(&#x27;cat&#x27;, &#x27;car&#x27;): %.4f (Expected: Low)\n&quot;</span>, simCCar)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Example 4: Same content (perfect similarity)</span></span><br><span class="line">	vecHello1 := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">	vecHello2 := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">	simHH := cosineSimilarity(vecHello1, vecHello2)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Similarity(&#x27;hello&#x27;, &#x27;hello&#x27;): %.4f (Expected: 1.0)\n&quot;</span>, simHH)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Example 5: Opposite directions (distinct meaning)</span></span><br><span class="line">	vecOpposite1 := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">0.0</span>&#125;</span><br><span class="line">	vecOpposite2 := []<span class="type">float64</span>&#123;<span class="number">-1.0</span>, <span class="number">0.0</span>&#125;</span><br><span class="line">	simOpp := cosineSimilarity(vecOpposite1, vecOpposite2)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Similarity(&#x27;good&#x27;, &#x27;bad&#x27; - theoretical opposite vectors): %.4f (Expected: -1.0)\n&quot;</span>, simOpp)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Error examples</span></span><br><span class="line">	fmt.Println(<span class="string">&quot;\n--- Error Handling Examples ---&quot;</span>)</span><br><span class="line">	vecErr1 := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">2.0</span>&#125;</span><br><span class="line">	vecErr2 := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>&#125;</span><br><span class="line">	simErrDim := cosineSimilarity(vecErr1, vecErr2)</span><br><span class="line">	<span class="keyword">if</span> simErrDim == <span class="number">-2.0</span> &#123;</span><br><span class="line">		fmt.Println(<span class="string">&quot;Similarity (different dimensions): Error handled.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	vecZero := []<span class="type">float64</span>&#123;<span class="number">0.0</span>, <span class="number">0.0</span>&#125;</span><br><span class="line">	vecNonZero := []<span class="type">float64</span>&#123;<span class="number">1.0</span>, <span class="number">1.0</span>&#125;</span><br><span class="line">	simZeroNonZero := cosineSimilarity(vecZero, vecNonZero)</span><br><span class="line">	<span class="keyword">if</span> simZeroNonZero == <span class="number">-2.0</span> &#123;</span><br><span class="line">		fmt.Println(<span class="string">&quot;Similarity (one zero vector): Error handled.&quot;</span>)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	vecBothZero := []<span class="type">float64</span>&#123;<span class="number">0.0</span>, <span class="number">0.0</span>&#125;</span><br><span class="line">	simBothZero := cosineSimilarity(vecZero, vecBothZero)</span><br><span class="line">	<span class="keyword">if</span> simBothZero == <span class="number">1.0</span> &#123; <span class="comment">// Our convention for both zero vectors</span></span><br><span class="line">		fmt.Printf(<span class="string">&quot;Similarity (both zero vectors): %.4f (Expected: 1.0)\n&quot;</span>, simBothZero)</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="3-2-2-欧几里得距离-Euclidean-Distance"><a href="#3-2-2-欧几里得距离-Euclidean-Distance" class="headerlink" title="3.2.2 欧几里得距离 (Euclidean Distance)"></a>3.2.2 欧几里得距离 (Euclidean Distance)</h4><ul>
<li><strong>定义</strong>: 欧几里得距离是两个向量在多维空间中最短的直线距离。距离越小，相似度越高。</li>
<li><strong>计算公式</strong>:<br>$$<br>\text{distance} &#x3D; \sqrt{\sum_{i&#x3D;1}^{n} (A_i - B_i)^2}<br>$$</li>
<li><strong>与余弦相似度的关系</strong>: 虽然是距离度量，但与余弦相似度高度相关。如果向量长度都被归一化到 1（即 <code>||A||=||B||=1</code>），那么欧几里得距离与余弦相似度之间存在简单关系：<code>distance^2 = 2 * (1 - cosine_similarity)</code>。</li>
</ul>
<h4 id="3-2-3-点积-Dot-Product"><a href="#3-2-3-点积-Dot-Product" class="headerlink" title="3.2.3 点积 (Dot Product)"></a>3.2.3 点积 (Dot Product)</h4><ul>
<li><strong>定义</strong>: 两个向量的点积衡量了它们在相同方向上的投影长度。如果向量是单位向量（长度为1），那么点积就等于余弦相似度。</li>
<li><strong>计算公式</strong>:<br>$$<br>\text{dot product} &#x3D; A \cdot B &#x3D; \sum_{i&#x3D;1}^{n} A_i B_i<br>$$</li>
<li><strong>在 LLM 中的应用</strong>: 在一些神经网络架构（如 Transformer 的注意力机制）中，点积被广泛用于计算查询 (Query) 向量和键 (Key) 向量之间的相似度。</li>
</ul>
<h3 id="3-3-相似性的类型"><a href="#3-3-相似性的类型" class="headerlink" title="3.3 相似性的类型"></a>3.3 相似性的类型</h3><ol>
<li><strong>词汇相似性 (Lexical Similarity)</strong>: 基于词语的共享或表面字符串匹配。<ul>
<li>示例: “run” 和 “running”。</li>
</ul>
</li>
<li><strong>句法相似性 (Syntactic Similarity)</strong>: 基于句子的语法结构或词序的相似性。<ul>
<li>示例: “猫追老鼠” 和 “老鼠被猫追” (虽然语义有差异，但结构可能某些方面相似)。</li>
</ul>
</li>
<li><strong>语义相似性 (Semantic Similarity)</strong>: 衡量文本片段在含义或意义上的相似程度。这是 LLM 最关注的。<ul>
<li>示例: “汽车” 和 “车辆”；”狗在花园里玩” 和 “一只小狗在后院嬉戏”。</li>
</ul>
</li>
<li><strong>语境相似性 (Contextual Similarity)</strong>: LLM 能够理解词语在不同语境下的含义。同一词语在不同语境中生成的嵌入向量可能大相径庭。语境相似性是指考虑了周围词语对目标词语含义的影响后的相似性。<ul>
<li>示例: “买了一个 <strong>Apple</strong> 手机”中的“Apple”和“树上结满了 <strong>apple</strong>”中的“apple”会生成不同的嵌入，因此它们与“科技公司”或“水果”的相似度也会不同。</li>
</ul>
</li>
</ol>
<h2 id="四、相关性-Relevance-详解"><a href="#四、相关性-Relevance-详解" class="headerlink" title="四、相关性 (Relevance) 详解"></a>四、相关性 (Relevance) 详解</h2><h3 id="4-1-定义"><a href="#4-1-定义" class="headerlink" title="4.1 定义"></a>4.1 定义</h3><p><strong>相关性</strong>是指一个文本片段对于特定的<strong>查询 (Query)</strong>、<strong>任务 (Task)</strong> 或<strong>上下文 (Context)</strong> 的有用性、重要性或匹配程度。它回答的是“这个文本对我的需求有多大帮助？”的问题。</p>
<h3 id="4-2-影响相关性的因素"><a href="#4-2-影响相关性的因素" class="headerlink" title="4.2 影响相关性的因素"></a>4.2 影响相关性的因素</h3><p>相关性不仅仅是语义相似度高那么简单，它还可能受以下因素影响：</p>
<ol>
<li><strong>查询意图 (Query Intent)</strong>: 用户的真实目的。例如，搜索“苹果”可能意图是水果，也可能是科技公司，也可能是某个品牌。</li>
<li><strong>时效性 (Timeliness)</strong>: 信息是否最新、是否过期。</li>
<li><strong>权威性 (Authority)</strong>: 信息来源是否可靠、可信。</li>
<li><strong>完整性 (Completeness)</strong>: 信息是否全面，能否解答用户的所有疑问。</li>
<li><strong>领域专业性 (Domain Specificity)</strong>: 在特定领域内，某些词语或概念的相关性可能更高。</li>
<li><strong>用户个性化 (User Personalization)</strong>: 用户的历史行为、偏好等也会影响相关性判断。</li>
<li><strong>上下文 (Context)</strong>: 当前的对话历史、之前的搜索结果等。</li>
</ol>
<h3 id="4-3-相关性的度量"><a href="#4-3-相关性的度量" class="headerlink" title="4.3 相关性的度量"></a>4.3 相关性的度量</h3><p>相关性通常不是通过简单的几何关系直接计算，而是通过更复杂的模型或系统来判断：</p>
<ul>
<li><strong>排序模型 (Ranking Models)</strong>: 在信息检索系统中，相关性是一个复杂的排序问题。模型会综合考虑文本的语义相似度、关键词匹配、文档质量、时效性、用户点击行为等多种特征来预测相关性分数。</li>
<li><strong>标注数据 (Labeled Data)</strong>: 通过人工标注大量查询-文档对的真实相关性，来训练监督学习模型。</li>
<li><strong>A&#x2F;B 测试与用户反馈</strong>: 通过在线实验，观察用户对不同排序结果的满意度、点击率等，来迭代优化相关性模型。</li>
<li><strong>结合 LLM 的评估</strong>: LLM 可以被训练来评估给定查询与文本之间的相关性，甚至可以解释为什么某个文本是相关的。例如，prompt LLM ：“请判断以下文本与查询’[查询]’的相关性，并给出评分（1-5分）及理由。”</li>
</ul>
<h2 id="五、相似性与相关性的区别与联系"><a href="#五、相似性与相关性的区别与联系" class="headerlink" title="五、相似性与相关性的区别与联系"></a>五、相似性与相关性的区别与联系</h2><table>
<thead>
<tr>
<th align="left">特征</th>
<th align="left">相似性 (Similarity)</th>
<th align="left">相关性 (Relevance)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>关注点</strong></td>
<td align="left">文本自身的语义、结构上的接近程度</td>
<td align="left">文本对于特定查询&#x2F;任务&#x2F;上下文的有用性或匹配程度</td>
</tr>
<tr>
<td align="left"><strong>标准</strong></td>
<td align="left">文本内在属性的匹配</td>
<td align="left">外部意图的功能性匹配</td>
</tr>
<tr>
<td align="left"><strong>是&#x2F;否</strong></td>
<td align="left">通常是一个连续的程度值（0-1 或 -1-1）</td>
<td align="left">也常是连续值，但可能受二元判断影响（相关&#x2F;不相关）</td>
</tr>
<tr>
<td align="left"><strong>度量方法</strong></td>
<td align="left">几何距离（余弦相似度、欧几里得距离）、点积等</td>
<td align="left">复杂的排序模型、机器学习、人工标注、用户反馈等</td>
</tr>
<tr>
<td align="left"><strong>评价对象</strong></td>
<td align="left">两个文本之间的静态关系</td>
<td align="left">文本与动态查询&#x2F;任务之间的动态关系</td>
</tr>
<tr>
<td align="left"><strong>例子</strong></td>
<td align="left">“汽车”与“车辆”高相似；“猫”与“狗”中度相似</td>
<td align="left">搜索“如何修复漏水的龙头”，关于水管维修的文章比关于猫咪健康的更相关</td>
</tr>
</tbody></table>
<p><strong>联系</strong>:</p>
<ul>
<li><strong>相关性经常以相似性为基础</strong>: 在很多情况下，语义相似度是判断相关性的一个重要（甚至是主要）特征。如果两个文本在语义上完全不相似，它们通常也不会相关。</li>
<li><strong>相关性是更复杂的判断</strong>: 相关性是在相似性基础上，结合了用户意图、上下文、时间、质量等多种因素的综合判断。</li>
</ul>
<p><strong>举例</strong>：</p>
<ul>
<li><p><strong>查询</strong>: “如何清洗我的运动鞋？”</p>
</li>
<li><p><strong>文档 A</strong>: “清洗鞋子的最佳方法” (包含关于洗涤剂、刷子、晾干等信息)</p>
</li>
<li><p><strong>文档 B</strong>: “如何保养你的真皮皮鞋” (详细介绍了皮鞋的护理)</p>
</li>
<li><p><strong>文档 C</strong>: “选择适合跑步的运动鞋” (关于购买运动鞋的建议)</p>
</li>
<li><p><strong>相似性视角</strong>：</p>
<ul>
<li>查询和文档A：高语义相似性（都关于“清洗鞋子”）。</li>
<li>查询和文档B：中低语义相似性（都关于“鞋子保养”，但类型和方法不同）。</li>
<li>查询和文档C：中等语义相似性（都关于“运动鞋”，但侧重“选择”而非“清洗”）。</li>
</ul>
</li>
<li><p><strong>相关性视角</strong>：</p>
<ul>
<li>对于查询“如何清洗我的运动鞋？”，<strong>文档 A 的相关性最高</strong>，因为它直接回答了如何清洗运动鞋。</li>
<li>文档 B 尽管语义上与“清洗”和“鞋子”都有所关联，但其<strong>相关性远低于文档 A</strong>，因为它涉及的是“皮鞋保养”而非“运动鞋清洗”。用户需要的信息无法从文档 B 中获取。</li>
<li>文档 C 尽管和“运动鞋”高度概念相关，但对“如何清洗”这一具体<strong>任务的相关性很低</strong>。</li>
</ul>
</li>
</ul>
<h2 id="六、在-LLM-中的应用场景"><a href="#六、在-LLM-中的应用场景" class="headerlink" title="六、在 LLM 中的应用场景"></a>六、在 LLM 中的应用场景</h2><h3 id="6-1-相似性的典型应用"><a href="#6-1-相似性的典型应用" class="headerlink" title="6.1 相似性的典型应用"></a>6.1 相似性的典型应用</h3><ol>
<li><strong>语义搜索 (Semantic Search)</strong>: 将查询和文档都嵌入，然后计算它们之间的余弦相似度来排序。</li>
<li><strong>文本聚类与发现</strong>: 将语义相似的文本嵌入分组，发现不同主题的文本簇。</li>
<li><strong>重复检测与去重</strong>: 识别出语义上重复或高度相似的文本片段。</li>
<li><strong>内容推薦</strong>: 基于用户已消费内容的嵌入，寻找相似的新内容。</li>
<li><strong>知识图谱构建</strong>: 发现实体和概念间的相似关系。</li>
</ol>
<h3 id="6-2-相关性的典型应用"><a href="#6-2-相关性的典型应用" class="headerlink" title="6.2 相关性的典型应用"></a>6.2 相关性的典型应用</h3><ol>
<li><strong>检索增强生成 (RAG - Retrieval-Augmented Generation)</strong>:<ul>
<li>用户提问后，系统将问题嵌入。</li>
<li>利用相似性搜索（通常是余弦相似度）从外部知识库中<strong>检索</strong>出语义相似的文本块。</li>
<li>在此基础上，RAG 更进一步，通过一个<strong>排序器 (Re-ranker)</strong> 模块对检索出的文本块进行相关性评估和重新排序。这个排序器通常是一个更小的、针对相关性任务微调过的 LLM，它不仅仅看语义相似度，还会考虑文本对回答当前查询的实际<strong>有用性</strong>。最后，将最相关的文本作为上下文提供给主 LLM。</li>
</ul>
</li>
<li><strong>问答系统 (Question Answering)</strong>: 找出对问题回答最有用的段落或句子。</li>
<li><strong>信息过滤与推荐</strong>: 确保推荐的内容不仅相似，而且是用户当前可能最感兴趣、最有价值的。</li>
<li><strong>搜索引擎排序</strong>: 这是相关性最经典的场景，搜索引擎的核心任务就是根据用户查询，返回最相关的网页。</li>
</ol>
<h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>相似性是文本的内在属性比较，主要通过向量的几何关系度量；相关性是文本对特定需求的外在有用性判断，它在相似性的基础上，融合了用户意图、上下文、时效性等更多复杂因素。</p>
<p>在 LLM 应用中，这两者相辅相成：</p>
<ul>
<li><strong>相似性</strong>通常作为<strong>初步筛选</strong>的有效工具，快速从海量数据中缩小范围，找出潜在的候选集合。</li>
<li><strong>相关性</strong>则在此基础上进行<strong>精细化排序和过滤</strong>，确保最终呈现给用户或 LLM 的信息是最准确、最有用的。</li>
</ul>
<p>尤其是在 RAG 这样的高级应用中，我们看到一个明确的流程：先用高效的相似性搜索进行粗粒度检索，再用更复杂的模型进行精确的相关性重排序。这种结合方式体现了对相似性和相关性两者深刻理解和有效利用的价值。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/d382adface17/">https://blog.tbf1211.xx.kg/d382adface17/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-04.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/49c9c17349e3/" title="Go语言常用设计模式详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Go语言常用设计模式详解</div></div><div class="info-2"><div class="info-item-1"> 设计模式是对在特定情境下，反复出现的问题提供一套成熟的、可复用的解决方案。Go 语言以其简洁、并发优先的特性，在实现设计模式时通常会有其独特的“Go 惯例”，有时会与传统面向对象设计模式的实现有所不同。本篇将探讨 Go 语言中常用的设计模式，并结合 Go 的特性给出实现示例。  核心思想：Go 语言的设计模式实现通常倾向于简洁、组合而非继承、接口优先以及利用 Goroutine 和 Channel 进行并发处理。   一、Go 语言与设计模式的哲学Go 语言在设计模式的实践上，有一些与传统 OOP 语言不同的哲学：  组合优于继承：Go 没有类继承的概念，而是通过结构体嵌入（Composition）和接口（Interfaces）来实现代码复用和多态。 接口优先：Go 的接口是隐式实现的（implicit interface satisfaction），任何类型只要实现了接口定义的所有方法，就自然地实现了该接口。这使得接口更加灵活，鼓励“小接口，大组合”的原则。 并发原语：Goroutine 和 Channel 是 Go 语言的核心并发原语，许多设计模式在 Go 中会自然融入并发...</div></div></div></a><a class="pagination-related" href="/d1dbcd98dd73/" title="LazyGit使用解析：你的Git命令行效率神器"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">LazyGit使用解析：你的Git命令行效率神器</div></div><div class="info-2"><div class="info-item-1"> 本文将带你深入了解 LazyGit，一个简单直观的终端 UI Git 客户端。如果你厌倦了反复输入 Git 命令，又觉得 GUI 客户端不够灵活，那么 LazyGit 可能会成为你的新宠。它将终端的强大与 GUI 的便捷完美结合，让你的 Git 工作流变得前所未有的高效和愉悦。  对于开发者而言，Git 无疑是日常工作中不可或缺的工具。然而，即使是最熟练的 Git 用户，也可能被一些重复、繁琐的命令行操作所困扰，例如 git add ., git status, git commit -m &quot;...&quot;, git log --oneline 等等。虽然有各种图形化 Git 客户端，但它们往往意味着脱离终端环境，或多或少牺牲了速度和灵活性。LazyGit 正是为了解决这一痛点而生的——它提供了一个文本用户界面 (TUI)，让你在终端中就能以图形化的方式快速、直观地执行 Git 操作，大幅提升工作效率。   一、为什么选择 LazyGit？LazyGit 并不是简单的 Git 命令别名集合，它提供了一个交互式的视图，将 git status, git branch...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/30bb7c0cff3b/" title="Transformer 模型深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-32.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-14</div><div class="info-item-2">Transformer 模型深度详解</div></div><div class="info-2"><div class="info-item-1"> Transformer 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了序列建模任务的主流架构。  核心思想：Transformer 放弃了传统的循环和卷积结构，完全依赖于注意力机制 (Attention Mechanism)来捕捉输入序列中的依赖关系。通过精心设计的自注意力 (Self-Attention) 机制，模型能够同时关注输入序列中的所有位置，从而实现高效的并行计算和对任意距离依赖的有效建模。   一、为什么需要 Transformer？在 Transformer 出现之前，RNN 及其变体 (如 LSTM 和 GRU) 是序列建模任务的主流。然而，它们存在一些固有的局限性：  顺序依赖：RNN 必须顺序地处理序列中的每个元素，后一个元素的计算依赖...</div></div></div></a><a class="pagination-related" href="/4b8301cdd035/" title="向量数据库 (Vector Database) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-03</div><div class="info-item-2">向量数据库 (Vector Database) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量数据库 (Vector Database &#x2F; Vector Store) 是一种专门设计用于高效存储、管理和检索向量嵌入 (Vector Embeddings) 的数据库。这些向量嵌入是高维的数值表示，由机器学习模型生成，能够捕捉文本、图像、音频或其他复杂数据的语义信息。向量数据库的核心能力在于通过计算向量之间的相似度 (Similarity) 来进行快速搜索，而非传统的精确匹配。  核心思想：将非结构化数据转化为机器可理解的低维或高维向量表示（嵌入），并在此基础上实现基于语义相似度的快速检索。它解决了传统数据库在处理语义搜索、推荐系统、多模态数据匹配等场景下的局限性。   一、什么是向量 (Vector)？在深入了解向量数据库之前，我们必须先理解“向量”这个核心概念。 1.1 向量的数学定义在数学和物理中，向量 (Vector) 是一个具有大小 (Magnitude) 和方向 (Direction) 的量。它可以被表示为一个有序的数值列表。  一维向量：一个标量，如 [5]。 二维向量：表示平面上的一个点或从原点指向该点的箭头，如 [x, y]。例如，[3, 4...</div></div></div></a><a class="pagination-related" href="/fdb8a5195f3e/" title="知识图谱 (Knowledge Graph) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-13.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-09</div><div class="info-item-2">知识图谱 (Knowledge Graph) 详解</div></div><div class="info-2"><div class="info-item-1"> 知识图谱 (Knowledge Graph, KG) 是一种结构化的知识表示形式，它以图 (Graph) 的形式来描述客观世界中存在的各种实体 (Entities)、概念 (Concepts) 及其之间的关系 (Relations)。通过将离散、异构的信息链接起来，知识图谱构建了一个庞大且相互关联的“知识网络”，使得机器能够像人类一样理解、组织和利用知识，从而支持复杂的推理和智能应用。  核心思想：将现实世界的知识抽象为“实体-关系-实体”或“实体-属性-属性值”的三元组结构，并通过图的形式直观地表示和存储这些知识，从而实现知识的机器可读、可理解和可推理。 它旨在解决传统关系型数据库在表示复杂、动态、多源异构数据时存在的语义鸿沟问题。   一、知识图谱的基本构成知识图谱的核心是其图结构中的基本元素：  实体 (Entities)  定义：指现实世界中具有明确指代和区分度的“事物”或“概念”，可以是具象的（如“苹果公司”、“埃菲尔铁塔”、“约翰·F·肯尼迪”）或抽象的（如“人工智能”、“经济学”、“创新”）。 表示：在知识图谱中，每个实体通常有一个唯一的标识符 (URI&#x2F...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/21e140c78f80/" title="大型语言模型如何理解人类文字：从Token到语义表征"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-25.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-16</div><div class="info-item-2">大型语言模型如何理解人类文字：从Token到语义表征</div></div><div class="info-2"><div class="info-item-1"> 大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。   一、基础构建模块：从文本到向量LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于分词 (Tokenization) 和词嵌入 (Word Embeddings)。 1.1 分词 (Tokenization)分词是将连续的文本序列切分成有意义的最小单位——Token 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。  词级别分词 (Word-level Tokenization)：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">438</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">224</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E7%9B%B8%E4%BC%BC%E6%80%A7%E4%B8%8E%E7%9B%B8%E5%85%B3%E6%80%A7%E5%9C%A8-LLM-%E4%B8%AD%E5%A6%82%E6%AD%A4%E9%87%8D%E8%A6%81%EF%BC%9F"><span class="toc-text">一、为什么相似性与相关性在 LLM 中如此重要？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%B5%8C%E5%85%A5%E4%B8%8E%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4"><span class="toc-text">二、核心概念：嵌入与向量空间</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%B5%8C%E5%85%A5-Embeddings"><span class="toc-text">2.1 嵌入 (Embeddings)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4-Vector-Space"><span class="toc-text">2.2 向量空间 (Vector Space)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%9B%B8%E4%BC%BC%E6%80%A7-Similarity-%E8%AF%A6%E8%A7%A3"><span class="toc-text">三、相似性 (Similarity) 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%AE%9A%E4%B9%89"><span class="toc-text">3.1 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%B8%BB%E8%A6%81%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95"><span class="toc-text">3.2 主要度量方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6-Cosine-Similarity"><span class="toc-text">3.2.1 余弦相似度 (Cosine Similarity)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB-Euclidean-Distance"><span class="toc-text">3.2.2 欧几里得距离 (Euclidean Distance)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-%E7%82%B9%E7%A7%AF-Dot-Product"><span class="toc-text">3.2.3 点积 (Dot Product)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E7%9B%B8%E4%BC%BC%E6%80%A7%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-text">3.3 相似性的类型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%9B%B8%E5%85%B3%E6%80%A7-Relevance-%E8%AF%A6%E8%A7%A3"><span class="toc-text">四、相关性 (Relevance) 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AE%9A%E4%B9%89"><span class="toc-text">4.1 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%BD%B1%E5%93%8D%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%9B%A0%E7%B4%A0"><span class="toc-text">4.2 影响相关性的因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%BA%A6%E9%87%8F"><span class="toc-text">4.3 相关性的度量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%9B%B8%E4%BC%BC%E6%80%A7%E4%B8%8E%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB"><span class="toc-text">五、相似性与相关性的区别与联系</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%9C%A8-LLM-%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">六、在 LLM 中的应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E7%9B%B8%E4%BC%BC%E6%80%A7%E7%9A%84%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="toc-text">6.1 相似性的典型应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="toc-text">6.2 相关性的典型应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">七、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CFFI (C Foreign Function Interface for Python) 详解"/></a><div class="content"><a class="title" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解">CFFI (C Foreign Function Interface for Python) 详解</a><time datetime="2025-12-23T22:24:00.000Z" title="发表于 2025-12-24 06:24:00">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-18.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="行为驱动开发 (BDD) 详解"/></a><div class="content"><a class="title" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解">行为驱动开发 (BDD) 详解</a><time datetime="2025-12-21T22:24:00.000Z" title="发表于 2025-12-22 06:24:00">2025-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="测试驱动开发 (TDD) 详解"/></a><div class="content"><a class="title" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解">测试驱动开发 (TDD) 详解</a><time datetime="2025-12-19T22:24:00.000Z" title="发表于 2025-12-20 06:24:00">2025-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPFS (InterPlanetary File System) 详解"/></a><div class="content"><a class="title" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解">IPFS (InterPlanetary File System) 详解</a><time datetime="2025-12-16T22:24:00.000Z" title="发表于 2025-12-17 06:24:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"/></a><div class="content"><a class="title" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)">L7 负载均衡详解 (Layer 7 Load Balancing Explained)</a><time datetime="2025-12-14T22:24:00.000Z" title="发表于 2025-12-15 06:24:00">2025-12-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-04.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>