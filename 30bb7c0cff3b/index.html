<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer 模型深度详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 模型深度详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/30bb7c0cff3b/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="Transformer 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg">
<meta property="article:published_time" content="2025-05-13T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-31T06:41:34.985Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Transformer 模型深度详解",
  "url": "https://blog.tbf1211.xx.kg/30bb7c0cff3b/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg",
  "datePublished": "2025-05-13T22:24:00.000Z",
  "dateModified": "2025-12-31T06:41:34.985Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/30bb7c0cff3b/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 模型深度详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">448</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">225</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-03.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">Transformer 模型深度详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 模型深度详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-05-13T22:24:00.000Z" title="发表于 2025-05-14 06:24:00">2025-05-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>Transformer</strong> 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了序列建模任务的主流架构。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>Transformer 放弃了传统的循环和卷积结构，完全依赖于</strong>注意力机制 (Attention Mechanism)<strong>来捕捉输入序列中的依赖关系。通过精心设计的自注意力 (Self-Attention) 机制，模型能够同时关注输入序列中的所有位置，从而实现高效的并行计算和对任意距离依赖的有效建模。</strong></p>
</div>
<hr>
<h2 id="一、为什么需要-Transformer？"><a href="#一、为什么需要-Transformer？" class="headerlink" title="一、为什么需要 Transformer？"></a>一、为什么需要 Transformer？</h2><p>在 Transformer 出现之前，RNN 及其变体 (如 LSTM 和 GRU) 是序列建模任务的主流。然而，它们存在一些固有的局限性：</p>
<ol>
<li><strong>顺序依赖</strong>：RNN 必须顺序地处理序列中的每个元素，后一个元素的计算依赖于前一个元素。这导致：<ul>
<li><strong>难以并行化</strong>：限制了训练速度，尤其是在处理长序列时。</li>
<li><strong>长距离依赖问题</strong>：随着序列长度增加，信息在传播过程中逐渐衰减，模型难以捕捉到相距较远元素之间的关联，即“梯度消失”或“梯度爆炸”问题。</li>
</ul>
</li>
<li><strong>计算效率低下</strong>：每个时间步的计算都需要等待前一个时间步的结果，导致训练和推理速度慢。</li>
</ol>
<p>Transformer 模型通过引入自注意力机制，有效地解决了这些问题，实现了序列建模的范式转变。</p>
<h2 id="二、Transformer-架构概览"><a href="#二、Transformer-架构概览" class="headerlink" title="二、Transformer 架构概览"></a>二、Transformer 架构概览</h2><p>Transformer 模型遵循经典的<strong>编码器-解码器 (Encoder-Decoder)</strong> 结构，但其内部实现完全基于注意力机制。</p>
<p><strong>整体结构：</strong></p>
<ul>
<li><strong>编码器 (Encoder)</strong>：由 <code>N</code> 个相同的编码器层堆叠而成。它负责将输入序列（例如，源语言句子）转换为一系列连续的表示。</li>
<li><strong>解码器 (Decoder)</strong>：由 <code>N</code> 个相同的解码器层堆叠而成。它负责将编码器的输出和之前生成的输出序列（例如，目标语言句子）作为输入，生成下一个输出序列的元素。</li>
</ul>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph LR
    subgraph Transformer
        Input[输入序列] --&gt; InputEmbedding[输入嵌入 + 位置编码];

        subgraph Encoder Stack
            E1[编码器层 1] --&gt; E2[编码器层 2] --&gt; ... --&gt; EN[编码器层 N];
        end
        InputEmbedding --&gt; E1;
        EN --&gt; EncoderOutput[编码器输出];

        OutputEmbedding[输出嵌入 + 位置编码] --&gt; D1[解码器层 1];
        D1 --&gt; D2[解码器层 2] --&gt; ... --&gt; DN[解码器层 N];

        EncoderOutput --&gt; D1;

        DN --&gt; Linear[线性层] --&gt; Softmax[Softmax];
        Softmax --&gt; Output[输出概率分布];
    end
  </pre></div>

<h2 id="三、编码器-Encoder-详解"><a href="#三、编码器-Encoder-详解" class="headerlink" title="三、编码器 (Encoder) 详解"></a>三、编码器 (Encoder) 详解</h2><p>每个编码器层包含两个主要的子层：</p>
<ol>
<li><strong>多头自注意力机制 (Multi-Head Self-Attention)</strong></li>
<li><strong>前馈神经网络 (Feed-Forward Network, FFN)</strong></li>
</ol>
<p>每个子层之后都跟着一个<strong>残差连接 (Residual Connection)</strong> 和<strong>层归一化 (Layer Normalization)</strong>。</p>
<h3 id="3-1-输入嵌入-Input-Embedding-与位置编码-Positional-Encoding"><a href="#3-1-输入嵌入-Input-Embedding-与位置编码-Positional-Encoding" class="headerlink" title="3.1 输入嵌入 (Input Embedding) 与位置编码 (Positional Encoding)"></a>3.1 输入嵌入 (Input Embedding) 与位置编码 (Positional Encoding)</h3><p>在将输入序列送入编码器之前，每个词（token）首先被转换成一个词向量 (Word Embedding)。由于 Transformer 模型不包含循环或卷积，它自身无法感知输入序列中词的顺序信息。因此，需要引入<strong>位置编码</strong>来注入词的位置信息。</p>
<p><strong>位置编码公式：</strong></p>
<p>我们使用不同频率的正弦和余弦函数来生成位置编码：</p>
<p>$$<br>PE_{(pos, 2i)} &#x3D; \sin(pos &#x2F; 10000^{2i&#x2F;d_{model}}) \<br>PE_{(pos, 2i+1)} &#x3D; \cos(pos &#x2F; 10000^{2i&#x2F;d_{model}})<br>$$</p>
<p>其中：</p>
<ul>
<li><code>pos</code> 是词在序列中的位置 (0 到 <code>max_sequence_length - 1</code>)。</li>
<li><code>i</code> 是位置编码向量的维度索引 (0 到 <code>d_model/2 - 1</code>)。</li>
<li><code>d_model</code> 是词向量的维度。</li>
</ul>
<p>最终的输入是词嵌入和位置编码的简单求和。</p>
<h3 id="3-2-多头自注意力机制-Multi-Head-Self-Attention"><a href="#3-2-多头自注意力机制-Multi-Head-Self-Attention" class="headerlink" title="3.2 多头自注意力机制 (Multi-Head Self-Attention)"></a>3.2 多头自注意力机制 (Multi-Head Self-Attention)</h3><p>这是 Transformer 的核心，允许模型在处理序列的某个词时，同时关注序列中的其他词，并计算它们之间的关联性。</p>
<p><strong>a. 自注意力 (Self-Attention)</strong></p>
<p>对于输入序列中的每个词，自注意力机制会计算三个向量：<strong>查询 (Query, Q)</strong>、<strong>键 (Key, K)</strong> 和<strong>值 (Value, V)</strong>。它们通过将词嵌入向量与三个不同的权重矩阵 $W^Q, W^K, W^V$ 相乘得到。</p>
<p>一旦有了 Q、K、V 向量，我们就可以计算自注意力的输出了：</p>
<ol>
<li><strong>计算注意力分数</strong>：将查询向量 $Q$ 与所有键向量 $K$ 进行点积，得到注意力分数。点积衡量了查询与每个键之间的相似度。</li>
<li><strong>缩放 (Scaling)</strong>：为了防止点积结果过大导致 softmax 函数进入梯度饱和区，将注意力分数除以 $\sqrt{d_k}$ (键向量的维度)。</li>
<li><strong>Softmax</strong>：对缩放后的分数应用 Softmax 函数，将其转换为概率分布，表示每个词对当前词的关注程度。</li>
<li><strong>加权求和</strong>：将 Softmax 结果（注意力权重）与所有值向量 $V$ 相乘并求和，得到当前词的自注意力输出。</li>
</ol>
<p><strong>Scaled Dot-Product Attention 公式：</strong></p>
<p>$$<br>Attention(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<p>其中：</p>
<ul>
<li>$Q$ 是查询矩阵 (queries, 维度 $n \times d_k$)。</li>
<li>$K$ 是键矩阵 (keys, 维度 $m \times d_k$)。</li>
<li>$V$ 是值矩阵 (values, 维度 $m \times d_v$)。</li>
<li>$n$ 是输出序列长度， $m$ 是输入序列长度。在自注意力中，$n&#x3D;m$。</li>
<li>$d_k$ 是键向量的维度。</li>
</ul>
<p><strong>b. 多头注意力 (Multi-Head Attention)</strong></p>
<p>多头注意力是自注意力机制的扩展。它将 Q、K、V 投影到 $h$ 个不同的子空间中（$h$ 为头的数量），然后独立地并行执行 $h$ 次自注意力计算。每个头都可以学习到不同的关注模式。最后，将所有头的输出拼接起来，再通过一个线性投影得到最终输出。</p>
<p>多头注意力使得模型能够：</p>
<ul>
<li>在不同的表示子空间中捕捉到不同类型的依赖关系。</li>
<li>提高模型捕捉复杂模式的能力。</li>
</ul>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    Input_X[输入 X] --&gt; Linear_Q[线性变换 WQ];
    Input_X --&gt; Linear_K[线性变换 WK];
    Input_X --&gt; Linear_V[线性变换 WV];

    Linear_Q --&gt; Q[Q];
    Linear_K --&gt; K[K];
    Linear_V --&gt; V[V];

    subgraph Head_1
        Q -- 1 --&gt; Attn_1[Scaled Dot-Product Attention];
        K -- 1 --&gt; Attn_1;
        V -- 1 --&gt; Attn_1;
        Attn_1 --&gt; Output_1[输出 1];
    end

    subgraph Head_2
        Q -- 2 --&gt; Attn_2[Scaled Dot-Product Attention];
        K -- 2 --&gt; Attn_2;
        V -- 2 --&gt; Attn_2;
        Attn_2 --&gt; Output_2[输出 2];
    end

    subgraph Head_h
        Q -- h --&gt; Attn_h[Scaled Dot-Product Attention];
        K -- h --&gt; Attn_h;
        V -- h --&gt; Attn_h;
        Attn_h --&gt; Output_h[输出 h];
    end

    Output_1 &amp; Output_2 &amp; ... &amp; Output_h --&gt; Concat[&quot;拼接 (Concatenate)&quot;];
    Concat --&gt; Linear_Output[线性变换 WO];
    Linear_Output --&gt; Final_Output[多头注意力输出];
  </pre></div>

<h3 id="3-3-Add-Norm-残差连接与层归一化"><a href="#3-3-Add-Norm-残差连接与层归一化" class="headerlink" title="3.3 Add &amp; Norm (残差连接与层归一化)"></a>3.3 Add &amp; Norm (残差连接与层归一化)</h3><p>每个子层（多头自注意力或前馈网络）的输出都会与该子层的输入进行残差连接，然后进行层归一化。</p>
<ul>
<li><strong>残差连接</strong>：$x + \text{Sublayer}(x)$。有助于解决深度网络中的梯度消失问题，使信息能够更容易地在网络中传播。</li>
<li><strong>层归一化 (Layer Normalization)</strong>：对每个样本的特征维度进行归一化，稳定了深层网络的训练过程。</li>
</ul>
<h3 id="3-4-前馈神经网络-Feed-Forward-Network-FFN"><a href="#3-4-前馈神经网络-Feed-Forward-Network-FFN" class="headerlink" title="3.4 前馈神经网络 (Feed-Forward Network, FFN)"></a>3.4 前馈神经网络 (Feed-Forward Network, FFN)</h3><p>FFN 是一个简单的两层全连接网络，通常包含一个 ReLU 激活函数：</p>
<p>$FFN(x) &#x3D; \max(0, xW_1 + b_1)W_2 + b_2$</p>
<p>它对每个位置独立地进行相同的转换。</p>
<h2 id="四、解码器-Decoder-详解"><a href="#四、解码器-Decoder-详解" class="headerlink" title="四、解码器 (Decoder) 详解"></a>四、解码器 (Decoder) 详解</h2><p>每个解码器层包含三个主要子层：</p>
<ol>
<li><strong>遮盖多头自注意力机制 (Masked Multi-Head Self-Attention)</strong></li>
<li><strong>多头编码器-解码器注意力机制 (Multi-Head Encoder-Decoder Attention)</strong></li>
<li><strong>前馈神经网络 (Feed-Forward Network, FFN)</strong></li>
</ol>
<p>同样，每个子层之后也跟着一个残差连接和层归一化。</p>
<h3 id="4-1-遮盖多头自注意力-Masked-Multi-Head-Self-Attention"><a href="#4-1-遮盖多头自注意力-Masked-Multi-Head-Self-Attention" class="headerlink" title="4.1 遮盖多头自注意力 (Masked Multi-Head Self-Attention)"></a>4.1 遮盖多头自注意力 (Masked Multi-Head Self-Attention)</h3><p>与编码器中的自注意力类似，但它在 Softmax 之前引入了一个“遮盖”机制。<br><strong>遮盖 (Masking)</strong>：确保解码器在预测当前位置的输出时，只能关注当前位置及之前的词，而不能“看到”未来的词。这模拟了序列生成的顺序性。</p>
<h3 id="4-2-多头编码器-解码器注意力-Multi-Head-Encoder-Decoder-Attention"><a href="#4-2-多头编码器-解码器注意力-Multi-Head-Encoder-Decoder-Attention" class="headerlink" title="4.2 多头编码器-解码器注意力 (Multi-Head Encoder-Decoder Attention)"></a>4.2 多头编码器-解码器注意力 (Multi-Head Encoder-Decoder Attention)</h3><p>这个注意力层用于连接编码器和解码器。</p>
<ul>
<li><strong>查询 (Q)</strong> 向量来自<strong>前一个解码器层</strong>的输出。</li>
<li><strong>键 (K)</strong> 和 <strong>值 (V)</strong> 向量来自<strong>编码器层</strong>的输出。</li>
</ul>
<p>这使得解码器在生成输出时，能够关注输入序列中的相关部分。</p>
<h3 id="4-3-最终输出层"><a href="#4-3-最终输出层" class="headerlink" title="4.3 最终输出层"></a>4.3 最终输出层</h3><p>解码器的输出经过一个线性层 (Linear Layer) 和一个 Softmax 层，转换为词汇表上每个词的概率分布，从而预测下一个词。</p>
<h2 id="五、Transformer-的训练"><a href="#五、Transformer-的训练" class="headerlink" title="五、Transformer 的训练"></a>五、Transformer 的训练</h2><p>Transformer 通常通过最大化下一个词的对数似然进行训练，即一个标准的序列到序列任务的交叉熵损失函数。优化器常采用 Adam，并结合自定义的学习率调度策略。</p>
<h2 id="六、Transformer-的优缺点与应用"><a href="#六、Transformer-的优缺点与应用" class="headerlink" title="六、Transformer 的优缺点与应用"></a>六、Transformer 的优缺点与应用</h2><h3 id="6-1-优点"><a href="#6-1-优点" class="headerlink" title="6.1 优点"></a>6.1 优点</h3><ol>
<li><strong>并行计算能力</strong>：自注意力机制允许模型同时处理序列中的所有词，极大地提高了训练效率。</li>
<li><strong>有效捕捉长距离依赖</strong>：通过注意力机制，模型可以轻松地连接相距很远的词，克服了 RNN 的长距离依赖问题。</li>
<li><strong>更少的归纳偏置 (Inductive Bias)</strong>：相较于 CNN 和 RNN，Transformer 对数据结构的假设更少，理论上可以学习更广泛的模式。</li>
<li><strong>模型可解释性</strong>：注意力权重可以部分揭示模型在决策时关注了哪些输入部分。</li>
</ol>
<h3 id="6-2-缺点"><a href="#6-2-缺点" class="headerlink" title="6.2 缺点"></a>6.2 缺点</h3><ol>
<li><strong>计算复杂度</strong>：自注意力机制的计算复杂度是序列长度 $L$ 的平方 ($O(L^2)$)，对于非常长的序列，计算成本非常高。</li>
<li><strong>位置编码依赖</strong>：Transformer 本身不具备处理顺序的能力，必须依赖于位置编码。</li>
<li><strong>内存消耗</strong>：Q、K、V 矩阵以及注意力矩阵的存储需要大量内存，尤其是在处理长序列时。</li>
</ol>
<h3 id="6-3-典型应用"><a href="#6-3-典型应用" class="headerlink" title="6.3 典型应用"></a>6.3 典型应用</h3><p>Transformer 模型及其变体已成为各种 AI 任务的核心：</p>
<ul>
<li><strong>自然语言处理 (NLP)</strong>：<ul>
<li>机器翻译 (Machine Translation)</li>
<li>文本摘要 (Text Summarization)</li>
<li>问答系统 (Question Answering)</li>
<li>情感分析 (Sentiment Analysis)</li>
<li>预训练语言模型 (如 BERT, GPT 系列, T5)</li>
</ul>
</li>
<li><strong>计算机视觉 (CV)</strong>：<ul>
<li>图像分类 (Vision Transformer, ViT)</li>
<li>目标检测 (DETR)</li>
<li>图像生成</li>
</ul>
</li>
<li><strong>语音识别 (Speech Recognition)</strong></li>
<li><strong>多模态任务</strong> (结合文本和图像等)</li>
</ul>
<h2 id="七、核心组件-Go-语言代码示例-概念性"><a href="#七、核心组件-Go-语言代码示例-概念性" class="headerlink" title="七、核心组件 Go 语言代码示例 (概念性)"></a>七、核心组件 Go 语言代码示例 (概念性)</h2><p>由于 Go 语言在深度学习模型构建方面不如 Python (TensorFlow&#x2F;PyTorch) 常用，以下提供一个概念性的 Go 语言实现，着重展示 Scaled Dot-Product Attention 的核心计算逻辑。实际的矩阵运算在 Go 中通常会依赖专门的线性代数库。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transformer</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;math&quot;</span></span><br><span class="line">	<span class="string">&quot;gonum.org/v1/gonum/mat&quot;</span> <span class="comment">// 假设使用gonum/mat进行矩阵操作</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScaledDotProductAttention 实现缩放点积注意力</span></span><br><span class="line"><span class="comment">// Q: Query 矩阵</span></span><br><span class="line"><span class="comment">// K: Key 矩阵</span></span><br><span class="line"><span class="comment">// V: Value 矩阵</span></span><br><span class="line"><span class="comment">// mask: 可选的注意力掩码 (例如，用于解码器自注意力)</span></span><br><span class="line"><span class="comment">// 返回注意力机制的输出矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">ScaledDotProductAttention</span><span class="params">(Q, K, V *mat.Dense, mask *mat.Dense)</span></span> (*mat.Dense, <span class="type">error</span>) &#123;</span><br><span class="line">	<span class="comment">// 1. QK^T</span></span><br><span class="line">	<span class="comment">// 获取 Q 的行数和 K 的列数，K^T 的行数是 K 的列数</span></span><br><span class="line">	rQ, cQ := Q.Dims()</span><br><span class="line">	rK, cK := K.Dims()</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 确保 Q 的列数等于 K 的列数 (d_k)</span></span><br><span class="line">	<span class="keyword">if</span> cQ != cK &#123;</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;query and key dimensions mismatch: cQ=%d, cK=%d&quot;</span>, cQ, cK)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// K 转置</span></span><br><span class="line">	Kt := mat.NewDense(cK, rK, <span class="literal">nil</span>)</span><br><span class="line">	Kt.CloneFrom(K.T()) <span class="comment">// K.T() 返回的是 View，需要CloneFrom</span></span><br><span class="line"></span><br><span class="line">	scores := mat.NewDense(rQ, rK, <span class="literal">nil</span>) <span class="comment">// rQ x rK (m x m for self-attention)</span></span><br><span class="line">	scores.Mul(Q, Kt) <span class="comment">// QK^T</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 2. 缩放 (Scaling)</span></span><br><span class="line">	dk := <span class="type">float64</span>(cQ) <span class="comment">// d_k 是 Q 的列数 (或 K 的列数)</span></span><br><span class="line">	scaleFactor := math.Sqrt(dk)</span><br><span class="line">	scores.Scale(<span class="number">1</span>/scaleFactor, scores)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 3. 应用掩码 (如果存在)</span></span><br><span class="line">	<span class="keyword">if</span> mask != <span class="literal">nil</span> &#123;</span><br><span class="line">		rMask, cMask := mask.Dims()</span><br><span class="line">		<span class="keyword">if</span> rMask != rQ || cMask != rK &#123;</span><br><span class="line">			<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;mask dimensions mismatch: expected %dx%d, got %dx%d&quot;</span>, rQ, rK, rMask, cMask)</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 遍历 scores 矩阵，将 mask 为 0 的位置替换为负无穷</span></span><br><span class="line">		scores.Apply(<span class="function"><span class="keyword">func</span><span class="params">(r, c <span class="type">int</span>, v <span class="type">float64</span>)</span></span> <span class="type">float64</span> &#123;</span><br><span class="line">			<span class="keyword">if</span> mask.At(r, c) == <span class="number">0</span> &#123; <span class="comment">// 假设 mask 为 0 表示该位置被遮盖</span></span><br><span class="line">				<span class="keyword">return</span> math.Inf(<span class="number">-1</span>) <span class="comment">// 负无穷</span></span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">return</span> v</span><br><span class="line">		&#125;, scores)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 4. Softmax</span></span><br><span class="line">	<span class="comment">// 需要对每一行独立进行 Softmax</span></span><br><span class="line">	rScores, cScores := scores.Dims()</span><br><span class="line">	attentionWeights := mat.NewDense(rScores, cScores, <span class="literal">nil</span>)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">for</span> r := <span class="number">0</span>; r &lt; rScores; r++ &#123;</span><br><span class="line">		row := mat.Row(<span class="literal">nil</span>, r, scores)</span><br><span class="line">		<span class="comment">// 计算行的指数和</span></span><br><span class="line">		sumExp := <span class="number">0.0</span></span><br><span class="line">		<span class="keyword">for</span> _, val := <span class="keyword">range</span> row &#123;</span><br><span class="line">			sumExp += math.Exp(val)</span><br><span class="line">		&#125;</span><br><span class="line">		<span class="comment">// 计算 softmax 值</span></span><br><span class="line">		<span class="keyword">for</span> c := <span class="number">0</span>; c &lt; cScores; c++ &#123;</span><br><span class="line">			attentionWeights.Set(r, c, math.Exp(row[c])/sumExp)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 5. 与 Value 矩阵相乘</span></span><br><span class="line">	rV, cV := V.Dims()</span><br><span class="line">	<span class="keyword">if</span> cScores != rV &#123; <span class="comment">// attentionWeights 的列数必须等于 V 的行数</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">&quot;attention weights column count %d mismatches Value matrix row count %d&quot;</span>, cScores, rV)</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	output := mat.NewDense(rQ, cV, <span class="literal">nil</span>) <span class="comment">// 输出维度 rQ x cV</span></span><br><span class="line">	output.Mul(attentionWeights, V)</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> output, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">// 示例用法：</span></span><br><span class="line"><span class="comment">func main() &#123;</span></span><br><span class="line"><span class="comment">	// 假设 d_model = 4, sequence_length = 3</span></span><br><span class="line"><span class="comment">	// Q, K, V 都是 (sequence_length, d_k) 矩阵</span></span><br><span class="line"><span class="comment">	// 在自注意力中，d_k 通常等于 d_model / num_heads</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	qData := []float64&#123;</span></span><br><span class="line"><span class="comment">		1.0, 0.0, 1.0, 0.0,</span></span><br><span class="line"><span class="comment">		0.0, 1.0, 0.0, 1.0,</span></span><br><span class="line"><span class="comment">		1.0, 1.0, 0.0, 0.0,</span></span><br><span class="line"><span class="comment">	&#125;</span></span><br><span class="line"><span class="comment">	kData := []float64&#123;</span></span><br><span class="line"><span class="comment">		1.0, 1.0, 0.0, 0.0,</span></span><br><span class="line"><span class="comment">		0.0, 0.0, 1.0, 1.0,</span></span><br><span class="line"><span class="comment">		1.0, 0.0, 1.0, 0.0,</span></span><br><span class="line"><span class="comment">	&#125;</span></span><br><span class="line"><span class="comment">	vData := []float64&#123;</span></span><br><span class="line"><span class="comment">		2.0, 0.0, 2.0, 0.0,</span></span><br><span class="line"><span class="comment">		0.0, 3.0, 0.0, 3.0,</span></span><br><span class="line"><span class="comment">		4.0, 4.0, 0.0, 0.0,</span></span><br><span class="line"><span class="comment">	&#125;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	Q := mat.NewDense(3, 4, qData)</span></span><br><span class="line"><span class="comment">	K := mat.NewDense(3, 4, kData)</span></span><br><span class="line"><span class="comment">	V := mat.NewDense(3, 4, vData)</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	fmt.Println(&quot;Q:\n&quot;, mat.Formatted(Q))</span></span><br><span class="line"><span class="comment">	fmt.Println(&quot;K:\n&quot;, mat.Formatted(K))</span></span><br><span class="line"><span class="comment">	fmt.Println(&quot;V:\n&quot;, mat.Formatted(V))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	// 模拟一个掩码 (例如，下三角矩阵用于 masked self-attention)</span></span><br><span class="line"><span class="comment">	maskData := []float64&#123;</span></span><br><span class="line"><span class="comment">		1, 0, 0,</span></span><br><span class="line"><span class="comment">		1, 1, 0,</span></span><br><span class="line"><span class="comment">		1, 1, 1,</span></span><br><span class="line"><span class="comment">	&#125;</span></span><br><span class="line"><span class="comment">	mask := mat.NewDense(3, 3, maskData)</span></span><br><span class="line"><span class="comment">	fmt.Println(&quot;Mask:\n&quot;, mat.Formatted(mask))</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">	output, err := ScaledDotProductAttention(Q, K, V, mask)</span></span><br><span class="line"><span class="comment">	if err != nil &#123;</span></span><br><span class="line"><span class="comment">		fmt.Println(&quot;Error:&quot;, err)</span></span><br><span class="line"><span class="comment">		return</span></span><br><span class="line"><span class="comment">	&#125;</span></span><br><span class="line"><span class="comment">	fmt.Println(&quot;Attention Output:\n&quot;, mat.Formatted(output))</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>Transformer 模型以其完全基于注意力机制的创新设计，彻底改变了深度学习在序列建模领域的应用。它通过并行计算能力和对长距离依赖的有效捕捉，超越了传统 RNN 模型的局限性，成为 NLP、CV 等众多领域的核心模型。从机器翻译到大语言模型，Transformer 及其变体持续推动着人工智能技术的发展。理解其编码器-解码器架构、多头自注意力机制和位置编码是理解现代 AI 模型工作原理的关键。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/30bb7c0cff3b/">https://blog.tbf1211.xx.kg/30bb7c0cff3b/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-03.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/289566312b0f/" title="Vision Transformer (ViT) 与 Residual Network (ResNet) 深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Vision Transformer (ViT) 与 Residual Network (ResNet) 深度详解</div></div><div class="info-2"><div class="info-item-1"> 在深度学习的计算机视觉领域，卷积神经网络 (CNN) 曾长期占据主导地位，而 Residual Network (ResNet) 则是其中一个里程碑式的创新，通过引入残差连接解决了深层网络训练中的梯度消失问题。近年来，随着 Transformer 模型在自然语言处理 (NLP) 领域取得巨大成功，研究人员尝试将其引入视觉领域，催生了 Vision Transformer (ViT)。ViT 颠覆了传统 CNN 的范式，直接将图像视为一系列序列化的图像块 (patches)，并用 Transformer 编码器进行处理。本文将对这两大具有代表性的模型进行深入剖析和比较。  ResNet 的核心思想： 通过残差连接 (Residual Connection) 允许网络学习残差函数，使得训练极深的网络变得可能，从而有效缓解了深度神经网络中的梯度消失和梯度爆炸问题，提高了模型性能。 ViT 的核心思想： 放弃了 CNN 的归纳偏置 (inductive bias)，直接将图像分割成固定大小的图像块 (patches)，并将其视为序列化的词向量 (tokens)，然后输入标准的 Tran...</div></div></div></a><a class="pagination-related" href="/7c1d07475a60/" title="图生图 (Image-to-Image) 原理详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-26.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">图生图 (Image-to-Image) 原理详解</div></div><div class="info-2"><div class="info-item-1"> 图生图 (Image-to-Image Generation) 是一种先进的人工智能技术，其核心是将一张输入图像作为条件，生成另一张与之相关的输出图像。这种技术能够实现图像风格转换、超分辨率、图像修复、语义分割图到真实图像转换等多种复杂的视觉任务。现代图生图模型通常建立在强大的生成模型之上，尤其是扩散模型 (Diffusion Models)，并通过精密的条件控制机制来引导图像的转换过程。  核心思想：图生图模型通过学习输入图像与目标输出图像之间的映射关系，将输入的视觉信息作为生成过程的条件。与文生图从随机噪声开始不同，图生图往往以输入图像的某种噪声化版本作为起点，然后通过迭代去噪过程，逐步生成符合条件的新图像。   一、为什么需要图生图？在计算机视觉领域，许多任务都可以被重新定义为图像到图像的转换问题。传统方法往往需要针对每个任务设计专门的算法，费时费力。图生图技术提供了一种统一且灵活的解决方案：  自动化复杂编辑：将耗时且专业的图像编辑工作（如图像修复、前景替换、风格化）自动化。 内容创作辅助：辅助艺术家和设计师快速生成不同风格的草图、渲染图或变体。 数据增强：为训练其他模...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/4b8301cdd035/" title="向量数据库 (Vector Database) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-21.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-03</div><div class="info-item-2">向量数据库 (Vector Database) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量数据库 (Vector Database &#x2F; Vector Store) 是一种专门设计用于高效存储、管理和检索向量嵌入 (Vector Embeddings) 的数据库。这些向量嵌入是高维的数值表示，由机器学习模型生成，能够捕捉文本、图像、音频或其他复杂数据的语义信息。向量数据库的核心能力在于通过计算向量之间的相似度 (Similarity) 来进行快速搜索，而非传统的精确匹配。  核心思想：将非结构化数据转化为机器可理解的低维或高维向量表示（嵌入），并在此基础上实现基于语义相似度的快速检索。它解决了传统数据库在处理语义搜索、推荐系统、多模态数据匹配等场景下的局限性。   一、什么是向量 (Vector)？在深入了解向量数据库之前，我们必须先理解“向量”这个核心概念。 1.1 向量的数学定义在数学和物理中，向量 (Vector) 是一个具有大小 (Magnitude) 和方向 (Direction) 的量。它可以被表示为一个有序的数值列表。  一维向量：一个标量，如 [5]。 二维向量：表示平面上的一个点或从原点指向该点的箭头，如 [x, y]。例如，[3, 4...</div></div></div></a><a class="pagination-related" href="/52b6fb1ec8d5/" title="文档嵌入模型 (Document Embedding Models) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-08</div><div class="info-item-2">文档嵌入模型 (Document Embedding Models) 详解</div></div><div class="info-2"><div class="info-item-1"> 文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的文档在多维向量空间中彼此靠近。这是构建高级信息检索、知识管理和内容理解系统的基石。   一、为什么需要文档嵌入模型？在大数据时代，我们面临着海量文档（如网页、报告、书籍、代码库、用户评论等）。传统处理这些文档的方法存在诸多局限：  关键词匹配的不足：搜索引擎通常依赖关键词匹配，但无法理解语义。例如，搜索“车祸”可能无法找到包含“交通事故”的文档。 句嵌入的局限性：虽然句嵌入能捕捉句子级别的语义，但在处理长文档时，简单地拼接或平均句嵌入会丢失文档整体的结构和主题信息。 高维稀疏性问题：传统的 Bag-of-Words (BOW) 或 TF-IDF 等模型将文档表示为高维稀疏向量，不仅计算效...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-15.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/aacac0039d65/" title="LangChain Chains 深度详解与应用实践"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-30</div><div class="info-item-2">LangChain Chains 深度详解与应用实践</div></div><div class="info-2"><div class="info-item-1"> LangChain 是一个强大的框架，旨在帮助开发者使用大语言模型（LLM）构建端到端的应用程序。在其众多核心模块中，Chains (链) 是最基础也是最重要的概念之一。它允许开发者将多个组件（如 LLM、提示模板、解析器、其他链）以逻辑顺序连接起来，形成一个完整的、可执行的流程，从而实现复杂的任务。  核心思想：Chains 的核心思想是将一系列操作（比如准备提示、调用 LLM、处理输出）串联起来，形成一个连贯的工作流。这使得开发者能够构建超越单一 LLM 调用的复杂应用程序，实现模块化、可组合和可扩展的 AI 应用。   一、为什么需要 Chains？大语言模型 (LLM) 固然强大，但直接调用 LLM 的 API 往往只能解决单一的、相对简单的问题。在实际应用中，我们面临的任务通常更加复杂：  多步骤任务：一个任务可能需要多次调用 LLM，每次调用基于上一次的输出。 输入预处理：可能需要根据用户输入动态地生成 LLM 提示。 输出后处理：LLM 的原始输出可能需要结构化、格式化或进一步处理才能使用。 数据检索：LLM 可能需要结合外部数据源（如数据库、文档）才能给出准确答...</div></div></div></a><a class="pagination-related" href="/51de73c05326/" title="LangGraph 库核心组件与调用方法详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-15</div><div class="info-item-2">LangGraph 库核心组件与调用方法详解</div></div><div class="info-2"><div class="info-item-1"> LangGraph 是 LangChain 生态系统中的一个高级库，它允许开发者使用有向无环图 (DAG) 的方式构建健壮、有状态且可控的 LLM 应用。它特别适用于需要多步骤推理、代理 (Agent) 行为、循环和人工干预的复杂工作流。LangGraph 的核心优势在于其明确的状态管理和对图结构的直接建模能力，使得构建和调试复杂代理系统变得更加直观和可靠。  核心思想：将多步骤的 LLM 应用程序建模为状态机，其中每个节点代表一个操作（LLM 调用、工具调用、函数等），边代表状态转换。通过在节点之间传递和修改状态，实现复杂、有循环的工作流。它解决了传统 LangChain Chain 在处理复杂逻辑（特别是循环和条件分支）时的局限性。    一、LangGraph 核心概念LangGraph 的设计基于图论和状态机的思想。理解以下核心概念是使用 LangGraph 的基础：  State (状态)：  表示整个应用程序在某个时间点的数据快照。 通过 StateDict 对象传递，它是一个字典或类似字典的结构。 节点操作通常会接收当前状态，并返回一个表示状态更新的 StateD...</div></div></div></a><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-27</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">448</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">225</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-Transformer%EF%BC%9F"><span class="toc-text">一、为什么需要 Transformer？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Transformer-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88"><span class="toc-text">二、Transformer 架构概览</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E7%BC%96%E7%A0%81%E5%99%A8-Encoder-%E8%AF%A6%E8%A7%A3"><span class="toc-text">三、编码器 (Encoder) 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%BE%93%E5%85%A5%E5%B5%8C%E5%85%A5-Input-Embedding-%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81-Positional-Encoding"><span class="toc-text">3.1 输入嵌入 (Input Embedding) 与位置编码 (Positional Encoding)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Multi-Head-Self-Attention"><span class="toc-text">3.2 多头自注意力机制 (Multi-Head Self-Attention)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Add-Norm-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E4%B8%8E%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">3.3 Add &amp; Norm (残差连接与层归一化)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-Feed-Forward-Network-FFN"><span class="toc-text">3.4 前馈神经网络 (Feed-Forward Network, FFN)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E8%A7%A3%E7%A0%81%E5%99%A8-Decoder-%E8%AF%A6%E8%A7%A3"><span class="toc-text">四、解码器 (Decoder) 详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%81%AE%E7%9B%96%E5%A4%9A%E5%A4%B4%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B-Masked-Multi-Head-Self-Attention"><span class="toc-text">4.1 遮盖多头自注意力 (Masked Multi-Head Self-Attention)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%A4%9A%E5%A4%B4%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%B3%A8%E6%84%8F%E5%8A%9B-Multi-Head-Encoder-Decoder-Attention"><span class="toc-text">4.2 多头编码器-解码器注意力 (Multi-Head Encoder-Decoder Attention)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E6%9C%80%E7%BB%88%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-text">4.3 最终输出层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Transformer-%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="toc-text">五、Transformer 的训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81Transformer-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="toc-text">六、Transformer 的优缺点与应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%BC%98%E7%82%B9"><span class="toc-text">6.1 优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E7%BC%BA%E7%82%B9"><span class="toc-text">6.2 缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8"><span class="toc-text">6.3 典型应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6-Go-%E8%AF%AD%E8%A8%80%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-%E6%A6%82%E5%BF%B5%E6%80%A7"><span class="toc-text">七、核心组件 Go 语言代码示例 (概念性)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">八、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CFFI (C Foreign Function Interface for Python) 详解"/></a><div class="content"><a class="title" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解">CFFI (C Foreign Function Interface for Python) 详解</a><time datetime="2025-12-23T22:24:00.000Z" title="发表于 2025-12-24 06:24:00">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-25.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="行为驱动开发 (BDD) 详解"/></a><div class="content"><a class="title" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解">行为驱动开发 (BDD) 详解</a><time datetime="2025-12-21T22:24:00.000Z" title="发表于 2025-12-22 06:24:00">2025-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="测试驱动开发 (TDD) 详解"/></a><div class="content"><a class="title" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解">测试驱动开发 (TDD) 详解</a><time datetime="2025-12-19T22:24:00.000Z" title="发表于 2025-12-20 06:24:00">2025-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-23.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPFS (InterPlanetary File System) 详解"/></a><div class="content"><a class="title" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解">IPFS (InterPlanetary File System) 详解</a><time datetime="2025-12-16T22:24:00.000Z" title="发表于 2025-12-17 06:24:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"/></a><div class="content"><a class="title" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)">L7 负载均衡详解 (Layer 7 Load Balancing Explained)</a><time datetime="2025-12-14T22:24:00.000Z" title="发表于 2025-12-15 06:24:00">2025-12-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-03.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>