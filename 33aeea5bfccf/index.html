<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大语言模型参数详解：规模、类型与意义 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型参数详解：规模、类型与意义">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/33aeea5bfccf/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-02.jpg">
<meta property="article:published_time" content="2025-01-21T22:24:00.000Z">
<meta property="article:modified_time" content="2026-02-28T09:37:21.872Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-02.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大语言模型参数详解：规模、类型与意义",
  "url": "https://blog.tbf1211.xx.kg/33aeea5bfccf/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-02.jpg",
  "datePublished": "2025-01-21T22:24:00.000Z",
  "dateModified": "2026-02-28T09:37:21.872Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/33aeea5bfccf/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const hour = new Date().getHours()
          const isNight = hour <= 6 || hour >= 7
          if (theme === undefined) isNight ? activateDarkMode() : activateLightMode()
          else theme === 'light' ? activateLightMode() : activateDarkMode()
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大语言模型参数详解：规模、类型与意义',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">567</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">233</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-02.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">大语言模型参数详解：规模、类型与意义</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大语言模型参数详解：规模、类型与意义</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-01-21T22:24:00.000Z" title="发表于 2025-01-22 06:24:00">2025-01-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>参数 (Parameters)</strong> 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。</strong></p>
</div>
<hr>
<h2 id="一、什么是大语言模型参数？"><a href="#一、什么是大语言模型参数？" class="headerlink" title="一、什么是大语言模型参数？"></a>一、什么是大语言模型参数？</h2><p>在神经网络的上下文中，<strong>参数</strong>是指模型在训练过程中需要学习和调整的所有<strong>权重 (weights)</strong> 和<strong>偏置 (biases)</strong>。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。</p>
<ul>
<li><strong>权重 (Weights)</strong>：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。</li>
<li><strong>偏置 (Biases)</strong>：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调整神经元的激活阈值，使得模型可以更好地拟合数据。</li>
</ul>
<p>LLMs 的参数通常是浮点数，存储在模型的各个层中，如：</p>
<ul>
<li><strong>词嵌入层 (Embedding Layer)</strong>：存储每个 Token 的嵌入向量。</li>
<li><strong>注意力机制 (Attention Mechanism)</strong>：包含用于计算 Query、Key、Value 矩阵的权重。</li>
<li><strong>前馈网络 (Feed-Forward Networks)</strong>：包含线性变换层的权重和偏置。</li>
<li><strong>层归一化 (Layer Normalization)</strong>：包含可学习的增益 (gain) 和偏置 (bias) 参数。</li>
</ul>
<p><strong>数学表示</strong>：<br>在一个简单的线性层中，输出 $y$ 可以通过输入 $x$、权重矩阵 $W$ 和偏置向量 $b$ 表示：<br>$$<br>y &#x3D; xW + b<br>$$<br>其中 $W$ 和 $b$ 就是这一层的参数。LLMs 由成百上千个这样的层堆叠而成，每个层都有其自己的 $W$ 和 $b$ 等参数。</p>
<h2 id="二、参数规模-Parameter-Scale"><a href="#二、参数规模-Parameter-Scale" class="headerlink" title="二、参数规模 (Parameter Scale)"></a>二、参数规模 (Parameter Scale)</h2><p>大语言模型的“大”字，很大程度上指的是其参数规模。从最初的几亿到现在的数万亿，参数数量的增长是推动 LLMs 能力飞跃的关键因素。</p>
<h3 id="2-1-参数规模的发展历程"><a href="#2-1-参数规模的发展历程" class="headerlink" title="2.1 参数规模的发展历程"></a>2.1 参数规模的发展历程</h3><table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">参数量</th>
<th align="left">发布年份</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ELMo</td>
<td align="left">93M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">BERT (base)</td>
<td align="left">110M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">BERT (large)</td>
<td align="left">340M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">GPT-2</td>
<td align="left">1.5B</td>
<td align="left">2019</td>
</tr>
<tr>
<td align="left">GPT-3</td>
<td align="left">175B</td>
<td align="left">2020</td>
</tr>
<tr>
<td align="left">Gopher</td>
<td align="left">280B</td>
<td align="left">2021</td>
</tr>
<tr>
<td align="left">Chinchilla</td>
<td align="left">70B</td>
<td align="left">2022</td>
</tr>
<tr>
<td align="left">LLaMA 2</td>
<td align="left">7B, 13B, 70B</td>
<td align="left">2023</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td align="left">估计万亿级（未公开）</td>
<td align="left">2023</td>
</tr>
</tbody></table>
<p><strong>M</strong> &#x3D; Million (百万), <strong>B</strong> &#x3D; Billion (十亿), <strong>T</strong> &#x3D; Trillion (万亿)</p>
<h3 id="2-2-参数规模的重要性"><a href="#2-2-参数规模的重要性" class="headerlink" title="2.2 参数规模的重要性"></a>2.2 参数规模的重要性</h3><ol>
<li><strong>能力涌现 (Emergent Abilities)</strong>：研究表明，当模型的参数规模达到一定阈值时，会涌现出新的能力，例如上下文学习 (In-context Learning)、多步推理 (Multi-step Reasoning) 等，这些能力在小规模模型中并不明显或根本不存在。</li>
<li><strong>知识存储容量</strong>：更多的参数意味着模型拥有更大的容量来存储在训练数据中学习到的语法、语义、事实知识和世界常识。</li>
<li><strong>泛化能力</strong>：参数越多的模型，理论上能够捕捉更复杂的数据模式，从而具有更强的泛化能力，能够更好地处理未见过的数据。</li>
<li><strong>计算资源需求</strong>：参数数量的增加直接导致训练和推理所需的计算资源（GPU、内存）呈指数级增长。</li>
</ol>
<h2 id="三、参数的类型与分布"><a href="#三、参数的类型与分布" class="headerlink" title="三、参数的类型与分布"></a>三、参数的类型与分布</h2><p>LLMs 的参数主要分布在 Transformer 架构的各个模块中。</p>
<h3 id="3-1-词嵌入层参数-Embedding-Layer-Parameters"><a href="#3-1-词嵌入层参数-Embedding-Layer-Parameters" class="headerlink" title="3.1 词嵌入层参数 (Embedding Layer Parameters)"></a>3.1 词嵌入层参数 (Embedding Layer Parameters)</h3><p>这部分参数存储了模型词汇表中每个 Token 的<strong>嵌入向量 (Embedding Vectors)</strong>。</p>
<ul>
<li><strong>计算方式</strong>：词汇表大小 $V \times$ 嵌入维度 $d_{model}$。</li>
<li><strong>示例</strong>：如果词汇表有 50,000 个 Token，嵌入维度是 768，则嵌入层有 $50,000 \times 768 \approx 3.8 \times 10^7$ (3800 万) 个参数。</li>
<li><strong>意义</strong>：这些参数是模型理解每个 Token 语义的基础。</li>
</ul>
<h3 id="3-2-Transformer-编码器-解码器层参数-Transformer-Layer-Parameters"><a href="#3-2-Transformer-编码器-解码器层参数-Transformer-Layer-Parameters" class="headerlink" title="3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)"></a>3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)</h3><p>每个 Transformer 层都包含多个子层，每个子层都有自己的参数。LLMs 通常由 $N$ 个这样的层堆叠而成。</p>
<ol>
<li><p><strong>多头自注意力机制 (Multi-Head Self-Attention)</strong>：</p>
<ul>
<li><strong>查询 (Query) $Q$、键 (Key) $K$、值 (Value) $V$ 的投影矩阵</strong>：每个头有三个线性层，将输入投影到 $Q, K, V$ 空间。如果有多头，这些层会独立存在或共享。</li>
<li><strong>输出投影矩阵</strong>：将所有头的输出拼接后，再经过一个线性层投影回原始维度。</li>
<li><strong>计算方式</strong>：主要参数量来自这些线性层，通常是 $4 \times d_{model} \times d_{model}$ 左右（考虑 $Q, K, V$ 投影和输出投影）。</li>
<li><strong>意义</strong>：这些参数让模型能够计算 Token 之间的注意力权重，捕捉上下文依赖关系。</li>
</ul>
</li>
<li><p><strong>前馈网络 (Feed-Forward Network, FFN)</strong>：</p>
<ul>
<li>每个 FFN 通常包含两个线性层，中间夹着一个非线性激活函数。</li>
<li><strong>计算方式</strong>：通常是 $d_{model} \times d_{ff} + d_{ff} \times d_{model}$，其中 $d_{ff}$ 通常是 $d_{model}$ 的 4 倍。因此，大约是 $4 \times d_{model}^2 + 4 \times d_{model}^2 &#x3D; 8 \times d_{model}^2$ 左右。</li>
<li><strong>意义</strong>：FFN 负责在每个 Token 的表示上进行更深层次的非线性变换和特征提取。</li>
</ul>
</li>
<li><p><strong>层归一化 (Layer Normalization)</strong>：</p>
<ul>
<li>每个归一化层有两个可学习参数：增益 (gain) $\gamma$ 和偏置 (bias) $\beta$，均为 $d_{model}$ 维度。</li>
<li><strong>计算方式</strong>：$2 \times d_{model}$。</li>
<li><strong>意义</strong>：稳定训练过程。</li>
</ul>
</li>
</ol>
<p><strong>总参数量粗略估算</strong>：<br>对于一个 Transformer 模型，总参数量大致可以估算为：<br>$$<br>\text{Total Parameters} \approx V \times d_{model} + N \times ( \text{Attention Params} + \text{FFN Params} + \text{Norm Params})<br>$$<br>其中 $V$ 是词汇表大小，$d_{model}$ 是模型维度，$N$ 是层数。<br>可以看出，大部分参数量集中在 FFN 和注意力机制中，并且随着层数 $N$ 和模型维度 $d_{model}$ 的增加，参数量呈二次方增长。</p>
<h2 id="四、参数训练与优化"><a href="#四、参数训练与优化" class="headerlink" title="四、参数训练与优化"></a>四、参数训练与优化</h2><p>LLMs 的训练是一个极其计算密集的过程，涉及到数万亿次浮点运算。</p>
<h3 id="4-1-预训练-Pre-training"><a href="#4-1-预训练-Pre-training" class="headerlink" title="4.1 预训练 (Pre-training)"></a>4.1 预训练 (Pre-training)</h3><ul>
<li><strong>目标</strong>：在海量无标注文本数据上学习语言的通用模式、语法、语义和世界知识。</li>
<li><strong>方法</strong>：通常采用<strong>自监督学习</strong>任务，如<strong>下一个词预测 (Next Token Prediction)</strong>（如 GPT 系列）或<strong>掩码语言建模 (Masked Language Modeling)</strong>（如 BERT 系列）。</li>
<li><strong>优化器</strong>：使用像 AdamW 这样的优化器，通过反向传播 (Backpropagation) 和梯度下降 (Gradient Descent) 算法，迭代地调整所有参数，以最小化预测误差。</li>
</ul>
<h3 id="4-2-微调-Fine-tuning"><a href="#4-2-微调-Fine-tuning" class="headerlink" title="4.2 微调 (Fine-tuning)"></a>4.2 微调 (Fine-tuning)</h3><ul>
<li><strong>目标</strong>：在特定任务的有标注数据集上进一步训练模型，使其适应特定应用。</li>
<li><strong>方法</strong>：通常是全参数微调，或者采用高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 方法，如 LoRA (Low-Rank Adaptation)。LoRA 只训练少量额外的低秩矩阵参数，大幅减少计算和存储开销。</li>
</ul>
<h3 id="4-3-知识蒸馏-Knowledge-Distillation"><a href="#4-3-知识蒸馏-Knowledge-Distillation" class="headerlink" title="4.3 知识蒸馏 (Knowledge Distillation)"></a>4.3 知识蒸馏 (Knowledge Distillation)</h3><ul>
<li><strong>目标</strong>：将一个大型模型的知识转移到一个小型模型中，以创建更高效、更易部署的模型。</li>
<li><strong>方法</strong>：小型模型（学生模型）在大型模型（教师模型）的指导下进行训练，尝试模仿教师模型的输出。</li>
</ul>
<h2 id="五、Go-语言中参数的概念性表示"><a href="#五、Go-语言中参数的概念性表示" class="headerlink" title="五、Go 语言中参数的概念性表示"></a>五、Go 语言中参数的概念性表示</h2><p>在 Go 语言中，我们通常不会直接操作数万亿个参数。但我们可以用结构体来概念性地表示一个简单线性层的权重和偏置。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;gonum.org/v1/gonum/mat&quot;</span> <span class="comment">// 使用gonum库进行矩阵运算</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinearLayerParams 结构体表示一个线性层的参数</span></span><br><span class="line"><span class="keyword">type</span> LinearLayerParams <span class="keyword">struct</span> &#123;</span><br><span class="line">	Weights *mat.Dense <span class="comment">// 权重矩阵</span></span><br><span class="line">	Biases  *mat.Dense <span class="comment">// 偏置向量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewLinearLayerParams 创建并初始化一个线性层的参数</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewLinearLayerParams</span><span class="params">(inputDim, outputDim <span class="type">int</span>)</span></span> *LinearLayerParams &#123;</span><br><span class="line">	<span class="comment">// 随机初始化权重矩阵 (通常用更复杂的初始化策略，这里简化)</span></span><br><span class="line">	weights := mat.NewDense(inputDim, outputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="comment">// 初始化为全零或小的随机数</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; inputDim; i++ &#123;</span><br><span class="line">		<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; outputDim; j++ &#123;</span><br><span class="line">			weights.Set(i, j, <span class="type">float64</span>(i*outputDim+j)/<span class="type">float64</span>(inputDim*outputDim*<span class="number">100</span>)) <span class="comment">// 示例值</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 初始化偏置向量</span></span><br><span class="line">	biases := mat.NewDense(<span class="number">1</span>, outputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="comment">// 初始化为全零或小的随机数</span></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; outputDim; j++ &#123;</span><br><span class="line">		biases.Set(<span class="number">0</span>, j, <span class="type">float64</span>(j)/<span class="type">float64</span>(outputDim*<span class="number">100</span>)) <span class="comment">// 示例值</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;LinearLayerParams&#123;</span><br><span class="line">		Weights: weights,</span><br><span class="line">		Biases:  biases,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Forward 实现线性层的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *LinearLayerParams)</span></span> Forward(input *mat.Dense) *mat.Dense &#123;</span><br><span class="line">	<span class="keyword">var</span> output mat.Dense</span><br><span class="line">	<span class="comment">// 计算 input * Weights</span></span><br><span class="line">	output.Mul(input, l.Weights)</span><br><span class="line">	<span class="comment">// 加上偏置 (偏置会广播到每一行)</span></span><br><span class="line">	output.Add(&amp;output, l.Biases)</span><br><span class="line">	<span class="keyword">return</span> &amp;output</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CountParameters 计算线性层的参数总量</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *LinearLayerParams)</span></span> CountParameters() <span class="type">int</span> &#123;</span><br><span class="line">	_, colsW := l.Weights.Dims()</span><br><span class="line">	rowsW, _ := l.Weights.Dims()</span><br><span class="line">	_, colsB := l.Biases.Dims()</span><br><span class="line">	<span class="keyword">return</span> rowsW*colsW + colsB <span class="comment">// 权重数量 + 偏置数量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	inputDim := <span class="number">768</span>  <span class="comment">// 输入特征维度 (例如：Token Embedding 维度)</span></span><br><span class="line">	outputDim := <span class="number">3072</span> <span class="comment">// 输出特征维度 (例如：FFN 的中间维度)</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 创建一个线性层</span></span><br><span class="line">	layer := NewLinearLayerParams(inputDim, outputDim)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 打印参数数量</span></span><br><span class="line">	fmt.Printf(<span class="string">&quot;Linear Layer Parameters: %d\n&quot;</span>, layer.CountParameters()) <span class="comment">// 768*3072 + 3072 = 2359296 + 3072 = 2362368</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 模拟输入</span></span><br><span class="line">	inputVector := mat.NewDense(<span class="number">1</span>, inputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; inputDim; i++ &#123;</span><br><span class="line">		inputVector.Set(<span class="number">0</span>, i, <span class="type">float64</span>(i)/<span class="type">float64</span>(inputDim))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 执行前向传播</span></span><br><span class="line">	outputVector := layer.Forward(inputVector)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Input shape: %v\n&quot;</span>, inputVector.Dims())</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Output shape: %v\n&quot;</span>, outputVector.Dims())</span><br><span class="line">	<span class="comment">// fmt.Printf(&quot;Output (first 5 values): %v\n&quot;, outputVector.Slice(0, 1, 0, 5)) // 打印部分输出</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 概念性地表示一个拥有多个层的简化LLM</span></span><br><span class="line">	<span class="comment">// 在一个真实的LLM中，会有几十到几百个这样的层</span></span><br><span class="line">	fmt.Println(<span class="string">&quot;\n--- Conceptual LLM Parameter Counting ---&quot;</span>)</span><br><span class="line">	totalLLMParams := <span class="number">0</span></span><br><span class="line">	numLayers := <span class="number">12</span> <span class="comment">// 假设有12个Transformer层</span></span><br><span class="line">	attentionParamsPerLayer := inputDim*inputDim*<span class="number">4</span> <span class="comment">// 粗略估算注意力头的参数</span></span><br><span class="line">	ffnParamsPerLayer := inputDim*outputDim*<span class="number">2</span> + outputDim*inputDim*<span class="number">2</span> <span class="comment">// 粗略估算FFN的参数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 嵌入层</span></span><br><span class="line">	vocabSize := <span class="number">50000</span></span><br><span class="line">	embeddingParams := vocabSize * inputDim</span><br><span class="line">	totalLLMParams += embeddingParams</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Embedding Layer Parameters: %d\n&quot;</span>, embeddingParams)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 每层参数</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; numLayers; i++ &#123;</span><br><span class="line">		layerParams := attentionParamsPerLayer + ffnParamsPerLayer + <span class="number">2</span>*inputDim <span class="comment">// 2*inputDim for LayerNorm</span></span><br><span class="line">		totalLLMParams += layerParams</span><br><span class="line">		<span class="comment">// fmt.Printf(&quot;Layer %d Parameters: %d\n&quot;, i+1, layerParams)</span></span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Total Estimated LLM Parameters (Simplified): %d\n&quot;</span>, totalLLMParams)</span><br><span class="line">	<span class="comment">// 注意：这是一个非常粗略的估算，真实LLM的参数计算远比这复杂，涉及到更详细的网络结构</span></span><br><span class="line">	<span class="comment">// 但这展示了参数如何累积，并说明了为什么LLMs的参数数量如此庞大</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>解释</strong>：上述 Go 代码使用了 <code>gonum/mat</code> 库来概念性地展示一个线性层（神经网络的基本构建块）如何包含权重和偏置。<code>CountParameters</code> 函数演示了如何计算这些参数的数量。在 <code>main</code> 函数中，进一步展示了一个<strong>高度简化和粗略估算</strong>的 LLM 参数计算，说明了即使是几个基本组件和少许层，参数量也能迅速累积到数百万甚至数十亿。这仅仅是为了概念性说明，真实的 LLM 参数计算要复杂得多，涉及到精确的网络拓扑和每个子模块的详细尺寸。</p>
<h2 id="六、挑战与未来方向"><a href="#六、挑战与未来方向" class="headerlink" title="六、挑战与未来方向"></a>六、挑战与未来方向</h2><ol>
<li><strong>计算与内存需求</strong>：训练和部署超大规模模型需要巨大的计算资源（数千甚至上万块 GPU）和内存（数百 GB 甚至数 TB）。</li>
<li><strong>训练稳定性</strong>：参数规模越大，训练越容易不稳定，需要更复杂的优化策略和技术。</li>
<li><strong>碳足迹</strong>：大型模型的训练消耗大量能源，对环境产生影响。</li>
<li><strong>模型压缩</strong>：研究人员正在探索参数高效微调 (PEFT)、量化 (Quantization)、剪枝 (Pruning) 等技术，以减小模型体积，降低部署成本，同时保持性能。</li>
<li><strong>模型架构优化</strong>：开发更高效的模型架构，以在相同参数量下实现更好的性能，或用更少的参数达到同等性能。</li>
</ol>
<h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>大语言模型的参数是其“大脑”和“知识库”的具象化。这些通过大规模训练学习到的数值权重和偏置，赋予了模型理解、生成和推理人类语言的能力。参数规模的增长是 LLMs 发展历程中的核心驱动力，带来了前所未有的能力涌现，但同时也带来了巨大的计算挑战。深入理解参数的结构、类型和优化过程，是理解 LLMs 本质及其未来发展方向的关键。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/33aeea5bfccf/">https://blog.tbf1211.xx.kg/33aeea5bfccf/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-02.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/fec0454e87d6/" title="Golang Gin 框架深度解析"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Golang Gin 框架深度解析</div></div><div class="info-2"><div class="info-item-1"> Gin 是一个用 Go 语言编写的 HTTP Web 框架，它以高性能和易用性著称。Gin 框架通过一个类似 Martini 的 API，但拥有显著更高的性能，这得益于其底层优化的路由引擎 httprouter。它非常适合构建 RESTful API 服务、微服务和高并发的 Web 应用程序。  核心思想：Gin 通过一个轻量级的路由引擎和可插拔的中间件机制，提供了一个快速、灵活且强大的 Web 开发骨架，将请求处理分解为一系列可管理的阶段。   一、为什么选择 Gin？在 Go 语言的 Web 框架中，Gin 凭借以下优势脱颖而出：  极高性能：Gin 宣称其性能比其他 Go 框架（如 net/http 原生路由器、Martini 等）高出 40 倍，因为它使用了优化的 httprouter 库，并且避免了反射。 易于使用：简洁的 API 设计使得学习曲线平缓，开发者可以快速上手并构建应用。 中间件支持：强大的中间件机制允许开发者在请求处理流程中插入自定义逻辑，如日志记录、认证、错误恢复等，实现代码复用和模块化。 路由灵活：支持丰富的路由定义，包括参数路由、通配符路由和路由组...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2ef7cb8bd831/" title="LangChain Text Splitters 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-03</div><div class="info-item-2">LangChain Text Splitters 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Text Splitters 是 LangChain 框架中的一个核心模块，用于将长文档或文本智能地分割成更小、更易于管理和处理的块 (chunks)。这个过程对于大语言模型 (LLM) 相关的应用至关重要，特别是当处理的文本长度超出 LLM 的上下文窗口限制时。  核心思想：将长文本分割成大小适中、语义连贯且包含一定重叠的块，以便 LLM 能够有效处理这些块，同时保持上下文完整性。LangChain 提供多种具有不同策略的 Text Splitters，以适应不同的文本结构和应用场景。   一、为什么需要 Text Splitters？在构建基于 LLM 的应用程序（尤其是问答 RAG (Retrieval Augmented Generation) 系统、文档摘要、聊天机器人等）时，我们经常遇到以下问题：  LLM 上下文窗口限制 (Context Window Limit)：大语言模型（如 GPT-3.5, GPT-4, Llama）通常有一个固定的最大输入长度。如果输入文本太长，会超出这个限制，导致模型无法处理。 性能和成本：即使模型支持很长的上下文...</div></div></div></a><a class="pagination-related" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-11</div><div class="info-item-2">智能体 (Agent) 详解：深入 LangChain 开发实践</div></div><div class="info-2"><div class="info-item-1"> 智能体 (Agent) 是人工智能领域中的一个核心概念，特指能够感知环境、进行决策并采取行动以实现特定目标或利益的实体。在当前的技术浪潮中，特别是随着大语言模型 (LLM) 的突破，智能体这一概念被赋予了新的活力和强大的实现路径。基于 LLM 的智能体能够理解复杂的指令、规划任务、执行外部工具并进行自我反思，从而展现出接近自主解决问题的能力。  核心思想：智能体是一个自主运行的系统，它通过感知 (Perception)、思考 (Thought&#x2F;Planning)、行动 (Action) 和反馈 (Feedback&#x2F;Memory) 的闭环循环，在动态环境中追求并实现预设目标。Python 中的 LangChain 库提供了一套强大的工具和框架，用于快速构建和部署基于 LLM 的智能体，使其能够与各种外部资源和工具交互。   一、智能体的基本概念1.1 什么是智能体？在广义的人工智能领域，智能体是一个能够自主地运作以影响其所处环境的实体。其核心能力体现在以下循环：  感知 (Perception)：接收来自环境的信息（传感器输入，如文本、图像、数据）。 思考&#...</div></div></div></a><a class="pagination-related" href="/30bb7c0cff3b/" title="Transformer 模型深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-14</div><div class="info-item-2">Transformer 模型深度详解</div></div><div class="info-2"><div class="info-item-1"> Transformer 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了序列建模任务的主流架构。  核心思想：Transformer 放弃了传统的循环和卷积结构，完全依赖于注意力机制 (Attention Mechanism)来捕捉输入序列中的依赖关系。通过精心设计的自注意力 (Self-Attention) 机制，模型能够同时关注输入序列中的所有位置，从而实现高效的并行计算和对任意距离依赖的有效建模。   一、为什么需要 Transformer？在 Transformer 出现之前，RNN 及其变体 (如 LSTM 和 GRU) 是序列建模任务的主流。然而，它们存在一些固有的局限性：  顺序依赖：RNN 必须顺序地处理序列中的每个元素，后一个元素的计算依赖...</div></div></div></a><a class="pagination-related" href="/51de73c05326/" title="LangGraph 库核心组件与调用方法详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-15</div><div class="info-item-2">LangGraph 库核心组件与调用方法详解</div></div><div class="info-2"><div class="info-item-1"> LangGraph 是 LangChain 生态系统中的一个高级库，它允许开发者使用有向无环图 (DAG) 的方式构建健壮、有状态且可控的 LLM 应用。它特别适用于需要多步骤推理、代理 (Agent) 行为、循环和人工干预的复杂工作流。LangGraph 的核心优势在于其明确的状态管理和对图结构的直接建模能力，使得构建和调试复杂代理系统变得更加直观和可靠。  核心思想：将多步骤的 LLM 应用程序建模为状态机，其中每个节点代表一个操作（LLM 调用、工具调用、函数等），边代表状态转换。通过在节点之间传递和修改状态，实现复杂、有循环的工作流。它解决了传统 LangChain Chain 在处理复杂逻辑（特别是循环和条件分支）时的局限性。    一、LangGraph 核心概念LangGraph 的设计基于图论和状态机的思想。理解以下核心概念是使用 LangGraph 的基础：  State (状态)：  表示整个应用程序在某个时间点的数据快照。 通过 StateDict 对象传递，它是一个字典或类似字典的结构。 节点操作通常会接收当前状态，并返回一个表示状态更新的 StateD...</div></div></div></a><a class="pagination-related" href="/52b6fb1ec8d5/" title="文档嵌入模型 (Document Embedding Models) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-26.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-08</div><div class="info-item-2">文档嵌入模型 (Document Embedding Models) 详解</div></div><div class="info-2"><div class="info-item-1"> 文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的文档在多维向量空间中彼此靠近。这是构建高级信息检索、知识管理和内容理解系统的基石。   一、为什么需要文档嵌入模型？在大数据时代，我们面临着海量文档（如网页、报告、书籍、代码库、用户评论等）。传统处理这些文档的方法存在诸多局限：  关键词匹配的不足：搜索引擎通常依赖关键词匹配，但无法理解语义。例如，搜索“车祸”可能无法找到包含“交通事故”的文档。 句嵌入的局限性：虽然句嵌入能捕捉句子级别的语义，但在处理长文档时，简单地拼接或平均句嵌入会丢失文档整体的结构和主题信息。 高维稀疏性问题：传统的 Bag-of-Words (BOW) 或 TF-IDF 等模型将文档表示为高维稀疏向量，不仅计算效...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">567</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">233</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">85</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-text">一、什么是大语言模型参数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1-Parameter-Scale"><span class="toc-text">二、参数规模 (Parameter Scale)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-text">2.1 参数规模的发展历程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">2.2 参数规模的重要性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8F%82%E6%95%B0%E7%9A%84%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%88%86%E5%B8%83"><span class="toc-text">三、参数的类型与分布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%B1%82%E5%8F%82%E6%95%B0-Embedding-Layer-Parameters"><span class="toc-text">3.1 词嵌入层参数 (Embedding Layer Parameters)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Transformer-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82%E5%8F%82%E6%95%B0-Transformer-Layer-Parameters"><span class="toc-text">3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-text">四、参数训练与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%A2%84%E8%AE%AD%E7%BB%83-Pre-training"><span class="toc-text">4.1 预训练 (Pre-training)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%BE%AE%E8%B0%83-Fine-tuning"><span class="toc-text">4.2 微调 (Fine-tuning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Knowledge-Distillation"><span class="toc-text">4.3 知识蒸馏 (Knowledge Distillation)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Go-%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%8F%82%E6%95%B0%E7%9A%84%E6%A6%82%E5%BF%B5%E6%80%A7%E8%A1%A8%E7%A4%BA"><span class="toc-text">五、Go 语言中参数的概念性表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%8C%91%E6%88%98%E4%B8%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-text">六、挑战与未来方向</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">七、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/cd5b6fd178d8/" title="go.sum 文件中特殊哈希计算详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="go.sum 文件中特殊哈希计算详解"/></a><div class="content"><a class="title" href="/cd5b6fd178d8/" title="go.sum 文件中特殊哈希计算详解">go.sum 文件中特殊哈希计算详解</a><time datetime="2026-02-23T22:24:00.000Z" title="发表于 2026-02-24 06:24:00">2026-02-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/d819fc26cbc7/" title="共识算法详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="共识算法详解"/></a><div class="content"><a class="title" href="/d819fc26cbc7/" title="共识算法详解">共识算法详解</a><time datetime="2026-02-19T22:24:00.000Z" title="发表于 2026-02-20 06:24:00">2026-02-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/1c5cbb334a2a/" title="计算机中熵的详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="计算机中熵的详解"/></a><div class="content"><a class="title" href="/1c5cbb334a2a/" title="计算机中熵的详解">计算机中熵的详解</a><time datetime="2026-02-17T22:24:00.000Z" title="发表于 2026-02-18 06:24:00">2026-02-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/9ed85cc18e8a/" title="CSP并发模型详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-10.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSP并发模型详解"/></a><div class="content"><a class="title" href="/9ed85cc18e8a/" title="CSP并发模型详解">CSP并发模型详解</a><time datetime="2026-02-15T22:24:00.000Z" title="发表于 2026-02-16 06:24:00">2026-02-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/0d177e0002e6/" title="程序错误处理详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-24.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="程序错误处理详解"/></a><div class="content"><a class="title" href="/0d177e0002e6/" title="程序错误处理详解">程序错误处理详解</a><time datetime="2026-02-13T22:24:00.000Z" title="发表于 2026-02-14 06:24:00">2026-02-14</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-02.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="🆕 2023">🆕 2023</a><a href="/archives/2024/" target="_blank" title="🆒 2024">🆒 2024</a><a href="/archives/2025/" target="_blank" title="👨‍👩‍👦 2025">👨‍👩‍👦 2025</a><a href="/archives/2026/" target="_blank" title="🆙 2026">🆙 2026</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="📁 分类">📁 分类</a><a href="/tags/" target="_blank" title="🔖 标签">🔖 标签</a><a href="/categories/" target="_blank" title="📽️ 时间线">📽️ 时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="💬 说说">💬 说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.9/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.1.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const parseViewBox = viewBox => {
    if (!viewBox) return null
    const parts = viewBox.trim().split(/[\s,]+/).map(n => Number(n))
    if (parts.length !== 4 || parts.some(n => Number.isNaN(n))) return null
    return parts
  }

  const getSvgViewBox = svg => {
    const attr = parseViewBox(svg.getAttribute('viewBox'))
    if (attr) return attr

    // Fallback: use bbox to build a viewBox
    try {
      const bbox = svg.getBBox()
      if (bbox && bbox.width && bbox.height) return [bbox.x, bbox.y, bbox.width, bbox.height]
    } catch (e) {
      // getBBox may fail on some edge cases; ignore
    }

    const w = Number(svg.getAttribute('width')) || 0
    const h = Number(svg.getAttribute('height')) || 0
    if (w > 0 && h > 0) return [0, 0, w, h]
    return [0, 0, 100, 100]
  }

  const setSvgViewBox = (svg, vb) => {
    svg.setAttribute('viewBox', `${vb[0]} ${vb[1]} ${vb[2]} ${vb[3]}`)
  }

  const clamp = (v, min, max) => Math.max(min, Math.min(max, v))

  const openSvgInNewTab = ({ source, initViewBox }) => {
    const getClonedSvg = () => {
      if (typeof source === 'string') {
        const template = document.createElement('template')
        template.innerHTML = source.trim()
        const svg = template.content.querySelector('svg')
        return svg ? svg.cloneNode(true) : null
      }
      if (source && typeof source.cloneNode === 'function') {
        return source.cloneNode(true)
      }
      return null
    }

    const clone = getClonedSvg()
    if (!clone) return
    if (initViewBox && initViewBox.length === 4) {
      clone.setAttribute('viewBox', initViewBox.join(' '))
    }
    if (!clone.getAttribute('xmlns')) clone.setAttribute('xmlns', 'http://www.w3.org/2000/svg')
    if (!clone.getAttribute('xmlns:xlink') && clone.outerHTML.includes('xlink:')) {
      clone.setAttribute('xmlns:xlink', 'http://www.w3.org/1999/xlink')
    }
    // inject background to match current theme
    const isDark = document.documentElement.getAttribute('data-theme') === 'dark'
    const bg = getComputedStyle(document.body).backgroundColor || (isDark ? '#1e1e1e' : '#ffffff')
    if (!clone.style.background) clone.style.background = bg

    const serializer = new XMLSerializer()
    const svgSource = serializer.serializeToString(clone)
    const htmlSource = `<!doctype html><html><head><meta charset="utf-8" />
      <style>
        html, body { width: 100%; height: 100%; margin: 0; display: flex; align-items: center; justify-content: center; background: ${bg}; }
        svg { max-width: 100%; max-height: 100%; height: auto; width: auto; }
      </style>
      </head><body>${svgSource}</body></html>`
    const blob = new Blob([htmlSource], { type: 'text/html;charset=utf-8' })
    const url = URL.createObjectURL(blob)
    window.open(url, '_blank', 'noopener')
    setTimeout(() => URL.revokeObjectURL(url), 30000)
  }

  const attachMermaidViewerButton = wrap => {
    let btn = wrap.querySelector('.mermaid-open-btn')
    if (!btn) {
      btn = document.createElement('button')
      btn.type = 'button'
      btn.className = 'mermaid-open-btn'
      wrap.appendChild(btn)
    }

    btn.innerHTML = '<i class="fa fa-search fa-fw" aria-hidden="true"></i>'

    if (!btn.__mermaidViewerBound) {
      btn.addEventListener('click', e => {
        e.preventDefault()
        e.stopPropagation()
        const svg = wrap.__mermaidOriginalSvg || wrap.querySelector('svg')
        if (!svg) return
        const initViewBox = wrap.__mermaidInitViewBox
        if (typeof svg === 'string') {
          openSvgInNewTab({ source: svg, initViewBox })
          return
        }
        openSvgInNewTab({ source: svg, initViewBox })
      })
      btn.__mermaidViewerBound = true
    }
  }

  // Zoom around a point (px, py) in the SVG viewport (in viewBox coordinates)
  const zoomAtPoint = (vb, factor, px, py) => {
    const w = vb[2] * factor
    const h = vb[3] * factor
    const nx = px - (px - vb[0]) * factor
    const ny = py - (py - vb[1]) * factor
    return [nx, ny, w, h]
  }

  const initMermaidGestures = wrap => {
    const svg = wrap.querySelector('svg')
    if (!svg) return

    // Ensure viewBox exists so gestures always work
    const initVb = getSvgViewBox(svg)
    wrap.__mermaidInitViewBox = initVb
    wrap.__mermaidCurViewBox = initVb.slice()
    setSvgViewBox(svg, initVb)

    // Avoid binding multiple times on themeChange/pjax
    if (wrap.__mermaidGestureBound) return
    wrap.__mermaidGestureBound = true

    // Helper: map client (viewport) coordinate -> viewBox coordinate
    const clientToViewBox = (clientX, clientY) => {
      const rect = svg.getBoundingClientRect()
      const vb = wrap.__mermaidCurViewBox || getSvgViewBox(svg)
      const x = vb[0] + (clientX - rect.left) * (vb[2] / rect.width)
      const y = vb[1] + (clientY - rect.top) * (vb[3] / rect.height)
      return { x, y, rect, vb }
    }

    const state = {
      pointers: new Map(),
      startVb: null,
      startDist: 0,
      startCenter: null
    }

    const clampVb = vb => {
      const init = wrap.__mermaidInitViewBox || vb
      const minW = init[2] * 0.1
      const maxW = init[2] * 10
      const minH = init[3] * 0.1
      const maxH = init[3] * 10
      vb[2] = clamp(vb[2], minW, maxW)
      vb[3] = clamp(vb[3], minH, maxH)
      return vb
    }

    const setCurVb = vb => {
      vb = clampVb(vb)
      wrap.__mermaidCurViewBox = vb
      setSvgViewBox(svg, vb)
    }

    const onPointerDown = e => {
      // Allow only primary button for mouse
      if (e.pointerType === 'mouse' && e.button !== 0) return
      svg.setPointerCapture(e.pointerId)
      state.pointers.set(e.pointerId, { x: e.clientX, y: e.clientY })

      if (state.pointers.size === 1) {
        state.startVb = (wrap.__mermaidCurViewBox || getSvgViewBox(svg)).slice()
      } else if (state.pointers.size === 2) {
        const pts = [...state.pointers.values()]
        const dx = pts[0].x - pts[1].x
        const dy = pts[0].y - pts[1].y
        state.startDist = Math.hypot(dx, dy)
        state.startVb = (wrap.__mermaidCurViewBox || getSvgViewBox(svg)).slice()
        state.startCenter = { x: (pts[0].x + pts[1].x) / 2, y: (pts[0].y + pts[1].y) / 2 }
      }
    }

    const onPointerMove = e => {
      if (!state.pointers.has(e.pointerId)) return
      state.pointers.set(e.pointerId, { x: e.clientX, y: e.clientY })

      // Pan with 1 pointer
      if (state.pointers.size === 1 && state.startVb) {
        const p = [...state.pointers.values()][0]
        const prev = { x: e.clientX - e.movementX, y: e.clientY - e.movementY }
        // movementX/Y unreliable on touch, compute from stored last position
        const last = wrap.__mermaidLastSinglePointer || p
        const dxClient = p.x - last.x
        const dyClient = p.y - last.y
        wrap.__mermaidLastSinglePointer = p

        const { rect } = clientToViewBox(p.x, p.y)
        const vb = (wrap.__mermaidCurViewBox || getSvgViewBox(svg)).slice()
        const dx = dxClient * (vb[2] / rect.width)
        const dy = dyClient * (vb[3] / rect.height)
        setCurVb([vb[0] - dx, vb[1] - dy, vb[2], vb[3]])
        return
      }

      // Pinch zoom with 2 pointers
      if (state.pointers.size === 2 && state.startVb && state.startDist > 0) {
        const pts = [...state.pointers.values()]
        const dx = pts[0].x - pts[1].x
        const dy = pts[0].y - pts[1].y
        const dist = Math.hypot(dx, dy)
        if (!dist) return
        const factor = state.startDist / dist // dist bigger => zoom in (viewBox smaller)

        const cx = (pts[0].x + pts[1].x) / 2
        const cy = (pts[0].y + pts[1].y) / 2
        const centerClient = { x: cx, y: cy }

        const pxy = clientToViewBox(centerClient.x, centerClient.y)
        const cpx = pxy.x
        const cpy = pxy.y

        const vb = zoomAtPoint(state.startVb, factor, cpx, cpy)
        setCurVb(vb)
      }
    }

    const onPointerUpOrCancel = e => {
      state.pointers.delete(e.pointerId)
      if (state.pointers.size === 0) {
        state.startVb = null
        state.startDist = 0
        state.startCenter = null
        wrap.__mermaidLastSinglePointer = null
      } else if (state.pointers.size === 1) {
        // reset single pointer baseline to avoid jump
        wrap.__mermaidLastSinglePointer = [...state.pointers.values()][0]
      }
    }

    // Wheel zoom (mouse/trackpad)
    const onWheel = e => {
      // ctrlKey on mac trackpad pinch; we treat both as zoom
      e.preventDefault()
      const delta = e.deltaY
      const zoomFactor = delta > 0 ? 1.1 : 0.9
      const { x, y } = clientToViewBox(e.clientX, e.clientY)
      const vb = (wrap.__mermaidCurViewBox || getSvgViewBox(svg)).slice()
      setCurVb(zoomAtPoint(vb, zoomFactor, x, y))
    }

    const onDblClick = () => {
      const init = wrap.__mermaidInitViewBox
      if (!init) return
      wrap.__mermaidCurViewBox = init.slice()
      setSvgViewBox(svg, init)
    }

    svg.addEventListener('pointerdown', onPointerDown)
    svg.addEventListener('pointermove', onPointerMove)
    svg.addEventListener('pointerup', onPointerUpOrCancel)
    svg.addEventListener('pointercancel', onPointerUpOrCancel)
    svg.addEventListener('wheel', onWheel, { passive: false })
    svg.addEventListener('dblclick', onDblClick)
  }

  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild

      // Clear old render (themeChange/pjax will rerun)
      const oldSvg = item.querySelector('svg')
      if (oldSvg) oldSvg.remove()
      item.__mermaidGestureBound = false

      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
        if (false) initMermaidGestures(item)
        item.__mermaidOriginalSvg = svg
        if (true) attachMermaidViewerButton(item)
      }


      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.4"></script></div></div></body></html>