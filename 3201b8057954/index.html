<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LangChain 详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LangChain 是一个用于开发由大型语言模型 (LLM) 驱动的应用程序的开源框架。它提供了一套工具、组件和接口，旨在简化 LLM 应用的开发流程，包括将 LLM 与外部数据源、计算逻辑和业务流程相结合，从而构建更复杂、更强大、更具上下文感知能力的应用程序。  核心思想：将 LLM 的能力扩展到超越单一提示的范围，通过链式组合不同的组件（如 LLM、提示模板、解析器、工具、内存等），构建具有">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain 详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/3201b8057954/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="LangChain 是一个用于开发由大型语言模型 (LLM) 驱动的应用程序的开源框架。它提供了一套工具、组件和接口，旨在简化 LLM 应用的开发流程，包括将 LLM 与外部数据源、计算逻辑和业务流程相结合，从而构建更复杂、更强大、更具上下文感知能力的应用程序。  核心思想：将 LLM 的能力扩展到超越单一提示的范围，通过链式组合不同的组件（如 LLM、提示模板、解析器、工具、内存等），构建具有">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-10.jpg">
<meta property="article:published_time" content="2025-10-11T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-09T07:36:00.185Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LangChain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-10.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LangChain 详解",
  "url": "https://blog.tbf1211.xx.kg/3201b8057954/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-10.jpg",
  "datePublished": "2025-10-11T22:24:00.000Z",
  "dateModified": "2025-12-09T07:36:00.185Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/3201b8057954/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LangChain 详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">365</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">211</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">75</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-10.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">LangChain 详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LangChain 详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-10-11T22:24:00.000Z" title="发表于 2025-10-12 06:24:00">2025-10-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/">开发框架</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">6.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/3201b8057954/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>LangChain</strong> 是一个用于开发由大型语言模型 (LLM) 驱动的应用程序的开源框架。它提供了一套工具、组件和接口，旨在简化 LLM 应用的开发流程，包括将 LLM 与外部数据源、计算逻辑和业务流程相结合，从而构建更复杂、更强大、更具上下文感知能力的应用程序。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将 LLM 的能力扩展到超越单一提示的范围，通过链式组合不同的组件（如 LLM、提示模板、解析器、工具、内存等），构建具有推理、记忆和外部交互能力的复杂智能体 (Agent)。</strong></p>
</div>
<hr>
<h2 id="一、为什么需要-LangChain？"><a href="#一、为什么需要-LangChain？" class="headerlink" title="一、为什么需要 LangChain？"></a>一、为什么需要 LangChain？</h2><p>大型语言模型（LLM），如 GPT 系列、Llama 系列等，具有强大的文本理解和生成能力。然而，在实际应用中，直接使用 LLM API 存在一些挑战：</p>
<ol>
<li><strong>上下文限制 (Context Window Limitations)</strong>：LLM 有输入令牌限制，无法处理过长的文本。</li>
<li><strong>知识截止 (Knowledge Cutoff)</strong>：LLM 的知识基于训练数据，无法获取实时或私有数据。</li>
<li><strong>幻觉 (Hallucination)</strong>：LLM 可能生成不准确或虚构的信息。</li>
<li><strong>缺乏记忆</strong>：LLM 默认是无状态的，无法记住之前的对话。</li>
<li><strong>缺乏外部工具</strong>：LLM 无法直接执行外部操作，如查询数据库、调用 API、执行代码等。</li>
<li><strong>复杂逻辑编排</strong>：构建多步骤、有条件的 LLM 应用需要复杂的逻辑编排。</li>
<li><strong>可维护性与可扩展性</strong>：硬编码的 LLM 应用难以维护和扩展。</li>
</ol>
<p>LangChain 旨在通过提供以下功能来解决这些问题：</p>
<ul>
<li><strong>模块化组件</strong>：将 LLM 应用的各个部分（模型、提示、解析器、工具、记忆等）抽象为可重用的模块。</li>
<li><strong>链式结构 (Chains)</strong>：允许将这些模块以特定的顺序组合起来，形成一个完整的处理流程。</li>
<li><strong>智能体 (Agents)</strong>：赋予 LLM 决策能力，使其能够根据用户的请求选择并使用合适的工具来完成任务。</li>
<li><strong>检索增强生成 (RAG - Retrieval Augmented Generation)</strong>：通过结合外部知识库（如向量数据库），突破 LLM 的知识截止限制。</li>
<li><strong>可观测性与调试</strong>：通过 LangSmith 等工具提供对链式执行过程的详细跟踪。</li>
</ul>
<h2 id="二、LangChain-的核心概念"><a href="#二、LangChain-的核心概念" class="headerlink" title="二、LangChain 的核心概念"></a>二、LangChain 的核心概念</h2><p>LangChain 的设计围绕着一系列核心组件，它们共同协作构建复杂的 LLM 应用程序。</p>
<ol>
<li><p><strong>LLM (Large Language Model)</strong>:</p>
<ul>
<li><strong>定义</strong>：LangChain 对各种语言模型 API 的封装。可以是基于文本的生成模型 (<code>LLM</code>) 或基于聊天消息的模型 (<code>ChatModel</code>)。</li>
<li><strong>作用</strong>：作为应用程序的“大脑”，执行文本生成、理解和推理。</li>
</ul>
</li>
<li><p><strong>Prompt (提示)</strong>:</p>
<ul>
<li><strong>定义</strong>：用户或应用程序提供给 LLM 的输入文本，用于指导 LLM 的行为。LangChain 提供了模板 (<code>PromptTemplate</code>) 来结构化提示。</li>
<li><strong>作用</strong>：决定 LLM 的输出质量和相关性。</li>
</ul>
</li>
<li><p><strong>Output Parser (输出解析器)</strong>:</p>
<ul>
<li><strong>定义</strong>：用于将 LLM 生成的原始文本输出转换为结构化的数据格式（如 JSON, Python 对象）。</li>
<li><strong>作用</strong>：使 LLM 输出能够被下游组件或应用程序逻辑方便地处理。</li>
</ul>
</li>
<li><p><strong>Chain (链)</strong>:</p>
<ul>
<li><strong>定义</strong>：将多个组件（如 Prompt, LLM, Parser, 其他 Chain）按顺序组合起来，形成一个端到端的处理流程。</li>
<li><strong>作用</strong>：实现复杂的业务逻辑，将多个 LLM 调用和数据处理步骤自动化。</li>
</ul>
</li>
<li><p><strong>Retrieval (检索)</strong>:</p>
<ul>
<li><strong>定义</strong>：从外部数据源（如文档、数据库、API）中获取相关信息，以增强 LLM 的上下文。通常涉及向量数据库和嵌入模型。</li>
<li><strong>作用</strong>：解决 LLM 的知识截止和上下文限制问题，实现 RAG 模式。</li>
</ul>
</li>
<li><p><strong>Agent (智能体)</strong>:</p>
<ul>
<li><strong>定义</strong>：由一个 LLM 驱动的决策循环。Agent 能够根据用户输入和当前状态，动态地选择并使用一个或多个 <code>Tools</code> 来完成任务。</li>
<li><strong>作用</strong>：赋予 LLM 规划和执行复杂任务的能力，使其能够与外部世界交互。</li>
</ul>
</li>
<li><p><strong>Tool (工具)</strong>:</p>
<ul>
<li><strong>定义</strong>：Agent 可以调用的特定功能。可以是搜索工具、计算器、API 调用、数据库查询等。</li>
<li><strong>作用</strong>：扩展 LLM 的能力，使其能够执行特定的外部操作。</li>
</ul>
</li>
<li><p><strong>Memory (记忆)</strong>:</p>
<ul>
<li><strong>定义</strong>：存储和管理对话历史或会话状态，使 LLM 能够记住之前的交互。</li>
<li><strong>作用</strong>：实现多轮对话，保持对话的连贯性和上下文。</li>
</ul>
</li>
<li><p><strong>Callbacks (回调)</strong>:</p>
<ul>
<li><strong>定义</strong>：在链或 Agent 执行的各个阶段触发的函数，用于监控、日志记录、调试、自定义处理等。</li>
<li><strong>作用</strong>：提供对 LLM 应用内部执行流程的可见性和控制能力。</li>
</ul>
</li>
</ol>
<h2 id="三、LangChain-架构与工作流程"><a href="#三、LangChain-架构与工作流程" class="headerlink" title="三、LangChain 架构与工作流程"></a>三、LangChain 架构与工作流程</h2><p>LangChain 的核心是它的模块化和可组合性。所有组件都是 <code>Runnable</code> 对象，可以通过 LangChain Expression Language (LCEL) 的 <code>|</code> 操作符轻松组合。</p>
<h3 id="3-1-架构图"><a href="#3-1-架构图" class="headerlink" title="3.1 架构图"></a>3.1 架构图</h3><div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    subgraph LangChain Application
        User[用户输入] --&gt;|请求| Agent[&quot;Agent (LLM + Tools + Reasoning)&quot;]
        Agent --&gt;|思考&#x2F;规划| LLM_Core[&quot;LLM (e.g., GPT-4)&quot;]
        LLM_Core --&gt;|输出决策| Agent

        subgraph Chains
            Chain1[Prompt + LLM + Parser]
            Chain2[Retrieval + Prompt + LLM]
            Chain3[Agentic Chain]
        end

        Agent -- &quot;使用&quot; --&gt; Tools[&quot;工具集 (e.g., Calculator, Search)&quot;]
        Agent -- &quot;使用&quot; --&gt; Memory[&quot;记忆 (对话历史)&quot;]
        Agent -- &quot;编排&quot; --&gt; Chain1
        Agent -- &quot;编排&quot; --&gt; Chain2

        Chain1 -- &quot;数据流&quot; --&gt; OutputParser[Output Parser]
        Chain2 -- &quot;数据流&quot; --&gt; Retrieval[&quot;Retrieval (Vector DB, Embeddings)&quot;]

        LLM_Core -- &quot;封装&quot; --&gt; LLM_API[&quot;LLM API (e.g., OpenAI, HuggingFace)&quot;]
    end

    Retrieval -- &quot;查询&quot; --&gt; ExternalData[&quot;外部数据源 (e.g., 文档, 数据库)&quot;]
    Tools -- &quot;执行&quot; --&gt; ExternalServices[&quot;外部服务 (e.g., API, Web)&quot;]
    Memory -- &quot;存储&#x2F;检索&quot; --&gt; DB[&quot;持久化存储 (可选)&quot;]

    LLM_API &lt;--&gt; CloudService[&quot;云服务 (e.g., OpenAI, Azure)&quot;]
  </pre></div>

<h3 id="3-2-工作流程-LCEL-范式"><a href="#3-2-工作流程-LCEL-范式" class="headerlink" title="3.2 工作流程 (LCEL 范式)"></a>3.2 工作流程 (LCEL 范式)</h3><p>一个典型的 LangChain 应用（特别是使用 LCEL）的工作流程如下：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    sequenceDiagram
    participant User as 用户
    participant App as LangChain 应用
    participant Prompt as 提示模板
    participant LLM as 大型语言模型
    participant Parser as 输出解析器
    participant Retriever as 检索器
    participant Tools as 工具
    participant Memory as 记忆
    participant ExternalDB as 外部数据库&#x2F;服务

    User-&gt;&gt;App: 1. 发送用户请求 (e.g., &quot;帮我总结文档 X&quot;)

    alt 如果是Agent
        App-&gt;&gt;LLM: 2. Agent LLM 思考如何处理请求
        LLM--&gt;&gt;App: 3. Agent LLM 输出行动计划 (Tool use&#x2F;Final Answer)
        alt 如果需要工具
            App-&gt;&gt;Tools: 4. 调用相应工具
            Tools-&gt;&gt;ExternalDB: 5. 工具执行外部操作
            ExternalDB--&gt;&gt;Tools: 6. 返回结果
            Tools--&gt;&gt;App: 7. 返回工具执行结果
            App-&gt;&gt;LLM: 8. Agent LLM 基于工具结果继续思考
        end
    end

    alt 如果是RAG (检索增强生成)
        App-&gt;&gt;Retriever: 2. 提取查询词，进行文档检索
        Retriever-&gt;&gt;ExternalDB: 3. 查询向量数据库
        ExternalDB--&gt;&gt;Retriever: 4. 返回相关文档片段
        Retriever--&gt;&gt;App: 5. 返回文档上下文
    end

    App-&gt;&gt;Prompt: 6. 将用户请求、(工具结果或文档上下文)、记忆等填充到提示模板
    Prompt--&gt;&gt;App: 7. 生成完整提示
    App-&gt;&gt;LLM: 8. 将完整提示发送给 LLM
    LLM--&gt;&gt;App: 9. LLM 生成原始文本输出
    App-&gt;&gt;Parser: 10. 解析 LLM 输出
    Parser--&gt;&gt;App: 11. 返回结构化结果

    alt 如果有记忆
        App-&gt;&gt;Memory: 12. 更新记忆 (存储当前对话)
    end

    App--&gt;&gt;User: 13. 返回最终结果
  </pre></div>

<h2 id="四、LangChain-模块详解与调用方法"><a href="#四、LangChain-模块详解与调用方法" class="headerlink" title="四、LangChain 模块详解与调用方法"></a>四、LangChain 模块详解与调用方法</h2><h3 id="4-1-安装"><a href="#4-1-安装" class="headerlink" title="4.1 安装"></a>4.1 安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip install langchain langchain-community langchain-openai</span><br><span class="line"><span class="comment"># langchain-community 包含各种第三方集成</span></span><br><span class="line"><span class="comment"># langchain-openai 包含 OpenAI 模型集成</span></span><br><span class="line"><span class="comment"># 其他模型如 HuggingFace, Anthropic 等需安装相应包</span></span><br><span class="line">pip install python-dotenv <span class="comment"># 用于加载环境变量</span></span><br></pre></td></tr></table></figure>

<p>创建 <code>.env</code> 文件并添加你的 API Key：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OPENAI_API_KEY=&quot;your_openai_api_key_here&quot;</span><br><span class="line"># HUGGINGFACEHUB_API_TOKEN=&quot;your_huggingface_token_here&quot;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-LLMs-大型语言模型"><a href="#4-2-LLMs-大型语言模型" class="headerlink" title="4.2 LLMs (大型语言模型)"></a>4.2 LLMs (大型语言模型)</h3><p>LangChain 将 LLM 分为 <code>LLM</code>（传统文本输入&#x2F;输出，如 completions API）和 <code>ChatModel</code>（消息列表输入&#x2F;输出，如 chat completions API）。推荐使用 <code>ChatModel</code>。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong>导入与实例化</strong>: 从 <code>langchain_openai</code> 或 <code>langchain_community</code> 导入相应的模型类。</li>
<li><strong><code>invoke(input)</code></strong>: 最简单的调用方法，直接传入字符串（对 <code>LLM</code>）或消息列表（对 <code>ChatModel</code>），返回单个结果。</li>
<li><strong><code>stream(input)</code></strong>: 流式传输结果，用于实时显示 LLM 生成的文本。</li>
<li><strong><code>batch(inputs)</code></strong>: 批量处理多个输入。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> dotenv <span class="keyword">import</span> load_dotenv</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI, OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line">load_dotenv() <span class="comment"># 加载 .env 文件中的环境变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 LLM (传统文本模型)</span></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0.7</span>, model_name=<span class="string">&quot;gpt-3.5-turbo-instruct&quot;</span>) <span class="comment"># model_name for legacy LLMs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化 ChatModel (推荐)</span></span><br><span class="line">chat_model = ChatOpenAI(temperature=<span class="number">0.7</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>) <span class="comment"># model for chat models</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- LLM 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. LLM 调用 (invoke)</span></span><br><span class="line">response_llm = llm.invoke(<span class="string">&quot;What is the capital of France?&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LLM Response (invoke): <span class="subst">&#123;response_llm.strip()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ChatModel 调用 (invoke)</span></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;You are a helpful assistant.&quot;</span>),</span><br><span class="line">    HumanMessage(content=<span class="string">&quot;What is the capital of France?&quot;</span>)</span><br><span class="line">]</span><br><span class="line">response_chat = chat_model.invoke(messages)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ChatModel Response (invoke): <span class="subst">&#123;response_chat.content&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. ChatModel 流式调用 (stream)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nChatModel Response (stream):&quot;</span>)</span><br><span class="line">stream_messages = [HumanMessage(content=<span class="string">&quot;Tell me a short story about a brave knight.&quot;</span>)]</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chat_model.stream(stream_messages):</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. ChatModel 批量调用 (batch)</span></span><br><span class="line">batch_inputs = [</span><br><span class="line">    [HumanMessage(content=<span class="string">&quot;What is the color of the sky?&quot;</span>)],</span><br><span class="line">    [HumanMessage(content=<span class="string">&quot;What is the color of grass?&quot;</span>)]</span><br><span class="line">]</span><br><span class="line">batch_responses = chat_model.batch(batch_inputs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nChatModel Response (batch):&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> res <span class="keyword">in</span> batch_responses:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;- <span class="subst">&#123;res.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-Prompts-提示"><a href="#4-3-Prompts-提示" class="headerlink" title="4.3 Prompts (提示)"></a>4.3 Prompts (提示)</h3><p>提示模板用于构建 LLM 的输入，确保提示结构化且可复用。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong><code>PromptTemplate</code></strong>: 适用于 <code>LLM</code> 模型的字符串输入。</li>
<li><strong><code>ChatPromptTemplate</code></strong>: 适用于 <code>ChatModel</code> 模型的聊天消息列表输入。<ul>
<li><code>from_messages()</code>: 从消息列表创建模板。</li>
<li><code>SystemMessagePromptTemplate</code>, <code>HumanMessagePromptTemplate</code>, <code>AIMessagePromptTemplate</code>: 组合不同角色消息的模板。</li>
<li><code>MessagesPlaceholder</code>: 占位符，用于插入其他消息（如记忆）。</li>
</ul>
</li>
<li><strong><code>FewShotPromptTemplate</code></strong>: 少样本提示，通过提供少量示例来指导 LLM。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate, ChatPromptTemplate, MessagesPlaceholder, FewShotPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Prompts 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. PromptTemplate (用于 LLM 模型)</span></span><br><span class="line">prompt_template = PromptTemplate.from_template(<span class="string">&quot;Tell me a &#123;adjective&#125; story about a &#123;noun&#125;.&quot;</span>)</span><br><span class="line">formatted_prompt = prompt_template.invoke(&#123;<span class="string">&quot;adjective&quot;</span>: <span class="string">&quot;funny&quot;</span>, <span class="string">&quot;noun&quot;</span>: <span class="string">&quot;cat&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;PromptTemplate 结果: <span class="subst">&#123;formatted_prompt&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ChatPromptTemplate (用于 ChatModel 模型)</span></span><br><span class="line"><span class="comment"># 使用 from_messages 快速创建</span></span><br><span class="line">chat_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;You are a helpful assistant that translates English to French.&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;Translate this sentence: &#123;sentence&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">formatted_chat_prompt = chat_prompt.invoke(&#123;<span class="string">&quot;sentence&quot;</span>: <span class="string">&quot;Hello, how are you?&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ChatPromptTemplate 结果: <span class="subst">&#123;formatted_chat_prompt.messages&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. ChatPromptTemplate 结合 MessagesPlaceholder (用于记忆或复杂消息注入)</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, AIMessage</span><br><span class="line">history = [HumanMessage(content=<span class="string">&quot;Hi there!&quot;</span>), AIMessage(content=<span class="string">&quot;Hello! How can I help you?&quot;</span>)]</span><br><span class="line">chat_prompt_with_history = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;You are a friendly chatbot.&quot;</span>),</span><br><span class="line">    MessagesPlaceholder(variable_name=<span class="string">&quot;chat_history&quot;</span>), <span class="comment"># 占位符用于插入历史消息</span></span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">formatted_prompt_with_history = chat_prompt_with_history.invoke(&#123;</span><br><span class="line">    <span class="string">&quot;chat_history&quot;</span>: history,</span><br><span class="line">    <span class="string">&quot;input&quot;</span>: <span class="string">&quot;What&#x27;s the weather like today?&quot;</span></span><br><span class="line">&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;ChatPromptTemplate (含历史) 结果: <span class="subst">&#123;formatted_prompt_with_history.messages&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. FewShotPromptTemplate</span></span><br><span class="line">examples = [</span><br><span class="line">    &#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;happy&quot;</span>, <span class="string">&quot;antonym&quot;</span>: <span class="string">&quot;sad&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;tall&quot;</span>, <span class="string">&quot;antonym&quot;</span>: <span class="string">&quot;short&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line">few_shot_prompt = FewShotPromptTemplate(</span><br><span class="line">    examples=examples,</span><br><span class="line">    example_prompt=PromptTemplate.from_template(<span class="string">&quot;Word: &#123;word&#125;\nAntonym: &#123;antonym&#125;&quot;</span>),</span><br><span class="line">    suffix=<span class="string">&quot;Word: &#123;word&#125;\nAntonym:&quot;</span>,</span><br><span class="line">    input_variables=[<span class="string">&quot;word&quot;</span>],</span><br><span class="line">)</span><br><span class="line">formatted_few_shot_prompt = few_shot_prompt.invoke(&#123;<span class="string">&quot;word&quot;</span>: <span class="string">&quot;big&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;FewShotPromptTemplate 结果:\n<span class="subst">&#123;formatted_few_shot_prompt.text&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-4-Output-Parsers-输出解析器"><a href="#4-4-Output-Parsers-输出解析器" class="headerlink" title="4.4 Output Parsers (输出解析器)"></a>4.4 Output Parsers (输出解析器)</h3><p>将 LLM 的文本输出解析成结构化的数据。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong><code>StrOutputParser</code></strong>: 最简单的解析器，直接返回字符串。</li>
<li><strong><code>JsonOutputParser</code></strong>: 将 LLM 输出解析为 JSON 格式。</li>
<li><strong><code>PydanticOutputParser</code></strong>: 基于 Pydantic 模型定义输出结构。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser, JsonOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Output Parsers 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">chat_model_parser = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. StrOutputParser</span></span><br><span class="line"><span class="comment"># 这是一个默认行为，但明确指定可读性更好</span></span><br><span class="line">chain_str = ChatPromptTemplate.from_template(<span class="string">&quot;Tell me a fact about &#123;animal&#125;.&quot;</span>) | chat_model_parser | StrOutputParser()</span><br><span class="line">fact = chain_str.invoke(&#123;<span class="string">&quot;animal&quot;</span>: <span class="string">&quot;dogs&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;StrOutputParser 结果: <span class="subst">&#123;fact&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. JsonOutputParser</span></span><br><span class="line">json_prompt = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;Return a JSON object with the &#x27;animal&#x27; and &#x27;fact&#x27; about &#123;animal&#125;.&quot;</span></span><br><span class="line">    <span class="string">&quot;Format: &#123;&#123;\&quot;animal\&quot;: \&quot;&lt;animal_name&gt;\&quot;, \&quot;fact\&quot;: \&quot;&lt;fact_about_animal&gt;\&quot;&#125;&#125;&quot;</span></span><br><span class="line">)</span><br><span class="line">chain_json = json_prompt | chat_model_parser | JsonOutputParser()</span><br><span class="line">json_output = chain_json.invoke(&#123;<span class="string">&quot;animal&quot;</span>: <span class="string">&quot;cats&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;JsonOutputParser 结果 (类型: <span class="subst">&#123;<span class="built_in">type</span>(json_output)&#125;</span>): <span class="subst">&#123;json_output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. PydanticOutputParser</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AnimalFact</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    animal: <span class="built_in">str</span> = Field(description=<span class="string">&quot;The name of the animal.&quot;</span>)</span><br><span class="line">    fact: <span class="built_in">str</span> = Field(description=<span class="string">&quot;An interesting fact about the animal.&quot;</span>)</span><br><span class="line"></span><br><span class="line">parser = PydanticOutputParser(pydantic_object=AnimalFact)</span><br><span class="line"></span><br><span class="line">pydantic_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;Answer the user query. &#123;format_instructions&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;Tell me a fact about &#123;animal&#125;.&quot;</span>)</span><br><span class="line">]).partial(format_instructions=parser.get_format_instructions()) <span class="comment"># 将格式指令注入到提示中</span></span><br><span class="line"></span><br><span class="line">chain_pydantic = pydantic_prompt | chat_model_parser | parser</span><br><span class="line">pydantic_output = chain_pydantic.invoke(&#123;<span class="string">&quot;animal&quot;</span>: <span class="string">&quot;pandas&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;PydanticOutputParser 结果 (类型: <span class="subst">&#123;<span class="built_in">type</span>(pydantic_output)&#125;</span>): <span class="subst">&#123;pydantic_output&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Pydantic 结果的 animal 字段: <span class="subst">&#123;pydantic_output.animal&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-Chains-链"><a href="#4-5-Chains-链" class="headerlink" title="4.5 Chains (链)"></a>4.5 Chains (链)</h3><p>链是 LangChain 的核心，通过 LangChain Expression Language (LCEL) 将可运行组件组合起来。</p>
<p><strong>调用方法:</strong></p>
<p>使用 <code>|</code> 操作符连接 <code>Runnable</code> 对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Chains 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最简单的链：Prompt -&gt; LLM -&gt; Parser</span></span><br><span class="line">llm_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;Tell me a short &#123;emotion&#125; story about a &#123;animal&#125;.&quot;</span>)</span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0.7</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">story = llm_chain.invoke(&#123;<span class="string">&quot;emotion&quot;</span>: <span class="string">&quot;happy&quot;</span>, <span class="string">&quot;animal&quot;</span>: <span class="string">&quot;rabbit&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;简单链执行结果:\n<span class="subst">&#123;story&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更复杂的链：包含多个 LLM 调用或中间步骤</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，作为链中的一个步骤</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">summarize_text</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># 模拟一个文本摘要服务</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(text) &gt; <span class="number">100</span>:</span><br><span class="line">        <span class="keyword">return</span> text[:<span class="number">100</span>] + <span class="string">&quot;...&quot;</span> <span class="comment"># 截断</span></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line">summary_chain = (</span><br><span class="line">    &#123;<span class="string">&quot;text&quot;</span>: llm_chain&#125; <span class="comment"># 将上一个链的输出作为 &#x27;text&#x27; 键传入下一个链</span></span><br><span class="line">    | RunnablePassthrough.assign(summary=<span class="keyword">lambda</span> x: summarize_text(x[<span class="string">&quot;text&quot;</span>]))</span><br><span class="line">    | ChatPromptTemplate.from_template(<span class="string">&quot;Original Story: &#123;text&#125;\nSummary: &#123;summary&#125;\n\nBased on the summary, what is the main theme of the story?&quot;</span>)</span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">main_theme = summary_chain.invoke(&#123;<span class="string">&quot;emotion&quot;</span>: <span class="string">&quot;happy&quot;</span>, <span class="string">&quot;animal&quot;</span>: <span class="string">&quot;rabbit&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n复杂链执行结果 (主旨):\n<span class="subst">&#123;main_theme&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-6-Retrieval-检索"><a href="#4-6-Retrieval-检索" class="headerlink" title="4.6 Retrieval (检索)"></a>4.6 Retrieval (检索)</h3><p>检索用于将外部数据引入 LLM 的上下文。通常涉及嵌入模型、文档加载器、文本分割器和向量存储。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong><code>VectorStoreRetriever</code></strong>: 从向量存储中获取相关文档。</li>
<li><strong><code>create_retrieval_chain</code></strong>: LangChain 提供的便捷函数，用于构建 RAG 链。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> Chroma</span><br><span class="line"><span class="keyword">from</span> langchain.chains.retrieval <span class="keyword">import</span> create_retrieval_chain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.combine_documents <span class="keyword">import</span> create_stuff_documents_chain</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Retrieval 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建一个模拟文档</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;example_document.txt&quot;</span>, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;LangChain is a framework for developing applications powered by large language models. &quot;</span></span><br><span class="line">            <span class="string">&quot;It enables applications that are context-aware and can reason. &quot;</span></span><br><span class="line">            <span class="string">&quot;LangChain also helps with data augmentation, agentic reasoning, and evaluation. &quot;</span></span><br><span class="line">            <span class="string">&quot;Key components include LLMs, prompts, chains, agents, and memory. &quot;</span></span><br><span class="line">            <span class="string">&quot;Vector databases like Chroma are often used for retrieval.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 加载文档</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;example_document.txt&quot;</span>)</span><br><span class="line">docs = loader.load()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 分割文本</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">500</span>, chunk_overlap=<span class="number">0</span>)</span><br><span class="line">splits = text_splitter.split_documents(docs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建嵌入模型和向量存储</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line">vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 创建检索器</span></span><br><span class="line">retriever = vectorstore.as_retriever()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 构建 RAG 链</span></span><br><span class="line"><span class="comment"># a. 定义一个文档组合链，它知道如何处理检索到的文档</span></span><br><span class="line">document_chain = create_stuff_documents_chain(</span><br><span class="line">    ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>),</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;Answer the question based only on the provided context:\n\n&#123;context&#125;\n\nQuestion: &#123;input&#125;&quot;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># b. 创建检索链</span></span><br><span class="line">retrieval_chain = create_retrieval_chain(retriever, document_chain)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 调用检索链</span></span><br><span class="line">response = retrieval_chain.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;What is LangChain useful for?&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;RAG 链的响应: <span class="subst">&#123;response[<span class="string">&#x27;answer&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"><span class="comment"># print(f&quot;检索到的文档: &#123;[d.page_content for d in response[&#x27;context&#x27;]]&#125;&quot;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理模拟文档</span></span><br><span class="line">os.remove(<span class="string">&quot;example_document.txt&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-7-Agents-智能体"><a href="#4-7-Agents-智能体" class="headerlink" title="4.7 Agents (智能体)"></a>4.7 Agents (智能体)</h3><p>Agent 赋予 LLM 决策和使用工具的能力。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong><code>Tool</code></strong>: 封装 Agent 可以调用的函数。</li>
<li><strong><code>create_react_agent</code></strong>: 创建基于 ReAct (Reasoning and Acting) 模式的 Agent。</li>
<li><strong><code>AgentExecutor</code></strong>: 运行 Agent，管理其与工具的交互循环。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_react_agent</span><br><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> hub</span><br><span class="line"><span class="keyword">from</span> langchain_core.tools <span class="keyword">import</span> Tool</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">import</span> operator <span class="comment"># 用于模拟一个简单的计算器</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Agents 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义工具 (Tools)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multiply</span>(<span class="params">a: <span class="built_in">float</span>, b: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Multiplies two numbers.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a * b</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a: <span class="built_in">float</span>, b: <span class="built_in">float</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Adds two numbers.&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 Python 函数封装成 LangChain 工具</span></span><br><span class="line">tools = [</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;Multiply&quot;</span>,</span><br><span class="line">        func=multiply,</span><br><span class="line">        description=<span class="string">&quot;Useful for multiplying two floating point numbers. Input should be two numbers separated by a comma.&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    Tool(</span><br><span class="line">        name=<span class="string">&quot;Add&quot;</span>,</span><br><span class="line">        func=add,</span><br><span class="line">        description=<span class="string">&quot;Useful for adding two floating point numbers. Input should be two numbers separated by a comma.&quot;</span></span><br><span class="line">    )</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建 Agent</span></span><br><span class="line"><span class="comment"># a. 从 LangChain Hub 加载 Agent 提示模板 (或自定义)</span></span><br><span class="line"><span class="comment"># pull_from_hub 函数需要安装 `langchainhub`</span></span><br><span class="line"><span class="comment"># pip install langchainhub</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    prompt = hub.pull(<span class="string">&quot;hwchase17/react&quot;</span>)</span><br><span class="line"><span class="keyword">except</span> Exception:</span><br><span class="line">    <span class="comment"># 如果无法访问 LangChain Hub，使用一个通用模板</span></span><br><span class="line">    prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&quot;&quot;Answer the following questions as best you can. You have access to the following tools:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;tools&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Use the following format:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: the input question you must answer</span></span><br><span class="line"><span class="string">Thought: you should always think about what to do</span></span><br><span class="line"><span class="string">Action: the action to take, should be one of [&#123;tool_names&#125;]</span></span><br><span class="line"><span class="string">Action Input: the input to the action</span></span><br><span class="line"><span class="string">Observation: the result of the action</span></span><br><span class="line"><span class="string"><span class="meta">... </span>(this Thought/Action/Action Input/Observation can repeat N times)</span></span><br><span class="line"><span class="string">Thought: I now know the final answer</span></span><br><span class="line"><span class="string">Final Answer: the final answer to the original input question</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Begin!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Question: &#123;input&#125;</span></span><br><span class="line"><span class="string">Thought:&#123;agent_scratchpad&#125;&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">llm_agent = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>) <span class="comment"># Agent 通常需要更强大的模型</span></span><br><span class="line"></span><br><span class="line">agent = create_react_agent(llm_agent, tools, prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建 AgentExecutor</span></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>) <span class="comment"># verbose=True 会打印 Agent 的思考过程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 调用 Agent</span></span><br><span class="line">question = <span class="string">&quot;What is 15.6 multiplied by 3.2 and then added to 10?&quot;</span></span><br><span class="line">response = agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: question&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nAgent 的最终响应: <span class="subst">&#123;response[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-8-Memory-记忆"><a href="#4-8-Memory-记忆" class="headerlink" title="4.8 Memory (记忆)"></a>4.8 Memory (记忆)</h3><p>记忆模块用于存储和管理对话历史，以便 LLM 能够记住之前的交互。</p>
<p><strong>调用方法:</strong></p>
<ol>
<li><strong><code>ConversationBufferMemory</code></strong>: 最简单的记忆类型，将所有对话存储在一个缓冲区中。</li>
<li><strong><code>ConversationBufferWindowMemory</code></strong>: 只记住最近 N 轮对话。</li>
<li><strong><code>ConversationSummaryMemory</code></strong>: 总结旧的对话内容，减少令牌消耗。</li>
<li>集成到链中：通常通过 <code>RunnablePassthrough.assign</code> 和 <code>MessagesPlaceholder</code>。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> ConversationChain</span><br><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, AIMessage</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Memory 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">chat_model_memory = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. ConversationBufferMemory</span></span><br><span class="line">memory = ConversationBufferMemory(return_messages=<span class="literal">True</span>)</span><br><span class="line">memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Hi!&quot;</span>&#125;, &#123;<span class="string">&quot;output&quot;</span>: <span class="string">&quot;What&#x27;s up?&quot;</span>&#125;)</span><br><span class="line">memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Not much, just chilling.&quot;</span>&#125;, &#123;<span class="string">&quot;output&quot;</span>: <span class="string">&quot;Cool.&quot;</span>&#125;)</span><br><span class="line"><span class="comment"># 获取历史消息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Buffer Memory 历史: <span class="subst">&#123;memory.load_memory_variables(&#123;&#125;</span>)[&#x27;history&#x27;]&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 将记忆集成到链中 (推荐使用 MessagesPlaceholder)</span></span><br><span class="line">prompt_with_memory = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;You are a friendly chatbot.&quot;</span>),</span><br><span class="line">    MessagesPlaceholder(variable_name=<span class="string">&quot;history&quot;</span>), <span class="comment"># 确保 variable_name 与 Memory 的键匹配</span></span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个链来演示记忆的运作</span></span><br><span class="line">conversation_with_memory = (</span><br><span class="line">    RunnablePassthrough.assign(</span><br><span class="line">        history=<span class="keyword">lambda</span> x: memory.load_memory_variables(&#123;&#125;)[<span class="string">&quot;history&quot;</span>]</span><br><span class="line">    )</span><br><span class="line">    | prompt_with_memory</span><br><span class="line">    | chat_model_memory</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 带记忆的对话示例 ---&quot;</span>)</span><br><span class="line">user_input1 = <span class="string">&quot;My name is Alice.&quot;</span></span><br><span class="line">response1 = conversation_with_memory.invoke(&#123;<span class="string">&quot;input&quot;</span>: user_input1&#125;)</span><br><span class="line">memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: user_input1&#125;, &#123;<span class="string">&quot;output&quot;</span>: response1&#125;) <span class="comment"># 手动保存上下文</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;用户: <span class="subst">&#123;user_input1&#125;</span>\nAI: <span class="subst">&#123;response1&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">user_input2 = <span class="string">&quot;What did I just tell you my name was?&quot;</span></span><br><span class="line">response2 = conversation_with_memory.invoke(&#123;<span class="string">&quot;input&quot;</span>: user_input2&#125;)</span><br><span class="line">memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: user_input2&#125;, &#123;<span class="string">&quot;output&quot;</span>: response2&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;用户: <span class="subst">&#123;user_input2&#125;</span>\nAI: <span class="subst">&#123;response2&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. ConversationBufferWindowMemory (只记住最后 N 轮)</span></span><br><span class="line">window_memory = ConversationBufferWindowMemory(k=<span class="number">1</span>, return_messages=<span class="literal">True</span>) <span class="comment"># 只记住最近1轮</span></span><br><span class="line">window_memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;First msg&quot;</span>&#125;, &#123;<span class="string">&quot;output&quot;</span>: <span class="string">&quot;First reply&quot;</span>&#125;)</span><br><span class="line">window_memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Second msg&quot;</span>&#125;, &#123;<span class="string">&quot;output&quot;</span>: <span class="string">&quot;Second reply&quot;</span>&#125;)</span><br><span class="line">window_memory.save_context(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Third msg&quot;</span>&#125;, &#123;<span class="string">&quot;output&quot;</span>: <span class="string">&quot;Third reply&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nWindow Memory (k=1) 历史: <span class="subst">&#123;window_memory.load_memory_variables(&#123;&#125;</span>)[&#x27;history&#x27;]&#125;&quot;</span>) <span class="comment"># 应该只剩最后1轮</span></span><br></pre></td></tr></table></figure>

<h3 id="4-9-Callbacks-回调"><a href="#4-9-Callbacks-回调" class="headerlink" title="4.9 Callbacks (回调)"></a>4.9 Callbacks (回调)</h3><p>回调允许你在链或 Agent 执行的各个阶段插入自定义逻辑。</p>
<p><strong>调用方法:</strong></p>
<p>通过 <code>callbacks</code> 参数传递回调处理器实例或列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> BaseCallbackHandler</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- Callbacks 模块调用示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 自定义回调处理器</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyCustomCallbackHandler</span>(<span class="title class_ inherited__">BaseCallbackHandler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_chain_start</span>(<span class="params">self, serialized, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;--- Chain started: <span class="subst">&#123;serialized[<span class="string">&#x27;lc_id&#x27;</span>]&#125;</span> ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_llm_start</span>(<span class="params">self, serialized, prompts, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;--- LLM started with prompts: <span class="subst">&#123;prompts&#125;</span> ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_llm_end</span>(<span class="params">self, response, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;--- LLM ended with response: <span class="subst">&#123;response.generations[<span class="number">0</span>][<span class="number">0</span>].text[:<span class="number">50</span>]&#125;</span>... ---&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_chain_end</span>(<span class="params">self, outputs, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;--- Chain ended with outputs: <span class="subst">&#123;outputs&#125;</span> ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 将回调处理器添加到链中</span></span><br><span class="line">chain_with_callbacks = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;What is a good name for a company that makes &#123;product&#125;?&quot;</span>)</span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0.7</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># invoke 方法接受 callbacks 参数</span></span><br><span class="line">response = chain_with_callbacks.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;product&quot;</span>: <span class="string">&quot;eco-friendly water bottles&quot;</span>&#125;,</span><br><span class="line">    config=&#123;<span class="string">&quot;callbacks&quot;</span>: [MyCustomCallbackHandler()]&#125;</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;链的最终响应 (带回调): <span class="subst">&#123;response&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 使用 LangChain 提供的默认回调 (如 StdOutCallbackHandler 用于打印详细日志)</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks.manager <span class="keyword">import</span> CallbackManager</span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks.stdout <span class="keyword">import</span> ConsoleCallbackHandler</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- StdOutCallbackHandler 示例 ---&quot;</span>)</span><br><span class="line">chain_with_stdout_callbacks = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;Explain the concept of &#123;concept&#125; in simple terms.&quot;</span>)</span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0.5</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response_stdout = chain_with_stdout_callbacks.invoke(</span><br><span class="line">    &#123;<span class="string">&quot;concept&quot;</span>: <span class="string">&quot;quantum entanglement&quot;</span>&#125;,</span><br><span class="line">    config=&#123;<span class="string">&quot;callbacks&quot;</span>: [ConsoleCallbackHandler()]&#125; <span class="comment"># 或者直接 verbose=True 在某些组件上</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;链的最终响应 (StdOut 回调): <span class="subst">&#123;response_stdout&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-10-LCEL-LangChain-Expression-Language"><a href="#4-10-LCEL-LangChain-Expression-Language" class="headerlink" title="4.10 LCEL (LangChain Expression Language)"></a>4.10 LCEL (LangChain Expression Language)</h3><p>LCEL 是 LangChain 的强大之处，它提供了一种声明式的方式来组合 <code>Runnable</code> 对象。所有 LangChain 的核心组件（LLM、Prompt、Parser、Retriever、Tools 等）都实现了 <code>Runnable</code> 接口。</p>
<p><strong>LCEL 的优势:</strong></p>
<ul>
<li><strong>可组合性</strong>：使用 <code>|</code> 运算符轻松连接组件。</li>
<li><strong>流式处理</strong>：原生支持流式输出。</li>
<li><strong>异步支持</strong>：支持 <code>async/await</code>。</li>
<li><strong>并行化</strong>：自动并行处理。</li>
<li><strong>回退机制</strong>：轻松添加重试和回退逻辑。</li>
<li><strong>类型安全</strong>：支持类型提示。</li>
<li><strong>调试和可观测性</strong>：与 LangSmith 深度集成。</li>
</ul>
<p>LCEL 的调用方法就是上面示例中大量使用的 <code>|</code> 操作符。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- LCEL 核心思想示例 ---&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是一个典型的 LCEL 链：</span></span><br><span class="line"><span class="comment"># 1. 定义 Prompt</span></span><br><span class="line"><span class="comment"># 2. 定义 LLM</span></span><br><span class="line"><span class="comment"># 3. 定义 Output Parser</span></span><br><span class="line"><span class="comment"># 4. 使用 | 连接它们</span></span><br><span class="line">final_lcel_chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;Say hi to &#123;person&#125;.&quot;</span>) <span class="comment"># Prompt 是 Runnable</span></span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0.8</span>, model=<span class="string">&quot;gpt-3.5-turbo&quot;</span>)    <span class="comment"># LLM 是 Runnable</span></span><br><span class="line">    | StrOutputParser()                                     <span class="comment"># Parser 是 Runnable</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">output = final_lcel_chain.invoke(&#123;<span class="string">&quot;person&quot;</span>: <span class="string">&quot;Alice&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LCEL 链输出: <span class="subst">&#123;output&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># LCEL 还可以结合 Python 函数 (包装成 RunnableLambda) 或 RunnablePassthrough.assign</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reverse_string</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">return</span> text[::-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">reverse_chain = (</span><br><span class="line">    final_lcel_chain</span><br><span class="line">    | reverse_string <span class="comment"># Python 函数会自动包装成 RunnableLambda</span></span><br><span class="line">)</span><br><span class="line">reversed_output = reverse_chain.invoke(&#123;<span class="string">&quot;person&quot;</span>: <span class="string">&quot;Bob&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;LCEL 链 (含 Python 函数) 输出: <span class="subst">&#123;reversed_output&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="五、LangChain-的优缺点与适用场景"><a href="#五、LangChain-的优缺点与适用场景" class="headerlink" title="五、LangChain 的优缺点与适用场景"></a>五、LangChain 的优缺点与适用场景</h2><h3 id="5-1-优点："><a href="#5-1-优点：" class="headerlink" title="5.1 优点："></a>5.1 优点：</h3><ol>
<li><strong>高度模块化和可组合性</strong>：所有组件都是可插拔的，易于构建、修改和复用。</li>
<li><strong>LCEL 提供强大编排能力</strong>：使得构建复杂链式应用变得直观、高效且可扩展。</li>
<li><strong>支持 RAG 模式</strong>：与向量数据库和嵌入模型深度集成，有效解决 LLM 知识限制。</li>
<li><strong>Agent 能力强大</strong>：通过工具扩展 LLM 边界，使其能够执行外部操作，实现复杂任务自动化。</li>
<li><strong>广泛的模型和集成支持</strong>：支持 OpenAI、HuggingFace、Anthropic 等多种 LLM 和各种工具集成。</li>
<li><strong>活跃的社区与生态系统</strong>：拥有庞大用户群、丰富的文档和工具，如 LangSmith 用于调试和评估。</li>
<li><strong>异步和流式处理</strong>：原生支持现代应用的需求。</li>
</ol>
<h3 id="5-2-缺点："><a href="#5-2-缺点：" class="headerlink" title="5.2 缺点："></a>5.2 缺点：</h3><ol>
<li><strong>学习曲线陡峭</strong>：概念多且抽象，API 繁杂，初学者需要投入较多时间理解。</li>
<li><strong>抽象层级有时过高</strong>：对于简单任务可能引入不必要的复杂性。</li>
<li><strong>调试复杂性</strong>：链式结构和 Agent 的决策循环可能难以调试和追踪。</li>
<li><strong>性能考量</strong>：多步骤链式调用可能增加延迟和成本。</li>
<li><strong>快速迭代带来变化</strong>：库更新频繁，API 可能有不兼容的改动。</li>
</ol>
<h3 id="5-3-适用场景："><a href="#5-3-适用场景：" class="headerlink" title="5.3 适用场景："></a>5.3 适用场景：</h3><ul>
<li><strong>智能问答系统 (Q&amp;A)</strong>：结合 RAG 实现基于特定文档或知识库的问答。</li>
<li><strong>聊天机器人</strong>：利用记忆模块实现多轮对话，通过工具扩展机器人功能。</li>
<li><strong>自动化工作流</strong>：Agent 可以编排多个工具和 LLM 调用，完成复杂自动化任务。</li>
<li><strong>数据分析与报告生成</strong>：结合代码解释器或数据分析工具，实现数据驱动的洞察和报告。</li>
<li><strong>代码生成与辅助</strong>：利用 LLM 的代码理解和生成能力。</li>
<li><strong>自定义 AI 助手</strong>：构建满足特定业务需求的智能助手。</li>
</ul>
<h2 id="六、安全性考虑"><a href="#六、安全性考虑" class="headerlink" title="六、安全性考虑"></a>六、安全性考虑</h2><p>开发基于 LangChain 的 LLM 应用程序时，安全性至关重要：</p>
<ol>
<li><strong>API Key 管理</strong>：<ul>
<li><strong>环境变量</strong>：绝不将 API Key 硬编码在代码中，应通过环境变量 (<code>.env</code> 文件, 操作系统环境变量) 加载。</li>
<li><strong>秘密管理服务</strong>：在生产环境中，使用专门的秘密管理服务 (如 AWS Secrets Manager, Azure Key Vault, HashiCorp Vault) 来存储和访问敏感凭据。</li>
</ul>
</li>
<li><strong>输入验证与过滤</strong>：<ul>
<li><strong>用户输入</strong>：对所有用户输入进行严格验证和清理，防止恶意注入，例如提示注入 (Prompt Injection)。</li>
<li><strong>RAG 内容</strong>：确保从外部数据源检索到的内容是可靠和无害的，防止恶意文档导致 LLM 偏离预期。</li>
</ul>
</li>
<li><strong>LLM 输出过滤</strong>：<ul>
<li><strong>敏感信息泄露</strong>：对 LLM 的输出进行审查，确保不泄露敏感信息。</li>
<li><strong>有害内容</strong>：过滤掉可能有害、不当或偏见的内容。</li>
<li><strong>工具执行风险</strong>：Agent 可能会调用外部工具。确保工具执行的指令是安全的，并且工具本身不会被滥用（例如，限制对文件系统或敏感 API 的访问）。</li>
</ul>
</li>
<li><strong>权限控制</strong>：<ul>
<li><strong>工具权限</strong>：为 Agent 提供的工具应遵循最小权限原则，仅授予完成任务所需的权限。</li>
<li><strong>用户权限</strong>：如果应用程序涉及多用户，应确保每个用户只能访问其有权操作的功能和数据。</li>
</ul>
</li>
<li><strong>数据隐私</strong>：<ul>
<li><strong>敏感数据处理</strong>：如果 LLM 处理敏感用户数据，必须确保符合 GDPR、HIPAA 等数据隐私法规。</li>
<li><strong>数据驻留</strong>：了解 LLM 服务提供商的数据处理政策和数据存储位置。</li>
</ul>
</li>
<li><strong>速率限制与成本管理</strong>：<ul>
<li>合理设置 LLM API 的速率限制和最大令牌数，防止意外的高成本或拒绝服务。</li>
</ul>
</li>
<li><strong>可观测性与日志</strong>：<ul>
<li>使用 LangSmith 或其他日志工具监控 Agent 和链的执行过程，及时发现异常行为和安全漏洞。</li>
</ul>
</li>
</ol>
<h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>LangChain 是 LLM 应用开发的强大基石，它通过提供模块化组件、灵活的编排能力和对复杂智能体的支持，极大地降低了开发门槛，加速了创新。理解其核心概念，并熟练运用 LLMs、Prompts、Parsers、Chains、Retrieval、Agents、Tools、Memory 等模块的调用方法，是构建强大、智能且可维护的 LLM 应用程序的关键。同时，开发者必须始终将安全性放在首位，确保应用程序的稳健和可靠。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/3201b8057954/">https://blog.tbf1211.xx.kg/3201b8057954/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/LangChain/">LangChain</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-10.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/c1c3c8b5b003/" title="LangChain Model I/O 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-19.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LangChain Model I/O 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Model I&#x2F;O 是 LangChain 框架的核心组成部分之一，它提供了一套标准化的接口和工具，用于与各种大型语言模型 (LLMs) 和聊天模型 (Chat Models) 进行交互，并对其输入和输出进行有效的管理和结构化。这是构建任何基于 LLM 的应用程序的基础。  核心思想：将与 LLM 的“对话”分解为可管理、可组合的组件：输入 (Prompt Templates)、模型调用 (LLM&#x2F;Chat Models) 和输出处理 (Output Parsers)。   一、为什么 Model I&#x2F;O 至关重要？在没有 LangChain Model I&#x2F;O 的情况下，直接与 LLM 交互通常意味着：  手动拼接 Prompt: 需要手动构建复杂的字符串，其中包含指令、上下文、示例和用户输入。这既繁琐又容易出错。 硬编码模型调用: 每次更换模型或供应商时，都需要修改底层代码。 非结构化的输出: LLM 的原始输出通常是自由文本，需要编写复杂的字符串解析逻辑来提取所需信息。 缺乏可复用性: 不同应用场景下的 Prom...</div></div></div></a><a class="pagination-related" href="/11a674c6a974/" title="NativeScript-Vue3详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">NativeScript-Vue3详解</div></div><div class="info-2"><div class="info-item-1"> NativeScript-Vue 3 是一个强大的框架组合，它允许开发者使用熟悉的 Vue 3 语法和工具链来构建真正的原生 iOS 和 Android 移动应用程序。与传统 Hybrid 应用（如 Cordova 或 Ionic）不同，NativeScript 直接操作原生 UI 组件，因此能够提供一流的性能和用户体验，同时避免了 Web 视图的性能瓶颈。  核心亮点：使用 Vue 3 渲染原生 UI 组件，实现高性能、媲美原生体验的跨平台移动应用开发。   一、什么是 NativeScript-Vue 3？1.1 NativeScript 简介NativeScript 是一个开源框架，用于使用 JavaScript、TypeScript 或其他编译到 JavaScript 的语言来构建原生移动应用程序。它的核心能力在于：  直接访问原生 API：无需编写任何 Objective-C&#x2F;Swift 或 Java&#x2F;Kotlin 代码，开发者可以直接从 JavaScript 访问设备的所有原生 API。 原生 UI 渲染：不使用 WebView，而是将 Java...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/c1c3c8b5b003/" title="LangChain Model I&#x2F;O 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-19.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-13</div><div class="info-item-2">LangChain Model I&#x2F;O 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Model I&#x2F;O 是 LangChain 框架的核心组成部分之一，它提供了一套标准化的接口和工具，用于与各种大型语言模型 (LLMs) 和聊天模型 (Chat Models) 进行交互，并对其输入和输出进行有效的管理和结构化。这是构建任何基于 LLM 的应用程序的基础。  核心思想：将与 LLM 的“对话”分解为可管理、可组合的组件：输入 (Prompt Templates)、模型调用 (LLM&#x2F;Chat Models) 和输出处理 (Output Parsers)。   一、为什么 Model I&#x2F;O 至关重要？在没有 LangChain Model I&#x2F;O 的情况下，直接与 LLM 交互通常意味着：  手动拼接 Prompt: 需要手动构建复杂的字符串，其中包含指令、上下文、示例和用户输入。这既繁琐又容易出错。 硬编码模型调用: 每次更换模型或供应商时，都需要修改底层代码。 非结构化的输出: LLM 的原始输出通常是自由文本，需要编写复杂的字符串解析逻辑来提取所需信息。 缺乏可复用性: 不同应用场景下的 Prom...</div></div></div></a><a class="pagination-related" href="/21e140c78f80/" title="大型语言模型如何理解人类文字：从Token到语义表征"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-16</div><div class="info-item-2">大型语言模型如何理解人类文字：从Token到语义表征</div></div><div class="info-2"><div class="info-item-1"> 大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。   一、基础构建模块：从文本到向量LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于分词 (Tokenization) 和词嵌入 (Word Embeddings)。 1.1 分词 (Tokenization)分词是将连续的文本序列切分成有意义的最小单位——Token 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。  词级别分词 (Word-level Tokenization)：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-10.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/9a400d225757/" title="对话模型与非对话模型详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-03.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-26</div><div class="info-item-2">对话模型与非对话模型详解</div></div><div class="info-2"><div class="info-item-1"> 在大型语言模型 (LLM) 的领域中，”对话模型” (Chat Models) 和 “非对话模型” (或称为 “文本模型” Text Models) 是两种基本但又有所区别的模型范式，它们在设计、训练数据、输入&#x2F;输出格式以及最佳应用场景上存在差异。理解这两种模型的区别是有效利用 LLM 进行开发的关键。  核心思想：对话模型优化用于多轮、上下文感知的交互，通过消息列表进行输入输出；非对话模型则擅长单次、直接的文本指令处理，通过字符串进行输入输出。    一、非对话模型 (Text Models &#x2F; LLMs)非对话模型是早期和传统的大型语言模型形式，它们通常设计为接收一个单一的字符串作为输入（通常称为 “prompt”），并生成一个单一的字符串作为输出。虽然这些模型也能在一定程度上处理对话，但通常需要通过在单次 Prompt 中手动构建对话历史来模拟。 1.1 特点 字符串输入&#x2F;输出：输入是一个字符串，输出也是一个字符串。 输入示例：&quot;把以下文本总结一下：[文本内容]&quot; 输出示例：&quot;这是一段总结后的文本。&quot; ...</div></div></div></a><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-19.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-27</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">365</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">211</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">75</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-LangChain%EF%BC%9F"><span class="toc-text">一、为什么需要 LangChain？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LangChain-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">二、LangChain 的核心概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81LangChain-%E6%9E%B6%E6%9E%84%E4%B8%8E%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">三、LangChain 架构与工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-text">3.1 架构图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B-LCEL-%E8%8C%83%E5%BC%8F"><span class="toc-text">3.2 工作流程 (LCEL 范式)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81LangChain-%E6%A8%A1%E5%9D%97%E8%AF%A6%E8%A7%A3%E4%B8%8E%E8%B0%83%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-text">四、LangChain 模块详解与调用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%AE%89%E8%A3%85"><span class="toc-text">4.1 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-LLMs-%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.2 LLMs (大型语言模型)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Prompts-%E6%8F%90%E7%A4%BA"><span class="toc-text">4.3 Prompts (提示)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Output-Parsers-%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90%E5%99%A8"><span class="toc-text">4.4 Output Parsers (输出解析器)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-Chains-%E9%93%BE"><span class="toc-text">4.5 Chains (链)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-Retrieval-%E6%A3%80%E7%B4%A2"><span class="toc-text">4.6 Retrieval (检索)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-Agents-%E6%99%BA%E8%83%BD%E4%BD%93"><span class="toc-text">4.7 Agents (智能体)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-Memory-%E8%AE%B0%E5%BF%86"><span class="toc-text">4.8 Memory (记忆)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-9-Callbacks-%E5%9B%9E%E8%B0%83"><span class="toc-text">4.9 Callbacks (回调)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-10-LCEL-LangChain-Expression-Language"><span class="toc-text">4.10 LCEL (LangChain Expression Language)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81LangChain-%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E4%B8%8E%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">五、LangChain 的优缺点与适用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E4%BC%98%E7%82%B9%EF%BC%9A"><span class="toc-text">5.1 优点：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E7%BC%BA%E7%82%B9%EF%BC%9A"><span class="toc-text">5.2 缺点：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9A"><span class="toc-text">5.3 适用场景：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%AE%89%E5%85%A8%E6%80%A7%E8%80%83%E8%99%91"><span class="toc-text">六、安全性考虑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">七、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-27.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bun.js 深度解析：冷启动与边缘函数优化"/></a><div class="content"><a class="title" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化">Bun.js 深度解析：冷启动与边缘函数优化</a><time datetime="2025-12-07T22:24:00.000Z" title="发表于 2025-12-08 06:24:00">2025-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-05.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go Jaeger 深度解析：分布式追踪实践"/></a><div class="content"><a class="title" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践">Go Jaeger 深度解析：分布式追踪实践</a><time datetime="2025-12-04T22:24:00.000Z" title="发表于 2025-12-05 06:24:00">2025-12-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/15920229f914/" title="Supabase 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Supabase 深度解析"/></a><div class="content"><a class="title" href="/15920229f914/" title="Supabase 深度解析">Supabase 深度解析</a><time datetime="2025-12-02T22:24:00.000Z" title="发表于 2025-12-03 06:24:00">2025-12-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/1ae20d2726d8/" title="MiniRTC 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MiniRTC 详解"/></a><div class="content"><a class="title" href="/1ae20d2726d8/" title="MiniRTC 详解">MiniRTC 详解</a><time datetime="2025-11-28T22:24:00.000Z" title="发表于 2025-11-29 06:24:00">2025-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/be24ef88e59a/" title="WebRTC 技术详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebRTC 技术详解"/></a><div class="content"><a class="title" href="/be24ef88e59a/" title="WebRTC 技术详解">WebRTC 技术详解</a><time datetime="2025-11-27T22:24:00.000Z" title="发表于 2025-11-28 06:24:00">2025-11-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-10.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            pagePV.textContent = data.pageviews.value
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors && typeof data.visitors.value !== 'undefined') {
            siteUV.textContent = data.visitors.value
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            sitePV.textContent = data.pageviews.value
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>