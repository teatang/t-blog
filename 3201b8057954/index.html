<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LangChain 详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LangChain 是一个开源框架，旨在帮助开发者使用大语言模型 (LLMs) 构建端到端的应用程序。它提供了一套工具、组件和接口，极大地简化了 LLMs 应用的开发流程，特别是对于那些需要多步骤推理、与外部数据源交互、具备记忆能力的复杂应用场景。LangChain 的核心理念是将 LLM 的能力与外部计算和数据源结合起来，使其不仅仅是一个文本生成器，而是一个能够执行复杂任务的智能代理。  核心">
<meta property="og:type" content="article">
<meta property="og:title" content="LangChain 详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/3201b8057954/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="LangChain 是一个开源框架，旨在帮助开发者使用大语言模型 (LLMs) 构建端到端的应用程序。它提供了一套工具、组件和接口，极大地简化了 LLMs 应用的开发流程，特别是对于那些需要多步骤推理、与外部数据源交互、具备记忆能力的复杂应用场景。LangChain 的核心理念是将 LLM 的能力与外部计算和数据源结合起来，使其不仅仅是一个文本生成器，而是一个能够执行复杂任务的智能代理。  核心">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-27.jpg">
<meta property="article:published_time" content="2025-10-11T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-01T02:41:23.752Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="LangChain">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-27.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LangChain 详解",
  "url": "https://blog.tbf1211.xx.kg/3201b8057954/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-27.jpg",
  "datePublished": "2025-10-11T22:24:00.000Z",
  "dateModified": "2025-12-01T02:41:23.752Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/3201b8057954/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LangChain 详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">331</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">202</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">71</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-27.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">LangChain 详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">LangChain 详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-10-11T22:24:00.000Z" title="发表于 2025-10-12 06:24:00">2025-10-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/%E5%BC%80%E5%8F%91%E6%A1%86%E6%9E%B6/">开发框架</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/3201b8057954/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>LangChain</strong> 是一个开源框架，旨在帮助开发者使用大语言模型 (LLMs) 构建端到端的应用程序。它提供了一套工具、组件和接口，极大地简化了 LLMs 应用的开发流程，特别是对于那些需要<strong>多步骤推理、与外部数据源交互、具备记忆能力</strong>的复杂应用场景。LangChain 的核心理念是<strong>将 LLM 的能力与外部计算和数据源结合起来</strong>，使其不仅仅是一个文本生成器，而是一个能够执行复杂任务的智能代理。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>连接 LLM 与外部世界，赋能 LLM 解决更复杂、更实际的问题。</strong></p>
</div>
<hr>
<h2 id="一、为什么需要-LangChain？"><a href="#一、为什么需要-LangChain？" class="headerlink" title="一、为什么需要 LangChain？"></a>一、为什么需要 LangChain？</h2><p>大语言模型（如 GPT-3&#x2F;4, Claude, Llama 等）拥有强大的文本理解和生成能力，但它们本身存在一些局限性：</p>
<ol>
<li><strong>知识截止日期 (Knowledge Cut-off)</strong>：LLMs 的知识库停留在其训练数据的截止日期，无法获取最新信息。</li>
<li><strong>幻觉 (Hallucination)</strong>：LLMs 可能会生成听起来合理但实际上错误或捏造的信息。</li>
<li><strong>缺乏上下文记忆 (Lack of Contextual Memory)</strong>：LLMs 默认是无状态的，无法记住之前的对话或交互。</li>
<li><strong>无法执行外部操作 (Cannot Perform External Actions)</strong>：LLMs 只能生成文本，无法直接调用 API、查询数据库或执行其他软件操作。</li>
<li><strong>单次调用限制 (Single-Shot Limit)</strong>：对于复杂任务，LLMs 需要多步骤的思考、规划和执行。</li>
<li><strong>Prompt 工程复杂 (Prompt Engineering Complexity)</strong>：构建有效的 Prompt 需要经验和技巧，且难以模块化和复用。</li>
</ol>
<p>LangChain 正是为了解决这些痛点而诞生的。它提供了一个结构化的方式来：</p>
<ul>
<li><strong>与外部数据源集成</strong>：让 LLM 获取最新或领域特定的信息。</li>
<li><strong>赋予记忆能力</strong>：使 LLM 能够进行有状态的对话。</li>
<li><strong>支持工具使用 (Tool Usage)</strong>：让 LLM 能够调用外部 API 或函数来执行动作。</li>
<li><strong>链式操作 (Chaining)</strong>：将多个 LLM 调用或其他组件组合成一个序列，实现复杂任务的分解和执行。</li>
<li><strong>可观测性 (Observability)</strong>：方便开发者理解和调试 LLM 链的运行过程。</li>
</ul>
<h2 id="二、LangChain-的核心模块与概念"><a href="#二、LangChain-的核心模块与概念" class="headerlink" title="二、LangChain 的核心模块与概念"></a>二、LangChain 的核心模块与概念</h2><p>LangChain 的架构设计由六个核心模块组成，它们共同协作，提供了构建 LLM 应用所需的所有基本组件。</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    User(用户) --&gt; Prompts(Prompts)
    Prompts --&gt; LLMs(LLMs)
    LLMs --&gt; OutputParsers(Output Parsers)
    OutputParsers --&gt; Chains(Chains)
    Chains --&gt; Agents(Agents)
    Agents --&gt; Tools(Tools)
    Tools --&gt; ExternalServices(外部服务&#x2F;数据源)
    Agents --&gt; Memory(Memory)
    Memory --&gt; Chains
    Memory --&gt; LLMs
  </pre></div>

<h3 id="2-1-LLMs-大语言模型"><a href="#2-1-LLMs-大语言模型" class="headerlink" title="2.1 LLMs (大语言模型)"></a>2.1 LLMs (大语言模型)</h3><p>这是 LangChain 的核心驱动力。</p>
<ul>
<li><strong>定义</strong>：LangChain 提供了与各种大语言模型交互的统一接口。无论是 OpenAI 的 GPT 系列、Google 的 Gemini&#x2F;PaLM、Hugging Face 上的开源模型，还是自定义的本地模型，都可以通过标准接口接入。</li>
<li><strong>功能</strong>：负责文本的生成、理解、摘要、翻译等核心能力。</li>
<li><strong>类型</strong>：<ul>
<li><strong>LLMs</strong>：纯文本输入输出模型（如 GPT-3.5 <code>text-davinci-003</code>，旧接口）。</li>
<li><strong>ChatModels</strong>：聊天模型，接收一系列消息（<code>HumanMessage</code>, <code>AIMessage</code>, <code>SystemMessage</code>），返回一条消息（如 GPT-3.5 <code>gpt-3.5-turbo</code>，GPT-4）。</li>
</ul>
</li>
</ul>
<p><strong>示例 (Python)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 OpenAI ChatModel</span></span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">chat = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">response = chat.invoke([</span><br><span class="line">    SystemMessage(<span class="string">&quot;你是一个专业的翻译助手。&quot;</span>),</span><br><span class="line">    HumanMessage(<span class="string">&quot;Hello, how are you today?&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.content) <span class="comment"># Output: &quot;你好，你今天过得怎么样？&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-Prompts-提示词"><a href="#2-2-Prompts-提示词" class="headerlink" title="2.2 Prompts (提示词)"></a>2.2 Prompts (提示词)</h3><ul>
<li><strong>定义</strong>：Prompt 是你给 LLM 的输入指令。LangChain 提供了工具来构造、管理和优化这些提示词。</li>
<li><strong>功能</strong>：<ul>
<li><strong><code>PromptTemplate</code></strong>：用于动态地构建 Prompt，将用户输入或变量插入到预定义的模板中。</li>
<li><strong><code>ChatPromptTemplate</code></strong>：专门为聊天模型设计，可以构造包含系统消息、人类消息、AI 消息等多角色的消息序列。</li>
<li><strong><code>ExampleSelector</code></strong>：根据输入动态选择最佳的示例来构造 Few-shot Prompt。</li>
</ul>
</li>
<li><strong>重要性</strong>：好的 Prompt 是 LLM 应用成功的关键，Prompt 工程是核心技能。</li>
</ul>
<p><strong>示例 (Python)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> PromptTemplate, ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. PromptTemplate (适用于 LLMs 或单个字符串输入)</span></span><br><span class="line">string_prompt_template = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;给我一个关于 &#123;product&#125; 的有趣名称。&quot;</span></span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(string_prompt_template.<span class="built_in">format</span>(product=<span class="string">&quot;智能吸尘器&quot;</span>))</span><br><span class="line"><span class="comment"># Output: &quot;给我一个关于 智能吸尘器 的有趣名称。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. ChatPromptTemplate (适用于 ChatModels)</span></span><br><span class="line">chat_template = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个起名专家，擅长为科技产品起名。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;给我一个关于 &#123;product&#125; 的有趣名称。&quot;</span>)</span><br><span class="line">])</span><br><span class="line"><span class="comment"># 将 PromptTemplate 与 LLM 结合</span></span><br><span class="line">model = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">chain = chat_template | model</span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;product&quot;</span>: <span class="string">&quot;智能吸尘器&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(response.content) <span class="comment"># Output: 可能是 &quot;旋风管家&quot; 或 &quot;扫地忍者&quot; 等</span></span><br></pre></td></tr></table></figure>

<h3 id="2-3-Output-Parsers-输出解析器"><a href="#2-3-Output-Parsers-输出解析器" class="headerlink" title="2.3 Output Parsers (输出解析器)"></a>2.3 Output Parsers (输出解析器)</h3><ul>
<li><strong>定义</strong>：LLM 的输出通常是自由格式的文本。Output Parsers 负责将这些文本结构化为特定的数据格式，如 JSON、列表或自定义对象。</li>
<li><strong>功能</strong>：确保 LLM 的输出能够被下游组件或应用逻辑方便地处理。</li>
<li><strong>重要性</strong>：弥合了 LLM 的文本输出与程序化数据结构之间的鸿沟。</li>
</ul>
<p><strong>示例 (Python)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field <span class="comment"># 用于定义结构化输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义期望的输出结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Joke</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    setup: <span class="built_in">str</span> = Field(description=<span class="string">&quot;笑话的开场白&quot;</span>)</span><br><span class="line">    punchline: <span class="built_in">str</span> = Field(description=<span class="string">&quot;笑话的包袱&quot;</span>)</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">parser = JsonOutputParser(pydantic_object=Joke)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取格式指导的 Prompt</span></span><br><span class="line">format_instructions = parser.get_format_instructions()</span><br><span class="line"></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个输出 JSON 格式的助手。\n&#123;format_instructions&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;给我讲一个关于 &#123;topic&#125; 的笑话。&quot;</span>)</span><br><span class="line">]).partial(format_instructions=format_instructions) <span class="comment"># 填充格式指令</span></span><br><span class="line"></span><br><span class="line">chain = prompt | model | parser</span><br><span class="line"></span><br><span class="line">joke = chain.invoke(&#123;<span class="string">&quot;topic&quot;</span>: <span class="string">&quot;猫&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(joke)</span><br><span class="line"><span class="comment"># Output: &#123;&#x27;setup&#x27;: &#x27;为什么猫是糟糕的扑克玩家？&#x27;, &#x27;punchline&#x27;: &#x27;因为它们总是藏着一张王牌！&#x27;&#125;</span></span><br><span class="line"><span class="built_in">print</span>(joke.setup) <span class="comment"># 可以像访问对象属性一样访问</span></span><br></pre></td></tr></table></figure>

<h3 id="2-4-Chains-链"><a href="#2-4-Chains-链" class="headerlink" title="2.4 Chains (链)"></a>2.4 Chains (链)</h3><ul>
<li><strong>定义</strong>：将多个 LLM 调用或其他组件（如 PromptTemplate、OutputParser）组合成一个逻辑序列，以完成更复杂的任务。</li>
<li><strong>功能</strong>：实现多步骤推理、数据转换、流程编排。</li>
<li><strong>核心理念</strong>：将复杂的 LLM 任务分解成一系列可管理的、相互连接的步骤。</li>
<li><strong>类型</strong>：<ul>
<li><strong>LLMChain</strong>：最基本的链，将 <code>PromptTemplate</code> 与 <code>LLM</code> 结合。</li>
<li><strong>SequentialChain</strong>：顺序执行多个链，将一个链的输出作为下一个链的输入。</li>
<li><strong>RetrievalQAChain</strong>：用于问答系统，结合检索器获取相关文档。</li>
<li><strong><code>LCEL (LangChain Expression Language)</code></strong>：是 LangChain 中构建链的推荐方式，提供了一种声明式、可组合的语法 (<code>|</code> 管道操作符)。</li>
</ul>
</li>
</ul>
<p><strong>示例 (Python - 使用 LCEL)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line">model = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line">output_parser = StrOutputParser() <span class="comment"># 简单地将 LLM 输出转换为字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤 1: 概念生成链</span></span><br><span class="line">concept_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个概念生成专家。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;给我生成一个关于 &#123;item&#125; 的有趣概念。&quot;</span>)</span><br><span class="line">])</span><br><span class="line">concept_chain = concept_prompt | model | output_parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤 2: 描述生成链 (使用上一步的输出)</span></span><br><span class="line">description_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个描述生成专家。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;描述一下这个概念: &#123;concept&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">description_chain = description_prompt | model | output_parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合成一个完整的链 (使用 LCEL 的 `RunnablePassthrough` 和 `chain.with_config`)</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line">full_chain = (</span><br><span class="line">    &#123;<span class="string">&quot;concept&quot;</span>: concept_chain&#125; <span class="comment"># 先执行 concept_chain，结果作为 &#x27;concept&#x27; 键值</span></span><br><span class="line">    | RunnablePassthrough.assign(</span><br><span class="line">        description=description_chain <span class="comment"># 再执行 description_chain，输入为 &#123;concept: ...&#125;，结果作为 &#x27;description&#x27; 键值</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = full_chain.invoke(&#123;<span class="string">&quot;item&quot;</span>: <span class="string">&quot;智能冰箱&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line"><span class="comment"># &#123;</span></span><br><span class="line"><span class="comment">#   &#x27;concept&#x27;: &#x27;“智能食物管家”：一个能追踪库存、建议食谱、自动补货的冰箱。&#x27;,</span></span><br><span class="line"><span class="comment">#   &#x27;description&#x27;: &#x27;智能食物管家是一款革命性的智能冰箱，它搭载了先进的传感器和AI系统，能够实时监控冰箱内食材的种类、数量和新鲜度。它不仅能根据现有食材智能推荐个性化食谱，还能在食材即将耗尽时自动生成购物清单，甚至通过合作平台一键完成在线补货。此外，它还能根据用户的饮食偏好和健康目标，提供营养分析和膳食建议，真正实现对家庭饮食的智能化管理。&#x27;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-5-Agents-代理"><a href="#2-5-Agents-代理" class="headerlink" title="2.5 Agents (代理)"></a>2.5 Agents (代理)</h3><ul>
<li><strong>定义</strong>：Agent 是 LangChain 中最强大的概念之一。它让 LLM 具备了<strong>推理 (Reasoning)</strong> 和<strong>行动 (Action)</strong> 的能力。Agent 会根据用户的输入，自主决定需要执行哪些 <code>Tool</code>，以及执行多少次，直到任务完成。</li>
<li><strong>功能</strong>：实现复杂、动态的、需要规划和多步操作的任务。</li>
<li><strong>工作原理 (ReAct 模式)</strong>：通常遵循 <strong>ReAct (Reasoning and Acting)</strong> 模式：<ol>
<li><strong>Thought (思考)</strong>：LLM 思考下一步应该做什么。</li>
<li><strong>Action (行动)</strong>：LLM 决定调用哪个 <code>Tool</code> 以及使用什么参数。</li>
<li><strong>Observation (观察)</strong>：<code>Tool</code> 执行后返回结果。</li>
<li>重复以上步骤，直到达到最终答案。</li>
</ol>
</li>
<li><strong>重要性</strong>：是构建能够自主解决问题的 LLM 应用程序的核心。</li>
</ul>
<p><strong>示例 (Python)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.agents <span class="keyword">import</span> AgentExecutor, create_react_agent, tool</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个工具</span></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_current_weather</span>(<span class="params">location: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;获取指定城市当前天气信息的工具。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> location == <span class="string">&quot;北京&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;北京：晴，25°C，微风。&quot;</span></span><br><span class="line">    <span class="keyword">elif</span> location == <span class="string">&quot;上海&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;上海：多云，28°C，阵雨。&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;未知城市天气。&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义另一个工具</span></span><br><span class="line"><span class="meta">@tool</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_expression</span>(<span class="params">expression: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算数学表达式的工具。&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>(<span class="built_in">eval</span>(expression)) <span class="comment"># 注意 eval 的安全风险，实际应用中应使用更安全的数学表达式解析器</span></span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;计算失败: <span class="subst">&#123;e&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">tools = [get_current_weather, calculate_expression]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 Agent Prompt</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个善于使用工具的助手。尽可能地回答用户的问题。&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;placeholder&quot;</span>, <span class="string">&quot;&#123;agent_scratchpad&#125;&quot;</span>) <span class="comment"># 代理的思考过程和工具输出会填充到这里</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 ReAct 代理</span></span><br><span class="line">agent = create_react_agent(llm, tools, prompt)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Agent 执行器</span></span><br><span class="line">agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=<span class="literal">True</span>) <span class="comment"># verbose=True 可以看到代理的思考过程</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用代理</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;--- 调用 Agent 1 ---&quot;</span>)</span><br><span class="line">result1 = agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;上海今天天气怎么样？&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终答案: <span class="subst">&#123;result1[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 调用 Agent 2 ---&quot;</span>)</span><br><span class="line">result2 = agent_executor.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;计算 123 乘以 456 再加上 789 等于多少？&quot;</span>&#125;)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最终答案: <span class="subst">&#123;result2[<span class="string">&#x27;output&#x27;</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出会包含 Thought, Action, Observation 的详细过程</span></span><br></pre></td></tr></table></figure>

<h3 id="2-6-Memory-记忆"><a href="#2-6-Memory-记忆" class="headerlink" title="2.6 Memory (记忆)"></a>2.6 Memory (记忆)</h3><ul>
<li><strong>定义</strong>：使 LLM 能够记住之前的对话或交互，从而进行有状态的交流。</li>
<li><strong>功能</strong>：将历史消息传递给 LLM，维持上下文。</li>
<li><strong>重要性</strong>：对于聊天机器人、对话代理等应用至关重要。</li>
<li><strong>类型</strong>：<ul>
<li><strong><code>ConversationBufferMemory</code></strong>：最简单，存储所有历史消息。</li>
<li><strong><code>ConversationSummaryMemory</code></strong>：总结历史消息，以节省 Token。</li>
<li><strong><code>ConversationBufferWindowMemory</code></strong>：只保留最近 <code>k</code> 条消息。</li>
<li><strong><code>ConversationTokenBufferMemory</code></strong>：根据 Token 数量限制保留消息。</li>
<li><strong><code>VectorStoreRetrieverMemory</code></strong>：将记忆存储到向量数据库中，进行检索。</li>
</ul>
</li>
</ul>
<p><strong>示例 (Python)</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate, MessagesPlaceholder</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> ConversationChain</span><br><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationBufferMemory</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0</span>, model=<span class="string">&quot;gpt-4&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 定义 PromptTemplate，包含 MessagesPlaceholder 用于记忆</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一个友好的聊天机器人。&quot;</span>),</span><br><span class="line">    MessagesPlaceholder(variable_name=<span class="string">&quot;history&quot;</span>), <span class="comment"># 记忆将填充到这里</span></span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;input&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 初始化记忆</span></span><br><span class="line">memory = ConversationBufferMemory(return_messages=<span class="literal">True</span>) <span class="comment"># return_messages=True 会返回消息对象列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建对话链</span></span><br><span class="line">conversation = ConversationChain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    memory=memory,</span><br><span class="line">    prompt=prompt</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行对话</span></span><br><span class="line"><span class="built_in">print</span>(conversation.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;你好！你叫什么名字？&quot;</span>&#125;))</span><br><span class="line"><span class="built_in">print</span>(conversation.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;你最喜欢什么颜色？&quot;</span>&#125;))</span><br><span class="line"><span class="built_in">print</span>(conversation.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;我刚才问了你什么？&quot;</span>&#125;)) <span class="comment"># 此时机器人会记得之前的对话</span></span><br></pre></td></tr></table></figure>

<h2 id="三、LangChain-的应用场景"><a href="#三、LangChain-的应用场景" class="headerlink" title="三、LangChain 的应用场景"></a>三、LangChain 的应用场景</h2><p>LangChain 极大地扩展了 LLM 的应用边界，常见的应用场景包括：</p>
<ol>
<li><strong>高级聊天机器人 (Advanced Chatbots)</strong>：具备记忆、工具调用能力的智能对话系统。</li>
<li><strong>问答系统 (Question Answering Systems)</strong>：基于私有文档或最新信息的检索增强生成 (RAG) 系统。</li>
<li><strong>智能代理 (Autonomous Agents)</strong>：能够自主规划和执行复杂任务的代理，例如自动化客服、代码生成和执行、数据分析等。</li>
<li><strong>数据提取与结构化 (Data Extraction &amp; Structuring)</strong>：从非结构化文本中提取信息并转换为 JSON 等结构化格式。</li>
<li><strong>内容创作与摘要 (Content Creation &amp; Summarization)</strong>：更灵活地生成和总结内容。</li>
<li><strong>代码生成与调试 (Code Generation &amp; Debugging)</strong>：结合代码解释器，实现更强大的代码辅助功能。</li>
</ol>
<h2 id="四、LangChain-的优势与挑战"><a href="#四、LangChain-的优势与挑战" class="headerlink" title="四、LangChain 的优势与挑战"></a>四、LangChain 的优势与挑战</h2><h3 id="4-1-优势："><a href="#4-1-优势：" class="headerlink" title="4.1 优势："></a>4.1 优势：</h3><ul>
<li><strong>模块化与可组合性</strong>：提供了丰富的可复用组件，可以像乐高积木一样自由组合。</li>
<li><strong>抽象层</strong>：为各种 LLM、Prompt 工程、外部工具等提供统一的抽象接口，降低开发难度。</li>
<li><strong>强大的 Agent 能力</strong>：通过 Agent 实现了 LLM 的自主规划和工具使用，是构建智能应用的关键。</li>
<li><strong>活跃的社区与生态</strong>：社区活跃，更新迭代快，支持多种语言 (Python, JavaScript&#x2F;TypeScript)。</li>
<li><strong>与外部系统集成</strong>：轻松连接各种数据库、API、文档存储等。</li>
</ul>
<h3 id="4-2-挑战："><a href="#4-2-挑战：" class="headerlink" title="4.2 挑战："></a>4.2 挑战：</h3><ul>
<li><strong>学习曲线</strong>：概念较多，初学者需要一定时间理解其核心模块和工作原理。</li>
<li><strong>性能与成本</strong>：多步骤链和 Agent 意味着更多的 LLM 调用，可能增加延迟和成本。</li>
<li><strong>调试复杂性</strong>：长链或 Agent 的运行过程可能难以调试和理解，虽然 LangSmith 等工具正在缓解这一问题。</li>
<li><strong>版本迭代快</strong>：LangChain 作为一个新兴框架，版本更新非常频繁，API 可能会有变动。</li>
<li><strong>幻觉与可靠性</strong>：尽管 LangChain 提供了工具来缓解，但 LLM 本身的幻觉问题依然存在，需要精心设计和测试以确保可靠性。</li>
</ul>
<h2 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h2><p>LangChain 是 LLM 应用开发领域一个极其重要的框架，它为开发者提供了一条清晰的路径，将大语言模型从一个简单的文本生成器，提升为能够理解、推理、行动并与外部世界交互的智能引擎。通过其模块化的设计和对核心 LLM 应用范式的抽象，LangChain 极大地降低了构建复杂、功能丰富 LLM 应用程序的门槛。</p>
<p>随着 LLM 技术的发展，LangChain 无疑将继续演进，成为构建下一代 AI 应用不可或缺的工具。要充分利用 LangChain，开发者需要深入理解其核心概念，并根据具体应用场景灵活运用这些组件，以平衡功能、性能和成本。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/3201b8057954/">https://blog.tbf1211.xx.kg/3201b8057954/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/LangChain/">LangChain</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-27.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/c1c3c8b5b003/" title="LangChain Model I/O 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">LangChain Model I/O 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Model I&#x2F;O 是 LangChain 框架的核心组成部分之一，它提供了一套标准化的接口和工具，用于与各种大型语言模型 (LLMs) 和聊天模型 (Chat Models) 进行交互，并对其输入和输出进行有效的管理和结构化。这是构建任何基于 LLM 的应用程序的基础。  核心思想：将与 LLM 的“对话”分解为可管理、可组合的组件：输入 (Prompt Templates)、模型调用 (LLM&#x2F;Chat Models) 和输出处理 (Output Parsers)。   一、为什么 Model I&#x2F;O 至关重要？在没有 LangChain Model I&#x2F;O 的情况下，直接与 LLM 交互通常意味着：  手动拼接 Prompt: 需要手动构建复杂的字符串，其中包含指令、上下文、示例和用户输入。这既繁琐又容易出错。 硬编码模型调用: 每次更换模型或供应商时，都需要修改底层代码。 非结构化的输出: LLM 的原始输出通常是自由文本，需要编写复杂的字符串解析逻辑来提取所需信息。 缺乏可复用性: 不同应用场景下的 Prom...</div></div></div></a><a class="pagination-related" href="/11a674c6a974/" title="NativeScript-Vue3详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-23.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">NativeScript-Vue3详解</div></div><div class="info-2"><div class="info-item-1"> NativeScript-Vue 3 是一个强大的框架组合，它允许开发者使用熟悉的 Vue 3 语法和工具链来构建真正的原生 iOS 和 Android 移动应用程序。与传统 Hybrid 应用（如 Cordova 或 Ionic）不同，NativeScript 直接操作原生 UI 组件，因此能够提供一流的性能和用户体验，同时避免了 Web 视图的性能瓶颈。  核心亮点：使用 Vue 3 渲染原生 UI 组件，实现高性能、媲美原生体验的跨平台移动应用开发。   一、什么是 NativeScript-Vue 3？1.1 NativeScript 简介NativeScript 是一个开源框架，用于使用 JavaScript、TypeScript 或其他编译到 JavaScript 的语言来构建原生移动应用程序。它的核心能力在于：  直接访问原生 API：无需编写任何 Objective-C&#x2F;Swift 或 Java&#x2F;Kotlin 代码，开发者可以直接从 JavaScript 访问设备的所有原生 API。 原生 UI 渲染：不使用 WebView，而是将 Java...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/c1c3c8b5b003/" title="LangChain Model I&#x2F;O 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-13</div><div class="info-item-2">LangChain Model I&#x2F;O 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Model I&#x2F;O 是 LangChain 框架的核心组成部分之一，它提供了一套标准化的接口和工具，用于与各种大型语言模型 (LLMs) 和聊天模型 (Chat Models) 进行交互，并对其输入和输出进行有效的管理和结构化。这是构建任何基于 LLM 的应用程序的基础。  核心思想：将与 LLM 的“对话”分解为可管理、可组合的组件：输入 (Prompt Templates)、模型调用 (LLM&#x2F;Chat Models) 和输出处理 (Output Parsers)。   一、为什么 Model I&#x2F;O 至关重要？在没有 LangChain Model I&#x2F;O 的情况下，直接与 LLM 交互通常意味着：  手动拼接 Prompt: 需要手动构建复杂的字符串，其中包含指令、上下文、示例和用户输入。这既繁琐又容易出错。 硬编码模型调用: 每次更换模型或供应商时，都需要修改底层代码。 非结构化的输出: LLM 的原始输出通常是自由文本，需要编写复杂的字符串解析逻辑来提取所需信息。 缺乏可复用性: 不同应用场景下的 Prom...</div></div></div></a><a class="pagination-related" href="/21e140c78f80/" title="大型语言模型如何理解人类文字：从Token到语义表征"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-16</div><div class="info-item-2">大型语言模型如何理解人类文字：从Token到语义表征</div></div><div class="info-2"><div class="info-item-1"> 大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。   一、基础构建模块：从文本到向量LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于分词 (Tokenization) 和词嵌入 (Word Embeddings)。 1.1 分词 (Tokenization)分词是将连续的文本序列切分成有意义的最小单位——Token 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。  词级别分词 (Word-level Tokenization)：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-25.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/9a400d225757/" title="对话模型与非对话模型详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-26</div><div class="info-item-2">对话模型与非对话模型详解</div></div><div class="info-2"><div class="info-item-1"> 在大型语言模型 (LLM) 的领域中，”对话模型” (Chat Models) 和 “非对话模型” (或称为 “文本模型” Text Models) 是两种基本但又有所区别的模型范式，它们在设计、训练数据、输入&#x2F;输出格式以及最佳应用场景上存在差异。理解这两种模型的区别是有效利用 LLM 进行开发的关键。  核心思想：对话模型优化用于多轮、上下文感知的交互，通过消息列表进行输入输出；非对话模型则擅长单次、直接的文本指令处理，通过字符串进行输入输出。    一、非对话模型 (Text Models &#x2F; LLMs)非对话模型是早期和传统的大型语言模型形式，它们通常设计为接收一个单一的字符串作为输入（通常称为 “prompt”），并生成一个单一的字符串作为输出。虽然这些模型也能在一定程度上处理对话，但通常需要通过在单次 Prompt 中手动构建对话历史来模拟。 1.1 特点 字符串输入&#x2F;输出：输入是一个字符串，输出也是一个字符串。 输入示例：&quot;把以下文本总结一下：[文本内容]&quot; 输出示例：&quot;这是一段总结后的文本。&quot; ...</div></div></div></a><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-27</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">331</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">202</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">71</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-LangChain%EF%BC%9F"><span class="toc-text">一、为什么需要 LangChain？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81LangChain-%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E4%B8%8E%E6%A6%82%E5%BF%B5"><span class="toc-text">二、LangChain 的核心模块与概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-LLMs-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.1 LLMs (大语言模型)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Prompts-%E6%8F%90%E7%A4%BA%E8%AF%8D"><span class="toc-text">2.2 Prompts (提示词)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Output-Parsers-%E8%BE%93%E5%87%BA%E8%A7%A3%E6%9E%90%E5%99%A8"><span class="toc-text">2.3 Output Parsers (输出解析器)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Chains-%E9%93%BE"><span class="toc-text">2.4 Chains (链)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Agents-%E4%BB%A3%E7%90%86"><span class="toc-text">2.5 Agents (代理)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Memory-%E8%AE%B0%E5%BF%86"><span class="toc-text">2.6 Memory (记忆)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81LangChain-%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">三、LangChain 的应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81LangChain-%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E6%8C%91%E6%88%98"><span class="toc-text">四、LangChain 的优势与挑战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%BC%98%E5%8A%BF%EF%BC%9A"><span class="toc-text">4.1 优势：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%8C%91%E6%88%98%EF%BC%9A"><span class="toc-text">4.2 挑战：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">五、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/1ae20d2726d8/" title="MiniRTC 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-13.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="MiniRTC 详解"/></a><div class="content"><a class="title" href="/1ae20d2726d8/" title="MiniRTC 详解">MiniRTC 详解</a><time datetime="2025-11-28T22:24:00.000Z" title="发表于 2025-11-29 06:24:00">2025-11-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/be24ef88e59a/" title="WebRTC 技术详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-24.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebRTC 技术详解"/></a><div class="content"><a class="title" href="/be24ef88e59a/" title="WebRTC 技术详解">WebRTC 技术详解</a><time datetime="2025-11-27T22:24:00.000Z" title="发表于 2025-11-28 06:24:00">2025-11-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/53e63dc49a04/" title="PyInstaller 深度解析与指令详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="PyInstaller 深度解析与指令详解"/></a><div class="content"><a class="title" href="/53e63dc49a04/" title="PyInstaller 深度解析与指令详解">PyInstaller 深度解析与指令详解</a><time datetime="2025-11-24T22:24:00.000Z" title="发表于 2025-11-25 06:24:00">2025-11-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/61fddda6a7a8/" title="Go 语言 GC (Garbage Collection) 机制详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-12.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go 语言 GC (Garbage Collection) 机制详解"/></a><div class="content"><a class="title" href="/61fddda6a7a8/" title="Go 语言 GC (Garbage Collection) 机制详解">Go 语言 GC (Garbage Collection) 机制详解</a><time datetime="2025-11-23T22:24:00.000Z" title="发表于 2025-11-24 06:24:00">2025-11-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/fae19f12de23/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"/></a><div class="content"><a class="title" href="/fae19f12de23/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析">压缩字典树 (Radix Trie/Patricia Trie) 深度解析</a><time datetime="2025-11-17T22:24:00.000Z" title="发表于 2025-11-18 06:24:00">2025-11-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-27.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            pagePV.textContent = data.pageviews.value
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors && typeof data.visitors.value !== 'undefined') {
            siteUV.textContent = data.visitors.value
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            sitePV.textContent = data.pageviews.value
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>