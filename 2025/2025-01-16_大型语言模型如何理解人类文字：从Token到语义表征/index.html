<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大型语言模型如何理解人类文字：从Token到语义表征 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕">
<meta property="og:type" content="article">
<meta property="og:title" content="大型语言模型如何理解人类文字：从Token到语义表征">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-31.jpg">
<meta property="article:published_time" content="2025-01-15T22:24:00.000Z">
<meta property="article:modified_time" content="2025-11-21T10:18:40.412Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-31.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大型语言模型如何理解人类文字：从Token到语义表征",
  "url": "https://blog.tbf1211.xx.kg/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-31.jpg",
  "datePublished": "2025-01-15T22:24:00.000Z",
  "dateModified": "2025-11-21T10:18:40.412Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大型语言模型如何理解人类文字：从Token到语义表征',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">306</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">199</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">69</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-31.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">大型语言模型如何理解人类文字：从Token到语义表征</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大型语言模型如何理解人类文字：从Token到语义表征</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-01-15T22:24:00.000Z" title="发表于 2025-01-16 06:24:00">2025-01-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>12分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>大型语言模型 (Large Language Models, LLMs)</strong> 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中<strong>统计模式和语义关联</strong>的深度学习，构建出高度复杂的语言表征。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。</strong></p>
</div>
<hr>
<h2 id="一、基础构建模块：从文本到向量"><a href="#一、基础构建模块：从文本到向量" class="headerlink" title="一、基础构建模块：从文本到向量"></a>一、基础构建模块：从文本到向量</h2><p>LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于<strong>分词 (Tokenization)</strong> 和<strong>词嵌入 (Word Embeddings)</strong>。</p>
<h3 id="1-1-分词-Tokenization"><a href="#1-1-分词-Tokenization" class="headerlink" title="1.1 分词 (Tokenization)"></a>1.1 分词 (Tokenization)</h3><p>分词是将连续的文本序列切分成有意义的最小单位——<strong>Token</strong> 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。</p>
<ul>
<li><strong>词级别分词 (Word-level Tokenization)</strong>：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且难以处理未登录词 (Out-Of-Vocabulary, OOV)。</li>
<li><strong>子词级别分词 (Subword-level Tokenization)</strong>：更常用的方法，如 Byte Pair Encoding (BPE)、WordPiece、SentencePiece。它将不常见的词分解成更小的、常见的子词单元，有效平衡了词汇量和 OOV 问题，同时保留了词语的形态学信息。例如，“unbelievable” 可能被分解为 “un”, “believe”, “able”。</li>
<li><strong>字符级别分词 (Character-level Tokenization)</strong>：将每个字符视为一个 Token。词汇量最小，但丢失了大部分语义信息。</li>
</ul>
<p><strong>示例 (Go 语言简化的子词分词概念)</strong>：</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;strings&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SimplifiedSubwordTokenizer 模拟子词分词的概念</span></span><br><span class="line"><span class="comment">// 实际的子词分词器（如BPE）要复杂得多，涉及频率统计和迭代合并</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">SimplifiedSubwordTokenizer</span><span class="params">(text <span class="type">string</span>)</span></span> []<span class="type">string</span> &#123;</span><br><span class="line">	<span class="comment">// 简单的预处理：将标点符号与词分离，并转换为小写</span></span><br><span class="line">	text = strings.ReplaceAll(text, <span class="string">&quot;.&quot;</span>, <span class="string">&quot; .&quot;</span>)</span><br><span class="line">	text = strings.ReplaceAll(text, <span class="string">&quot;,&quot;</span>, <span class="string">&quot; ,&quot;</span>)</span><br><span class="line">	text = strings.ToLower(text)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 分割成基础词/子词单元</span></span><br><span class="line">	words := strings.Fields(text)</span><br><span class="line">	<span class="keyword">var</span> tokens []<span class="type">string</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 模拟子词分解规则（非常简化）</span></span><br><span class="line">	<span class="keyword">for</span> _, word := <span class="keyword">range</span> words &#123;</span><br><span class="line">		<span class="keyword">if</span> strings.HasPrefix(word, <span class="string">&quot;un&quot;</span>) &amp;&amp; <span class="built_in">len</span>(word) &gt; <span class="number">2</span> &#123;</span><br><span class="line">			tokens = <span class="built_in">append</span>(tokens, <span class="string">&quot;un&quot;</span>, strings.TrimPrefix(word, <span class="string">&quot;un&quot;</span>))</span><br><span class="line">		&#125; <span class="keyword">else</span> <span class="keyword">if</span> strings.Contains(word, <span class="string">&quot;ing&quot;</span>) &amp;&amp; <span class="built_in">len</span>(word) &gt; <span class="number">3</span> &#123;</span><br><span class="line">			<span class="comment">// 如果词汇包含 &#x27;ing&#x27; 且不是它本身</span></span><br><span class="line">			parts := strings.Split(word, <span class="string">&quot;ing&quot;</span>)</span><br><span class="line">			<span class="keyword">if</span> <span class="built_in">len</span>(parts[<span class="number">0</span>]) &gt; <span class="number">0</span> &#123;</span><br><span class="line">				tokens = <span class="built_in">append</span>(tokens, parts[<span class="number">0</span>])</span><br><span class="line">			&#125;</span><br><span class="line">			tokens = <span class="built_in">append</span>(tokens, <span class="string">&quot;ing&quot;</span>)</span><br><span class="line">			<span class="keyword">if</span> <span class="built_in">len</span>(parts) &gt; <span class="number">1</span> &amp;&amp; <span class="built_in">len</span>(parts[<span class="number">1</span>]) &gt; <span class="number">0</span> &#123; <span class="comment">// 处理 &#x27;ing&#x27; 后还有内容的情况</span></span><br><span class="line">				tokens = <span class="built_in">append</span>(tokens, parts[<span class="number">1</span>])</span><br><span class="line">			&#125;</span><br><span class="line">		&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">			tokens = <span class="built_in">append</span>(tokens, word)</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">return</span> tokens</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	sentence := <span class="string">&quot;The unbelievable cat is running quickly.&quot;</span></span><br><span class="line">	tokens := SimplifiedSubwordTokenizer(sentence)</span><br><span class="line">	fmt.Println(<span class="string">&quot;Original sentence:&quot;</span>, sentence)</span><br><span class="line">	fmt.Println(<span class="string">&quot;Simplified tokens:&quot;</span>, tokens)</span><br><span class="line">	<span class="comment">// 预期输出可能类似：</span></span><br><span class="line">	<span class="comment">// Original sentence: The unbelievable cat is running quickly.</span></span><br><span class="line">	<span class="comment">// Simplified tokens: [the un believe able cat is run ing quick ly .]</span></span><br><span class="line">	<span class="comment">// (注意：这是高度简化的，实际BPE会更复杂和精确)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>解释</strong>：上述 Go 代码展示了一个极度简化的分词概念。在实际的LLMs中，分词器会基于一个预训练的词汇表，通过复杂的算法（如BPE）将文本切分为模型能够处理的Token序列。</p>
<h3 id="1-2-词嵌入-Word-Embeddings-向量表示"><a href="#1-2-词嵌入-Word-Embeddings-向量表示" class="headerlink" title="1.2 词嵌入 (Word Embeddings) &#x2F; 向量表示"></a>1.2 词嵌入 (Word Embeddings) &#x2F; 向量表示</h3><p>在分词之后，每个 Token 都需要被转换成一个<strong>数值向量</strong>。这个向量就是 Token 的“嵌入”(Embedding)。</p>
<ul>
<li><strong>高维向量</strong>：嵌入向量通常是几百到几千维的浮点数。</li>
<li><strong>语义信息</strong>：这些向量的特点是，语义相似的词（例如 “king” 和 “queen”）在向量空间中会彼此靠近，而语义不相关的词则相距较远。这种距离关系反映了词语的语义关联。</li>
<li><strong>上下文感知</strong>：现代的词嵌入（如ELMo, BERT, GPT中的嵌入）是<strong>上下文感知</strong>的，意味着同一个词在不同的语境下会有不同的嵌入向量。例如，“bank” 在表示“银行”时和表示“河岸”时，其嵌入向量是不同的。</li>
</ul>
<p><strong>数学表示</strong>：<br>对于一个词 $w$，其嵌入向量表示为 $\mathbf{e}_w \in \mathbb{R}^d$，其中 $d$ 是嵌入向量的维度。</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[人类文字] --&gt; B[&quot;分词 (Tokenization)&quot;]
    B --&gt; C{Token序列}
    C --&gt; D[&quot;词嵌入 (Word Embeddings)&quot;]
    D --&gt; E[&quot;高维向量序列 (数值表示)&quot;]
  </pre></div>

<h2 id="二、Transformer-架构：理解语言的核心"><a href="#二、Transformer-架构：理解语言的核心" class="headerlink" title="二、Transformer 架构：理解语言的核心"></a>二、Transformer 架构：理解语言的核心</h2><p>LLMs “理解”能力的核心是其采用的 <strong>Transformer 架构</strong>，特别是其中的<strong>自注意力机制 (Self-Attention Mechanism)</strong>。</p>
<h3 id="2-1-Transformer-概览"><a href="#2-1-Transformer-概览" class="headerlink" title="2.1 Transformer 概览"></a>2.1 Transformer 概览</h3><p>Transformer 架构摒弃了传统的循环神经网络 (RNN) 和卷积神经网络 (CNN)，完全依赖于注意力机制来处理序列数据。它能够并行处理输入序列，大大提高了训练效率，并能有效捕捉长距离依赖关系。</p>
<p><strong>结构示意</strong>：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    Input[输入 Token 序列] --&gt; Embedding[词嵌入 + 位置编码]
    Embedding --&gt; EncoderStack[N 个编码器层]
    EncoderStack --&gt; DecoderStack[&quot;N 个解码器层 (对于生成任务)&quot;]
    DecoderStack --&gt; Output[输出概率分布&#x2F;下一个 Token]

    subgraph Encoder Layer
        SA_enc[多头自注意力] --&gt; FFN_enc[前馈神经网络]
        SA_enc -- 残差连接 &amp; 层归一化 --&gt; FFN_enc
    end

    subgraph Decoder Layer
        SA_dec[带掩码多头自注意力] --&gt; CrossA[交叉注意力]
        CrossA --&gt; FFN_dec[前馈神经网络]
        SA_dec -- 残差连接 &amp; 层归一化 --&gt; CrossA
        CrossA -- 残差连接 &amp; 层归一化 --&gt; FFN_dec
    end

    EncoderStack --&gt; CrossA
  </pre></div>

<h3 id="2-2-自注意力机制-Self-Attention-Mechanism"><a href="#2-2-自注意力机制-Self-Attention-Mechanism" class="headerlink" title="2.2 自注意力机制 (Self-Attention Mechanism)"></a>2.2 自注意力机制 (Self-Attention Mechanism)</h3><p>自注意力机制是 Transformer 的核心，它允许模型在处理序列中的每个 Token 时，都能同时考虑到序列中的所有其他 Token，并根据它们之间的相关性分配不同的“注意力权重”。这使得模型能够捕捉到复杂的上下文依赖关系，无论这些依赖关系是近距离的还是远距离的。</p>
<p><strong>工作原理</strong>：<br>对于输入序列中的每个 Token，模型会生成三个向量：</p>
<ul>
<li><strong>查询 (Query) $Q$</strong>：当前 Token 的信息。</li>
<li><strong>键 (Key) $K$</strong>：序列中所有 Token 的信息。</li>
<li><strong>值 (Value) $V$</strong>：序列中所有 Token 的内容信息。</li>
</ul>
<p>通过计算 $Q$ 和 $K$ 的点积，得到<strong>注意力分数</strong>，这个分数表示当前 Token 与序列中其他 Token 的相关性。将这些分数经过 Softmax 函数归一化后，得到<strong>注意力权重</strong>。最后，将注意力权重与 $V$ 相乘并求和，得到当前 Token 的加权表示。</p>
<p><strong>数学公式</strong>：<br>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$<br>其中 $d_k$ 是键向量的维度，用于缩放点积，防止梯度过大。</p>
<p><strong>多头注意力 (Multi-Head Attention)</strong>：<br>为了让模型能从不同角度和不同表示子空间捕获信息，自注意力机制通常会并行运行多个“头”(Head)，每个头学习一组独立的 $Q, K, V$ 投影矩阵。然后，将所有头的输出拼接起来，再经过一个线性变换，得到最终的多头注意力输出。</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[Token 序列的嵌入] --&gt; B[&quot;线性变换 (Q, K, V)&quot;]
    B --&gt; C1[自注意力头 1]
    B --&gt; C2[自注意力头 2]
    B --&gt; ...
    B --&gt; Cn[自注意力头 n]
    C1 --&gt; D[输出向量 1]
    C2 --&gt; D[输出向量 2]
    Cn --&gt; D[输出向量 n]
    D --&gt; E[拼接所有输出向量]
    E --&gt; F[线性变换]
    F --&gt; G[多头注意力输出]
  </pre></div>
<p><strong>解释</strong>：通过自注意力，模型能够识别出句子中不同词语之间的语义联系。例如，在“The <strong>animal</strong> didn’t cross the street because <strong>it</strong> was too tired.”中，模型能通过注意力机制理解“it”指的是“animal”。</p>
<h2 id="三、训练机制：从模式识别到泛化"><a href="#三、训练机制：从模式识别到泛化" class="headerlink" title="三、训练机制：从模式识别到泛化"></a>三、训练机制：从模式识别到泛化</h2><p>LLMs 并非被明确地“编程”来理解语言，而是通过在海量文本数据上进行<strong>无监督预训练</strong>和随后的<strong>有监督微调</strong>来学习语言模式。</p>
<h3 id="3-1-预训练-Pre-training"><a href="#3-1-预训练-Pre-training" class="headerlink" title="3.1 预训练 (Pre-training)"></a>3.1 预训练 (Pre-training)</h3><p>这是 LLMs 学习语言“理解”的关键阶段。模型在数万亿词的文本语料库（如Common Crawl、维基百科、书籍等）上进行训练。</p>
<ul>
<li><strong>目标</strong>：预测序列中的下一个词 (如 GPT 系列) 或恢复被遮盖的词 (如 BERT)。</li>
<li><strong>方法</strong>：通过大规模的“猜词”任务，模型被迫学习语言的语法、语义、事实知识以及世界常识。它会发现词语出现的频率、共现模式、上下文关系等。</li>
<li><strong>结果</strong>：预训练后的模型获得了一个强大的<strong>语言模型头</strong> (Language Model Head)，能够生成语法正确、语义连贯的文本，并对它所见过的语言模式建立了深刻的“直觉”。</li>
</ul>
<h3 id="3-2-微调-Fine-tuning"><a href="#3-2-微调-Fine-tuning" class="headerlink" title="3.2 微调 (Fine-tuning)"></a>3.2 微调 (Fine-tuning)</h3><p>在预训练之后，模型会针对特定的下游任务（如问答、摘要、情感分析、翻译等）进行有监督的微调。</p>
<ul>
<li><strong>目标</strong>：使模型更好地适应特定任务的需求。</li>
<li><strong>方法</strong>：使用较小的、标注过的数据集进行训练，调整模型的参数。</li>
</ul>
<h3 id="3-3-人类反馈强化学习-RLHF-Reinforcement-Learning-from-Human-Feedback"><a href="#3-3-人类反馈强化学习-RLHF-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="3.3 人类反馈强化学习 (RLHF - Reinforcement Learning from Human Feedback)"></a>3.3 人类反馈强化学习 (RLHF - Reinforcement Learning from Human Feedback)</h3><p>GPT-3.5 和 GPT-4 等模型引入了 RLHF，进一步提升了模型的对齐能力。</p>
<ul>
<li><strong>目标</strong>：让模型生成更符合人类偏好、更有帮助、更安全、更无害的回复。</li>
<li><strong>方法</strong>：训练一个奖励模型 (Reward Model)，由人类对模型的多个输出进行排序和评分。然后，使用强化学习算法（如 PPO - Proximal Policy Optimization）来优化语言模型，使其生成能获得更高奖励分数的输出。</li>
</ul>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[&quot;海量文本数据 (互联网、书籍)&quot;] --&gt; B[无监督预训练]
    B --&gt; C[&quot;基础语言能力 (语法、语义、事实)&quot;]
    C --&gt; D[特定任务数据集]
    D --&gt; E[有监督微调]
    E --&gt; F[&quot;任务专用能力 (问答、摘要等)&quot;]
    F --&gt; G[&quot;人类偏好数据 (排名&#x2F;评分)&quot;]
    G --&gt; H[奖励模型训练]
    F --&gt; I[&quot;强化学习 (PPO)&quot;]
    I --&gt; J[与人类意图对齐的模型]
  </pre></div>

<h2 id="四、理解的实现：从统计模式到语义关联"><a href="#四、理解的实现：从统计模式到语义关联" class="headerlink" title="四、理解的实现：从统计模式到语义关联"></a>四、理解的实现：从统计模式到语义关联</h2><p>LLMs 通过上述机制，在统计层面实现了对语言的“理解”，这体现在以下几个方面：</p>
<h3 id="4-1-模式识别与语义关联"><a href="#4-1-模式识别与语义关联" class="headerlink" title="4.1 模式识别与语义关联"></a>4.1 模式识别与语义关联</h3><p>模型通过大量的预训练数据，学习了词语、短语、句子之间的<strong>共现模式</strong>和<strong>统计关联</strong>。当它遇到一个词时，能够预测其后续可能出现的词，或根据上下文推断其含义。这种能力并非基于词典定义，而是基于词语在不同语境中出现的概率分布。</p>
<h3 id="4-2-上下文依赖"><a href="#4-2-上下文依赖" class="headerlink" title="4.2 上下文依赖"></a>4.2 上下文依赖</h3><p>Transformer 的自注意力机制允许模型在处理长文本时，有效地捕捉到相距较远的词语之间的依赖关系。这使得模型能够维持上下文连贯性，并进行更准确的语义推理。例如，在多轮对话中，模型能记住之前的对话内容。</p>
<h3 id="4-3-知识内化"><a href="#4-3-知识内化" class="headerlink" title="4.3 知识内化"></a>4.3 知识内化</h3><p>在预训练过程中，模型隐式地学习并“存储”了大量的<strong>事实知识</strong>和<strong>世界常识</strong>。这些知识不是以结构化的数据库形式存储，而是以神经网络参数的分布式表示形式存在。当被问及相关问题时，模型能够通过其学到的语言模式和知识进行生成。</p>
<h3 id="4-4-泛化与推理"><a href="#4-4-泛化与推理" class="headerlink" title="4.4 泛化与推理"></a>4.4 泛化与推理</h3><p>LLMs 展现出惊人的泛化能力，能够处理和理解训练数据中未曾见过的句子结构和表达方式。它们也能进行一定程度的<strong>链式推理</strong>，通过多步生成来解决复杂问题，这通常通过“思维链”(Chain of Thought) 提示工程来实现。</p>
<h2 id="五、挑战与局限"><a href="#五、挑战与局限" class="headerlink" title="五、挑战与局限"></a>五、挑战与局限</h2><p>尽管 LLMs 取得了巨大成功，但它们的“理解”仍存在显著局限：</p>
<h3 id="5-1-缺乏真正的世界模型与常识"><a href="#5-1-缺乏真正的世界模型与常识" class="headerlink" title="5.1 缺乏真正的世界模型与常识"></a>5.1 缺乏真正的世界模型与常识</h3><p>LLMs 仅通过文本学习，它们没有真实世界的感官体验，因此缺乏人类所拥有的<strong>深层世界模型和常识性推理能力</strong>。它们在面对需要跨领域推理或与物理世界深度交互的问题时，可能会表现出弱点。</p>
<h3 id="5-2-幻觉-Hallucinations"><a href="#5-2-幻觉-Hallucinations" class="headerlink" title="5.2 幻觉 (Hallucinations)"></a>5.2 幻觉 (Hallucinations)</h3><p>模型可能会生成听起来非常合理但实际上是<strong>错误或虚构</strong>的信息。这源于其基于概率预测的本质，有时会“编造”不存在的答案。</p>
<h3 id="5-3-偏见-Bias"><a href="#5-3-偏见-Bias" class="headerlink" title="5.3 偏见 (Bias)"></a>5.3 偏见 (Bias)</h3><p>由于训练数据中包含了人类社会的偏见（如性别、种族偏见），LLMs 可能会学习并<strong>放大这些偏见</strong>，在生成内容时表现出来。</p>
<h3 id="5-4-上下文窗口限制"><a href="#5-4-上下文窗口限制" class="headerlink" title="5.4 上下文窗口限制"></a>5.4 上下文窗口限制</h3><p>尽管 Transformer 擅长处理长距离依赖，但其注意力机制的计算复杂度随着序列长度的增加而呈二次方增长。这导致 LLMs 仍然有<strong>上下文窗口的限制</strong>，无法一次性处理无限长的文本。</p>
<h3 id="5-5-可解释性差"><a href="#5-5-可解释性差" class="headerlink" title="5.5 可解释性差"></a>5.5 可解释性差</h3><p>LLMs 是复杂的深度神经网络，其内部工作机制是高度不透明的**“黑箱”**。我们很难准确解释模型做出特定决策或生成特定文本的原因。</p>
<h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><p>大模型对人类文字的“理解”是一种基于<strong>大规模统计学习</strong>的复杂能力。它通过将文本转化为高维向量，利用 Transformer 架构的自注意力机制捕捉词语间的复杂关联，并在海量数据上通过预训练和微调，内化了语言的语法、语义和事实知识。这种理解是<strong>统计性、表征性</strong>的，而非人类意识层面的认知。</p>
<p>尽管 LLMs 在语言处理上取得了里程碑式的进展，但其缺乏真正的世界模型、存在幻觉和偏见等问题，提醒我们其“理解”与人类的认知存在本质区别。未来的研究将继续探索如何弥补这些差距，使 AI 能够更深层次地与人类世界互动和理解。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/">https://blog.tbf1211.xx.kg/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-31.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/2025-01-18_Metasploit%20%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3/" title="Metasploit 框架详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Metasploit 框架详解</div></div><div class="info-2"><div class="info-item-1"> Metasploit 框架 是一个广为人知且功能强大的开源渗透测试工具。它提供了一个全面的平台，用于开发、测试和执行漏洞利用（exploit）。无论是安全研究人员、渗透测试工程师还是红队成员，Metasploit 都是他们工具箱中不可或缺的一部分。  核心思想：将漏洞利用、载荷生成、后渗透模块等功能模块化，提供统一的接口和工具链，简化复杂的渗透测试流程。   一、Metasploit 简介1.1 什么是 Metasploit？Metasploit 是由 Rapid7 公司维护的一个著名的开源项目。它是一个漏洞利用框架，旨在协助渗透测试人员识别、利用和验证漏洞。它不仅仅是一个简单的漏洞扫描器，更是一个提供多种工具和方法的集成环境，几乎覆盖了渗透测试的整个生命周期。 1.2 Metasploit 的发展历史 2003年：由 H.D. Moore 发起，最初是一个 Perl 语言的项目。 2004年：发布 2.0 版本，首次引入了模块化架构。 2007年：框架被重写，使用 Ruby 语言，提高了灵活性和可维护性。 209年：Rapid7 收购 Metasploit 项目，并继续其开发...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-14_%E5%A5%87%E5%81%B6%E6%A3%80%E9%AA%8C%E8%AF%A6%E8%A7%A3/" title="奇偶检验详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-21.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">奇偶检验详解</div></div><div class="info-2"><div class="info-item-1"> 奇偶检验 (Parity Check) 是一种最简单、最古老的错误检测方法，用于验证数据在传输或存储过程中是否发生了一位或奇数位的错误。它通过在原始数据的基础上添加一个额外的比特位（称为奇偶校验位）来实现。  核心思想： 通过统计数据位中 ‘1’ 的数量是奇数还是偶数，并添加一个校验位来使其总数符合预设的奇偶性，从而在接收端检测数据是否被意外翻转。   一、奇偶检验的基本原理奇偶检验的基本思想是确保一组二进制位中 ‘1’ 的总数（包括校验位）始终是奇数或偶数。 1.1 两种类型根据要求的奇偶性，奇偶检验分为两种：  奇校验 (Odd Parity Check)：  发送方统计数据位中 ‘1’ 的个数。 如果 ‘1’ 的个数为偶数，则奇偶校验位设置为 ‘1’，使包括校验位在内的所有位中 ‘1’ 的总数为奇数。 如果 ‘1’ 的个数为奇数，则奇偶校验位设置为 ‘0’，使包括校验位在内的所有位中 ‘1’ 的总数仍为奇数。 目标：传输的整个数据串（数据位 + 校验位）中 ‘1’ 的个数为奇数。   偶校验 (Even Parity Check)：  发送方统计数据位中 ‘1’ 的个数。...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/2025-01-20_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Token%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%95%B0%E6%8D%AE%E3%80%81%E5%A4%84%E7%90%86%E4%B8%8E%E6%84%8F%E4%B9%89/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-10.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-05</div><div class="info-item-2">Claude Code 详解：Anthropic 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Claude Code 是 Anthropic 公司开发的 Claude 系列大型语言模型在代码理解、生成和辅助方面的一个特定能力集合或优化方向。Anthropic 以其在 AI 安全和可控性方面的研究而闻名，Claude 模型旨在成为一个有益、无害且诚实的 AI 助手。因此，Claude Code 不仅关注代码的正确性，也强调生成代码的安全性、可读性和遵循最佳实践。  核心思想：结合 Anthropic 的安全和伦理原则，提供安全、有益、高质量的代码生成与辅助能力，旨在成为开发者的“无害”智能编程伙伴。   一、Claude Code 的背景与 Anthropic 理念Anthropic 由前 OpenAI 员工创立，致力于开发安全、可控且有益的人工智能系统。其核心产品 Claude 语言模型系列被设计为更易于对齐人类价值观，并通过“宪法 AI (Constitutional AI)”等方法进行训练，减少有害、偏见或不真实内容的生成。 在代码领域，这种理念意味着 Claude Code 不仅仅是生成能运行的代码，更关注：  安全性：避免生成包含已知漏洞或不良安全实践的代码。 ...</div></div></div></a><a class="pagination-related" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-07</div><div class="info-item-2">Codex 详解与使用技巧：OpenAI 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Codex 是由 OpenAI 训练的一个大型语言模型，其核心能力在于理解自然语言并将其转换为代码，或者理解代码并解释其含义。它是 GPT 系列模型的一个特化版本，专门针对编程语言进行了大量训练。Codex 不仅能生成 Python 代码，还能处理多种其他编程语言，是 OpenAI 在人工智能编程领域迈出的重要一步，也是 GitHub Copilot 等工具的基石。  核心思想：将自然语言描述的问题转化为可执行的代码，实现人机协作编程，降低编程门槛，提升开发效率。 掌握有效的指令（Prompt）是充分发挥 Codex 能力的关键。   一、Codex 的起源与核心能力Codex 的开发是基于 OpenAI 的 GPT-3 模型。GPT-3 以其强大的文本生成能力震惊业界，但其在代码生成方面虽然有一定表现，但仍缺乏专业性和精准度。为了弥补这一差距，OpenAI 进一步对 GPT-3 进行了微调，使用了海量的代码数据，最终诞生了 Codex。 1.1 背景：GPT-3 的局限性与代码生成的需求GPT-3 在零样本（zero-shot）和少样本（few-shot）学习方面表现出色，能...</div></div></div></a><a class="pagination-related" href="/2024/2024-08-03_Go%20Context%E8%AF%A6%E8%A7%A3%EF%BC%9A%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6%E4%B8%8E%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E7%9A%84%E5%88%A9%E5%99%A8/" title="Golang context 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-03</div><div class="info-item-2">Golang context 详解</div></div><div class="info-2"><div class="info-item-1"> context 包 是 Go 语言标准库中的一个关键组件，自 Go 1.7 版本引入，它提供了一种在 Goroutine 之间传递请求范围的数据 (request-scoped data)、取消信号 (cancellation signals) 和截止时间 (deadlines) 的标准机制。在构建复杂的并发系统、微服务架构以及处理网络请求链时，context 包是管理 Goroutine 生命周期和避免资源泄露的基石。  核心思想：context.Context 接口允许在 Goroutine 树中安全地传递控制流信息。其核心价值在于实现对计算任务的统一取消、超时控制和值传递，从而提升程序的健壮性和资源利用效率。   一、context 包的必要性在 Go 语言中，Goroutine 是轻量级并发的基础。然而，当应用程序的并发逻辑变得复杂时，以下问题会变得突出：  并发操作的取消：当一个上游操作（如用户取消请求）不再需要其下游的所有并发子任务时，如何有效地通知并停止这些子任务，避免不必要的计算和资源消耗？ 操作超时控制：如何在复杂的请求链中，为整个链条或其中某个环节设置统一的...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-12_Go%E8%AF%AD%E8%A8%80embed%E5%8C%85%E8%AF%A6%E8%A7%A3/" title="Go语言embed包详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-03.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="info-item-2">Go语言embed包详解</div></div><div class="info-2"><div class="info-item-1"> Go 1.16 版本引入了一个内置的 embed包，它提供了一种将文件和文件系统内容直接嵌入到 Go 程序的可执行二进制文件中的简单、高效的方式。这使得开发者可以方便地将网页模板、静态资源（如 HTML、CSS、JavaScript、图片）、配置文件等打包进编译后的程序中，从而创建一个完全自包含 (self-contained) 的应用程序，无需在部署时额外管理静态文件。  核心思想：通过编译时指令将文件内容注入到 Go 程序的数据段，使其在运行时像普通变量一样被访问，实现单文件部署。   一、为什么需要 embed 包？在 embed 包出现之前，Go 程序处理静态资源通常有以下几种方式：  外部文件引用：将静态资源放在程序运行目录的相对路径下。 缺点：部署时需要额外管理这些文件，容易出现文件丢失或路径错误。   go:generate 工具 + bytes 包：利用 go:generate 生成 Go 代码文件，将静态资源转换为 []byte 或 string 变量。 缺点：需要引入额外的第三方工具（如 go-bindata），增加项目复杂度；生成的代码文件通常很大，不便于...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">306</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">199</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">69</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E5%9F%BA%E7%A1%80%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9D%97%EF%BC%9A%E4%BB%8E%E6%96%87%E6%9C%AC%E5%88%B0%E5%90%91%E9%87%8F"><span class="toc-text">一、基础构建模块：从文本到向量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%88%86%E8%AF%8D-Tokenization"><span class="toc-text">1.1 分词 (Tokenization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%AF%8D%E5%B5%8C%E5%85%A5-Word-Embeddings-%E5%90%91%E9%87%8F%E8%A1%A8%E7%A4%BA"><span class="toc-text">1.2 词嵌入 (Word Embeddings) &#x2F; 向量表示</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Transformer-%E6%9E%B6%E6%9E%84%EF%BC%9A%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E7%9A%84%E6%A0%B8%E5%BF%83"><span class="toc-text">二、Transformer 架构：理解语言的核心</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Transformer-%E6%A6%82%E8%A7%88"><span class="toc-text">2.1 Transformer 概览</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-Attention-Mechanism"><span class="toc-text">2.2 自注意力机制 (Self-Attention Mechanism)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%88%B6%EF%BC%9A%E4%BB%8E%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%88%B0%E6%B3%9B%E5%8C%96"><span class="toc-text">三、训练机制：从模式识别到泛化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%A2%84%E8%AE%AD%E7%BB%83-Pre-training"><span class="toc-text">3.1 预训练 (Pre-training)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%BE%AE%E8%B0%83-Fine-tuning"><span class="toc-text">3.2 微调 (Fine-tuning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0-RLHF-Reinforcement-Learning-from-Human-Feedback"><span class="toc-text">3.3 人类反馈强化学习 (RLHF - Reinforcement Learning from Human Feedback)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E7%90%86%E8%A7%A3%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%9A%E4%BB%8E%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%88%B0%E8%AF%AD%E4%B9%89%E5%85%B3%E8%81%94"><span class="toc-text">四、理解的实现：从统计模式到语义关联</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E8%AF%AD%E4%B9%89%E5%85%B3%E8%81%94"><span class="toc-text">4.1 模式识别与语义关联</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BE%9D%E8%B5%96"><span class="toc-text">4.2 上下文依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%9F%A5%E8%AF%86%E5%86%85%E5%8C%96"><span class="toc-text">4.3 知识内化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%B3%9B%E5%8C%96%E4%B8%8E%E6%8E%A8%E7%90%86"><span class="toc-text">4.4 泛化与推理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%8C%91%E6%88%98%E4%B8%8E%E5%B1%80%E9%99%90"><span class="toc-text">五、挑战与局限</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E7%BC%BA%E4%B9%8F%E7%9C%9F%E6%AD%A3%E7%9A%84%E4%B8%96%E7%95%8C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%B8%B8%E8%AF%86"><span class="toc-text">5.1 缺乏真正的世界模型与常识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E5%B9%BB%E8%A7%89-Hallucinations"><span class="toc-text">5.2 幻觉 (Hallucinations)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%81%8F%E8%A7%81-Bias"><span class="toc-text">5.3 偏见 (Bias)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E9%99%90%E5%88%B6"><span class="toc-text">5.4 上下文窗口限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7%E5%B7%AE"><span class="toc-text">5.5 可解释性差</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">六、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-18_%E5%8E%8B%E7%BC%A9%E5%AD%97%E5%85%B8%E6%A0%91%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-23.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"/></a><div class="content"><a class="title" href="/2025/2025-11-18_%E5%8E%8B%E7%BC%A9%E5%AD%97%E5%85%B8%E6%A0%91%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析">压缩字典树 (Radix Trie/Patricia Trie) 深度解析</a><time datetime="2025-11-17T22:24:00.000Z" title="发表于 2025-11-18 06:24:00">2025-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-13_Golang%20%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%E8%AF%A6%E8%A7%A3/" title="Golang 内存对齐详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang 内存对齐详解"/></a><div class="content"><a class="title" href="/2025/2025-11-13_Golang%20%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%E8%AF%A6%E8%A7%A3/" title="Golang 内存对齐详解">Golang 内存对齐详解</a><time datetime="2025-11-12T22:24:00.000Z" title="发表于 2025-11-13 06:24:00">2025-11-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-11_Golang%20%E7%A9%BA%E7%BB%93%E6%9E%84%E4%BD%93%20(struct%7B%7D)%20%E8%AF%A6%E8%A7%A3/" title="Golang 空结构体 (struct{}) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-30.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang 空结构体 (struct{}) 详解"/></a><div class="content"><a class="title" href="/2025/2025-11-11_Golang%20%E7%A9%BA%E7%BB%93%E6%9E%84%E4%BD%93%20(struct%7B%7D)%20%E8%AF%A6%E8%A7%A3/" title="Golang 空结构体 (struct{}) 详解">Golang 空结构体 (struct{}) 详解</a><time datetime="2025-11-10T22:24:00.000Z" title="发表于 2025-11-11 06:24:00">2025-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Codex 详解与使用技巧：OpenAI 的代码智能模型"/></a><div class="content"><a class="title" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型">Codex 详解与使用技巧：OpenAI 的代码智能模型</a><time datetime="2025-11-06T22:24:00.000Z" title="发表于 2025-11-07 06:24:00">2025-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Claude Code 详解：Anthropic 的代码智能模型"/></a><div class="content"><a class="title" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型">Claude Code 详解：Anthropic 的代码智能模型</a><time datetime="2025-11-04T22:24:00.000Z" title="发表于 2025-11-05 06:24:00">2025-11-05</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-31.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2021 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            pagePV.textContent = data.pageviews.value
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors && typeof data.visitors.value !== 'undefined') {
            siteUV.textContent = data.visitors.value
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            sitePV.textContent = data.pageviews.value
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>