<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>大语言模型参数详解：规模、类型与意义 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而">
<meta property="og:type" content="article">
<meta property="og:title" content="大语言模型参数详解：规模、类型与意义">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-13.jpg">
<meta property="article:published_time" content="2025-01-21T22:24:00.000Z">
<meta property="article:modified_time" content="2025-11-21T06:14:45.231Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-13.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "大语言模型参数详解：规模、类型与意义",
  "url": "https://blog.tbf1211.xx.kg/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-13.jpg",
  "datePublished": "2025-01-21T22:24:00.000Z",
  "dateModified": "2025-11-21T06:14:45.231Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '大语言模型参数详解：规模、类型与意义',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">198</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">68</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-13.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">大语言模型参数详解：规模、类型与意义</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">大语言模型参数详解：规模、类型与意义</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-01-21T22:24:00.000Z" title="发表于 2025-01-22 06:24:00">2025-01-22</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>参数 (Parameters)</strong> 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。</strong></p>
</div>
<hr>
<h2 id="一、什么是大语言模型参数？"><a href="#一、什么是大语言模型参数？" class="headerlink" title="一、什么是大语言模型参数？"></a>一、什么是大语言模型参数？</h2><p>在神经网络的上下文中，<strong>参数</strong>是指模型在训练过程中需要学习和调整的所有<strong>权重 (weights)</strong> 和<strong>偏置 (biases)</strong>。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。</p>
<ul>
<li><strong>权重 (Weights)</strong>：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。</li>
<li><strong>偏置 (Biases)</strong>：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调整神经元的激活阈值，使得模型可以更好地拟合数据。</li>
</ul>
<p>LLMs 的参数通常是浮点数，存储在模型的各个层中，如：</p>
<ul>
<li><strong>词嵌入层 (Embedding Layer)</strong>：存储每个 Token 的嵌入向量。</li>
<li><strong>注意力机制 (Attention Mechanism)</strong>：包含用于计算 Query、Key、Value 矩阵的权重。</li>
<li><strong>前馈网络 (Feed-Forward Networks)</strong>：包含线性变换层的权重和偏置。</li>
<li><strong>层归一化 (Layer Normalization)</strong>：包含可学习的增益 (gain) 和偏置 (bias) 参数。</li>
</ul>
<p><strong>数学表示</strong>：<br>在一个简单的线性层中，输出 $y$ 可以通过输入 $x$、权重矩阵 $W$ 和偏置向量 $b$ 表示：<br>$$<br>y &#x3D; xW + b<br>$$<br>其中 $W$ 和 $b$ 就是这一层的参数。LLMs 由成百上千个这样的层堆叠而成，每个层都有其自己的 $W$ 和 $b$ 等参数。</p>
<h2 id="二、参数规模-Parameter-Scale"><a href="#二、参数规模-Parameter-Scale" class="headerlink" title="二、参数规模 (Parameter Scale)"></a>二、参数规模 (Parameter Scale)</h2><p>大语言模型的“大”字，很大程度上指的是其参数规模。从最初的几亿到现在的数万亿，参数数量的增长是推动 LLMs 能力飞跃的关键因素。</p>
<h3 id="2-1-参数规模的发展历程"><a href="#2-1-参数规模的发展历程" class="headerlink" title="2.1 参数规模的发展历程"></a>2.1 参数规模的发展历程</h3><table>
<thead>
<tr>
<th align="left">模型</th>
<th align="left">参数量</th>
<th align="left">发布年份</th>
</tr>
</thead>
<tbody><tr>
<td align="left">ELMo</td>
<td align="left">93M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">BERT (base)</td>
<td align="left">110M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">BERT (large)</td>
<td align="left">340M</td>
<td align="left">2018</td>
</tr>
<tr>
<td align="left">GPT-2</td>
<td align="left">1.5B</td>
<td align="left">2019</td>
</tr>
<tr>
<td align="left">GPT-3</td>
<td align="left">175B</td>
<td align="left">2020</td>
</tr>
<tr>
<td align="left">Gopher</td>
<td align="left">280B</td>
<td align="left">2021</td>
</tr>
<tr>
<td align="left">Chinchilla</td>
<td align="left">70B</td>
<td align="left">2022</td>
</tr>
<tr>
<td align="left">LLaMA 2</td>
<td align="left">7B, 13B, 70B</td>
<td align="left">2023</td>
</tr>
<tr>
<td align="left">GPT-4</td>
<td align="left">估计万亿级（未公开）</td>
<td align="left">2023</td>
</tr>
</tbody></table>
<p><strong>M</strong> &#x3D; Million (百万), <strong>B</strong> &#x3D; Billion (十亿), <strong>T</strong> &#x3D; Trillion (万亿)</p>
<h3 id="2-2-参数规模的重要性"><a href="#2-2-参数规模的重要性" class="headerlink" title="2.2 参数规模的重要性"></a>2.2 参数规模的重要性</h3><ol>
<li><strong>能力涌现 (Emergent Abilities)</strong>：研究表明，当模型的参数规模达到一定阈值时，会涌现出新的能力，例如上下文学习 (In-context Learning)、多步推理 (Multi-step Reasoning) 等，这些能力在小规模模型中并不明显或根本不存在。</li>
<li><strong>知识存储容量</strong>：更多的参数意味着模型拥有更大的容量来存储在训练数据中学习到的语法、语义、事实知识和世界常识。</li>
<li><strong>泛化能力</strong>：参数越多的模型，理论上能够捕捉更复杂的数据模式，从而具有更强的泛化能力，能够更好地处理未见过的数据。</li>
<li><strong>计算资源需求</strong>：参数数量的增加直接导致训练和推理所需的计算资源（GPU、内存）呈指数级增长。</li>
</ol>
<h2 id="三、参数的类型与分布"><a href="#三、参数的类型与分布" class="headerlink" title="三、参数的类型与分布"></a>三、参数的类型与分布</h2><p>LLMs 的参数主要分布在 Transformer 架构的各个模块中。</p>
<h3 id="3-1-词嵌入层参数-Embedding-Layer-Parameters"><a href="#3-1-词嵌入层参数-Embedding-Layer-Parameters" class="headerlink" title="3.1 词嵌入层参数 (Embedding Layer Parameters)"></a>3.1 词嵌入层参数 (Embedding Layer Parameters)</h3><p>这部分参数存储了模型词汇表中每个 Token 的<strong>嵌入向量 (Embedding Vectors)</strong>。</p>
<ul>
<li><strong>计算方式</strong>：词汇表大小 $V \times$ 嵌入维度 $d_{model}$。</li>
<li><strong>示例</strong>：如果词汇表有 50,000 个 Token，嵌入维度是 768，则嵌入层有 $50,000 \times 768 \approx 3.8 \times 10^7$ (3800 万) 个参数。</li>
<li><strong>意义</strong>：这些参数是模型理解每个 Token 语义的基础。</li>
</ul>
<h3 id="3-2-Transformer-编码器-解码器层参数-Transformer-Layer-Parameters"><a href="#3-2-Transformer-编码器-解码器层参数-Transformer-Layer-Parameters" class="headerlink" title="3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)"></a>3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)</h3><p>每个 Transformer 层都包含多个子层，每个子层都有自己的参数。LLMs 通常由 $N$ 个这样的层堆叠而成。</p>
<ol>
<li><p><strong>多头自注意力机制 (Multi-Head Self-Attention)</strong>：</p>
<ul>
<li><strong>查询 (Query) $Q$、键 (Key) $K$、值 (Value) $V$ 的投影矩阵</strong>：每个头有三个线性层，将输入投影到 $Q, K, V$ 空间。如果有多头，这些层会独立存在或共享。</li>
<li><strong>输出投影矩阵</strong>：将所有头的输出拼接后，再经过一个线性层投影回原始维度。</li>
<li><strong>计算方式</strong>：主要参数量来自这些线性层，通常是 $4 \times d_{model} \times d_{model}$ 左右（考虑 $Q, K, V$ 投影和输出投影）。</li>
<li><strong>意义</strong>：这些参数让模型能够计算 Token 之间的注意力权重，捕捉上下文依赖关系。</li>
</ul>
</li>
<li><p><strong>前馈网络 (Feed-Forward Network, FFN)</strong>：</p>
<ul>
<li>每个 FFN 通常包含两个线性层，中间夹着一个非线性激活函数。</li>
<li><strong>计算方式</strong>：通常是 $d_{model} \times d_{ff} + d_{ff} \times d_{model}$，其中 $d_{ff}$ 通常是 $d_{model}$ 的 4 倍。因此，大约是 $4 \times d_{model}^2 + 4 \times d_{model}^2 &#x3D; 8 \times d_{model}^2$ 左右。</li>
<li><strong>意义</strong>：FFN 负责在每个 Token 的表示上进行更深层次的非线性变换和特征提取。</li>
</ul>
</li>
<li><p><strong>层归一化 (Layer Normalization)</strong>：</p>
<ul>
<li>每个归一化层有两个可学习参数：增益 (gain) $\gamma$ 和偏置 (bias) $\beta$，均为 $d_{model}$ 维度。</li>
<li><strong>计算方式</strong>：$2 \times d_{model}$。</li>
<li><strong>意义</strong>：稳定训练过程。</li>
</ul>
</li>
</ol>
<p><strong>总参数量粗略估算</strong>：<br>对于一个 Transformer 模型，总参数量大致可以估算为：<br>$$<br>\text{Total Parameters} \approx V \times d_{model} + N \times ( \text{Attention Params} + \text{FFN Params} + \text{Norm Params})<br>$$<br>其中 $V$ 是词汇表大小，$d_{model}$ 是模型维度，$N$ 是层数。<br>可以看出，大部分参数量集中在 FFN 和注意力机制中，并且随着层数 $N$ 和模型维度 $d_{model}$ 的增加，参数量呈二次方增长。</p>
<h2 id="四、参数训练与优化"><a href="#四、参数训练与优化" class="headerlink" title="四、参数训练与优化"></a>四、参数训练与优化</h2><p>LLMs 的训练是一个极其计算密集的过程，涉及到数万亿次浮点运算。</p>
<h3 id="4-1-预训练-Pre-training"><a href="#4-1-预训练-Pre-training" class="headerlink" title="4.1 预训练 (Pre-training)"></a>4.1 预训练 (Pre-training)</h3><ul>
<li><strong>目标</strong>：在海量无标注文本数据上学习语言的通用模式、语法、语义和世界知识。</li>
<li><strong>方法</strong>：通常采用<strong>自监督学习</strong>任务，如<strong>下一个词预测 (Next Token Prediction)</strong>（如 GPT 系列）或<strong>掩码语言建模 (Masked Language Modeling)</strong>（如 BERT 系列）。</li>
<li><strong>优化器</strong>：使用像 AdamW 这样的优化器，通过反向传播 (Backpropagation) 和梯度下降 (Gradient Descent) 算法，迭代地调整所有参数，以最小化预测误差。</li>
</ul>
<h3 id="4-2-微调-Fine-tuning"><a href="#4-2-微调-Fine-tuning" class="headerlink" title="4.2 微调 (Fine-tuning)"></a>4.2 微调 (Fine-tuning)</h3><ul>
<li><strong>目标</strong>：在特定任务的有标注数据集上进一步训练模型，使其适应特定应用。</li>
<li><strong>方法</strong>：通常是全参数微调，或者采用高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 方法，如 LoRA (Low-Rank Adaptation)。LoRA 只训练少量额外的低秩矩阵参数，大幅减少计算和存储开销。</li>
</ul>
<h3 id="4-3-知识蒸馏-Knowledge-Distillation"><a href="#4-3-知识蒸馏-Knowledge-Distillation" class="headerlink" title="4.3 知识蒸馏 (Knowledge Distillation)"></a>4.3 知识蒸馏 (Knowledge Distillation)</h3><ul>
<li><strong>目标</strong>：将一个大型模型的知识转移到一个小型模型中，以创建更高效、更易部署的模型。</li>
<li><strong>方法</strong>：小型模型（学生模型）在大型模型（教师模型）的指导下进行训练，尝试模仿教师模型的输出。</li>
</ul>
<h2 id="五、Go-语言中参数的概念性表示"><a href="#五、Go-语言中参数的概念性表示" class="headerlink" title="五、Go 语言中参数的概念性表示"></a>五、Go 语言中参数的概念性表示</h2><p>在 Go 语言中，我们通常不会直接操作数万亿个参数。但我们可以用结构体来概念性地表示一个简单线性层的权重和偏置。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;gonum.org/v1/gonum/mat&quot;</span> <span class="comment">// 使用gonum库进行矩阵运算</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// LinearLayerParams 结构体表示一个线性层的参数</span></span><br><span class="line"><span class="keyword">type</span> LinearLayerParams <span class="keyword">struct</span> &#123;</span><br><span class="line">	Weights *mat.Dense <span class="comment">// 权重矩阵</span></span><br><span class="line">	Biases  *mat.Dense <span class="comment">// 偏置向量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// NewLinearLayerParams 创建并初始化一个线性层的参数</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewLinearLayerParams</span><span class="params">(inputDim, outputDim <span class="type">int</span>)</span></span> *LinearLayerParams &#123;</span><br><span class="line">	<span class="comment">// 随机初始化权重矩阵 (通常用更复杂的初始化策略，这里简化)</span></span><br><span class="line">	weights := mat.NewDense(inputDim, outputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="comment">// 初始化为全零或小的随机数</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; inputDim; i++ &#123;</span><br><span class="line">		<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; outputDim; j++ &#123;</span><br><span class="line">			weights.Set(i, j, <span class="type">float64</span>(i*outputDim+j)/<span class="type">float64</span>(inputDim*outputDim*<span class="number">100</span>)) <span class="comment">// 示例值</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 初始化偏置向量</span></span><br><span class="line">	biases := mat.NewDense(<span class="number">1</span>, outputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="comment">// 初始化为全零或小的随机数</span></span><br><span class="line">	<span class="keyword">for</span> j := <span class="number">0</span>; j &lt; outputDim; j++ &#123;</span><br><span class="line">		biases.Set(<span class="number">0</span>, j, <span class="type">float64</span>(j)/<span class="type">float64</span>(outputDim*<span class="number">100</span>)) <span class="comment">// 示例值</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">return</span> &amp;LinearLayerParams&#123;</span><br><span class="line">		Weights: weights,</span><br><span class="line">		Biases:  biases,</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Forward 实现线性层的前向传播</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *LinearLayerParams)</span></span> Forward(input *mat.Dense) *mat.Dense &#123;</span><br><span class="line">	<span class="keyword">var</span> output mat.Dense</span><br><span class="line">	<span class="comment">// 计算 input * Weights</span></span><br><span class="line">	output.Mul(input, l.Weights)</span><br><span class="line">	<span class="comment">// 加上偏置 (偏置会广播到每一行)</span></span><br><span class="line">	output.Add(&amp;output, l.Biases)</span><br><span class="line">	<span class="keyword">return</span> &amp;output</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CountParameters 计算线性层的参数总量</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(l *LinearLayerParams)</span></span> CountParameters() <span class="type">int</span> &#123;</span><br><span class="line">	_, colsW := l.Weights.Dims()</span><br><span class="line">	rowsW, _ := l.Weights.Dims()</span><br><span class="line">	_, colsB := l.Biases.Dims()</span><br><span class="line">	<span class="keyword">return</span> rowsW*colsW + colsB <span class="comment">// 权重数量 + 偏置数量</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	inputDim := <span class="number">768</span>  <span class="comment">// 输入特征维度 (例如：Token Embedding 维度)</span></span><br><span class="line">	outputDim := <span class="number">3072</span> <span class="comment">// 输出特征维度 (例如：FFN 的中间维度)</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 创建一个线性层</span></span><br><span class="line">	layer := NewLinearLayerParams(inputDim, outputDim)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 打印参数数量</span></span><br><span class="line">	fmt.Printf(<span class="string">&quot;Linear Layer Parameters: %d\n&quot;</span>, layer.CountParameters()) <span class="comment">// 768*3072 + 3072 = 2359296 + 3072 = 2362368</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 模拟输入</span></span><br><span class="line">	inputVector := mat.NewDense(<span class="number">1</span>, inputDim, <span class="literal">nil</span>)</span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; inputDim; i++ &#123;</span><br><span class="line">		inputVector.Set(<span class="number">0</span>, i, <span class="type">float64</span>(i)/<span class="type">float64</span>(inputDim))</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 执行前向传播</span></span><br><span class="line">	outputVector := layer.Forward(inputVector)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Input shape: %v\n&quot;</span>, inputVector.Dims())</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Output shape: %v\n&quot;</span>, outputVector.Dims())</span><br><span class="line">	<span class="comment">// fmt.Printf(&quot;Output (first 5 values): %v\n&quot;, outputVector.Slice(0, 1, 0, 5)) // 打印部分输出</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 概念性地表示一个拥有多个层的简化LLM</span></span><br><span class="line">	<span class="comment">// 在一个真实的LLM中，会有几十到几百个这样的层</span></span><br><span class="line">	fmt.Println(<span class="string">&quot;\n--- Conceptual LLM Parameter Counting ---&quot;</span>)</span><br><span class="line">	totalLLMParams := <span class="number">0</span></span><br><span class="line">	numLayers := <span class="number">12</span> <span class="comment">// 假设有12个Transformer层</span></span><br><span class="line">	attentionParamsPerLayer := inputDim*inputDim*<span class="number">4</span> <span class="comment">// 粗略估算注意力头的参数</span></span><br><span class="line">	ffnParamsPerLayer := inputDim*outputDim*<span class="number">2</span> + outputDim*inputDim*<span class="number">2</span> <span class="comment">// 粗略估算FFN的参数</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 嵌入层</span></span><br><span class="line">	vocabSize := <span class="number">50000</span></span><br><span class="line">	embeddingParams := vocabSize * inputDim</span><br><span class="line">	totalLLMParams += embeddingParams</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Embedding Layer Parameters: %d\n&quot;</span>, embeddingParams)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 每层参数</span></span><br><span class="line">	<span class="keyword">for</span> i := <span class="number">0</span>; i &lt; numLayers; i++ &#123;</span><br><span class="line">		layerParams := attentionParamsPerLayer + ffnParamsPerLayer + <span class="number">2</span>*inputDim <span class="comment">// 2*inputDim for LayerNorm</span></span><br><span class="line">		totalLLMParams += layerParams</span><br><span class="line">		<span class="comment">// fmt.Printf(&quot;Layer %d Parameters: %d\n&quot;, i+1, layerParams)</span></span><br><span class="line">	&#125;</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Total Estimated LLM Parameters (Simplified): %d\n&quot;</span>, totalLLMParams)</span><br><span class="line">	<span class="comment">// 注意：这是一个非常粗略的估算，真实LLM的参数计算远比这复杂，涉及到更详细的网络结构</span></span><br><span class="line">	<span class="comment">// 但这展示了参数如何累积，并说明了为什么LLMs的参数数量如此庞大</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>解释</strong>：上述 Go 代码使用了 <code>gonum/mat</code> 库来概念性地展示一个线性层（神经网络的基本构建块）如何包含权重和偏置。<code>CountParameters</code> 函数演示了如何计算这些参数的数量。在 <code>main</code> 函数中，进一步展示了一个<strong>高度简化和粗略估算</strong>的 LLM 参数计算，说明了即使是几个基本组件和少许层，参数量也能迅速累积到数百万甚至数十亿。这仅仅是为了概念性说明，真实的 LLM 参数计算要复杂得多，涉及到精确的网络拓扑和每个子模块的详细尺寸。</p>
<h2 id="六、挑战与未来方向"><a href="#六、挑战与未来方向" class="headerlink" title="六、挑战与未来方向"></a>六、挑战与未来方向</h2><ol>
<li><strong>计算与内存需求</strong>：训练和部署超大规模模型需要巨大的计算资源（数千甚至上万块 GPU）和内存（数百 GB 甚至数 TB）。</li>
<li><strong>训练稳定性</strong>：参数规模越大，训练越容易不稳定，需要更复杂的优化策略和技术。</li>
<li><strong>碳足迹</strong>：大型模型的训练消耗大量能源，对环境产生影响。</li>
<li><strong>模型压缩</strong>：研究人员正在探索参数高效微调 (PEFT)、量化 (Quantization)、剪枝 (Pruning) 等技术，以减小模型体积，降低部署成本，同时保持性能。</li>
<li><strong>模型架构优化</strong>：开发更高效的模型架构，以在相同参数量下实现更好的性能，或用更少的参数达到同等性能。</li>
</ol>
<h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>大语言模型的参数是其“大脑”和“知识库”的具象化。这些通过大规模训练学习到的数值权重和偏置，赋予了模型理解、生成和推理人类语言的能力。参数规模的增长是 LLMs 发展历程中的核心驱动力，带来了前所未有的能力涌现，但同时也带来了巨大的计算挑战。深入理解参数的结构、类型和优化过程，是理解 LLMs 本质及其未来发展方向的关键。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/">https://blog.tbf1211.xx.kg/2025/2025-01-22_%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%EF%BC%9A%E8%A7%84%E6%A8%A1%E3%80%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%84%8F%E4%B9%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-13.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/2025-01-24_Golang%20Gin%20%E6%A1%86%E6%9E%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" title="Golang Gin 框架深度解析"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Golang Gin 框架深度解析</div></div><div class="info-2"><div class="info-item-1"> Gin 是一个用 Go 语言编写的 HTTP Web 框架，它以高性能和易用性著称。Gin 框架通过一个类似 Martini 的 API，但拥有显著更高的性能，这得益于其底层优化的路由引擎 httprouter。它非常适合构建 RESTful API 服务、微服务和高并发的 Web 应用程序。  核心思想：Gin 通过一个轻量级的路由引擎和可插拔的中间件机制，提供了一个快速、灵活且强大的 Web 开发骨架，将请求处理分解为一系列可管理的阶段。   一、为什么选择 Gin？在 Go 语言的 Web 框架中，Gin 凭借以下优势脱颖而出：  极高性能：Gin 宣称其性能比其他 Go 框架（如 net/http 原生路由器、Martini 等）高出 40 倍，因为它使用了优化的 httprouter 库，并且避免了反射。 易于使用：简洁的 API 设计使得学习曲线平缓，开发者可以快速上手并构建应用。 中间件支持：强大的中间件机制允许开发者在请求处理流程中插入自定义逻辑，如日志记录、认证、错误恢复等，实现代码复用和模块化。 路由灵活：支持丰富的路由定义，包括参数路由、通配符路由和路由组...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-20_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Token%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%95%B0%E6%8D%AE%E3%80%81%E5%A4%84%E7%90%86%E4%B8%8E%E6%84%8F%E4%B9%89/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-20.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/2025-01-16_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BA%BA%E7%B1%BB%E6%96%87%E5%AD%97%EF%BC%9A%E4%BB%8EToken%E5%88%B0%E8%AF%AD%E4%B9%89%E8%A1%A8%E5%BE%81/" title="大型语言模型如何理解人类文字：从Token到语义表征"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-16</div><div class="info-item-2">大型语言模型如何理解人类文字：从Token到语义表征</div></div><div class="info-2"><div class="info-item-1"> 大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。   一、基础构建模块：从文本到向量LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于分词 (Tokenization) 和词嵌入 (Word Embeddings)。 1.1 分词 (Tokenization)分词是将连续的文本序列切分成有意义的最小单位——Token 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。  词级别分词 (Word-level Tokenization)：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-20_%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84Token%E8%AF%A6%E8%A7%A3%EF%BC%9A%E6%95%B0%E6%8D%AE%E3%80%81%E5%A4%84%E7%90%86%E4%B8%8E%E6%84%8F%E4%B9%89/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-20.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-05</div><div class="info-item-2">Claude Code 详解：Anthropic 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Claude Code 是 Anthropic 公司开发的 Claude 系列大型语言模型在代码理解、生成和辅助方面的一个特定能力集合或优化方向。Anthropic 以其在 AI 安全和可控性方面的研究而闻名，Claude 模型旨在成为一个有益、无害且诚实的 AI 助手。因此，Claude Code 不仅关注代码的正确性，也强调生成代码的安全性、可读性和遵循最佳实践。  核心思想：结合 Anthropic 的安全和伦理原则，提供安全、有益、高质量的代码生成与辅助能力，旨在成为开发者的“无害”智能编程伙伴。   一、Claude Code 的背景与 Anthropic 理念Anthropic 由前 OpenAI 员工创立，致力于开发安全、可控且有益的人工智能系统。其核心产品 Claude 语言模型系列被设计为更易于对齐人类价值观，并通过“宪法 AI (Constitutional AI)”等方法进行训练，减少有害、偏见或不真实内容的生成。 在代码领域，这种理念意味着 Claude Code 不仅仅是生成能运行的代码，更关注：  安全性：避免生成包含已知漏洞或不良安全实践的代码。 ...</div></div></div></a><a class="pagination-related" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-18.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-07</div><div class="info-item-2">Codex 详解与使用技巧：OpenAI 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Codex 是由 OpenAI 训练的一个大型语言模型，其核心能力在于理解自然语言并将其转换为代码，或者理解代码并解释其含义。它是 GPT 系列模型的一个特化版本，专门针对编程语言进行了大量训练。Codex 不仅能生成 Python 代码，还能处理多种其他编程语言，是 OpenAI 在人工智能编程领域迈出的重要一步，也是 GitHub Copilot 等工具的基石。  核心思想：将自然语言描述的问题转化为可执行的代码，实现人机协作编程，降低编程门槛，提升开发效率。 掌握有效的指令（Prompt）是充分发挥 Codex 能力的关键。   一、Codex 的起源与核心能力Codex 的开发是基于 OpenAI 的 GPT-3 模型。GPT-3 以其强大的文本生成能力震惊业界，但其在代码生成方面虽然有一定表现，但仍缺乏专业性和精准度。为了弥补这一差距，OpenAI 进一步对 GPT-3 进行了微调，使用了海量的代码数据，最终诞生了 Codex。 1.1 背景：GPT-3 的局限性与代码生成的需求GPT-3 在零样本（zero-shot）和少样本（few-shot）学习方面表现出色，能...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-12_Go%E8%AF%AD%E8%A8%80embed%E5%8C%85%E8%AF%A6%E8%A7%A3/" title="Go语言embed包详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-12</div><div class="info-item-2">Go语言embed包详解</div></div><div class="info-2"><div class="info-item-1"> Go 1.16 版本引入了一个内置的 embed包，它提供了一种将文件和文件系统内容直接嵌入到 Go 程序的可执行二进制文件中的简单、高效的方式。这使得开发者可以方便地将网页模板、静态资源（如 HTML、CSS、JavaScript、图片）、配置文件等打包进编译后的程序中，从而创建一个完全自包含 (self-contained) 的应用程序，无需在部署时额外管理静态文件。  核心思想：通过编译时指令将文件内容注入到 Go 程序的数据段，使其在运行时像普通变量一样被访问，实现单文件部署。   一、为什么需要 embed 包？在 embed 包出现之前，Go 程序处理静态资源通常有以下几种方式：  外部文件引用：将静态资源放在程序运行目录的相对路径下。 缺点：部署时需要额外管理这些文件，容易出现文件丢失或路径错误。   go:generate 工具 + bytes 包：利用 go:generate 生成 Go 代码文件，将静态资源转换为 []byte 或 string 变量。 缺点：需要引入额外的第三方工具（如 go-bindata），增加项目复杂度；生成的代码文件通常很大，不便于...</div></div></div></a><a class="pagination-related" href="/2025/2025-01-14_%E5%A5%87%E5%81%B6%E6%A3%80%E9%AA%8C%E8%AF%A6%E8%A7%A3/" title="奇偶检验详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-18.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-14</div><div class="info-item-2">奇偶检验详解</div></div><div class="info-2"><div class="info-item-1"> 奇偶检验 (Parity Check) 是一种最简单、最古老的错误检测方法，用于验证数据在传输或存储过程中是否发生了一位或奇数位的错误。它通过在原始数据的基础上添加一个额外的比特位（称为奇偶校验位）来实现。  核心思想： 通过统计数据位中 ‘1’ 的数量是奇数还是偶数，并添加一个校验位来使其总数符合预设的奇偶性，从而在接收端检测数据是否被意外翻转。   一、奇偶检验的基本原理奇偶检验的基本思想是确保一组二进制位中 ‘1’ 的总数（包括校验位）始终是奇数或偶数。 1.1 两种类型根据要求的奇偶性，奇偶检验分为两种：  奇校验 (Odd Parity Check)：  发送方统计数据位中 ‘1’ 的个数。 如果 ‘1’ 的个数为偶数，则奇偶校验位设置为 ‘1’，使包括校验位在内的所有位中 ‘1’ 的总数为奇数。 如果 ‘1’ 的个数为奇数，则奇偶校验位设置为 ‘0’，使包括校验位在内的所有位中 ‘1’ 的总数仍为奇数。 目标：传输的整个数据串（数据位 + 校验位）中 ‘1’ 的个数为奇数。   偶校验 (Even Parity Check)：  发送方统计数据位中 ‘1’ 的个数。...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">304</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">198</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">68</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%9F"><span class="toc-text">一、什么是大语言模型参数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1-Parameter-Scale"><span class="toc-text">二、参数规模 (Parameter Scale)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B"><span class="toc-text">2.1 参数规模的发展历程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%8F%82%E6%95%B0%E8%A7%84%E6%A8%A1%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">2.2 参数规模的重要性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8F%82%E6%95%B0%E7%9A%84%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%88%86%E5%B8%83"><span class="toc-text">三、参数的类型与分布</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E8%AF%8D%E5%B5%8C%E5%85%A5%E5%B1%82%E5%8F%82%E6%95%B0-Embedding-Layer-Parameters"><span class="toc-text">3.1 词嵌入层参数 (Embedding Layer Parameters)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Transformer-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82%E5%8F%82%E6%95%B0-Transformer-Layer-Parameters"><span class="toc-text">3.2 Transformer 编码器&#x2F;解码器层参数 (Transformer Layer Parameters)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%8F%82%E6%95%B0%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-text">四、参数训练与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%A2%84%E8%AE%AD%E7%BB%83-Pre-training"><span class="toc-text">4.1 预训练 (Pre-training)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%BE%AE%E8%B0%83-Fine-tuning"><span class="toc-text">4.2 微调 (Fine-tuning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F-Knowledge-Distillation"><span class="toc-text">4.3 知识蒸馏 (Knowledge Distillation)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81Go-%E8%AF%AD%E8%A8%80%E4%B8%AD%E5%8F%82%E6%95%B0%E7%9A%84%E6%A6%82%E5%BF%B5%E6%80%A7%E8%A1%A8%E7%A4%BA"><span class="toc-text">五、Go 语言中参数的概念性表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E6%8C%91%E6%88%98%E4%B8%8E%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="toc-text">六、挑战与未来方向</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">七、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-18_%E5%8E%8B%E7%BC%A9%E5%AD%97%E5%85%B8%E6%A0%91%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="压缩字典树 (Radix Trie/Patricia Trie) 深度解析"/></a><div class="content"><a class="title" href="/2025/2025-11-18_%E5%8E%8B%E7%BC%A9%E5%AD%97%E5%85%B8%E6%A0%91%20%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" title="压缩字典树 (Radix Trie/Patricia Trie) 深度解析">压缩字典树 (Radix Trie/Patricia Trie) 深度解析</a><time datetime="2025-11-17T22:24:00.000Z" title="发表于 2025-11-18 06:24:00">2025-11-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-13_Golang%20%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%E8%AF%A6%E8%A7%A3/" title="Golang 内存对齐详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-16.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang 内存对齐详解"/></a><div class="content"><a class="title" href="/2025/2025-11-13_Golang%20%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90%E8%AF%A6%E8%A7%A3/" title="Golang 内存对齐详解">Golang 内存对齐详解</a><time datetime="2025-11-12T22:24:00.000Z" title="发表于 2025-11-13 06:24:00">2025-11-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-11_Golang%20%E7%A9%BA%E7%BB%93%E6%9E%84%E4%BD%93%20(struct%7B%7D)%20%E8%AF%A6%E8%A7%A3/" title="Golang 空结构体 (struct{}) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang 空结构体 (struct{}) 详解"/></a><div class="content"><a class="title" href="/2025/2025-11-11_Golang%20%E7%A9%BA%E7%BB%93%E6%9E%84%E4%BD%93%20(struct%7B%7D)%20%E8%AF%A6%E8%A7%A3/" title="Golang 空结构体 (struct{}) 详解">Golang 空结构体 (struct{}) 详解</a><time datetime="2025-11-10T22:24:00.000Z" title="发表于 2025-11-11 06:24:00">2025-11-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-18.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Codex 详解与使用技巧：OpenAI 的代码智能模型"/></a><div class="content"><a class="title" href="/2025/2025-11-07_Codex%20%E8%AF%A6%E8%A7%A3%E4%B8%8E%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%EF%BC%9AOpenAI%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型">Codex 详解与使用技巧：OpenAI 的代码智能模型</a><time datetime="2025-11-06T22:24:00.000Z" title="发表于 2025-11-07 06:24:00">2025-11-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Claude Code 详解：Anthropic 的代码智能模型"/></a><div class="content"><a class="title" href="/2025/2025-11-05_Claude%20Code%20%E8%AF%A6%E8%A7%A3%EF%BC%9AAnthropic%20%E7%9A%84%E4%BB%A3%E7%A0%81%E6%99%BA%E8%83%BD%E6%A8%A1%E5%9E%8B/" title="Claude Code 详解：Anthropic 的代码智能模型">Claude Code 详解：Anthropic 的代码智能模型</a><time datetime="2025-11-04T22:24:00.000Z" title="发表于 2025-11-05 06:24:00">2025-11-05</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-13.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2021 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.2"></script><script src="/js/main.js?v=5.5.2"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.4/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.1/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            pagePV.textContent = data.pageviews.value
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors && typeof data.visitors.value !== 'undefined') {
            siteUV.textContent = data.visitors.value
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews && typeof data.pageviews.value !== 'undefined') {
            sitePV.textContent = data.pageviews.value
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.2"></script></div></div></body></html>