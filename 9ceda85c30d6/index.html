<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>向量嵌入 (Vector Embeddings) 详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="向量嵌入 (Vector Embeddings) 是人工智能和机器学习领域的一个核心概念，它指的是将复杂的数据对象（如文本、图像、音频、图形节点、用户行为等）映射到高维实数向量空间中的一种技术。在这个向量空间中，语义或功能上相似的数据对象会映射到彼此接近的向量点。  通过向量嵌入，我们可以将非结构化数据转化为机器可理解和处理的数值形式，并且能够通过计算向量之间的距离来量化数据对象之间的相似性。它">
<meta property="og:type" content="article">
<meta property="og:title" content="向量嵌入 (Vector Embeddings) 详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/9ceda85c30d6/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="向量嵌入 (Vector Embeddings) 是人工智能和机器学习领域的一个核心概念，它指的是将复杂的数据对象（如文本、图像、音频、图形节点、用户行为等）映射到高维实数向量空间中的一种技术。在这个向量空间中，语义或功能上相似的数据对象会映射到彼此接近的向量点。  通过向量嵌入，我们可以将非结构化数据转化为机器可理解和处理的数值形式，并且能够通过计算向量之间的距离来量化数据对象之间的相似性。它">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg">
<meta property="article:published_time" content="2025-05-04T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-20T13:04:06.836Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "向量嵌入 (Vector Embeddings) 详解",
  "url": "https://blog.tbf1211.xx.kg/9ceda85c30d6/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-03.jpg",
  "datePublished": "2025-05-04T22:24:00.000Z",
  "dateModified": "2025-12-20T13:04:06.836Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/9ceda85c30d6/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '向量嵌入 (Vector Embeddings) 详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">411</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">217</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-03.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">向量嵌入 (Vector Embeddings) 详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">向量嵌入 (Vector Embeddings) 详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-05-04T22:24:00.000Z" title="发表于 2025-05-05 06:24:00">2025-05-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/9ceda85c30d6/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>向量嵌入 (Vector Embeddings)</strong> 是人工智能和机器学习领域的一个核心概念，它指的是将复杂的数据对象（如文本、图像、音频、图形节点、用户行为等）映射到<strong>高维实数向量空间</strong>中的一种技术。在这个向量空间中，语义或功能上相似的数据对象会映射到彼此接近的向量点。</p>
</blockquote>
<div class="note info flat"><p>通过向量嵌入，我们可以将非结构化数据转化为机器可理解和处理的数值形式，并且能够通过计算向量之间的距离来量化数据对象之间的相似性。它是许多现代AI应用（如推荐系统、搜索引擎、自然语言处理、图像识别等）的基石。</p>
</div>
<hr>
<h2 id="一、为什么需要向量嵌入？"><a href="#一、为什么需要向量嵌入？" class="headerlink" title="一、为什么需要向量嵌入？"></a>一、为什么需要向量嵌入？</h2><p>传统上，机器处理数据的方式通常是基于符号匹配或离散的分类。然而，这种方式在处理复杂、非结构化数据时面临诸多局限：</p>
<ol>
<li><strong>语义鸿沟 (Semantic Gap)</strong>：计算机无法直接理解词语、句子、图像甚至用户偏好背后的“含义”。例如，“汽车”和“车辆”在语义上相近，但在符号匹配中是不同的字符串。</li>
<li><strong>高维稀疏性 (High-Dimensional Sparsity)</strong>：传统的 One-Hot 编码等方法会产生维度极高且稀疏的向量，这不仅浪费存储和计算资源，而且无法捕捉词语之间的关系。</li>
<li><strong>计算复杂性</strong>：直接比较非结构化数据非常困难，例如如何判断两篇文章的相似度？</li>
<li><strong>无法泛化</strong>：没有见过的新数据点，传统方法难以有效处理。</li>
</ol>
<p>向量嵌入旨在解决这些问题，将数据的深层语义信息编码成紧密的数值表示，从而：</p>
<ul>
<li><strong>捕捉语义和上下文关系</strong>：相似的词、概念或数据点在向量空间中距离更近。</li>
<li><strong>降低维度</strong>：将高维稀疏表示映射到低维稠密向量。</li>
<li><strong>计算效率高</strong>：在向量空间中，可以通过数学方法（如欧氏距离、余弦相似度）快速计算相似性。</li>
<li><strong>实现泛化</strong>：模型可以学习到未见过的词语的上下文含义。</li>
<li><strong>赋能下游任务</strong>：作为特征输入，显著提升分类、聚类、搜索、推荐等任务的性能。</li>
</ul>
<h2 id="二、向量嵌入的核心概念"><a href="#二、向量嵌入的核心概念" class="headerlink" title="二、向量嵌入的核心概念"></a>二、向量嵌入的核心概念</h2><h3 id="2-1-维度-Dimension"><a href="#2-1-维度-Dimension" class="headerlink" title="2.1 维度 (Dimension)"></a>2.1 维度 (Dimension)</h3><ul>
<li><strong>定义</strong>：一个向量有多少个数值。例如，一个 768 维的向量包含 768 个浮点数。</li>
<li><strong>影响</strong>：维度越高，通常能编码的信息越多，捕捉的细粒度特征也越多，但计算和存储成本也越高。常见的 embedding 维度范围从几十到几千不等 (如 BERT family 的 768，OpenAI embedding 的 1536，Cohere embedding 的 1024)。</li>
</ul>
<h3 id="2-2-向量空间-Vector-Space"><a href="#2-2-向量空间-Vector-Space" class="headerlink" title="2.2 向量空间 (Vector Space)"></a>2.2 向量空间 (Vector Space)</h3><ul>
<li><strong>定义</strong>：嵌入向量存在的多维数学空间。在这个空间中，每个维度代表数据的一个抽象特征。</li>
<li><strong>直观理解</strong>：尽管我们难以想象超过三维的空间，但可以类比：如果“水果”、“车辆”各有一个维度，那么苹果、香蕉、汽车就会在这个空间中有它们的位置。</li>
</ul>
<h3 id="2-3-相似性度量-Similarity-Metrics"><a href="#2-3-相似性度量-Similarity-Metrics" class="headerlink" title="2.3 相似性度量 (Similarity Metrics)"></a>2.3 相似性度量 (Similarity Metrics)</h3><p>如何判断两个向量在语义上是否相似？通过计算它们在向量空间中的距离或夹角。</p>
<ol>
<li><p><strong>余弦相似度 (Cosine Similarity)</strong>：</p>
<ul>
<li><strong>定义</strong>：测量两个向量之间夹角的余弦值。值范围在 -1 (完全相反) 到 1 (完全相同) 之间。</li>
<li><strong>特点</strong>：只关注向量方向，不关注向量长度。即使两个文档的词频相差很大，只要它们的主题相似，余弦相似度也会很高。在 NLP 中应用最广泛。</li>
<li><strong>公式</strong>：$similarity &#x3D; \frac{A \cdot B}{|A| |B|}$</li>
</ul>
</li>
<li><p><strong>欧氏距离 (Euclidean Distance)</strong>：</p>
<ul>
<li><strong>定义</strong>：测量两个向量在多维空间中的直线距离。距离越小，相似度越高。</li>
<li><strong>特点</strong>：同时考虑向量方向和长度。</li>
<li><strong>公式</strong>：$distance &#x3D; \sqrt{\sum_{i&#x3D;1}^{n}(A_i - B_i)^2}$</li>
</ul>
</li>
<li><p><strong>点积 (Dot Product)</strong>：</p>
<ul>
<li><strong>定义</strong>：两个向量对应位相乘后再求和。</li>
<li><strong>特点</strong>：与余弦相似度高度相关（如果向量经过归一化），且计算更高效。在推荐系统中常用。</li>
<li><strong>公式</strong>：$similarity &#x3D; A \cdot B &#x3D; \sum_{i&#x3D;1}^{n}A_i B_i$</li>
</ul>
</li>
</ol>
<h3 id="2-4-嵌入模型-Embedding-Model"><a href="#2-4-嵌入模型-Embedding-Model" class="headerlink" title="2.4 嵌入模型 (Embedding Model)"></a>2.4 嵌入模型 (Embedding Model)</h3><ul>
<li><strong>定义</strong>：用于生成向量嵌入的预训练模型。它们通常是深度学习模型（如 Transformer 架构），在海量数据上进行训练，学习如何将输入文本、图像等映射为语义丰富的向量。</li>
<li><strong>示例</strong>：<ul>
<li><strong>文本</strong>：BERT、RoBERTa、Word2Vec、GloVe、fastText、OpenAI Embeddings (text-embedding-ada-002)、Cohere Embeddings、Sentence-BERT (SBERT) 系列模型等。</li>
<li><strong>图像</strong>：ResNet、ViT、CLIP 等。</li>
<li><strong>多模态</strong>：CLIP (将图像和文本映射到同一向量空间)。</li>
</ul>
</li>
</ul>
<h2 id="三、向量嵌入的生成过程"><a href="#三、向量嵌入的生成过程" class="headerlink" title="三、向量嵌入的生成过程"></a>三、向量嵌入的生成过程</h2><p>以文本嵌入为例，通常的生成过程如下：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[原始文本数据] --&gt; B{&quot;预处理 (分词, 清洗)&quot;}
    B --&gt; C{&quot;嵌入模型 (如 BERT, SBERT)&quot;}
    C -- 经过多层神经网络处理 --&gt; D[&quot;高维向量 (Vector Embedding)&quot;]
    D -- (进一步处理, 如归一化) --&gt; E[稠密数值向量, 可用于比较]
  </pre></div>

<p><strong>详细步骤</strong>：</p>
<ol>
<li><strong>输入数据</strong>：可以是单个词语、句子、段落或整篇文章。</li>
<li><strong>预处理</strong>：对文本进行分词 (tokenization)、去除停用词、Punctuation 等操作，将其转化为模型可以理解的输入格式（通常是 token ID 序列）。</li>
<li><strong>嵌入模型</strong>：将预处理后的输入喂给预训练的嵌入模型。<ul>
<li>模型内部通常包含一个底层的 token embedding 层，将每个 token 映射为初始向量。</li>
<li>然后通过多层 Transformer 编码器等结构，捕捉词语间的上下文关系和深层语义。</li>
</ul>
</li>
<li><strong>池化 (Pooling)</strong>：如果输入是序列（如句子），模型会输出每个 token 的 contextualized embedding。通常需要一个池化操作（如取平均 <code>mean pooling</code>、取 <code>[CLS]</code> token 对应的 embedding）将整个序列的向量表示聚合为一个固定维度的向量。</li>
<li><strong>输出向量</strong>：最终得到一个稠密的高维实数向量，这就是输入文本的向量嵌入。</li>
<li><strong>归一化 (Normalization)</strong> (可选但推荐)：将向量的长度归一化到 1，这对于余弦相似度计算很有用。</li>
</ol>
<h2 id="四、向量嵌入的应用场景"><a href="#四、向量嵌入的应用场景" class="headerlink" title="四、向量嵌入的应用场景"></a>四、向量嵌入的应用场景</h2><p>向量嵌入是现代AI的“瑞士军刀”，广泛应用于各种任务：</p>
<ol>
<li><p><strong>语义搜索 (Semantic Search)</strong>：</p>
<ul>
<li><strong>原理</strong>：将查询 (query) 文本和文档 (document) 文本都转换为向量嵌入。通过计算查询向量与所有文档向量的相似度，返回语义上最相关的文档，即使它们不包含相同的关键词。</li>
<li><strong>示例</strong>：问答系统、智能客服、RAG (Retrieval Augmented Generation) 知识检索。</li>
</ul>
</li>
<li><p><strong>推荐系统 (Recommendation Systems)</strong>：</p>
<ul>
<li><strong>原理</strong>：将用户、商品、电影等转换为向量嵌入。相似的用户（或用户与商品）在向量空间中距离相近。</li>
<li><strong>示例</strong>：个性化商品推荐、内容推荐、电影推荐。</li>
</ul>
</li>
<li><p><strong>文本相似度&#x2F;去重 (Text Similarity&#x2F;Deduplication)</strong>：</p>
<ul>
<li><strong>原理</strong>：将文本片段转换为向量，计算它们之间的相似度来判断是否重复或语义相近。</li>
<li><strong>示例</strong>：检测抄袭、新闻去重、清理数据集。</li>
</ul>
</li>
<li><p><strong>文本分类&#x2F;聚类 (Text Classification&#x2F;Clustering)</strong>：</p>
<ul>
<li><strong>原理</strong>：将文本转换为向量后，可以使用传统的分类器（如 SVM、逻辑回归）或聚类算法（如 K-Means）进行分类和聚类。</li>
<li><strong>示例</strong>：情感分析、垃圾邮件检测、主题分类。</li>
</ul>
</li>
<li><p><strong>图像检索&#x2F;识别 (Image Retrieval&#x2F;Recognition)</strong>：</p>
<ul>
<li><strong>原理</strong>：将图像转换为向量，从而实现以图搜图、图像分类等功能。</li>
<li><strong>示例</strong>：Pinterest 的视觉搜索，产品识别。</li>
</ul>
</li>
<li><p><strong>异常检测 (Anomaly Detection)</strong>：</p>
<ul>
<li><strong>原理</strong>：正常的数据点在向量空间中趋于密集，而异常点则可能孤立。</li>
<li><strong>示例</strong>：网络入侵检测、欺诈交易检测。</li>
</ul>
</li>
<li><p><strong>数据可视化 (Data Visualization)</strong>：</p>
<ul>
<li><strong>原理</strong>：通过降维技术（如 t-SNE, UMAP），将高维嵌入向量映射到 2D&#x2F;3D 空间，以便观察数据点的分布和聚类。</li>
</ul>
</li>
</ol>
<h2 id="五、向量嵌入的实践：使用-Python-和-OpenAI-Embeddings"><a href="#五、向量嵌入的实践：使用-Python-和-OpenAI-Embeddings" class="headerlink" title="五、向量嵌入的实践：使用 Python 和 OpenAI Embeddings"></a>五、向量嵌入的实践：使用 Python 和 OpenAI Embeddings</h2><p>我们将使用 OpenAI 的 <code>text-embedding-ada-002</code> 模型来生成文本嵌入。这是目前业界常用的高性能模型之一。</p>
<p><strong>准备工作</strong>:</p>
<ol>
<li><strong>安装 OpenAI 库</strong>:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install openai</span><br></pre></td></tr></table></figure></li>
<li><strong>设置 OpenAI API Key</strong>:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> OPENAI_API_KEY=<span class="string">&quot;sk-YOUR_OPENAI_API_KEY&quot;</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>Python 代码示例</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 初始化 OpenAI 客户端 ---</span></span><br><span class="line"><span class="comment"># 确保你的 OPENAI_API_KEY 环境变量已设置</span></span><br><span class="line">client = openai.OpenAI(api_key=os.environ.get(<span class="string">&quot;OPENAI_API_KEY&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用的嵌入模型名称</span></span><br><span class="line">EMBEDDING_MODEL = <span class="string">&quot;text-embedding-ada-002&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 定义函数生成嵌入 ---</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_embedding</span>(<span class="params">text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>[<span class="built_in">float</span>]:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用 OpenAI Embedding 模型生成给定文本的向量嵌入。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># 确保文本是字符串类型，并且进行一些基本清理</span></span><br><span class="line">        text = <span class="built_in">str</span>(text).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot; &quot;</span>)</span><br><span class="line">        response = client.embeddings.create(</span><br><span class="line">            <span class="built_in">input</span>=[text],</span><br><span class="line">            model=EMBEDDING_MODEL</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> response.data[<span class="number">0</span>].embedding</span><br><span class="line">    <span class="keyword">except</span> openai.APIStatusError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;OpenAI API error: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error generating embedding: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 生成文本嵌入 ---</span></span><br><span class="line">texts_to_embed = [</span><br><span class="line">    <span class="string">&quot;苹果公司是一家科技巨头，以iPhone和Mac产品闻名。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;香蕉是一种热带水果，富含钾。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;特斯拉是一家电动汽车和清洁能源公司。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;手机是现代生活中不可或缺的通讯工具。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我喜欢吃香蕉和苹果。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;我更喜欢驾驶电动汽车。&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">embeddings = [get_embedding(text) <span class="keyword">for</span> text <span class="keyword">in</span> texts_to_embed]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查嵌入向量的维度</span></span><br><span class="line"><span class="keyword">if</span> embeddings <span class="keyword">and</span> embeddings[<span class="number">0</span>]:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;生成的嵌入向量维度: <span class="subst">&#123;<span class="built_in">len</span>(embeddings[<span class="number">0</span>])&#125;</span>&quot;</span>) <span class="comment"># text-embedding-ada-002 维度是 1536</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;未能成功生成嵌入向量，请检查API Key和网络连接。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 计算相似性并进行语义搜索 ---</span></span><br><span class="line"><span class="comment"># 将列表转换为 numpy 数组以便进行批处理计算</span></span><br><span class="line">embeddings_np = np.array(embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：查找与某个查询最相似的文本</span></span><br><span class="line">query_text = <span class="string">&quot;我喜欢健康的水果。&quot;</span></span><br><span class="line">query_embedding = get_embedding(query_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> query_embedding:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;查询嵌入生成失败。&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">query_embedding_np = np.array(query_embedding).reshape(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># 转换为2D数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算查询向量与所有文本向量的余弦相似度</span></span><br><span class="line"><span class="comment"># cosine_similarity 返回一个二维数组，形状为 (n_samples_X, n_samples_Y)</span></span><br><span class="line">similarities = cosine_similarity(query_embedding_np, embeddings_np)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 语义搜索结果 ---&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i, text <span class="keyword">in</span> <span class="built_in">enumerate</span>(texts_to_embed):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文本: &#x27;<span class="subst">&#123;text&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  与查询 &#x27;<span class="subst">&#123;query_text&#125;</span>&#x27; 的相似度: <span class="subst">&#123;similarities[i]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到最相似的文本</span></span><br><span class="line">most_similar_index = np.argmax(similarities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n最相似的文本是: &#x27;<span class="subst">&#123;texts_to_embed[most_similar_index]&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;相似度: <span class="subst">&#123;similarities[most_similar_index]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：文本去重或分组 (通过计算两两相似度)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n--- 文本两两相似度矩阵 ---&quot;</span>)</span><br><span class="line"><span class="comment"># 文本之间两两相似度矩阵</span></span><br><span class="line">pairwise_similarities = cosine_similarity(embeddings_np, embeddings_np)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(texts_to_embed)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(texts_to_embed)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;&#x27;<span class="subst">&#123;texts_to_embed[i]&#125;</span>&#x27; vs &#x27;<span class="subst">&#123;texts_to_embed[j]&#125;</span>&#x27;: <span class="subst">&#123;pairwise_similarities[i, j]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察结果：</span></span><br><span class="line"><span class="comment"># &quot;苹果公司...&quot; 和 &quot;手机...&quot; 相似度可能较高，因为都涉及科技产品。</span></span><br><span class="line"><span class="comment"># &quot;香蕉...&quot; 和 &quot;我喜欢吃香蕉和苹果。&quot; 相似度会很高。</span></span><br><span class="line"><span class="comment"># &quot;特斯拉...&quot; 和 &quot;我更喜欢驾驶电动汽车。&quot; 相似度会很高。</span></span><br></pre></td></tr></table></figure>

<p><strong>代码解析</strong>:</p>
<ol>
<li><strong><code>openai.OpenAI</code></strong>: 初始化 OpenAI API 客户端。</li>
<li><strong><code>get_embedding</code> 函数</strong>: 封装了调用 <code>client.embeddings.create</code> 的逻辑。它将输入的文本发送给 OpenAI API，选择 <code>text-embedding-ada-002</code> 模型，并返回生成的 1536 维向量。</li>
<li><strong><code>texts_to_embed</code></strong>: 准备了一组示例文本。</li>
<li><strong><code>embeddings</code></strong>: 批量生成所有文本的嵌入向量。</li>
<li><strong><code>cosine_similarity</code></strong>: 使用 <code>sklearn</code> 库计算余弦相似度。这是一个高效且常用的方法。</li>
<li><strong>语义搜索示例</strong>: 通过计算查询与所有文本的相似度，找到最匹配的文本。即使查询中没有“手机”或“汽车”的明确词语，语义相似度也能捕捉到“科技产品”的关联。</li>
<li><strong>两两相似度</strong>: 展示了如何计算数据集中任意两段文本之间的相似度，这在去重、聚类等场景中非常有用。</li>
</ol>
<h2 id="六、向量嵌入的局限性"><a href="#六、向量嵌入的局限性" class="headerlink" title="六、向量嵌入的局限性"></a>六、向量嵌入的局限性</h2><p>尽管向量嵌入功能强大，但并非没有缺点：</p>
<ol>
<li><strong>黑盒性 (Black Box)</strong>：嵌入模型的内部机制复杂，难以解释为什么某些文本相似而另一些不相似。这给调试和理解模型行为带来了挑战。</li>
<li><strong>上下文依赖 (Context Dependence)</strong>：某些嵌入模型可能难以处理复杂、长距离的上下文关系，或对歧义词的处理不够精确。</li>
<li><strong>训练数据偏差 (Training Data Bias)</strong>：嵌入模型是在大规模文本数据上训练的，如果训练数据存在偏见（如性别歧视、种族偏见），这些偏见可能会被编码到嵌入中，并传播到下游应用。</li>
<li><strong>计算资源</strong>：生成高质量的嵌入（尤其是对于大型文档）和在海量向量中进行高效检索（需要向量数据库）仍然需要一定的计算资源。</li>
<li><strong>维度诅咒 (Curse of Dimensionality)</strong>：虽然嵌入旨在降低维度，但在极高维空间中，点之间的距离概念可能变得不那么直观，各种距离度量的区分度也会下降。</li>
</ol>
<h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>向量嵌入是现代人工智能，特别是自然语言处理和信息检索领域的一项革命性技术。它将非结构化数据转化为机器可处理的稠密数值向量，从而为语义理解、相似性计算和一系列高级AI应用奠定了基础。</p>
<p>通过理解其核心概念（维度、向量空间、相似性度量）和生成过程，并熟练运用如 OpenAI Embeddings 等工具，开发者能够构建出更智能、更高效的语义搜索、推荐系统、智能问答等应用。尽管存在黑盒性、偏见和计算资源等挑战，但向量嵌入无疑是当前AI技术栈中不可或缺的组成部分，并将持续推动AI领域的发展与创新。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/9ceda85c30d6/">https://blog.tbf1211.xx.kg/9ceda85c30d6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-03.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/84d87a2eca4d/" title="Caddy Web Server详解：现代Web服务器的优雅选择"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-24.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Caddy Web Server详解：现代Web服务器的优雅选择</div></div><div class="info-2"><div class="info-item-1"> Caddy 是一款用 Go 语言编写的开源 Web 服务器，以其自动 HTTPS 功能、简洁的配置以及强大的功能而闻名。它被设计成现代 Web 的瑞士军刀，能够胜任静态文件服务、反向代理、负载均衡、API 网关等多种任务，并且在安全性和易用性方面表现出色。  “Caddy 是未来 Web 服务器的样子：默认安全、易于管理、功能强大，并且能够自动处理 HTTPS 证书的申请和续期，让你的网站在几秒钟内上线并享受加密连接。”   一、Caddy 简介1.1 什么是 Caddy？Caddy 是一个高性能、可扩展的 Web 服务器，其核心特性包括：  自动 HTTPS：这是 Caddy 最吸引人的特性之一。对于绝大多数公共可访问的域名，Caddy 可以自动从 Let’s Encrypt 申请、配置和续期 SSL&#x2F;TLS 证书，无需手动干预。 配置简洁：Caddyfile 配置文件语法非常直观易懂，相比 Nginx 和 Apache 更加简洁。 HTTP&#x2F;2 和 HTTP&#x2F;3 支持：Caddy 默认启用 HTTP&#x2F;2，并且是首批支持 QUIC (H...</div></div></div></a><a class="pagination-related" href="/4b8301cdd035/" title="向量数据库 (Vector Database) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">向量数据库 (Vector Database) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量数据库 (Vector Database &#x2F; Vector Store) 是一种专门设计用于高效存储、管理和检索向量嵌入 (Vector Embeddings) 的数据库。这些向量嵌入是高维的数值表示，由机器学习模型生成，能够捕捉文本、图像、音频或其他复杂数据的语义信息。向量数据库的核心能力在于通过计算向量之间的相似度 (Similarity) 来进行快速搜索，而非传统的精确匹配。  核心思想：将非结构化数据转化为机器可理解的低维或高维向量表示（嵌入），并在此基础上实现基于语义相似度的快速检索。它解决了传统数据库在处理语义搜索、推荐系统、多模态数据匹配等场景下的局限性。   一、什么是向量 (Vector)？在深入了解向量数据库之前，我们必须先理解“向量”这个核心概念。 1.1 向量的数学定义在数学和物理中，向量 (Vector) 是一个具有大小 (Magnitude) 和方向 (Direction) 的量。它可以被表示为一个有序的数值列表。  一维向量：一个标量，如 [5]。 二维向量：表示平面上的一个点或从原点指向该点的箭头，如 [x, y]。例如，[3, 4...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/82498674876b/" title="RAG（检索增强生成）技术详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-01</div><div class="info-item-2">RAG（检索增强生成）技术详解</div></div><div class="info-2"><div class="info-item-1"> RAG (Retrieval Augmented Generation)，即检索增强生成，是一种结合了检索系统与大型语言模型 (LLM) 的人工智能技术。它旨在提高 LLM 在回答问题、生成文本时的准确性、及时性和事实可靠性，尤其是在处理特定领域知识、最新信息或内部数据时。RAG 通过在生成答案之前，从外部知识库中检索相关信息，并将这些信息作为上下文提供给 LLM，从而“增强”其生成能力。  核心思想：克服大语言模型在知识时效性、幻觉和领域特异性方面的局限性。它通过动态地从权威数据源检索相关、准确的事实依据，并以此为基础指导 LLM 进行生成，使得 LLM 的输出更加准确、可追溯且富含最新信息。    一、为什么需要 RAG？大语言模型的局限性大语言模型（LLMs）在处理自然语言任务方面展现出惊人的能力，但它们也存在一些固有的局限性，RAG 正是为了解决这些问题而生：  知识时效性与更新难题 (Knowledge Staleness)  LLM 的知识来源于其训练数据，这些数据在模型发布后就成为了静态的。它们无法获取最新的事件、实时数据或新形成的知识。 每次需要更新知识时，都可...</div></div></div></a><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-27</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a><a class="pagination-related" href="/d382adface17/" title="LLM中相似性与相关性：概念、度量与应用详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM中相似性与相关性：概念、度量与应用详解</div></div><div class="info-2"><div class="info-item-1"> 在大型语言模型 (LLM) 和更广泛的自然语言处理 (NLP) 领域中，相似性 (Similarity) 和 相关性 (Relevance) 是两个经常被提及但又有所区别的核心概念。它们都量化了两个文本片段之间的某种关联程度，但在具体含义、度量方法和应用场景上存在微妙但重要的差异。理解这两者的区别与联系，对于构建和优化基于 LLM 的智能系统至关重要。  核心思想：相似性通常指文本内容在语义或结构上的“形似”或“意近”，强调固有属性的匹配；而相关性则指文本内容与特定“查询”、“任务”或“上下文”之间的“关联程度”或“有用性”，强调功能性匹配。   一、为什么相似性与相关性在 LLM 中如此重要？LLM 通过将文本数据转换为高维向量空间中的数值向量（即嵌入），从而能够捕捉词语和文本的复杂语义。这种表示方法使得计算机可以进行超越简单关键词匹配的语义理解。而相似性和相关性正是这种语义理解的两个重要视角：  语义理解的基石：它们让 LLM 能够理解文本的实际含义，而不仅仅是表面文字。 信息检索的核心：无论是搜索、问答还是推荐，核心都是找出“最相似”或“最相关”的信息。 生成质量的衡量：...</div></div></div></a><a class="pagination-related" href="/e9adc93372f4/" title="LangChain Expression Language (LCEL) 深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-09.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-29</div><div class="info-item-2">LangChain Expression Language (LCEL) 深度详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Expression Language (LCEL) 是 LangChain 框架中一种强大、灵活且声明式的编程范式，用于构建和编排复杂的链 (Chains) 和代理 (Agents)。它提供了一种简洁、易读且高性能的方式来组合 LangChain 的各种组件（如提示模板、大语言模型、输出解析器、检索器、自定义函数等），从而构建出端到端的 LLM 应用程序。  核心思想：LCEL 的核心在于提供一个统一的 Runnable 接口和管道操作符 (|)，允许开发者以类似 Unix 管道的方式将不同的组件连接起来。这种声明式组合方式不仅提高了代码的可读性和可维护性，还带来了自动化的并行处理、流式传输、异步支持、类型安全以及与 LangSmith 等调试工具的深度集成等诸多优势。   一、为什么选择 LCEL？在 LCEL 出现之前，LangChain 主要通过传统的 Chain 类来构建应用程序。虽然这些 Chain 也有效，但 LCEL 解决了它们的一些局限性，并带来了显著的改进：  更简洁的语法和可读性：LCEL 使用管道操作符 (|)，使得链式调用直观，像数...</div></div></div></a><a class="pagination-related" href="/0998285408b6/" title="Agentic RAG (智能体RAG) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-02</div><div class="info-item-2">Agentic RAG (智能体RAG) 详解</div></div><div class="info-2"><div class="info-item-1"> Agentic RAG (智能体检索增强生成) 是在传统 RAG (Retrieval Augmented Generation) 范式基础上的一次重大演进。它将大型语言模型 (LLM) 的推理能力与AI 智能体 (Agent) 的规划、工具使用和自我反思能力相结合，以更智能、更动态的方式执行信息检索和内容生成。传统 RAG 主要关注在检索到相关信息后直接由 LLM 进行生成，而 Agentic RAG 则通过引入智能体层，使得检索过程、生成过程甚至整个解决问题的流程都更加具有策略性、可控性和适应性。   一、背景：从 RAG 到 Agentic RAG1.1 传统 RAG 的局限性Retrieval Augmented Generation (RAG) 是一种将 LLM 的生成能力与外部知识检索系统相结合的技术。当用户提出问题时，RAG 系统会首先从一个大型的、通常是向量化的知识库中检索出最相关的文档片段，然后将这些片段与用户问题一并通过 Prompt 喂给 LLM，让 LLM 基于这些检索到的信息生成回答。 传统 RAG 带来了显著的性能提升，特别是在处理事实性问题和减少幻...</div></div></div></a><a class="pagination-related" href="/c1c3c8b5b003/" title="LangChain Model I&#x2F;O 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-13</div><div class="info-item-2">LangChain Model I&#x2F;O 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Model I&#x2F;O 是 LangChain 框架的核心组成部分之一，它提供了一套标准化的接口和工具，用于与各种大型语言模型 (LLMs) 和聊天模型 (Chat Models) 进行交互，并对其输入和输出进行有效的管理和结构化。这是构建任何基于 LLM 的应用程序的基础。  核心思想：将与 LLM 的“对话”分解为可管理、可组合的组件：输入 (Prompt Templates)、模型调用 (LLM&#x2F;Chat Models) 和输出处理 (Output Parsers)。   一、为什么 Model I&#x2F;O 至关重要？在没有 LangChain Model I&#x2F;O 的情况下，直接与 LLM 交互通常意味着：  手动拼接 Prompt: 需要手动构建复杂的字符串，其中包含指令、上下文、示例和用户输入。这既繁琐又容易出错。 硬编码模型调用: 每次更换模型或供应商时，都需要修改底层代码。 非结构化的输出: LLM 的原始输出通常是自由文本，需要编写复杂的字符串解析逻辑来提取所需信息。 缺乏可复用性: 不同应用场景下的 Prom...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">411</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">217</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%EF%BC%9F"><span class="toc-text">一、为什么需要向量嵌入？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">二、向量嵌入的核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E7%BB%B4%E5%BA%A6-Dimension"><span class="toc-text">2.1 维度 (Dimension)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4-Vector-Space"><span class="toc-text">2.2 向量空间 (Vector Space)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F-Similarity-Metrics"><span class="toc-text">2.3 相似性度量 (Similarity Metrics)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B-Embedding-Model"><span class="toc-text">2.4 嵌入模型 (Embedding Model)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-text">三、向量嵌入的生成过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">四、向量嵌入的应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E7%9A%84%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8-Python-%E5%92%8C-OpenAI-Embeddings"><span class="toc-text">五、向量嵌入的实践：使用 Python 和 OpenAI Embeddings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E5%90%91%E9%87%8F%E5%B5%8C%E5%85%A5%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">六、向量嵌入的局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">七、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-25.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPFS (InterPlanetary File System) 详解"/></a><div class="content"><a class="title" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解">IPFS (InterPlanetary File System) 详解</a><time datetime="2025-12-16T22:24:00.000Z" title="发表于 2025-12-17 06:24:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能体 (Agent) 详解：深入 LangChain 开发实践"/></a><div class="content"><a class="title" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践">智能体 (Agent) 详解：深入 LangChain 开发实践</a><time datetime="2025-12-10T22:24:00.000Z" title="发表于 2025-12-11 06:24:00">2025-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bun.js 深度解析：冷启动与边缘函数优化"/></a><div class="content"><a class="title" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化">Bun.js 深度解析：冷启动与边缘函数优化</a><time datetime="2025-12-07T22:24:00.000Z" title="发表于 2025-12-08 06:24:00">2025-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-15.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go Jaeger 深度解析：分布式追踪实践"/></a><div class="content"><a class="title" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践">Go Jaeger 深度解析：分布式追踪实践</a><time datetime="2025-12-04T22:24:00.000Z" title="发表于 2025-12-05 06:24:00">2025-12-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/15920229f914/" title="Supabase 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Supabase 深度解析"/></a><div class="content"><a class="title" href="/15920229f914/" title="Supabase 深度解析">Supabase 深度解析</a><time datetime="2025-12-02T22:24:00.000Z" title="发表于 2025-12-03 06:24:00">2025-12-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-03.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        loader: {
          load: [
            // Four font extension packages (optional)
            //- '[tex]/bbm',
            //- '[tex]/bboldx',
            //- '[tex]/dsfont',
            '[tex]/mhchem'
          ],
          paths: {
            'mathjax-newcm': '[mathjax]/../@mathjax/mathjax-newcm-font',

            //- // Four font extension packages (optional)
            //- 'mathjax-bbm-extension': '[mathjax]/../@mathjax/mathjax-bbm-font-extension',
            //- 'mathjax-bboldx-extension': '[mathjax]/../@mathjax/mathjax-bboldx-font-extension',
            //- 'mathjax-dsfont-extension': '[mathjax]/../@mathjax/mathjax-dsfont-font-extension',
            'mathjax-mhchem-extension': '[mathjax]/../@mathjax/mathjax-mhchem-font-extension'
          }
        },
        output: {
          font: 'mathjax-newcm',
        },
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
          packages: {
            '[+]': [
              'mhchem'
            ]
          }
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          menuOptions: {
            settings: {
              enrich: false  // Turn off Braille and voice narration text automatic generation
            }
          },
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@4.0.0/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}&path=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews) {
            pagePV.textContent = typeof data.pageviews.value !== 'undefined' ? data.pageviews.value : data.pageviews
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors) {
            siteUV.textContent = typeof data.visitors.value !== 'undefined' ? data.visitors.value : data.visitors
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews) {
            sitePV.textContent = typeof data.pageviews.value !== 'undefined' ? data.pageviews.value : data.pageviews
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>