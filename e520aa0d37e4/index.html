<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ollama 深度解析 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:type" content="article">
<meta property="og:title" content="Ollama 深度解析">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-18.jpg">
<meta property="article:published_time" content="2025-03-02T22:24:00.000Z">
<meta property="article:modified_time" content="2026-01-07T03:33:23.895Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="代码生成">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-18.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ollama 深度解析",
  "url": "https://blog.tbf1211.xx.kg/e520aa0d37e4/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-18.jpg",
  "datePublished": "2025-03-02T22:24:00.000Z",
  "dateModified": "2026-01-07T03:33:23.895Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ollama 深度解析',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">472</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">230</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">82</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-18.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">Ollama 深度解析</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ollama 深度解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-03-02T22:24:00.000Z" title="发表于 2025-03-03 06:24:00">2025-03-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>Ollama</strong> 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将复杂的大语言模型本地化运行过程封装成简单命令，让用户能轻松部署、交互和定制开源 LLM，实现AI的民主化和去中心化。</strong></p>
</div>
<hr>
<h2 id="一、Ollama-简介"><a href="#一、Ollama-简介" class="headerlink" title="一、Ollama 简介"></a>一、Ollama 简介</h2><p>随着大语言模型技术的飞速发展，越来越多的开发者和企业希望在本地环境中运行这些模型，以实现数据隐私、降低成本、离线可用以及更灵活的定制化。然而，直接在本地部署和管理 LLM 往往涉及复杂的依赖安装、模型格式转换、GPU 配置等挑战。</p>
<p>Ollama 应运而生，它旨在解决这些痛点，提供一个“一站式”解决方案：</p>
<ul>
<li><strong>极简的用户体验</strong>：通过单个可执行文件和直观的命令行指令，即可完成模型的下载、运行和管理。</li>
<li><strong>广泛的模型支持</strong>：支持多种流行的开源 LLM，并且不断有新模型加入。</li>
<li><strong>GPU 加速</strong>：自动检测并利用本地的 GPU 资源 (如 NVIDIA CUDA, Apple Metal) 进行推理加速，显著提升性能。</li>
<li><strong>RESTful API</strong>：提供标准化的 API 接口，方便其他应用程序和框架（如 LangChain, LlamaIndex）集成。</li>
<li><strong>Modelfile 定制</strong>：允许用户通过简单的文本文件定制自己的模型，包括修改提示词、参数和模型行为。</li>
</ul>
<h2 id="二、Ollama-的工作原理"><a href="#二、Ollama-的工作原理" class="headerlink" title="二、Ollama 的工作原理"></a>二、Ollama 的工作原理</h2><p>Ollama 的核心是一个轻量级的服务器程序，负责管理模型的生命周期、处理推理请求以及与底层硬件进行交互。</p>
<h3 id="2-1-架构概述"><a href="#2-1-架构概述" class="headerlink" title="2.1 架构概述"></a>2.1 架构概述</h3><div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[用户&#x2F;客户端应用] --&gt;|命令行指令 或 REST API 请求| B(Ollama 服务)
    B --&gt; C{模型仓库}
    C --&gt; |&quot;下载模型 (GGUF)&quot;| B
    B --&gt; D[模型运行时]
    D --&gt; E[GPU &#x2F; CPU]
    E --&gt; |执行推理| D
    D --&gt; |返回结果| B
    B --&gt; |响应| A
  </pre></div>

<ol>
<li><strong>Ollama 服务 (Server)</strong>：这是 Ollama 的核心组件，通常在后台运行。它监听来自用户或客户端应用程序的请求。</li>
<li><strong>模型仓库 (Model Repository)</strong>：Ollama 维护着一个模型仓库，用户可以通过 <code>ollama pull</code> 命令从其中下载预训练模型。</li>
<li><strong>模型运行时 (Model Runtime)</strong>：Ollama 使用 <code>llama.cpp</code> 等高效的 C++ 推理引擎来加载和运行模型。</li>
<li><strong>硬件加速 (Hardware Acceleration)</strong>：Ollama 能够智能地利用本地的 GPU (如 NVIDIA CUDA, Apple Metal) 或 CPU 进行模型推理，显著提高效率。</li>
<li><strong>REST API</strong>：Ollama 服务默认在 <code>localhost:11434</code> 监听，提供标准的 REST API 接口，允许程序化地与模型进行交互。</li>
<li><strong>GGUF 格式</strong>：Ollama 使用 GGUF (GPT-Generated Unified Format) 格式的模型文件。GGUF 是一种高度优化的二进制格式，专为 CPU 和 GPU 上的高效推理而设计，支持多种量化级别，可以显著减少模型的内存占用和计算需求。</li>
</ol>
<h3 id="2-2-Modelfiles-模型文件"><a href="#2-2-Modelfiles-模型文件" class="headerlink" title="2.2 Modelfiles (模型文件)"></a>2.2 Modelfiles (模型文件)</h3><p>Modelfile 是 Ollama 中一个强大的功能，它允许用户基于现有模型创建自定义模型。通过 Modelfile，你可以：</p>
<ul>
<li>指定基础模型 (<code>FROM &lt;base_model&gt;</code>)。</li>
<li>设置模型参数 (<code>PARAMETER temperature 0.8</code>)。</li>
<li>定义系统提示词 (<code>SYSTEM &quot;You are a helpful assistant.&quot;</code>)。</li>
<li>添加自定义消息。</li>
<li>甚至可以组合不同的模型层。</li>
</ul>
<h2 id="三、安装-Ollama"><a href="#三、安装-Ollama" class="headerlink" title="三、安装 Ollama"></a>三、安装 Ollama</h2><p>Ollama 支持 macOS、Linux 和 Windows (通过 WSL2)。安装过程非常简单，通常只需下载并运行一个可执行文件或通过包管理器安装。</p>
<p><strong>官方下载地址</strong>：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://ollama.com/download">https://ollama.com/download</a></p>
<p><strong>示例 (Linux)</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p>安装后，Ollama 服务通常会自动启动并在后台运行。</p>
<h2 id="四、基本使用"><a href="#四、基本使用" class="headerlink" title="四、基本使用"></a>四、基本使用</h2><h3 id="4-1-运行模型-交互式"><a href="#4-1-运行模型-交互式" class="headerlink" title="4.1 运行模型 (交互式)"></a>4.1 运行模型 (交互式)</h3><p>下载并运行一个模型最简单的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama2</span><br></pre></td></tr></table></figure>
<p>这会先检查本地是否有 <code>llama2</code> 模型。如果没有，它会自动下载模型文件，然后启动一个交互式的聊天会话。</p>
<h3 id="4-2-下载模型"><a href="#4-2-下载模型" class="headerlink" title="4.2 下载模型"></a>4.2 下载模型</h3><p>显式下载一个模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull mistral</span><br></pre></td></tr></table></figure>
<p>你也可以指定模型版本，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull llama2:13b</span><br></pre></td></tr></table></figure>

<h3 id="4-3-列出已安装模型"><a href="#4-3-列出已安装模型" class="headerlink" title="4.3 列出已安装模型"></a>4.3 列出已安装模型</h3><p>查看本地已下载的所有模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure>

<h3 id="4-4-删除模型"><a href="#4-4-删除模型" class="headerlink" title="4.4 删除模型"></a>4.4 删除模型</h3><p>删除不再需要的模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama <span class="built_in">rm</span> llama2</span><br></pre></td></tr></table></figure>

<h3 id="4-5-启动-Ollama-服务"><a href="#4-5-启动-Ollama-服务" class="headerlink" title="4.5 启动 Ollama 服务"></a>4.5 启动 Ollama 服务</h3><p>如果你需要手动启动 Ollama 服务（例如在服务器环境中），可以使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></table></figure>
<p>此命令会阻塞当前终端，但 Ollama 服务将在 <code>localhost:11434</code> 上运行，接受 API 请求。</p>
<h2 id="五、高级用法"><a href="#五、高级用法" class="headerlink" title="五、高级用法"></a>五、高级用法</h2><h3 id="5-1-定制模型-Modelfiles"><a href="#5-1-定制模型-Modelfiles" class="headerlink" title="5.1 定制模型 (Modelfiles)"></a>5.1 定制模型 (Modelfiles)</h3><p>你可以通过创建 Modelfile 来定制现有模型。</p>
<p><strong>示例 Modelfile (名为 <code>MyAssistant</code>)</strong>：<br>创建一个名为 <code>Modelfile</code> 的文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM mistral</span><br><span class="line">PARAMETER temperature 0.7</span><br><span class="line">SYSTEM &quot;&quot;&quot;你是一个严谨且乐于助人的中文助手。</span><br><span class="line">你的回答需要清晰、准确，并尽量提供详细的解释。&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>然后使用 <code>ollama create</code> 命令基于这个 Modelfile 创建新模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama create my-assistant -f ./Modelfile</span><br></pre></td></tr></table></figure>
<p>现在你可以运行你的定制模型了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run my-assistant</span><br></pre></td></tr></table></figure>

<h3 id="5-2-使用-REST-API-进行编程交互"><a href="#5-2-使用-REST-API-进行编程交互" class="headerlink" title="5.2 使用 REST API 进行编程交互"></a>5.2 使用 REST API 进行编程交互</h3><p>Ollama 提供了一个 RESTful API，方便集成到其他应用程序中。默认服务地址为 <code>http://localhost:11434</code>。</p>
<p><strong>Go 语言代码示例</strong>：向 Ollama 发送生成请求。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;bytes&quot;</span></span><br><span class="line">	<span class="string">&quot;encoding/json&quot;</span></span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;io/ioutil&quot;</span></span><br><span class="line">	<span class="string">&quot;net/http&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateRequest represents the request payload for Ollama&#x27;s /api/generate endpoint</span></span><br><span class="line"><span class="keyword">type</span> GenerateRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model  <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	Prompt <span class="type">string</span> <span class="string">`json:&quot;prompt&quot;`</span></span><br><span class="line">	Stream <span class="type">bool</span>   <span class="string">`json:&quot;stream&quot;`</span> <span class="comment">// Set to false for a single response</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateResponse represents the basic response structure</span></span><br><span class="line"><span class="keyword">type</span> GenerateResponse <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model     <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	CreatedAt <span class="type">string</span> <span class="string">`json:&quot;created_at&quot;`</span></span><br><span class="line">	Response  <span class="type">string</span> <span class="string">`json:&quot;response&quot;`</span></span><br><span class="line">	Done      <span class="type">bool</span>   <span class="string">`json:&quot;done&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ollamaURL := <span class="string">&quot;http://localhost:11434/api/generate&quot;</span></span><br><span class="line">	modelName := <span class="string">&quot;llama2&quot;</span> <span class="comment">// 确保本地已 pull llama2 模型</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 构造请求体</span></span><br><span class="line">	requestPayload := GenerateRequest&#123;</span><br><span class="line">		Model:  modelName,</span><br><span class="line">		Prompt: <span class="string">&quot;What is the capital of France?&quot;</span>,</span><br><span class="line">		Stream: <span class="literal">false</span>,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	jsonPayload, err := json.Marshal(requestPayload)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error marshalling JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 发送 POST 请求</span></span><br><span class="line">	resp, err := http.Post(ollamaURL, <span class="string">&quot;application/json&quot;</span>, bytes.NewBuffer(jsonPayload))</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error sending request to Ollama: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> resp.StatusCode != http.StatusOK &#123;</span><br><span class="line">		bodyBytes, _ := ioutil.ReadAll(resp.Body)</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Ollama API returned non-OK status: %s, Body: %s\n&quot;</span>, resp.Status, <span class="type">string</span>(bodyBytes))</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 读取并解析响应</span></span><br><span class="line">	body, err := ioutil.ReadAll(resp.Body)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error reading response body: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> generateResponse GenerateResponse</span><br><span class="line">	err = json.Unmarshal(body, &amp;generateResponse)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error unmarshalling response JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">&quot;Model: %s\n&quot;</span>, generateResponse.Model)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Response: %s\n&quot;</span>, generateResponse.Response)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：上述 Go 代码是一个简化示例，未处理 <code>stream: true</code> 模式下的分块响应。</p>
<h3 id="5-3-与其他工具集成"><a href="#5-3-与其他工具集成" class="headerlink" title="5.3 与其他工具集成"></a>5.3 与其他工具集成</h3><p>Ollama 作为本地 LLM 运行时的核心，已被广泛集成到各种 AI 开发工具和框架中：</p>
<ul>
<li><strong>LangChain</strong> 和 <strong>LlamaIndex</strong>：直接支持 Ollama 作为本地推理后端。</li>
<li><strong>LiteLLM</strong>：一个统一的 LLM API 接口，可以轻松切换 Ollama 和其他云端&#x2F;本地模型。</li>
<li><strong>VS Code Extensions</strong>：例如 <code>Continue</code> 等插件可以配置为使用 Ollama 驱动的本地 LLM 进行代码补全、问答等。</li>
<li><strong>Web UIs</strong>：许多开源的本地 AI 聊天界面也支持 Ollama。</li>
</ul>
<h2 id="六、优势与应用场景"><a href="#六、优势与应用场景" class="headerlink" title="六、优势与应用场景"></a>六、优势与应用场景</h2><h3 id="6-1-优势"><a href="#6-1-优势" class="headerlink" title="6.1 优势"></a>6.1 优势</h3><ol>
<li><strong>数据隐私与安全</strong>：所有推理都在本地完成，敏感数据无需发送到外部云服务。</li>
<li><strong>成本效益</strong>：无需支付 API 调用费用，只需本地硬件的电力消耗。</li>
<li><strong>离线可用</strong>：模型下载后，可在没有互联网连接的情况下使用。</li>
<li><strong>模型定制与实验</strong>：Modelfile 提供了极大的灵活性，方便用户修改模型行为、尝试不同的提示词策略。</li>
<li><strong>易用性</strong>：简化了本地 LLM 部署的复杂性，降低了使用门槛。</li>
</ol>
<h3 id="6-2-应用场景"><a href="#6-2-应用场景" class="headerlink" title="6.2 应用场景"></a>6.2 应用场景</h3><ul>
<li><strong>本地开发与原型设计</strong>：开发者可以在本地快速测试和迭代基于 LLM 的应用程序。</li>
<li><strong>隐私敏感型应用</strong>：处理医疗、金融或个人敏感数据的企业和个人。</li>
<li><strong>研究与教育</strong>：学生和研究人员可以在受控环境中深入学习和实验 LLM。</li>
<li><strong>离线环境</strong>：在网络受限或无网络的场景下（如飞机、野外作业）使用 LLM。</li>
<li><strong>本地知识库问答</strong>：结合 RAG (Retrieval Augmented Generation) 技术，构建基于本地文档的智能问答系统。</li>
</ul>
<h2 id="七、限制与注意事项"><a href="#七、限制与注意事项" class="headerlink" title="七、限制与注意事项"></a>七、限制与注意事项</h2><ol>
<li><strong>硬件要求</strong>：运行 LLM 需要足够的内存 (RAM) 和计算资源 (CPU&#x2F;GPU)。较大的模型（如 7B, 13B, 70B 参数）对内存和 GPU 显存有较高要求。<ul>
<li><strong>RAM</strong>：通常建议至少 8GB RAM 用于小型模型，32GB+ 用于中型模型。</li>
<li><strong>GPU (VRAM)</strong>：GPU 加速效果显著，但需要足够的显存。例如，一个 7B 模型可能需要 4-6GB VRAM，13B 模型可能需要 8-10GB VRAM，而 70B 模型则需要 40GB+ VRAM。</li>
</ul>
</li>
<li><strong>性能</strong>：本地硬件的性能直接决定了推理速度。消费级 GPU 的性能通常无法与云端专业的 AI 加速器相媲美。</li>
<li><strong>模型选择</strong>：Ollama 支持的模型虽然众多，但并非所有开源 LLM 都能在 Ollama 中运行（需要转换为 GGUF 格式）。</li>
<li><strong>持续维护</strong>：Ollama 项目正在快速发展，新功能和模型不断涌现，但也可能存在一些 bug 或兼容性问题，需要社区支持。</li>
</ol>
<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>Ollama 是一个改变游戏规则的工具，它极大地简化了在本地运行大语言模型的过程。通过提供直观的命令行界面、强大的 Modelfile 定制功能和标准化的 REST API，Ollama 正在推动 LLM 技术的普及和应用。对于那些关注隐私、追求成本效益或希望进行深度模型实验的开发者和企业而言，Ollama 无疑是一个不可或缺的工具，它使得每个人都能在自己的机器上掌握 AI 的力量。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/e520aa0d37e4/">https://blog.tbf1211.xx.kg/e520aa0d37e4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/">代码生成</a><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-18.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/34476d6d7b8c/" title="Java 各版本新特性详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Java 各版本新特性详解</div></div><div class="info-2"><div class="info-item-1"> Java 作为一门历史悠久且持续演进的编程语言，自其诞生以来，便不断通过新版本的发布引入众多创新特性，以适应现代软件开发的需求。本文将详尽地剖析 Java 8 至今（直至 Java 21 作为当前主流 LTS 版本）各重要版本所带来的核心新特性，旨在帮助开发者理解这些特性如何提升开发效率、代码质量及程序性能。  核心思想：理解 Java 各版本的新特性，能够使开发者编写出更现代、更简洁、更高性能的代码，并有效利用 JVM 的最新优化。   一、Java 8 (LTS - 发布于 2014 年)Java 8 是 Java 发展史上的一个里程碑版本，引入了大量旨在提升生产力的新特性，尤其是在函数式编程和并发领域。 1.1 Lambda 表达式定义：Lambda 表达式提供了一种简洁的方式来表示可传递的匿名函数。它使得函数可以作为方法参数，并且使代码更加简洁、可读性更强。这实质上是支持了函数式编程范式。 语法：(parameters) -&gt; expression 或 (parameters) -&gt; &#123; statements; &#125; 示例 (Java)： ...</div></div></div></a><a class="pagination-related" href="/21a6ae00fb43/" title="哈希表负载因子详解(Load Factor)"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">哈希表负载因子详解(Load Factor)</div></div><div class="info-2"><div class="info-item-1"> 哈希表 (Hash Table) 是一种高效的数据结构，用于存储键值对 (key-value pairs)，提供快速的查找、插入和删除操作。它的核心思想是利用哈希函数 (Hash Function) 将键映射到数组的某个索引位置。然而，哈希表的性能高度依赖于负载因子 (Load Factor) 的管理，它在空间利用率、查找效率和再哈希 (Resizing&#x2F;Rehashing) 成本之间扮演着关键的平衡角色。  核心思想：负载因子衡量了哈希表的“满”程度，是决定何时以及如何调整哈希表大小的关键指标，直接影响其性能和资源消耗。   一、哈希表简介与冲突在深入了解负载因子之前，我们先回顾哈希表的基本概念和冲突问题。 1.1 哈希表工作原理哈希表使用一个数组（通常称为桶数组或槽数组）来存储数据。当需要插入一个键值对时：  哈希函数：对键进行哈希计算，得到一个哈希值。 取模运算：将哈希值与桶数组的长度取模，得到一个数组索引。 存储：将键值对存储到该索引位置。  1.2 哈希冲突 (Hash Collision)不同的键经过哈希函数计算后，可能会得到相同的哈希值，进而映射到桶数组...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/6d28d801758d/" title="Claude Code 详解：Anthropic 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-26.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-05</div><div class="info-item-2">Claude Code 详解：Anthropic 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Claude Code 是 Anthropic 公司开发的 Claude 系列大型语言模型在代码理解、生成和辅助方面的一个特定能力集合或优化方向。Anthropic 以其在 AI 安全和可控性方面的研究而闻名，Claude 模型旨在成为一个有益、无害且诚实的 AI 助手。因此，Claude Code 不仅关注代码的正确性，也强调生成代码的安全性、可读性和遵循最佳实践。  核心思想：结合 Anthropic 的安全和伦理原则，提供安全、有益、高质量的代码生成与辅助能力，旨在成为开发者的“无害”智能编程伙伴。   一、Claude Code 的背景与 Anthropic 理念Anthropic 由前 OpenAI 员工创立，致力于开发安全、可控且有益的人工智能系统。其核心产品 Claude 语言模型系列被设计为更易于对齐人类价值观，并通过“宪法 AI (Constitutional AI)”等方法进行训练，减少有害、偏见或不真实内容的生成。 在代码领域，这种理念意味着 Claude Code 不仅仅是生成能运行的代码，更关注：  安全性：避免生成包含已知漏洞或不良安全实践的代码。 ...</div></div></div></a><a class="pagination-related" href="/b100840425a8/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-19.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-07</div><div class="info-item-2">Codex 详解与使用技巧：OpenAI 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Codex 是由 OpenAI 训练的一个大型语言模型，其核心能力在于理解自然语言并将其转换为代码，或者理解代码并解释其含义。它是 GPT 系列模型的一个特化版本，专门针对编程语言进行了大量训练。Codex 不仅能生成 Python 代码，还能处理多种其他编程语言，是 OpenAI 在人工智能编程领域迈出的重要一步，也是 GitHub Copilot 等工具的基石。  核心思想：将自然语言描述的问题转化为可执行的代码，实现人机协作编程，降低编程门槛，提升开发效率。 掌握有效的指令（Prompt）是充分发挥 Codex 能力的关键。   一、Codex 的起源与核心能力Codex 的开发是基于 OpenAI 的 GPT-3 模型。GPT-3 以其强大的文本生成能力震惊业界，但其在代码生成方面虽然有一定表现，但仍缺乏专业性和精准度。为了弥补这一差距，OpenAI 进一步对 GPT-3 进行了微调，使用了海量的代码数据，最终诞生了 Codex。 1.1 背景：GPT-3 的局限性与代码生成的需求GPT-3 在零样本（zero-shot）和少样本（few-shot）学习方面表现出色，能...</div></div></div></a><a class="pagination-related" href="/9ceda85c30d6/" title="向量嵌入 (Vector Embeddings) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-05</div><div class="info-item-2">向量嵌入 (Vector Embeddings) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量嵌入 (Vector Embeddings) 是人工智能和机器学习领域的一个核心概念，它指的是将复杂的数据对象（如文本、图像、音频、图形节点、用户行为等）映射到高维实数向量空间中的一种技术。在这个向量空间中，语义或功能上相似的数据对象会映射到彼此接近的向量点。  通过向量嵌入，我们可以将非结构化数据转化为机器可理解和处理的数值形式，并且能够通过计算向量之间的距离来量化数据对象之间的相似性。它是许多现代AI应用（如推荐系统、搜索引擎、自然语言处理、图像识别等）的基石。   一、为什么需要向量嵌入？传统上，机器处理数据的方式通常是基于符号匹配或离散的分类。然而，这种方式在处理复杂、非结构化数据时面临诸多局限：  语义鸿沟 (Semantic Gap)：计算机无法直接理解词语、句子、图像甚至用户偏好背后的“含义”。例如，“汽车”和“车辆”在语义上相近，但在符号匹配中是不同的字符串。 高维稀疏性 (High-Dimensional Sparsity)：传统的 One-Hot 编码等方法会产生维度极高且稀疏的向量，这不仅浪费存储和计算资源，而且无法捕捉词语之间的关系。 计算复杂性：直...</div></div></div></a><a class="pagination-related" href="/01a529e01857/" title="Diffusion Models (扩散模型) 深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-22</div><div class="info-item-2">Diffusion Models (扩散模型) 深度详解</div></div><div class="info-2"><div class="info-item-1"> 扩散模型 (Diffusion Models) 是一类新兴的生成模型 (Generative Models)，近年来在图像生成、音频合成、视频生成等领域取得了突破性的进展，特别是在高保真度图像生成方面展现出无与伦比的性能，超越了传统的 GANs (生成对抗网络) 和 VAEs (变分自编码器)。其核心思想是模仿物理学中的扩散过程，通过逐步添加噪声来破坏数据结构，然后学习如何逆转这个过程，从随机噪声中逐渐恢复出清晰的数据。  核心思想：扩散模型将数据生成视为一个迭代的去噪过程。它包含两个核心阶段：前向扩散过程（加噪）和反向去噪过程（学习去噪以生成数据）。通过训练一个神经网络来预测并去除前向过程中添加的噪声，模型学会了如何从纯噪声中一步步“去噪”并生成符合真实数据分布的样本。   一、为什么需要扩散模型？在扩散模型出现之前，主流的生成模型有：  生成对抗网络 (GANs)：以其出色的图像生成质量而闻名。然而，GANs 的训练过程以对抗性方式进行，往往不稳定且难以收敛，存在模式崩溃 (mode collapse) 问题，即生成多样性不足。 变分自编码器 (VAEs)：训练更稳定，但生...</div></div></div></a><a class="pagination-related" href="/52b6fb1ec8d5/" title="文档嵌入模型 (Document Embedding Models) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-32.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-08</div><div class="info-item-2">文档嵌入模型 (Document Embedding Models) 详解</div></div><div class="info-2"><div class="info-item-1"> 文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的文档在多维向量空间中彼此靠近。这是构建高级信息检索、知识管理和内容理解系统的基石。   一、为什么需要文档嵌入模型？在大数据时代，我们面临着海量文档（如网页、报告、书籍、代码库、用户评论等）。传统处理这些文档的方法存在诸多局限：  关键词匹配的不足：搜索引擎通常依赖关键词匹配，但无法理解语义。例如，搜索“车祸”可能无法找到包含“交通事故”的文档。 句嵌入的局限性：虽然句嵌入能捕捉句子级别的语义，但在处理长文档时，简单地拼接或平均句嵌入会丢失文档整体的结构和主题信息。 高维稀疏性问题：传统的 Bag-of-Words (BOW) 或 TF-IDF 等模型将文档表示为高维稀疏向量，不仅计算效...</div></div></div></a><a class="pagination-related" href="/d382adface17/" title="LLM中相似性与相关性：概念、度量与应用详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-32.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-03</div><div class="info-item-2">LLM中相似性与相关性：概念、度量与应用详解</div></div><div class="info-2"><div class="info-item-1"> 在大型语言模型 (LLM) 和更广泛的自然语言处理 (NLP) 领域中，相似性 (Similarity) 和 相关性 (Relevance) 是两个经常被提及但又有所区别的核心概念。它们都量化了两个文本片段之间的某种关联程度，但在具体含义、度量方法和应用场景上存在微妙但重要的差异。理解这两者的区别与联系，对于构建和优化基于 LLM 的智能系统至关重要。  核心思想：相似性通常指文本内容在语义或结构上的“形似”或“意近”，强调固有属性的匹配；而相关性则指文本内容与特定“查询”、“任务”或“上下文”之间的“关联程度”或“有用性”，强调功能性匹配。   一、为什么相似性与相关性在 LLM 中如此重要？LLM 通过将文本数据转换为高维向量空间中的数值向量（即嵌入），从而能够捕捉词语和文本的复杂语义。这种表示方法使得计算机可以进行超越简单关键词匹配的语义理解。而相似性和相关性正是这种语义理解的两个重要视角：  语义理解的基石：它们让 LLM 能够理解文本的实际含义，而不仅仅是表面文字。 信息检索的核心：无论是搜索、问答还是推荐，核心都是找出“最相似”或“最相关”的信息。 生成质量的衡量：...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">472</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">230</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">82</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Ollama-%E7%AE%80%E4%BB%8B"><span class="toc-text">一、Ollama 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Ollama-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">二、Ollama 的工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-text">2.1 架构概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Modelfiles-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6"><span class="toc-text">2.2 Modelfiles (模型文件)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85-Ollama"><span class="toc-text">三、安装 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">四、基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B-%E4%BA%A4%E4%BA%92%E5%BC%8F"><span class="toc-text">4.1 运行模型 (交互式)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.2 下载模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%88%97%E5%87%BA%E5%B7%B2%E5%AE%89%E8%A3%85%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.3 列出已安装模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%88%A0%E9%99%A4%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.4 删除模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%90%AF%E5%8A%A8-Ollama-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.5 启动 Ollama 服务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="toc-text">五、高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%AE%9A%E5%88%B6%E6%A8%A1%E5%9E%8B-Modelfiles"><span class="toc-text">5.1 定制模型 (Modelfiles)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BD%BF%E7%94%A8-REST-API-%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A8%8B%E4%BA%A4%E4%BA%92"><span class="toc-text">5.2 使用 REST API 进行编程交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%B8%8E%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90"><span class="toc-text">5.3 与其他工具集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">六、优势与应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%BC%98%E5%8A%BF"><span class="toc-text">6.1 优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">6.2 应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E9%99%90%E5%88%B6%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">七、限制与注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">八、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/82bf00e3828f/" title="传统命令行工具的现代补强与替代方案详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="传统命令行工具的现代补强与替代方案详解"/></a><div class="content"><a class="title" href="/82bf00e3828f/" title="传统命令行工具的现代补强与替代方案详解">传统命令行工具的现代补强与替代方案详解</a><time datetime="2026-01-06T22:24:00.000Z" title="发表于 2026-01-07 06:24:00">2026-01-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/cecf96486ef4/" title="IPC (Inter-Process Communication) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPC (Inter-Process Communication) 详解"/></a><div class="content"><a class="title" href="/cecf96486ef4/" title="IPC (Inter-Process Communication) 详解">IPC (Inter-Process Communication) 详解</a><time datetime="2026-01-04T22:24:00.000Z" title="发表于 2026-01-05 06:24:00">2026-01-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/3059cdc5f529/" title="Golang sqlc 框架详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-32.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Golang sqlc 框架详解"/></a><div class="content"><a class="title" href="/3059cdc5f529/" title="Golang sqlc 框架详解">Golang sqlc 框架详解</a><time datetime="2026-01-01T22:24:00.000Z" title="发表于 2026-01-02 06:24:00">2026-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/4e0d929e572f/" title="如何防止 Golang Goroutine 泄漏"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="如何防止 Golang Goroutine 泄漏"/></a><div class="content"><a class="title" href="/4e0d929e572f/" title="如何防止 Golang Goroutine 泄漏">如何防止 Golang Goroutine 泄漏</a><time datetime="2025-12-30T22:24:00.000Z" title="发表于 2025-12-31 06:24:00">2025-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-29.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CFFI (C Foreign Function Interface for Python) 详解"/></a><div class="content"><a class="title" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解">CFFI (C Foreign Function Interface for Python) 详解</a><time datetime="2025-12-23T22:24:00.000Z" title="发表于 2025-12-24 06:24:00">2025-12-24</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-18.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="🆕 2023">🆕 2023</a><a href="/archives/2024/" target="_blank" title="🆒 2024">🆒 2024</a><a href="/archives/2025/" target="_blank" title="👨‍👩‍👦 2025">👨‍👩‍👦 2025</a><a href="/archives/2026/" target="_blank" title="🆙 2026">🆙 2026</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="📁 分类">📁 分类</a><a href="/tags/" target="_blank" title="🔖 标签">🔖 标签</a><a href="/categories/" target="_blank" title="📽️ 时间线">📽️ 时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="💬 说说">💬 说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>