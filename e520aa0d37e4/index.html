<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ollama 深度解析 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:type" content="article">
<meta property="og:title" content="Ollama 深度解析">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-30.jpg">
<meta property="article:published_time" content="2025-03-02T22:24:00.000Z">
<meta property="article:modified_time" content="2026-02-01T09:29:10.744Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="代码生成">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-30.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ollama 深度解析",
  "url": "https://blog.tbf1211.xx.kg/e520aa0d37e4/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-30.jpg",
  "datePublished": "2025-03-02T22:24:00.000Z",
  "dateModified": "2026-02-01T09:29:10.744Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ollama 深度解析',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">536</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">229</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">84</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-30.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">Ollama 深度解析</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li><li><a class="site-page child" href="/archives/2026/"><i class="fa-fw fa-solid fa-code-branch"></i><span> 2026</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ollama 深度解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-03-02T22:24:00.000Z" title="发表于 2025-03-03 06:24:00">2025-03-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>Ollama</strong> 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将复杂的大语言模型本地化运行过程封装成简单命令，让用户能轻松部署、交互和定制开源 LLM，实现AI的民主化和去中心化。</strong></p>
</div>
<hr>
<h2 id="一、Ollama-简介"><a href="#一、Ollama-简介" class="headerlink" title="一、Ollama 简介"></a>一、Ollama 简介</h2><p>随着大语言模型技术的飞速发展，越来越多的开发者和企业希望在本地环境中运行这些模型，以实现数据隐私、降低成本、离线可用以及更灵活的定制化。然而，直接在本地部署和管理 LLM 往往涉及复杂的依赖安装、模型格式转换、GPU 配置等挑战。</p>
<p>Ollama 应运而生，它旨在解决这些痛点，提供一个“一站式”解决方案：</p>
<ul>
<li><strong>极简的用户体验</strong>：通过单个可执行文件和直观的命令行指令，即可完成模型的下载、运行和管理。</li>
<li><strong>广泛的模型支持</strong>：支持多种流行的开源 LLM，并且不断有新模型加入。</li>
<li><strong>GPU 加速</strong>：自动检测并利用本地的 GPU 资源 (如 NVIDIA CUDA, Apple Metal) 进行推理加速，显著提升性能。</li>
<li><strong>RESTful API</strong>：提供标准化的 API 接口，方便其他应用程序和框架（如 LangChain, LlamaIndex）集成。</li>
<li><strong>Modelfile 定制</strong>：允许用户通过简单的文本文件定制自己的模型，包括修改提示词、参数和模型行为。</li>
</ul>
<h2 id="二、Ollama-的工作原理"><a href="#二、Ollama-的工作原理" class="headerlink" title="二、Ollama 的工作原理"></a>二、Ollama 的工作原理</h2><p>Ollama 的核心是一个轻量级的服务器程序，负责管理模型的生命周期、处理推理请求以及与底层硬件进行交互。</p>
<h3 id="2-1-架构概述"><a href="#2-1-架构概述" class="headerlink" title="2.1 架构概述"></a>2.1 架构概述</h3><div class="mermaid-wrap"><pre class="mermaid-src" hidden>
    graph TD
    A[用户&#x2F;客户端应用] --&gt;|命令行指令 或 REST API 请求| B(Ollama 服务)
    B --&gt; C{模型仓库}
    C --&gt; |&quot;下载模型 (GGUF)&quot;| B
    B --&gt; D[模型运行时]
    D --&gt; E[GPU &#x2F; CPU]
    E --&gt; |执行推理| D
    D --&gt; |返回结果| B
    B --&gt; |响应| A
  </pre></div>

<ol>
<li><strong>Ollama 服务 (Server)</strong>：这是 Ollama 的核心组件，通常在后台运行。它监听来自用户或客户端应用程序的请求。</li>
<li><strong>模型仓库 (Model Repository)</strong>：Ollama 维护着一个模型仓库，用户可以通过 <code>ollama pull</code> 命令从其中下载预训练模型。</li>
<li><strong>模型运行时 (Model Runtime)</strong>：Ollama 使用 <code>llama.cpp</code> 等高效的 C++ 推理引擎来加载和运行模型。</li>
<li><strong>硬件加速 (Hardware Acceleration)</strong>：Ollama 能够智能地利用本地的 GPU (如 NVIDIA CUDA, Apple Metal) 或 CPU 进行模型推理，显著提高效率。</li>
<li><strong>REST API</strong>：Ollama 服务默认在 <code>localhost:11434</code> 监听，提供标准的 REST API 接口，允许程序化地与模型进行交互。</li>
<li><strong>GGUF 格式</strong>：Ollama 使用 GGUF (GPT-Generated Unified Format) 格式的模型文件。GGUF 是一种高度优化的二进制格式，专为 CPU 和 GPU 上的高效推理而设计，支持多种量化级别，可以显著减少模型的内存占用和计算需求。</li>
</ol>
<h3 id="2-2-Modelfiles-模型文件"><a href="#2-2-Modelfiles-模型文件" class="headerlink" title="2.2 Modelfiles (模型文件)"></a>2.2 Modelfiles (模型文件)</h3><p>Modelfile 是 Ollama 中一个强大的功能，它允许用户基于现有模型创建自定义模型。通过 Modelfile，你可以：</p>
<ul>
<li>指定基础模型 (<code>FROM &lt;base_model&gt;</code>)。</li>
<li>设置模型参数 (<code>PARAMETER temperature 0.8</code>)。</li>
<li>定义系统提示词 (<code>SYSTEM &quot;You are a helpful assistant.&quot;</code>)。</li>
<li>添加自定义消息。</li>
<li>甚至可以组合不同的模型层。</li>
</ul>
<h2 id="三、安装-Ollama"><a href="#三、安装-Ollama" class="headerlink" title="三、安装 Ollama"></a>三、安装 Ollama</h2><p>Ollama 支持 macOS、Linux 和 Windows (通过 WSL2)。安装过程非常简单，通常只需下载并运行一个可执行文件或通过包管理器安装。</p>
<p><strong>官方下载地址</strong>：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://ollama.com/download">https://ollama.com/download</a></p>
<p><strong>示例 (Linux)</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p>安装后，Ollama 服务通常会自动启动并在后台运行。</p>
<h2 id="四、基本使用"><a href="#四、基本使用" class="headerlink" title="四、基本使用"></a>四、基本使用</h2><h3 id="4-1-运行模型-交互式"><a href="#4-1-运行模型-交互式" class="headerlink" title="4.1 运行模型 (交互式)"></a>4.1 运行模型 (交互式)</h3><p>下载并运行一个模型最简单的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama2</span><br></pre></td></tr></table></figure>
<p>这会先检查本地是否有 <code>llama2</code> 模型。如果没有，它会自动下载模型文件，然后启动一个交互式的聊天会话。</p>
<h3 id="4-2-下载模型"><a href="#4-2-下载模型" class="headerlink" title="4.2 下载模型"></a>4.2 下载模型</h3><p>显式下载一个模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull mistral</span><br></pre></td></tr></table></figure>
<p>你也可以指定模型版本，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull llama2:13b</span><br></pre></td></tr></table></figure>

<h3 id="4-3-列出已安装模型"><a href="#4-3-列出已安装模型" class="headerlink" title="4.3 列出已安装模型"></a>4.3 列出已安装模型</h3><p>查看本地已下载的所有模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure>

<h3 id="4-4-删除模型"><a href="#4-4-删除模型" class="headerlink" title="4.4 删除模型"></a>4.4 删除模型</h3><p>删除不再需要的模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama <span class="built_in">rm</span> llama2</span><br></pre></td></tr></table></figure>

<h3 id="4-5-启动-Ollama-服务"><a href="#4-5-启动-Ollama-服务" class="headerlink" title="4.5 启动 Ollama 服务"></a>4.5 启动 Ollama 服务</h3><p>如果你需要手动启动 Ollama 服务（例如在服务器环境中），可以使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></table></figure>
<p>此命令会阻塞当前终端，但 Ollama 服务将在 <code>localhost:11434</code> 上运行，接受 API 请求。</p>
<h2 id="五、高级用法"><a href="#五、高级用法" class="headerlink" title="五、高级用法"></a>五、高级用法</h2><h3 id="5-1-定制模型-Modelfiles"><a href="#5-1-定制模型-Modelfiles" class="headerlink" title="5.1 定制模型 (Modelfiles)"></a>5.1 定制模型 (Modelfiles)</h3><p>你可以通过创建 Modelfile 来定制现有模型。</p>
<p><strong>示例 Modelfile (名为 <code>MyAssistant</code>)</strong>：<br>创建一个名为 <code>Modelfile</code> 的文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM mistral</span><br><span class="line">PARAMETER temperature 0.7</span><br><span class="line">SYSTEM &quot;&quot;&quot;你是一个严谨且乐于助人的中文助手。</span><br><span class="line">你的回答需要清晰、准确，并尽量提供详细的解释。&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>然后使用 <code>ollama create</code> 命令基于这个 Modelfile 创建新模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama create my-assistant -f ./Modelfile</span><br></pre></td></tr></table></figure>
<p>现在你可以运行你的定制模型了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run my-assistant</span><br></pre></td></tr></table></figure>

<h3 id="5-2-使用-REST-API-进行编程交互"><a href="#5-2-使用-REST-API-进行编程交互" class="headerlink" title="5.2 使用 REST API 进行编程交互"></a>5.2 使用 REST API 进行编程交互</h3><p>Ollama 提供了一个 RESTful API，方便集成到其他应用程序中。默认服务地址为 <code>http://localhost:11434</code>。</p>
<p><strong>Go 语言代码示例</strong>：向 Ollama 发送生成请求。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;bytes&quot;</span></span><br><span class="line">	<span class="string">&quot;encoding/json&quot;</span></span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;io/ioutil&quot;</span></span><br><span class="line">	<span class="string">&quot;net/http&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateRequest represents the request payload for Ollama&#x27;s /api/generate endpoint</span></span><br><span class="line"><span class="keyword">type</span> GenerateRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model  <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	Prompt <span class="type">string</span> <span class="string">`json:&quot;prompt&quot;`</span></span><br><span class="line">	Stream <span class="type">bool</span>   <span class="string">`json:&quot;stream&quot;`</span> <span class="comment">// Set to false for a single response</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateResponse represents the basic response structure</span></span><br><span class="line"><span class="keyword">type</span> GenerateResponse <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model     <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	CreatedAt <span class="type">string</span> <span class="string">`json:&quot;created_at&quot;`</span></span><br><span class="line">	Response  <span class="type">string</span> <span class="string">`json:&quot;response&quot;`</span></span><br><span class="line">	Done      <span class="type">bool</span>   <span class="string">`json:&quot;done&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ollamaURL := <span class="string">&quot;http://localhost:11434/api/generate&quot;</span></span><br><span class="line">	modelName := <span class="string">&quot;llama2&quot;</span> <span class="comment">// 确保本地已 pull llama2 模型</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 构造请求体</span></span><br><span class="line">	requestPayload := GenerateRequest&#123;</span><br><span class="line">		Model:  modelName,</span><br><span class="line">		Prompt: <span class="string">&quot;What is the capital of France?&quot;</span>,</span><br><span class="line">		Stream: <span class="literal">false</span>,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	jsonPayload, err := json.Marshal(requestPayload)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error marshalling JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 发送 POST 请求</span></span><br><span class="line">	resp, err := http.Post(ollamaURL, <span class="string">&quot;application/json&quot;</span>, bytes.NewBuffer(jsonPayload))</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error sending request to Ollama: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> resp.StatusCode != http.StatusOK &#123;</span><br><span class="line">		bodyBytes, _ := ioutil.ReadAll(resp.Body)</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Ollama API returned non-OK status: %s, Body: %s\n&quot;</span>, resp.Status, <span class="type">string</span>(bodyBytes))</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 读取并解析响应</span></span><br><span class="line">	body, err := ioutil.ReadAll(resp.Body)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error reading response body: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> generateResponse GenerateResponse</span><br><span class="line">	err = json.Unmarshal(body, &amp;generateResponse)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error unmarshalling response JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">&quot;Model: %s\n&quot;</span>, generateResponse.Model)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Response: %s\n&quot;</span>, generateResponse.Response)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：上述 Go 代码是一个简化示例，未处理 <code>stream: true</code> 模式下的分块响应。</p>
<h3 id="5-3-与其他工具集成"><a href="#5-3-与其他工具集成" class="headerlink" title="5.3 与其他工具集成"></a>5.3 与其他工具集成</h3><p>Ollama 作为本地 LLM 运行时的核心，已被广泛集成到各种 AI 开发工具和框架中：</p>
<ul>
<li><strong>LangChain</strong> 和 <strong>LlamaIndex</strong>：直接支持 Ollama 作为本地推理后端。</li>
<li><strong>LiteLLM</strong>：一个统一的 LLM API 接口，可以轻松切换 Ollama 和其他云端&#x2F;本地模型。</li>
<li><strong>VS Code Extensions</strong>：例如 <code>Continue</code> 等插件可以配置为使用 Ollama 驱动的本地 LLM 进行代码补全、问答等。</li>
<li><strong>Web UIs</strong>：许多开源的本地 AI 聊天界面也支持 Ollama。</li>
</ul>
<h2 id="六、优势与应用场景"><a href="#六、优势与应用场景" class="headerlink" title="六、优势与应用场景"></a>六、优势与应用场景</h2><h3 id="6-1-优势"><a href="#6-1-优势" class="headerlink" title="6.1 优势"></a>6.1 优势</h3><ol>
<li><strong>数据隐私与安全</strong>：所有推理都在本地完成，敏感数据无需发送到外部云服务。</li>
<li><strong>成本效益</strong>：无需支付 API 调用费用，只需本地硬件的电力消耗。</li>
<li><strong>离线可用</strong>：模型下载后，可在没有互联网连接的情况下使用。</li>
<li><strong>模型定制与实验</strong>：Modelfile 提供了极大的灵活性，方便用户修改模型行为、尝试不同的提示词策略。</li>
<li><strong>易用性</strong>：简化了本地 LLM 部署的复杂性，降低了使用门槛。</li>
</ol>
<h3 id="6-2-应用场景"><a href="#6-2-应用场景" class="headerlink" title="6.2 应用场景"></a>6.2 应用场景</h3><ul>
<li><strong>本地开发与原型设计</strong>：开发者可以在本地快速测试和迭代基于 LLM 的应用程序。</li>
<li><strong>隐私敏感型应用</strong>：处理医疗、金融或个人敏感数据的企业和个人。</li>
<li><strong>研究与教育</strong>：学生和研究人员可以在受控环境中深入学习和实验 LLM。</li>
<li><strong>离线环境</strong>：在网络受限或无网络的场景下（如飞机、野外作业）使用 LLM。</li>
<li><strong>本地知识库问答</strong>：结合 RAG (Retrieval Augmented Generation) 技术，构建基于本地文档的智能问答系统。</li>
</ul>
<h2 id="七、限制与注意事项"><a href="#七、限制与注意事项" class="headerlink" title="七、限制与注意事项"></a>七、限制与注意事项</h2><ol>
<li><strong>硬件要求</strong>：运行 LLM 需要足够的内存 (RAM) 和计算资源 (CPU&#x2F;GPU)。较大的模型（如 7B, 13B, 70B 参数）对内存和 GPU 显存有较高要求。<ul>
<li><strong>RAM</strong>：通常建议至少 8GB RAM 用于小型模型，32GB+ 用于中型模型。</li>
<li><strong>GPU (VRAM)</strong>：GPU 加速效果显著，但需要足够的显存。例如，一个 7B 模型可能需要 4-6GB VRAM，13B 模型可能需要 8-10GB VRAM，而 70B 模型则需要 40GB+ VRAM。</li>
</ul>
</li>
<li><strong>性能</strong>：本地硬件的性能直接决定了推理速度。消费级 GPU 的性能通常无法与云端专业的 AI 加速器相媲美。</li>
<li><strong>模型选择</strong>：Ollama 支持的模型虽然众多，但并非所有开源 LLM 都能在 Ollama 中运行（需要转换为 GGUF 格式）。</li>
<li><strong>持续维护</strong>：Ollama 项目正在快速发展，新功能和模型不断涌现，但也可能存在一些 bug 或兼容性问题，需要社区支持。</li>
</ol>
<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>Ollama 是一个改变游戏规则的工具，它极大地简化了在本地运行大语言模型的过程。通过提供直观的命令行界面、强大的 Modelfile 定制功能和标准化的 REST API，Ollama 正在推动 LLM 技术的普及和应用。对于那些关注隐私、追求成本效益或希望进行深度模型实验的开发者和企业而言，Ollama 无疑是一个不可或缺的工具，它使得每个人都能在自己的机器上掌握 AI 的力量。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/e520aa0d37e4/">https://blog.tbf1211.xx.kg/e520aa0d37e4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/">代码生成</a><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-30.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/34476d6d7b8c/" title="Java 各版本新特性详解"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-08.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Java 各版本新特性详解</div></div><div class="info-2"><div class="info-item-1"> Java 作为一门历史悠久且持续演进的编程语言，自其诞生以来，便不断通过新版本的发布引入众多创新特性，以适应现代软件开发的需求。本文将详尽地剖析 Java 8 至今（直至 Java 21 作为当前主流 LTS 版本）各重要版本所带来的核心新特性，旨在帮助开发者理解这些特性如何提升开发效率、代码质量及程序性能。  核心思想：理解 Java 各版本的新特性，能够使开发者编写出更现代、更简洁、更高性能的代码，并有效利用 JVM 的最新优化。   一、Java 8 (LTS - 发布于 2014 年)Java 8 是 Java 发展史上的一个里程碑版本，引入了大量旨在提升生产力的新特性，尤其是在函数式编程和并发领域。 1.1 Lambda 表达式定义：Lambda 表达式提供了一种简洁的方式来表示可传递的匿名函数。它使得函数可以作为方法参数，并且使代码更加简洁、可读性更强。这实质上是支持了函数式编程范式。 语法：(parameters) -&gt; expression 或 (parameters) -&gt; &#123; statements; &#125; 示例 (Java)： ...</div></div></div></a><a class="pagination-related" href="/21a6ae00fb43/" title="哈希表负载因子详解(Load Factor)"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">哈希表负载因子详解(Load Factor)</div></div><div class="info-2"><div class="info-item-1"> 哈希表 (Hash Table) 是一种高效的数据结构，用于存储键值对 (key-value pairs)，提供快速的查找、插入和删除操作。它的核心思想是利用哈希函数 (Hash Function) 将键映射到数组的某个索引位置。然而，哈希表的性能高度依赖于负载因子 (Load Factor) 的管理，它在空间利用率、查找效率和再哈希 (Resizing&#x2F;Rehashing) 成本之间扮演着关键的平衡角色。  核心思想：负载因子衡量了哈希表的“满”程度，是决定何时以及如何调整哈希表大小的关键指标，直接影响其性能和资源消耗。   一、哈希表简介与冲突在深入了解负载因子之前，我们先回顾哈希表的基本概念和冲突问题。 1.1 哈希表工作原理哈希表使用一个数组（通常称为桶数组或槽数组）来存储数据。当需要插入一个键值对时：  哈希函数：对键进行哈希计算，得到一个哈希值。 取模运算：将哈希值与桶数组的长度取模，得到一个数组索引。 存储：将键值对存储到该索引位置。  1.2 哈希冲突 (Hash Collision)不同的键经过哈希函数计算后，可能会得到相同的哈希值，进而映射到桶数组...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/e62a1e8acade/" title="AI 辅助编程的关键要点与代码幻觉防范"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-16</div><div class="info-item-2">AI 辅助编程的关键要点与代码幻觉防范</div></div><div class="info-2"><div class="info-item-1"> AI 辅助编程，通常指利用大型语言模型 (LLM) 如 GPT、Claude、Copilot 等来帮助开发者完成代码生成、代码补全、错误检查、文档编写等任务。它极大地提高了开发效率，但同时也引入了新的挑战，其中最突出的就是 “代码幻觉 (Code Hallucinations)”。代码幻觉是指 AI 生成了看似合理但实际上错误、不存在、或与需求不符的代码、API 调用或概念。  核心思想：AI 是强大的工具而非万能的替代品。在使用 AI 辅助编程时，开发者必须保持批判性思维，通过有效的“提示工程”和严谨的“人工验证”来驾驭 AI，防止其产生误导性的“代码幻觉”。   一、AI 辅助编程的核心优势与风险1.1 核心优势 提高效率：快速生成样板代码、函数骨架、测试用例等，减少重复劳动。 知识获取：作为“超级Stack Overflow”，快速查询 API 用法、框架最佳实践、算法实现等。 学习辅助：解释复杂代码、概念，帮助新手快速理解。 跨语言&#x2F;框架能力：在不熟悉的语言或框架中提供初步帮助。 重构与优化建议：提出改进代码结构、性能或可读性的建议。  1.2 主要风险 代码...</div></div></div></a><a class="pagination-related" href="/6d28d801758d/" title="Claude Code 详解：Anthropic 的代码智能模型"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-27.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-05</div><div class="info-item-2">Claude Code 详解：Anthropic 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Claude Code 是 Anthropic 公司推出的官方 CLI 工具，专为软件工程设计。它将 Claude 的强大 AI 能力带入终端，帮助开发者完成代码编写、调试、重构、测试等各类编程任务。作为一款专业的 AI 编程助手，Claude Code 特别强调安全性，仅协助处理授权的安全测试、CTF 挑战等合法场景。  核心特性：Claude Code 通过自然语言对话完成复杂编程任务，支持多文件编辑、智能体任务、代码库探索等功能，是提升开发效率的利器。    一、Claude Code 概述1.1 什么是 Claude Code？Claude Code 是 Anthropic 推出的官方命令行界面 (CLI) 工具，它将 Claude AI 模型的能力带入开发者的终端环境。与传统的代码补全工具不同，Claude Code 是一个完整的 AI 编程助手，能够理解项目上下文、执行复杂任务、与文件系统交互，并生成高质量的代码。 作为 Claude Agent SDK 的官方实现，Claude Code 采用了专业的软件工程方法论，强调：  安全性优先：仅协助授权的安全测试和防御...</div></div></div></a><a class="pagination-related" href="/b100840425a8/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-24.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-07</div><div class="info-item-2">Codex 详解与使用技巧：OpenAI 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Codex 是由 OpenAI 训练的一个大型语言模型，其核心能力在于理解自然语言并将其转换为代码，或者理解代码并解释其含义。它是 GPT 系列模型的一个特化版本，专门针对编程语言进行了大量训练。Codex 不仅能生成 Python 代码，还能处理多种其他编程语言，是 OpenAI 在人工智能编程领域迈出的重要一步，也是 GitHub Copilot 等工具的基石。  核心思想：将自然语言描述的问题转化为可执行的代码，实现人机协作编程，降低编程门槛，提升开发效率。 掌握有效的指令（Prompt）是充分发挥 Codex 能力的关键。   一、Codex 的起源与核心能力Codex 的开发是基于 OpenAI 的 GPT-3 模型。GPT-3 以其强大的文本生成能力震惊业界，但其在代码生成方面虽然有一定表现，但仍缺乏专业性和精准度。为了弥补这一差距，OpenAI 进一步对 GPT-3 进行了微调，使用了海量的代码数据，最终诞生了 Codex。 1.1 背景：GPT-3 的局限性与代码生成的需求GPT-3 在零样本（zero-shot）和少样本（few-shot）学习方面表现出色，能...</div></div></div></a><a class="pagination-related" href="/21e140c78f80/" title="大型语言模型如何理解人类文字：从Token到语义表征"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-16</div><div class="info-item-2">大型语言模型如何理解人类文字：从Token到语义表征</div></div><div class="info-2"><div class="info-item-1"> 大型语言模型 (Large Language Models, LLMs) 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中统计模式和语义关联的深度学习，构建出高度复杂的语言表征。  核心思想：LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。   一、基础构建模块：从文本到向量LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于分词 (Tokenization) 和词嵌入 (Word Embeddings)。 1.1 分词 (Tokenization)分词是将连续的文本序列切分成有意义的最小单位——Token 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。  词级别分词 (Word-level Tokenization)：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-05.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">536</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">229</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">84</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Ollama-%E7%AE%80%E4%BB%8B"><span class="toc-text">一、Ollama 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Ollama-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">二、Ollama 的工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-text">2.1 架构概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Modelfiles-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6"><span class="toc-text">2.2 Modelfiles (模型文件)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85-Ollama"><span class="toc-text">三、安装 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">四、基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B-%E4%BA%A4%E4%BA%92%E5%BC%8F"><span class="toc-text">4.1 运行模型 (交互式)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.2 下载模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%88%97%E5%87%BA%E5%B7%B2%E5%AE%89%E8%A3%85%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.3 列出已安装模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%88%A0%E9%99%A4%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.4 删除模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%90%AF%E5%8A%A8-Ollama-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.5 启动 Ollama 服务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="toc-text">五、高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%AE%9A%E5%88%B6%E6%A8%A1%E5%9E%8B-Modelfiles"><span class="toc-text">5.1 定制模型 (Modelfiles)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BD%BF%E7%94%A8-REST-API-%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A8%8B%E4%BA%A4%E4%BA%92"><span class="toc-text">5.2 使用 REST API 进行编程交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%B8%8E%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90"><span class="toc-text">5.3 与其他工具集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">六、优势与应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%BC%98%E5%8A%BF"><span class="toc-text">6.1 优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">6.2 应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E9%99%90%E5%88%B6%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">七、限制与注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">八、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/8fc7e3e72510/" title="前端渲染模式：CSR, SSR, SSG, ISR, DPR 详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="前端渲染模式：CSR, SSR, SSG, ISR, DPR 详解"/></a><div class="content"><a class="title" href="/8fc7e3e72510/" title="前端渲染模式：CSR, SSR, SSG, ISR, DPR 详解">前端渲染模式：CSR, SSR, SSG, ISR, DPR 详解</a><time datetime="2026-01-27T22:24:00.000Z" title="发表于 2026-01-28 06:24:00">2026-01-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/5341a0037256/" title="CSS-in-JS 详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-27.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CSS-in-JS 详解"/></a><div class="content"><a class="title" href="/5341a0037256/" title="CSS-in-JS 详解">CSS-in-JS 详解</a><time datetime="2026-01-25T22:24:00.000Z" title="发表于 2026-01-26 06:24:00">2026-01-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/0b52cb819619/" title="Git 核心对象：Commit, Tree, Blob 详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Git 核心对象：Commit, Tree, Blob 详解"/></a><div class="content"><a class="title" href="/0b52cb819619/" title="Git 核心对象：Commit, Tree, Blob 详解">Git 核心对象：Commit, Tree, Blob 详解</a><time datetime="2026-01-21T22:24:00.000Z" title="发表于 2026-01-22 06:24:00">2026-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/1d2a942bda1e/" title="Terraform 详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Terraform 详解"/></a><div class="content"><a class="title" href="/1d2a942bda1e/" title="Terraform 详解">Terraform 详解</a><time datetime="2026-01-19T22:24:00.000Z" title="发表于 2026-01-20 06:24:00">2026-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/26cdb2447b3d/" title="WebView 详解"><img src= "/img/loading.gif" data-lazy-src="/img/cover/default_cover-28.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebView 详解"/></a><div class="content"><a class="title" href="/26cdb2447b3d/" title="WebView 详解">WebView 详解</a><time datetime="2026-01-17T22:24:00.000Z" title="发表于 2026-01-18 06:24:00">2026-01-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-30.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="🆕 2023">🆕 2023</a><a href="/archives/2024/" target="_blank" title="🆒 2024">🆒 2024</a><a href="/archives/2025/" target="_blank" title="👨‍👩‍👦 2025">👨‍👩‍👦 2025</a><a href="/archives/2026/" target="_blank" title="🆙 2026">🆙 2026</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="📁 分类">📁 分类</a><a href="/tags/" target="_blank" title="🔖 标签">🔖 标签</a><a href="/categories/" target="_blank" title="📽️ 时间线">📽️ 时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="💬 说说">💬 说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2026 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const mermaidThemeConfig = `%%{init:{ 'theme':'${theme}'}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      true
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>