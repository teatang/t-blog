<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Ollama 深度解析 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:type" content="article">
<meta property="og:title" content="Ollama 深度解析">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="Ollama 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-01.jpg">
<meta property="article:published_time" content="2025-03-02T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-18T06:51:26.840Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="代码生成">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-01.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Ollama 深度解析",
  "url": "https://blog.tbf1211.xx.kg/e520aa0d37e4/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-01.jpg",
  "datePublished": "2025-03-02T22:24:00.000Z",
  "dateModified": "2025-12-18T06:51:26.840Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/e520aa0d37e4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ollama 深度解析',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">388</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">213</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">76</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-01.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">Ollama 深度解析</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Ollama 深度解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-03-02T22:24:00.000Z" title="发表于 2025-03-03 06:24:00">2025-03-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/">开发工具</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">2.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="umamiPV" data-path="/e520aa0d37e4/"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>Ollama</strong> 是一个开源项目，旨在简化在本地机器上运行大型语言模型 (LLM) 的过程。它提供了一个易于使用的命令行界面和 API，让用户能够快速下载、运行、创建和管理各种预训练的开源 LLM，如 Llama 2, Mistral, Gemma 等。Ollama 专注于提供流畅的用户体验，使个人开发者、研究人员和企业能够在自己的硬件上，以隐私保护和成本效益的方式探索和利用 LLM 的强大功能。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将复杂的大语言模型本地化运行过程封装成简单命令，让用户能轻松部署、交互和定制开源 LLM，实现AI的民主化和去中心化。</strong></p>
</div>
<hr>
<h2 id="一、Ollama-简介"><a href="#一、Ollama-简介" class="headerlink" title="一、Ollama 简介"></a>一、Ollama 简介</h2><p>随着大语言模型技术的飞速发展，越来越多的开发者和企业希望在本地环境中运行这些模型，以实现数据隐私、降低成本、离线可用以及更灵活的定制化。然而，直接在本地部署和管理 LLM 往往涉及复杂的依赖安装、模型格式转换、GPU 配置等挑战。</p>
<p>Ollama 应运而生，它旨在解决这些痛点，提供一个“一站式”解决方案：</p>
<ul>
<li><strong>极简的用户体验</strong>：通过单个可执行文件和直观的命令行指令，即可完成模型的下载、运行和管理。</li>
<li><strong>广泛的模型支持</strong>：支持多种流行的开源 LLM，并且不断有新模型加入。</li>
<li><strong>GPU 加速</strong>：自动检测并利用本地的 GPU 资源 (如 NVIDIA CUDA, Apple Metal) 进行推理加速，显著提升性能。</li>
<li><strong>RESTful API</strong>：提供标准化的 API 接口，方便其他应用程序和框架（如 LangChain, LlamaIndex）集成。</li>
<li><strong>Modelfile 定制</strong>：允许用户通过简单的文本文件定制自己的模型，包括修改提示词、参数和模型行为。</li>
</ul>
<h2 id="二、Ollama-的工作原理"><a href="#二、Ollama-的工作原理" class="headerlink" title="二、Ollama 的工作原理"></a>二、Ollama 的工作原理</h2><p>Ollama 的核心是一个轻量级的服务器程序，负责管理模型的生命周期、处理推理请求以及与底层硬件进行交互。</p>
<h3 id="2-1-架构概述"><a href="#2-1-架构概述" class="headerlink" title="2.1 架构概述"></a>2.1 架构概述</h3><div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[用户&#x2F;客户端应用] --&gt;|命令行指令 或 REST API 请求| B(Ollama 服务)
    B --&gt; C{模型仓库}
    C --&gt; |&quot;下载模型 (GGUF)&quot;| B
    B --&gt; D[模型运行时]
    D --&gt; E[GPU &#x2F; CPU]
    E --&gt; |执行推理| D
    D --&gt; |返回结果| B
    B --&gt; |响应| A
  </pre></div>

<ol>
<li><strong>Ollama 服务 (Server)</strong>：这是 Ollama 的核心组件，通常在后台运行。它监听来自用户或客户端应用程序的请求。</li>
<li><strong>模型仓库 (Model Repository)</strong>：Ollama 维护着一个模型仓库，用户可以通过 <code>ollama pull</code> 命令从其中下载预训练模型。</li>
<li><strong>模型运行时 (Model Runtime)</strong>：Ollama 使用 <code>llama.cpp</code> 等高效的 C++ 推理引擎来加载和运行模型。</li>
<li><strong>硬件加速 (Hardware Acceleration)</strong>：Ollama 能够智能地利用本地的 GPU (如 NVIDIA CUDA, Apple Metal) 或 CPU 进行模型推理，显著提高效率。</li>
<li><strong>REST API</strong>：Ollama 服务默认在 <code>localhost:11434</code> 监听，提供标准的 REST API 接口，允许程序化地与模型进行交互。</li>
<li><strong>GGUF 格式</strong>：Ollama 使用 GGUF (GPT-Generated Unified Format) 格式的模型文件。GGUF 是一种高度优化的二进制格式，专为 CPU 和 GPU 上的高效推理而设计，支持多种量化级别，可以显著减少模型的内存占用和计算需求。</li>
</ol>
<h3 id="2-2-Modelfiles-模型文件"><a href="#2-2-Modelfiles-模型文件" class="headerlink" title="2.2 Modelfiles (模型文件)"></a>2.2 Modelfiles (模型文件)</h3><p>Modelfile 是 Ollama 中一个强大的功能，它允许用户基于现有模型创建自定义模型。通过 Modelfile，你可以：</p>
<ul>
<li>指定基础模型 (<code>FROM &lt;base_model&gt;</code>)。</li>
<li>设置模型参数 (<code>PARAMETER temperature 0.8</code>)。</li>
<li>定义系统提示词 (<code>SYSTEM &quot;You are a helpful assistant.&quot;</code>)。</li>
<li>添加自定义消息。</li>
<li>甚至可以组合不同的模型层。</li>
</ul>
<h2 id="三、安装-Ollama"><a href="#三、安装-Ollama" class="headerlink" title="三、安装 Ollama"></a>三、安装 Ollama</h2><p>Ollama 支持 macOS、Linux 和 Windows (通过 WSL2)。安装过程非常简单，通常只需下载并运行一个可执行文件或通过包管理器安装。</p>
<p><strong>官方下载地址</strong>：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://ollama.com/download">https://ollama.com/download</a></p>
<p><strong>示例 (Linux)</strong>：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></table></figure>
<p>安装后，Ollama 服务通常会自动启动并在后台运行。</p>
<h2 id="四、基本使用"><a href="#四、基本使用" class="headerlink" title="四、基本使用"></a>四、基本使用</h2><h3 id="4-1-运行模型-交互式"><a href="#4-1-运行模型-交互式" class="headerlink" title="4.1 运行模型 (交互式)"></a>4.1 运行模型 (交互式)</h3><p>下载并运行一个模型最简单的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama2</span><br></pre></td></tr></table></figure>
<p>这会先检查本地是否有 <code>llama2</code> 模型。如果没有，它会自动下载模型文件，然后启动一个交互式的聊天会话。</p>
<h3 id="4-2-下载模型"><a href="#4-2-下载模型" class="headerlink" title="4.2 下载模型"></a>4.2 下载模型</h3><p>显式下载一个模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull mistral</span><br></pre></td></tr></table></figure>
<p>你也可以指定模型版本，例如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama pull llama2:13b</span><br></pre></td></tr></table></figure>

<h3 id="4-3-列出已安装模型"><a href="#4-3-列出已安装模型" class="headerlink" title="4.3 列出已安装模型"></a>4.3 列出已安装模型</h3><p>查看本地已下载的所有模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama list</span><br></pre></td></tr></table></figure>

<h3 id="4-4-删除模型"><a href="#4-4-删除模型" class="headerlink" title="4.4 删除模型"></a>4.4 删除模型</h3><p>删除不再需要的模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama <span class="built_in">rm</span> llama2</span><br></pre></td></tr></table></figure>

<h3 id="4-5-启动-Ollama-服务"><a href="#4-5-启动-Ollama-服务" class="headerlink" title="4.5 启动 Ollama 服务"></a>4.5 启动 Ollama 服务</h3><p>如果你需要手动启动 Ollama 服务（例如在服务器环境中），可以使用：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></table></figure>
<p>此命令会阻塞当前终端，但 Ollama 服务将在 <code>localhost:11434</code> 上运行，接受 API 请求。</p>
<h2 id="五、高级用法"><a href="#五、高级用法" class="headerlink" title="五、高级用法"></a>五、高级用法</h2><h3 id="5-1-定制模型-Modelfiles"><a href="#5-1-定制模型-Modelfiles" class="headerlink" title="5.1 定制模型 (Modelfiles)"></a>5.1 定制模型 (Modelfiles)</h3><p>你可以通过创建 Modelfile 来定制现有模型。</p>
<p><strong>示例 Modelfile (名为 <code>MyAssistant</code>)</strong>：<br>创建一个名为 <code>Modelfile</code> 的文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM mistral</span><br><span class="line">PARAMETER temperature 0.7</span><br><span class="line">SYSTEM &quot;&quot;&quot;你是一个严谨且乐于助人的中文助手。</span><br><span class="line">你的回答需要清晰、准确，并尽量提供详细的解释。&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>

<p>然后使用 <code>ollama create</code> 命令基于这个 Modelfile 创建新模型：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama create my-assistant -f ./Modelfile</span><br></pre></td></tr></table></figure>
<p>现在你可以运行你的定制模型了：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run my-assistant</span><br></pre></td></tr></table></figure>

<h3 id="5-2-使用-REST-API-进行编程交互"><a href="#5-2-使用-REST-API-进行编程交互" class="headerlink" title="5.2 使用 REST API 进行编程交互"></a>5.2 使用 REST API 进行编程交互</h3><p>Ollama 提供了一个 RESTful API，方便集成到其他应用程序中。默认服务地址为 <code>http://localhost:11434</code>。</p>
<p><strong>Go 语言代码示例</strong>：向 Ollama 发送生成请求。</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">&quot;bytes&quot;</span></span><br><span class="line">	<span class="string">&quot;encoding/json&quot;</span></span><br><span class="line">	<span class="string">&quot;fmt&quot;</span></span><br><span class="line">	<span class="string">&quot;io/ioutil&quot;</span></span><br><span class="line">	<span class="string">&quot;net/http&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateRequest represents the request payload for Ollama&#x27;s /api/generate endpoint</span></span><br><span class="line"><span class="keyword">type</span> GenerateRequest <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model  <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	Prompt <span class="type">string</span> <span class="string">`json:&quot;prompt&quot;`</span></span><br><span class="line">	Stream <span class="type">bool</span>   <span class="string">`json:&quot;stream&quot;`</span> <span class="comment">// Set to false for a single response</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// GenerateResponse represents the basic response structure</span></span><br><span class="line"><span class="keyword">type</span> GenerateResponse <span class="keyword">struct</span> &#123;</span><br><span class="line">	Model     <span class="type">string</span> <span class="string">`json:&quot;model&quot;`</span></span><br><span class="line">	CreatedAt <span class="type">string</span> <span class="string">`json:&quot;created_at&quot;`</span></span><br><span class="line">	Response  <span class="type">string</span> <span class="string">`json:&quot;response&quot;`</span></span><br><span class="line">	Done      <span class="type">bool</span>   <span class="string">`json:&quot;done&quot;`</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">	ollamaURL := <span class="string">&quot;http://localhost:11434/api/generate&quot;</span></span><br><span class="line">	modelName := <span class="string">&quot;llama2&quot;</span> <span class="comment">// 确保本地已 pull llama2 模型</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">// 构造请求体</span></span><br><span class="line">	requestPayload := GenerateRequest&#123;</span><br><span class="line">		Model:  modelName,</span><br><span class="line">		Prompt: <span class="string">&quot;What is the capital of France?&quot;</span>,</span><br><span class="line">		Stream: <span class="literal">false</span>,</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	jsonPayload, err := json.Marshal(requestPayload)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error marshalling JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 发送 POST 请求</span></span><br><span class="line">	resp, err := http.Post(ollamaURL, <span class="string">&quot;application/json&quot;</span>, bytes.NewBuffer(jsonPayload))</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error sending request to Ollama: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">defer</span> resp.Body.Close()</span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> resp.StatusCode != http.StatusOK &#123;</span><br><span class="line">		bodyBytes, _ := ioutil.ReadAll(resp.Body)</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Ollama API returned non-OK status: %s, Body: %s\n&quot;</span>, resp.Status, <span class="type">string</span>(bodyBytes))</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 读取并解析响应</span></span><br><span class="line">	body, err := ioutil.ReadAll(resp.Body)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error reading response body: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">var</span> generateResponse GenerateResponse</span><br><span class="line">	err = json.Unmarshal(body, &amp;generateResponse)</span><br><span class="line">	<span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">		fmt.Printf(<span class="string">&quot;Error unmarshalling response JSON: %v\n&quot;</span>, err)</span><br><span class="line">		<span class="keyword">return</span></span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	fmt.Printf(<span class="string">&quot;Model: %s\n&quot;</span>, generateResponse.Model)</span><br><span class="line">	fmt.Printf(<span class="string">&quot;Response: %s\n&quot;</span>, generateResponse.Response)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：上述 Go 代码是一个简化示例，未处理 <code>stream: true</code> 模式下的分块响应。</p>
<h3 id="5-3-与其他工具集成"><a href="#5-3-与其他工具集成" class="headerlink" title="5.3 与其他工具集成"></a>5.3 与其他工具集成</h3><p>Ollama 作为本地 LLM 运行时的核心，已被广泛集成到各种 AI 开发工具和框架中：</p>
<ul>
<li><strong>LangChain</strong> 和 <strong>LlamaIndex</strong>：直接支持 Ollama 作为本地推理后端。</li>
<li><strong>LiteLLM</strong>：一个统一的 LLM API 接口，可以轻松切换 Ollama 和其他云端&#x2F;本地模型。</li>
<li><strong>VS Code Extensions</strong>：例如 <code>Continue</code> 等插件可以配置为使用 Ollama 驱动的本地 LLM 进行代码补全、问答等。</li>
<li><strong>Web UIs</strong>：许多开源的本地 AI 聊天界面也支持 Ollama。</li>
</ul>
<h2 id="六、优势与应用场景"><a href="#六、优势与应用场景" class="headerlink" title="六、优势与应用场景"></a>六、优势与应用场景</h2><h3 id="6-1-优势"><a href="#6-1-优势" class="headerlink" title="6.1 优势"></a>6.1 优势</h3><ol>
<li><strong>数据隐私与安全</strong>：所有推理都在本地完成，敏感数据无需发送到外部云服务。</li>
<li><strong>成本效益</strong>：无需支付 API 调用费用，只需本地硬件的电力消耗。</li>
<li><strong>离线可用</strong>：模型下载后，可在没有互联网连接的情况下使用。</li>
<li><strong>模型定制与实验</strong>：Modelfile 提供了极大的灵活性，方便用户修改模型行为、尝试不同的提示词策略。</li>
<li><strong>易用性</strong>：简化了本地 LLM 部署的复杂性，降低了使用门槛。</li>
</ol>
<h3 id="6-2-应用场景"><a href="#6-2-应用场景" class="headerlink" title="6.2 应用场景"></a>6.2 应用场景</h3><ul>
<li><strong>本地开发与原型设计</strong>：开发者可以在本地快速测试和迭代基于 LLM 的应用程序。</li>
<li><strong>隐私敏感型应用</strong>：处理医疗、金融或个人敏感数据的企业和个人。</li>
<li><strong>研究与教育</strong>：学生和研究人员可以在受控环境中深入学习和实验 LLM。</li>
<li><strong>离线环境</strong>：在网络受限或无网络的场景下（如飞机、野外作业）使用 LLM。</li>
<li><strong>本地知识库问答</strong>：结合 RAG (Retrieval Augmented Generation) 技术，构建基于本地文档的智能问答系统。</li>
</ul>
<h2 id="七、限制与注意事项"><a href="#七、限制与注意事项" class="headerlink" title="七、限制与注意事项"></a>七、限制与注意事项</h2><ol>
<li><strong>硬件要求</strong>：运行 LLM 需要足够的内存 (RAM) 和计算资源 (CPU&#x2F;GPU)。较大的模型（如 7B, 13B, 70B 参数）对内存和 GPU 显存有较高要求。<ul>
<li><strong>RAM</strong>：通常建议至少 8GB RAM 用于小型模型，32GB+ 用于中型模型。</li>
<li><strong>GPU (VRAM)</strong>：GPU 加速效果显著，但需要足够的显存。例如，一个 7B 模型可能需要 4-6GB VRAM，13B 模型可能需要 8-10GB VRAM，而 70B 模型则需要 40GB+ VRAM。</li>
</ul>
</li>
<li><strong>性能</strong>：本地硬件的性能直接决定了推理速度。消费级 GPU 的性能通常无法与云端专业的 AI 加速器相媲美。</li>
<li><strong>模型选择</strong>：Ollama 支持的模型虽然众多，但并非所有开源 LLM 都能在 Ollama 中运行（需要转换为 GGUF 格式）。</li>
<li><strong>持续维护</strong>：Ollama 项目正在快速发展，新功能和模型不断涌现，但也可能存在一些 bug 或兼容性问题，需要社区支持。</li>
</ol>
<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>Ollama 是一个改变游戏规则的工具，它极大地简化了在本地运行大语言模型的过程。通过提供直观的命令行界面、强大的 Modelfile 定制功能和标准化的 REST API，Ollama 正在推动 LLM 技术的普及和应用。对于那些关注隐私、追求成本效益或希望进行深度模型实验的开发者和企业而言，Ollama 无疑是一个不可或缺的工具，它使得每个人都能在自己的机器上掌握 AI 的力量。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/e520aa0d37e4/">https://blog.tbf1211.xx.kg/e520aa0d37e4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/">代码生成</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-01.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/fc0a62ae56f9/" title="OAuth2.0详解：现代授权框架的核心原理与应用"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">OAuth2.0详解：现代授权框架的核心原理与应用</div></div><div class="info-2"><div class="info-item-1"> OAuth 2.0（Open Authorization）是一个开放标准，定义了一套授权流程，允许用户（资源所有者）授权第三方应用访问他们在另一个服务提供者（授权服务器）上的受保护资源（资源服务器），而无需将自己的用户名和密码直接提供给第三方应用。它主要解决的是委托授权的问题，即“我授权应用A去访问我在服务B上的某些数据”。  核心区分：OAuth 2.0 是一个授权（Authorization）框架，而不是用来做认证（Authentication）。尽管它常常与认证机制（如 OpenID Connect）结合使用，但其核心职责是授予对资源的访问权限，而非验证用户身份。   一、OAuth 2.0 产生的背景与解决的问题在 OAuth 出现之前，如果一个第三方应用需要访问用户在其他服务（如 Google 相册、GitHub 代码库）上的数据，用户通常需要将自己的账号密码直接告知第三方应用。这种做法带来了严重的安全和便捷性问题：  凭据泄露风险：第三方应用一旦被攻破，或恶意使用，用户的完整凭据就会泄露，导致所有关联服务面临风险。 权限过大：第三方应用获得的是用户的完全控制权，无法...</div></div></div></a><a class="pagination-related" href="/21a6ae00fb43/" title="哈希表负载因子详解(Load Factor)"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">哈希表负载因子详解(Load Factor)</div></div><div class="info-2"><div class="info-item-1"> 哈希表 (Hash Table) 是一种高效的数据结构，用于存储键值对 (key-value pairs)，提供快速的查找、插入和删除操作。它的核心思想是利用哈希函数 (Hash Function) 将键映射到数组的某个索引位置。然而，哈希表的性能高度依赖于负载因子 (Load Factor) 的管理，它在空间利用率、查找效率和再哈希 (Resizing&#x2F;Rehashing) 成本之间扮演着关键的平衡角色。  核心思想：负载因子衡量了哈希表的“满”程度，是决定何时以及如何调整哈希表大小的关键指标，直接影响其性能和资源消耗。   一、哈希表简介与冲突在深入了解负载因子之前，我们先回顾哈希表的基本概念和冲突问题。 1.1 哈希表工作原理哈希表使用一个数组（通常称为桶数组或槽数组）来存储数据。当需要插入一个键值对时：  哈希函数：对键进行哈希计算，得到一个哈希值。 取模运算：将哈希值与桶数组的长度取模，得到一个数组索引。 存储：将键值对存储到该索引位置。  1.2 哈希冲突 (Hash Collision)不同的键经过哈希函数计算后，可能会得到相同的哈希值，进而映射到桶数组...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/6d28d801758d/" title="Claude Code 详解：Anthropic 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-23.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-05</div><div class="info-item-2">Claude Code 详解：Anthropic 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Claude Code 是 Anthropic 公司开发的 Claude 系列大型语言模型在代码理解、生成和辅助方面的一个特定能力集合或优化方向。Anthropic 以其在 AI 安全和可控性方面的研究而闻名，Claude 模型旨在成为一个有益、无害且诚实的 AI 助手。因此，Claude Code 不仅关注代码的正确性，也强调生成代码的安全性、可读性和遵循最佳实践。  核心思想：结合 Anthropic 的安全和伦理原则，提供安全、有益、高质量的代码生成与辅助能力，旨在成为开发者的“无害”智能编程伙伴。   一、Claude Code 的背景与 Anthropic 理念Anthropic 由前 OpenAI 员工创立，致力于开发安全、可控且有益的人工智能系统。其核心产品 Claude 语言模型系列被设计为更易于对齐人类价值观，并通过“宪法 AI (Constitutional AI)”等方法进行训练，减少有害、偏见或不真实内容的生成。 在代码领域，这种理念意味着 Claude Code 不仅仅是生成能运行的代码，更关注：  安全性：避免生成包含已知漏洞或不良安全实践的代码。 ...</div></div></div></a><a class="pagination-related" href="/b100840425a8/" title="Codex 详解与使用技巧：OpenAI 的代码智能模型"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-13.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-07</div><div class="info-item-2">Codex 详解与使用技巧：OpenAI 的代码智能模型</div></div><div class="info-2"><div class="info-item-1"> Codex 是由 OpenAI 训练的一个大型语言模型，其核心能力在于理解自然语言并将其转换为代码，或者理解代码并解释其含义。它是 GPT 系列模型的一个特化版本，专门针对编程语言进行了大量训练。Codex 不仅能生成 Python 代码，还能处理多种其他编程语言，是 OpenAI 在人工智能编程领域迈出的重要一步，也是 GitHub Copilot 等工具的基石。  核心思想：将自然语言描述的问题转化为可执行的代码，实现人机协作编程，降低编程门槛，提升开发效率。 掌握有效的指令（Prompt）是充分发挥 Codex 能力的关键。   一、Codex 的起源与核心能力Codex 的开发是基于 OpenAI 的 GPT-3 模型。GPT-3 以其强大的文本生成能力震惊业界，但其在代码生成方面虽然有一定表现，但仍缺乏专业性和精准度。为了弥补这一差距，OpenAI 进一步对 GPT-3 进行了微调，使用了海量的代码数据，最终诞生了 Codex。 1.1 背景：GPT-3 的局限性与代码生成的需求GPT-3 在零样本（zero-shot）和少样本（few-shot）学习方面表现出色，能...</div></div></div></a><a class="pagination-related" href="/9ceda85c30d6/" title="向量嵌入 (Vector Embeddings) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-03.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-05</div><div class="info-item-2">向量嵌入 (Vector Embeddings) 详解</div></div><div class="info-2"><div class="info-item-1"> 向量嵌入 (Vector Embeddings) 是人工智能和机器学习领域的一个核心概念，它指的是将复杂的数据对象（如文本、图像、音频、图形节点、用户行为等）映射到高维实数向量空间中的一种技术。在这个向量空间中，语义或功能上相似的数据对象会映射到彼此接近的向量点。  通过向量嵌入，我们可以将非结构化数据转化为机器可理解和处理的数值形式，并且能够通过计算向量之间的距离来量化数据对象之间的相似性。它是许多现代AI应用（如推荐系统、搜索引擎、自然语言处理、图像识别等）的基石。   一、为什么需要向量嵌入？传统上，机器处理数据的方式通常是基于符号匹配或离散的分类。然而，这种方式在处理复杂、非结构化数据时面临诸多局限：  语义鸿沟 (Semantic Gap)：计算机无法直接理解词语、句子、图像甚至用户偏好背后的“含义”。例如，“汽车”和“车辆”在语义上相近，但在符号匹配中是不同的字符串。 高维稀疏性 (High-Dimensional Sparsity)：传统的 One-Hot 编码等方法会产生维度极高且稀疏的向量，这不仅浪费存储和计算资源，而且无法捕捉词语之间的关系。 计算复杂性：直...</div></div></div></a><a class="pagination-related" href="/289566312b0f/" title="Vision Transformer (ViT) 与 Residual Network (ResNet) 深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-30.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-16</div><div class="info-item-2">Vision Transformer (ViT) 与 Residual Network (ResNet) 深度详解</div></div><div class="info-2"><div class="info-item-1"> 在深度学习的计算机视觉领域，卷积神经网络 (CNN) 曾长期占据主导地位，而 Residual Network (ResNet) 则是其中一个里程碑式的创新，通过引入残差连接解决了深层网络训练中的梯度消失问题。近年来，随着 Transformer 模型在自然语言处理 (NLP) 领域取得巨大成功，研究人员尝试将其引入视觉领域，催生了 Vision Transformer (ViT)。ViT 颠覆了传统 CNN 的范式，直接将图像视为一系列序列化的图像块 (patches)，并用 Transformer 编码器进行处理。本文将对这两大具有代表性的模型进行深入剖析和比较。  ResNet 的核心思想： 通过残差连接 (Residual Connection) 允许网络学习残差函数，使得训练极深的网络变得可能，从而有效缓解了深度神经网络中的梯度消失和梯度爆炸问题，提高了模型性能。 ViT 的核心思想： 放弃了 CNN 的归纳偏置 (inductive bias)，直接将图像分割成固定大小的图像块 (patches)，并将其视为序列化的词向量 (tokens)，然后输入标准的 Tran...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-15.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/52b6fb1ec8d5/" title="文档嵌入模型 (Document Embedding Models) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-18.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-08</div><div class="info-item-2">文档嵌入模型 (Document Embedding Models) 详解</div></div><div class="info-2"><div class="info-item-1"> 文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的文档在多维向量空间中彼此靠近。这是构建高级信息检索、知识管理和内容理解系统的基石。   一、为什么需要文档嵌入模型？在大数据时代，我们面临着海量文档（如网页、报告、书籍、代码库、用户评论等）。传统处理这些文档的方法存在诸多局限：  关键词匹配的不足：搜索引擎通常依赖关键词匹配，但无法理解语义。例如，搜索“车祸”可能无法找到包含“交通事故”的文档。 句嵌入的局限性：虽然句嵌入能捕捉句子级别的语义，但在处理长文档时，简单地拼接或平均句嵌入会丢失文档整体的结构和主题信息。 高维稀疏性问题：传统的 Bag-of-Words (BOW) 或 TF-IDF 等模型将文档表示为高维稀疏向量，不仅计算效...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">388</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">213</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">76</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81Ollama-%E7%AE%80%E4%BB%8B"><span class="toc-text">一、Ollama 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81Ollama-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">二、Ollama 的工作原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0"><span class="toc-text">2.1 架构概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Modelfiles-%E6%A8%A1%E5%9E%8B%E6%96%87%E4%BB%B6"><span class="toc-text">2.2 Modelfiles (模型文件)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%AE%89%E8%A3%85-Ollama"><span class="toc-text">三、安装 Ollama</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">四、基本使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B-%E4%BA%A4%E4%BA%92%E5%BC%8F"><span class="toc-text">4.1 运行模型 (交互式)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.2 下载模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E5%88%97%E5%87%BA%E5%B7%B2%E5%AE%89%E8%A3%85%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.3 列出已安装模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%88%A0%E9%99%A4%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.4 删除模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E5%90%AF%E5%8A%A8-Ollama-%E6%9C%8D%E5%8A%A1"><span class="toc-text">4.5 启动 Ollama 服务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%AB%98%E7%BA%A7%E7%94%A8%E6%B3%95"><span class="toc-text">五、高级用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%AE%9A%E5%88%B6%E6%A8%A1%E5%9E%8B-Modelfiles"><span class="toc-text">5.1 定制模型 (Modelfiles)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BD%BF%E7%94%A8-REST-API-%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A8%8B%E4%BA%A4%E4%BA%92"><span class="toc-text">5.2 使用 REST API 进行编程交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%B8%8E%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7%E9%9B%86%E6%88%90"><span class="toc-text">5.3 与其他工具集成</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">六、优势与应用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%BC%98%E5%8A%BF"><span class="toc-text">6.1 优势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">6.2 应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E9%99%90%E5%88%B6%E4%B8%8E%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-text">七、限制与注意事项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">八、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-05.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPFS (InterPlanetary File System) 详解"/></a><div class="content"><a class="title" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解">IPFS (InterPlanetary File System) 详解</a><time datetime="2025-12-16T22:24:00.000Z" title="发表于 2025-12-17 06:24:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-04.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="智能体 (Agent) 详解：深入 LangChain 开发实践"/></a><div class="content"><a class="title" href="/bfcc84247c6a/" title="智能体 (Agent) 详解：深入 LangChain 开发实践">智能体 (Agent) 详解：深入 LangChain 开发实践</a><time datetime="2025-12-10T22:24:00.000Z" title="发表于 2025-12-11 06:24:00">2025-12-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-14.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Bun.js 深度解析：冷启动与边缘函数优化"/></a><div class="content"><a class="title" href="/28f993be5cfe/" title="Bun.js 深度解析：冷启动与边缘函数优化">Bun.js 深度解析：冷启动与边缘函数优化</a><time datetime="2025-12-07T22:24:00.000Z" title="发表于 2025-12-08 06:24:00">2025-12-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-15.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go Jaeger 深度解析：分布式追踪实践"/></a><div class="content"><a class="title" href="/bcbe0f78ef1d/" title="Go Jaeger 深度解析：分布式追踪实践">Go Jaeger 深度解析：分布式追踪实践</a><time datetime="2025-12-04T22:24:00.000Z" title="发表于 2025-12-05 06:24:00">2025-12-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/15920229f914/" title="Supabase 深度解析"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Supabase 深度解析"/></a><div class="content"><a class="title" href="/15920229f914/" title="Supabase 深度解析">Supabase 深度解析</a><time datetime="2025-12-02T22:24:00.000Z" title="发表于 2025-12-03 06:24:00">2025-12-03</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-01.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><script>(() => {
  const option = null
  const config = {"site_uv":true,"site_pv":true,"page_pv":true,"token":"qTvz1SkmPDt785fgh6BpiA5qiFFIVUwxj8Ft+rPW+cdN59v1hXjwRgSmy0+ji9m+oLlcxvo2NfDSMa6epVl3NTlsN3ejCIwWeP8Y51aEJ0Sbem4UexGmJLspB7AkOBId2SdtT6QWEBlGmFIIQgchQ2zAKYxTmc/kpBED5aLSr+3uvmQ9/G7FJQeVFpveDkK0xM1hu36xq4a6/FSeROxtoEp5zabzTWiYTlLsQzIl/NlELnCq3nxK+oo/vl3UQo/oM/rae/gJX/MaVKsgIUCd2ABJogNkx2KTenBIBpbPki5FzOgPh6/z4GPa4HvhNO51DDVG1SEQZooqEYmt/gnybLBWFbN+7liZWw=="}

  const runTrack = () => {
    if (typeof umami !== 'undefined' && typeof umami.track === 'function') {
      umami.track(props => ({ ...props, url: window.location.pathname, title: GLOBAL_CONFIG_SITE.title }))
    } else {
      console.warn('Umami Analytics: umami.track is not available')
    }
  }

  const loadUmamiJS = () => {
    btf.getScript('https://umami.012700.xyz/script.js', {
      'data-website-id': '2a796d6c-6499-42d8-8eb4-d9a2930b0ff3',
      'data-auto-track': 'false',
      ...option
    }).then(() => {
      runTrack()
    }).catch(error => {
      console.error('Umami Analytics: Error loading script', error)
    })
  }

  const getData = async (isPost) => {
    try {
      const now = Date.now()
      const keyUrl = isPost ? `&url=${window.location.pathname}&path=${window.location.pathname}` : ''
      const headerList = { 'Accept': 'application/json' }

      if (true) {
        headerList['Authorization'] = `Bearer ${config.token}`
      } else {
        headerList['x-umami-api-key'] = config.token
      }

      const res = await fetch(`https://umami.012700.xyz/api/websites/2a796d6c-6499-42d8-8eb4-d9a2930b0ff3/stats?startAt=0000000000&endAt=${now}${keyUrl}`, {
        method: "GET",
        headers: headerList
      })

      if (!res.ok) {
        throw new Error(`HTTP error! status: ${res.status}`)
      }

      return await res.json()
    } catch (error) {
      console.error('Umami Analytics: Failed to fetch data', error)
      throw error
    }
  }

  const insertData = async () => {
    try {
      if (GLOBAL_CONFIG_SITE.pageType === 'post' && config.page_pv) {
        const pagePV = document.getElementById('umamiPV')
        if (pagePV) {
          const data = await getData(true)
          if (data && data.pageviews) {
            pagePV.textContent = typeof data.pageviews.value !== 'undefined' ? data.pageviews.value : data.pageviews
          } else {
            console.warn('Umami Analytics: Invalid page view data received')
          }
        }
      }

      if (config.site_uv || config.site_pv) {
        const data = await getData(false)

        if (config.site_uv) {
          const siteUV = document.getElementById('umami-site-uv')
          if (siteUV && data && data.visitors) {
            siteUV.textContent = typeof data.visitors.value !== 'undefined' ? data.visitors.value : data.visitors
          } else if (siteUV) {
            console.warn('Umami Analytics: Invalid site UV data received')
          }
        }

        if (config.site_pv) {
          const sitePV = document.getElementById('umami-site-pv')
          if (sitePV && data && data.pageviews) {
            sitePV.textContent = typeof data.pageviews.value !== 'undefined' ? data.pageviews.value : data.pageviews
          } else if (sitePV) {
            console.warn('Umami Analytics: Invalid site PV data received')
          }
        }
      }
    } catch (error) {
      console.error('Umami Analytics: Failed to insert data', error)
    }
  }

  btf.addGlobalFn('pjaxComplete', runTrack, 'umami_analytics_run_track')
  btf.addGlobalFn('pjaxComplete', insertData, 'umami_analytics_insert')


  loadUmamiJS()

  if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', insertData)
  } else {
    setTimeout(insertData, 100)
  }
})()</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>