---
title: 大型语言模型如何理解人类文字：从Token到语义表征
date: 2025-01-16 06:24:00
tags:
  - 2025
  - AI
  - LLM
categories:
  - AI
  - LLM
mathjax: true
---

> **大型语言模型 (Large Language Models, LLMs)** 在处理和生成人类语言方面展现出了前所未有的能力，这引发了一个核心问题：它们是如何“理解”人类文字的？这种理解并非传统意义上的认知或意识，而是通过对海量文本数据中**统计模式和语义关联**的深度学习，构建出高度复杂的语言表征。

{% note info %}
核心思想：**LLMs 将人类语言转化为高维数学向量，并通过 Transformer 架构中的注意力机制，捕捉词语、句子乃至篇章间的复杂关联，从而在统计层面模拟人类对语言的理解和生成。**
{% endnote %}
------

## 一、基础构建模块：从文本到向量

LLMs 的“理解”始于将人类可读的文字转化为机器可处理的数值形式。这一过程主要依赖于**分词 (Tokenization)** 和**词嵌入 (Word Embeddings)**。

### 1.1 分词 (Tokenization)

分词是将连续的文本序列切分成有意义的最小单位——**Token** 的过程。Token 可以是一个词、一个子词 (subword) 甚至一个字符。

*   **词级别分词 (Word-level Tokenization)**：以空格或标点符号为界，将文本切分为词。简单直观，但词汇量庞大，且难以处理未登录词 (Out-Of-Vocabulary, OOV)。
*   **子词级别分词 (Subword-level Tokenization)**：更常用的方法，如 Byte Pair Encoding (BPE)、WordPiece、SentencePiece。它将不常见的词分解成更小的、常见的子词单元，有效平衡了词汇量和 OOV 问题，同时保留了词语的形态学信息。例如，“unbelievable” 可能被分解为 “un”, “believe”, “able”。
*   **字符级别分词 (Character-level Tokenization)**：将每个字符视为一个 Token。词汇量最小，但丢失了大部分语义信息。

**示例 (Go 语言简化的子词分词概念)**：

```go
package main

import (
	"fmt"
	"strings"
)

// SimplifiedSubwordTokenizer 模拟子词分词的概念
// 实际的子词分词器（如BPE）要复杂得多，涉及频率统计和迭代合并
func SimplifiedSubwordTokenizer(text string) []string {
	// 简单的预处理：将标点符号与词分离，并转换为小写
	text = strings.ReplaceAll(text, ".", " .")
	text = strings.ReplaceAll(text, ",", " ,")
	text = strings.ToLower(text)

	// 分割成基础词/子词单元
	words := strings.Fields(text)
	var tokens []string

	// 模拟子词分解规则（非常简化）
	for _, word := range words {
		if strings.HasPrefix(word, "un") && len(word) > 2 {
			tokens = append(tokens, "un", strings.TrimPrefix(word, "un"))
		} else if strings.Contains(word, "ing") && len(word) > 3 {
			// 如果词汇包含 'ing' 且不是它本身
			parts := strings.Split(word, "ing")
			if len(parts[0]) > 0 {
				tokens = append(tokens, parts[0])
			}
			tokens = append(tokens, "ing")
			if len(parts) > 1 && len(parts[1]) > 0 { // 处理 'ing' 后还有内容的情况
				tokens = append(tokens, parts[1])
			}
		} else {
			tokens = append(tokens, word)
		}
	}
	return tokens
}

func main() {
	sentence := "The unbelievable cat is running quickly."
	tokens := SimplifiedSubwordTokenizer(sentence)
	fmt.Println("Original sentence:", sentence)
	fmt.Println("Simplified tokens:", tokens)
	// 预期输出可能类似：
	// Original sentence: The unbelievable cat is running quickly.
	// Simplified tokens: [the un believe able cat is run ing quick ly .]
	// (注意：这是高度简化的，实际BPE会更复杂和精确)
}
```
**解释**：上述 Go 代码展示了一个极度简化的分词概念。在实际的LLMs中，分词器会基于一个预训练的词汇表，通过复杂的算法（如BPE）将文本切分为模型能够处理的Token序列。

### 1.2 词嵌入 (Word Embeddings) / 向量表示

在分词之后，每个 Token 都需要被转换成一个**数值向量**。这个向量就是 Token 的“嵌入”(Embedding)。

*   **高维向量**：嵌入向量通常是几百到几千维的浮点数。
*   **语义信息**：这些向量的特点是，语义相似的词（例如 "king" 和 "queen"）在向量空间中会彼此靠近，而语义不相关的词则相距较远。这种距离关系反映了词语的语义关联。
*   **上下文感知**：现代的词嵌入（如ELMo, BERT, GPT中的嵌入）是**上下文感知**的，意味着同一个词在不同的语境下会有不同的嵌入向量。例如，“bank” 在表示“银行”时和表示“河岸”时，其嵌入向量是不同的。

**数学表示**：
对于一个词 $w$，其嵌入向量表示为 $\mathbf{e}_w \in \mathbb{R}^d$，其中 $d$ 是嵌入向量的维度。

{% mermaid %}
graph TD
    A[人类文字] --> B["分词 (Tokenization)"]
    B --> C{Token序列}
    C --> D["词嵌入 (Word Embeddings)"]
    D --> E["高维向量序列 (数值表示)"]
{% endmermaid %}

## 二、Transformer 架构：理解语言的核心

LLMs “理解”能力的核心是其采用的 **Transformer 架构**，特别是其中的**自注意力机制 (Self-Attention Mechanism)**。

### 2.1 Transformer 概览

Transformer 架构摒弃了传统的循环神经网络 (RNN) 和卷积神经网络 (CNN)，完全依赖于注意力机制来处理序列数据。它能够并行处理输入序列，大大提高了训练效率，并能有效捕捉长距离依赖关系。

**结构示意**：
{% mermaid %}
graph TD
    Input[输入 Token 序列] --> Embedding[词嵌入 + 位置编码]
    Embedding --> EncoderStack[N 个编码器层]
    EncoderStack --> DecoderStack["N 个解码器层 (对于生成任务)"]
    DecoderStack --> Output[输出概率分布/下一个 Token]

    subgraph Encoder Layer
        SA_enc[多头自注意力] --> FFN_enc[前馈神经网络]
        SA_enc -- 残差连接 & 层归一化 --> FFN_enc
    end

    subgraph Decoder Layer
        SA_dec[带掩码多头自注意力] --> CrossA[交叉注意力]
        CrossA --> FFN_dec[前馈神经网络]
        SA_dec -- 残差连接 & 层归一化 --> CrossA
        CrossA -- 残差连接 & 层归一化 --> FFN_dec
    end

    EncoderStack --> CrossA
{% endmermaid %}

### 2.2 自注意力机制 (Self-Attention Mechanism)

自注意力机制是 Transformer 的核心，它允许模型在处理序列中的每个 Token 时，都能同时考虑到序列中的所有其他 Token，并根据它们之间的相关性分配不同的“注意力权重”。这使得模型能够捕捉到复杂的上下文依赖关系，无论这些依赖关系是近距离的还是远距离的。

**工作原理**：
对于输入序列中的每个 Token，模型会生成三个向量：
*   **查询 (Query) $Q$**：当前 Token 的信息。
*   **键 (Key) $K$**：序列中所有 Token 的信息。
*   **值 (Value) $V$**：序列中所有 Token 的内容信息。

通过计算 $Q$ 和 $K$ 的点积，得到**注意力分数**，这个分数表示当前 Token 与序列中其他 Token 的相关性。将这些分数经过 Softmax 函数归一化后，得到**注意力权重**。最后，将注意力权重与 $V$ 相乘并求和，得到当前 Token 的加权表示。

**数学公式**：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
其中 $d_k$ 是键向量的维度，用于缩放点积，防止梯度过大。

**多头注意力 (Multi-Head Attention)**：
为了让模型能从不同角度和不同表示子空间捕获信息，自注意力机制通常会并行运行多个“头”(Head)，每个头学习一组独立的 $Q, K, V$ 投影矩阵。然后，将所有头的输出拼接起来，再经过一个线性变换，得到最终的多头注意力输出。

{% mermaid %}
graph TD
    A[Token 序列的嵌入] --> B["线性变换 (Q, K, V)"]
    B --> C1[自注意力头 1]
    B --> C2[自注意力头 2]
    B --> ...
    B --> Cn[自注意力头 n]
    C1 --> D[输出向量 1]
    C2 --> D[输出向量 2]
    Cn --> D[输出向量 n]
    D --> E[拼接所有输出向量]
    E --> F[线性变换]
    F --> G[多头注意力输出]
{% endmermaid %}
**解释**：通过自注意力，模型能够识别出句子中不同词语之间的语义联系。例如，在“The **animal** didn't cross the street because **it** was too tired.”中，模型能通过注意力机制理解“it”指的是“animal”。

## 三、训练机制：从模式识别到泛化

LLMs 并非被明确地“编程”来理解语言，而是通过在海量文本数据上进行**无监督预训练**和随后的**有监督微调**来学习语言模式。

### 3.1 预训练 (Pre-training)

这是 LLMs 学习语言“理解”的关键阶段。模型在数万亿词的文本语料库（如Common Crawl、维基百科、书籍等）上进行训练。

*   **目标**：预测序列中的下一个词 (如 GPT 系列) 或恢复被遮盖的词 (如 BERT)。
*   **方法**：通过大规模的“猜词”任务，模型被迫学习语言的语法、语义、事实知识以及世界常识。它会发现词语出现的频率、共现模式、上下文关系等。
*   **结果**：预训练后的模型获得了一个强大的**语言模型头** (Language Model Head)，能够生成语法正确、语义连贯的文本，并对它所见过的语言模式建立了深刻的“直觉”。

### 3.2 微调 (Fine-tuning)

在预训练之后，模型会针对特定的下游任务（如问答、摘要、情感分析、翻译等）进行有监督的微调。

*   **目标**：使模型更好地适应特定任务的需求。
*   **方法**：使用较小的、标注过的数据集进行训练，调整模型的参数。

### 3.3 人类反馈强化学习 (RLHF - Reinforcement Learning from Human Feedback)

GPT-3.5 和 GPT-4 等模型引入了 RLHF，进一步提升了模型的对齐能力。

*   **目标**：让模型生成更符合人类偏好、更有帮助、更安全、更无害的回复。
*   **方法**：训练一个奖励模型 (Reward Model)，由人类对模型的多个输出进行排序和评分。然后，使用强化学习算法（如 PPO - Proximal Policy Optimization）来优化语言模型，使其生成能获得更高奖励分数的输出。

{% mermaid %}
graph TD
    A["海量文本数据 (互联网、书籍)"] --> B[无监督预训练]
    B --> C["基础语言能力 (语法、语义、事实)"]
    C --> D[特定任务数据集]
    D --> E[有监督微调]
    E --> F["任务专用能力 (问答、摘要等)"]
    F --> G["人类偏好数据 (排名/评分)"]
    G --> H[奖励模型训练]
    F --> I["强化学习 (PPO)"]
    I --> J[与人类意图对齐的模型]
{% endmermaid %}

## 四、理解的实现：从统计模式到语义关联

LLMs 通过上述机制，在统计层面实现了对语言的“理解”，这体现在以下几个方面：

### 4.1 模式识别与语义关联

模型通过大量的预训练数据，学习了词语、短语、句子之间的**共现模式**和**统计关联**。当它遇到一个词时，能够预测其后续可能出现的词，或根据上下文推断其含义。这种能力并非基于词典定义，而是基于词语在不同语境中出现的概率分布。

### 4.2 上下文依赖

Transformer 的自注意力机制允许模型在处理长文本时，有效地捕捉到相距较远的词语之间的依赖关系。这使得模型能够维持上下文连贯性，并进行更准确的语义推理。例如，在多轮对话中，模型能记住之前的对话内容。

### 4.3 知识内化

在预训练过程中，模型隐式地学习并“存储”了大量的**事实知识**和**世界常识**。这些知识不是以结构化的数据库形式存储，而是以神经网络参数的分布式表示形式存在。当被问及相关问题时，模型能够通过其学到的语言模式和知识进行生成。

### 4.4 泛化与推理

LLMs 展现出惊人的泛化能力，能够处理和理解训练数据中未曾见过的句子结构和表达方式。它们也能进行一定程度的**链式推理**，通过多步生成来解决复杂问题，这通常通过“思维链”(Chain of Thought) 提示工程来实现。

## 五、挑战与局限

尽管 LLMs 取得了巨大成功，但它们的“理解”仍存在显著局限：

### 5.1 缺乏真正的世界模型与常识

LLMs 仅通过文本学习，它们没有真实世界的感官体验，因此缺乏人类所拥有的**深层世界模型和常识性推理能力**。它们在面对需要跨领域推理或与物理世界深度交互的问题时，可能会表现出弱点。

### 5.2 幻觉 (Hallucinations)

模型可能会生成听起来非常合理但实际上是**错误或虚构**的信息。这源于其基于概率预测的本质，有时会“编造”不存在的答案。

### 5.3 偏见 (Bias)

由于训练数据中包含了人类社会的偏见（如性别、种族偏见），LLMs 可能会学习并**放大这些偏见**，在生成内容时表现出来。

### 5.4 上下文窗口限制

尽管 Transformer 擅长处理长距离依赖，但其注意力机制的计算复杂度随着序列长度的增加而呈二次方增长。这导致 LLMs 仍然有**上下文窗口的限制**，无法一次性处理无限长的文本。

### 5.5 可解释性差

LLMs 是复杂的深度神经网络，其内部工作机制是高度不透明的**“黑箱”**。我们很难准确解释模型做出特定决策或生成特定文本的原因。

## 六、总结

大模型对人类文字的“理解”是一种基于**大规模统计学习**的复杂能力。它通过将文本转化为高维向量，利用 Transformer 架构的自注意力机制捕捉词语间的复杂关联，并在海量数据上通过预训练和微调，内化了语言的语法、语义和事实知识。这种理解是**统计性、表征性**的，而非人类意识层面的认知。

尽管 LLMs 在语言处理上取得了里程碑式的进展，但其缺乏真正的世界模型、存在幻觉和偏见等问题，提醒我们其“理解”与人类的认知存在本质区别。未来的研究将继续探索如何弥补这些差距，使 AI 能够更深层次地与人类世界互动和理解。