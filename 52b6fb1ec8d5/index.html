<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>文档嵌入模型 (Document Embedding Models) 详解 | 1024 维度</title><meta name="author" content="TeaTang"><meta name="copyright" content="TeaTang"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的">
<meta property="og:type" content="article">
<meta property="og:title" content="文档嵌入模型 (Document Embedding Models) 详解">
<meta property="og:url" content="https://blog.tbf1211.xx.kg/52b6fb1ec8d5/index.html">
<meta property="og:site_name" content="1024 维度">
<meta property="og:description" content="文档嵌入模型 (Document Embedding Models) 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。  核心思想：将非结构化文档转化为机器可理解的深层语义表示，使相似的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-19.jpg">
<meta property="article:published_time" content="2025-05-07T22:24:00.000Z">
<meta property="article:modified_time" content="2025-12-29T14:27:26.319Z">
<meta property="article:author" content="TeaTang">
<meta property="article:tag" content="2025">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.tbf1211.xx.kg/img/cover/default_cover-19.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "文档嵌入模型 (Document Embedding Models) 详解",
  "url": "https://blog.tbf1211.xx.kg/52b6fb1ec8d5/",
  "image": "https://blog.tbf1211.xx.kg/img/cover/default_cover-19.jpg",
  "datePublished": "2025-05-07T22:24:00.000Z",
  "dateModified": "2025-12-29T14:27:26.319Z",
  "author": [
    {
      "@type": "Person",
      "name": "TeaTang",
      "url": "https://blog.tbf1211.xx.kg"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon-1.ico"><link rel="canonical" href="https://blog.tbf1211.xx.kg/52b6fb1ec8d5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><meta name="google-site-verification" content="NdIUXAOVyGnnBhcrip0ksCawbdAzT0hlBZDE9u4jx6k"/><meta name="msvalidate.01" content="567E47D75E8DCF1282B9623AD914701E"/><meta name="baidu-site-verification" content="code-pE5rnuxcfD"/><link rel="stylesheet" href="/css/index.css?v=5.5.3"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          const mediaQueryDark = window.matchMedia('(prefers-color-scheme: dark)')
          const mediaQueryLight = window.matchMedia('(prefers-color-scheme: light)')

          if (theme === undefined) {
            if (mediaQueryLight.matches) activateLightMode()
            else if (mediaQueryDark.matches) activateDarkMode()
            else {
              const hour = new Date().getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            mediaQueryDark.addEventListener('change', () => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else {
            theme === 'light' ? activateLightMode() : activateDarkMode()
          }
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":false,"top_n_per_article":1,"unescape":false,"pagination":{"enable":true,"hitsPerPage":8},"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400,"highlightFullpage":true,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":150,"languages":{"author":"作者: TeaTang","link":"链接: ","source":"来源: 1024 维度","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '文档嵌入模型 (Document Embedding Models) 详解',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="preconnect" href="https://jsd.012700.xyz"><link href="/self/btf.css" rel="stylesheet"><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="1024 维度" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">443</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">225</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url(/img/cover/default_cover-19.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">1024 维度</span></a><a class="nav-page-title" href="/"><span class="site-name">文档嵌入模型 (Document Embedding Models) 详解</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 我的轨迹</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/2023/"><i class="fa-fw fa-solid fa-bug"></i><span> 2023</span></a></li><li><a class="site-page child" href="/archives/2024/"><i class="fa-fw fa-solid fa-code"></i><span> 2024</span></a></li><li><a class="site-page child" href="/archives/2025/"><i class="fa-fw fa-solid fa-network-wired"></i><span> 2025</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa-solid fa-calendar-days"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo"><i class="fa-fw fas fa-comment"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">文档嵌入模型 (Document Embedding Models) 详解</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-05-07T22:24:00.000Z" title="发表于 2025-05-08 06:24:00">2025-05-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>15分钟</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p><strong>文档嵌入模型 (Document Embedding Models)</strong> 是将整个文档（包括句子、段落或更长的文本）映射到高维实数向量空间的技术。与传统的词嵌入（如 Word2Vec）和句嵌入相比，文档嵌入旨在捕捉文档更宏观、更复杂的语义和上下文信息，使其在向量空间中表示为一个能够与其他文档进行高效相似性比较、检索和分析的稠密向量。</p>
</blockquote>
<div class="note info flat"><p>核心思想：<strong>将非结构化文档转化为机器可理解的深层语义表示，使相似的文档在多维向量空间中彼此靠近。这是构建高级信息检索、知识管理和内容理解系统的基石。</strong></p>
</div>
<hr>
<h2 id="一、为什么需要文档嵌入模型？"><a href="#一、为什么需要文档嵌入模型？" class="headerlink" title="一、为什么需要文档嵌入模型？"></a>一、为什么需要文档嵌入模型？</h2><p>在大数据时代，我们面临着海量文档（如网页、报告、书籍、代码库、用户评论等）。传统处理这些文档的方法存在诸多局限：</p>
<ol>
<li><strong>关键词匹配的不足</strong>：搜索引擎通常依赖关键词匹配，但无法理解语义。例如，搜索“车祸”可能无法找到包含“交通事故”的文档。</li>
<li><strong>句嵌入的局限性</strong>：虽然句嵌入能捕捉句子级别的语义，但在处理长文档时，简单地拼接或平均句嵌入会丢失文档整体的结构和主题信息。</li>
<li><strong>高维稀疏性问题</strong>：传统的 Bag-of-Words (BOW) 或 TF-IDF 等模型将文档表示为高维稀疏向量，不仅计算效率低下，也无法捕捉词语的语义关系和上下文信息。</li>
<li><strong>无法泛化</strong>：对于未见过的词语或组合，传统方法难以有效处理。</li>
<li><strong>下游任务的挑战</strong>：文档分类、聚类、摘要、推荐等任务需要文档的统一、语义丰富的表示作为输入。</li>
</ol>
<p>文档嵌入模型通过将整个文档编码为单一的、稠密的向量，解决了上述问题，使得计算机能够“理解”文档的内在含义，并能够基于语义进行高效操作。</p>
<h2 id="二、文档嵌入的核心概念与方法"><a href="#二、文档嵌入的核心概念与方法" class="headerlink" title="二、文档嵌入的核心概念与方法"></a>二、文档嵌入的核心概念与方法</h2><p>文档嵌入模型在向量嵌入 (Vector Embeddings) 的基础上，更侧重于处理长文本的语义表示。其核心概念包括：</p>
<h3 id="2-1-向量空间与相似性度量"><a href="#2-1-向量空间与相似性度量" class="headerlink" title="2.1 向量空间与相似性度量"></a>2.1 向量空间与相似性度量</h3><p>与所有向量嵌入一样，文档嵌入模型将文档映射到高维向量空间。在这个空间中，语义相似的文档的向量表示会彼此接近。常用的相似性度量包括：</p>
<ul>
<li><strong>余弦相似度 (Cosine Similarity)</strong>：衡量两个向量方向的相似性，是文本相似度任务中最常用的度量，范围从 -1 到 1。</li>
<li><strong>欧氏距离 (Euclidean Distance)</strong>：衡量两个向量空间中的直线距离，距离越小相似度越高。</li>
<li><strong>点积 (Dot Product)</strong>：如果向量经过归一化，点积与余弦相似度等价，计算更高效。</li>
</ul>
<h3 id="2-2-上下文关联-Contextualization"><a href="#2-2-上下文关联-Contextualization" class="headerlink" title="2.2 上下文关联 (Contextualization)"></a>2.2 上下文关联 (Contextualization)</h3><p>优质的文档嵌入能够捕捉词语乃至句子在特定文档中的上下文含义。例如，“Bank”在“Bank of America”和“river bank”中的含义是不同的，上下文嵌入模型能够区分这些差异。</p>
<h3 id="2-3-迁移学习-Transfer-Learning"><a href="#2-3-迁移学习-Transfer-Learning" class="headerlink" title="2.3 迁移学习 (Transfer Learning)"></a>2.3 迁移学习 (Transfer Learning)</h3><p>大多数强大的文档嵌入模型都基于大规模预训练模型。这些模型在海量文本数据上学习了通用的语言理解能力，然后通过微调或直接用作特征提取器，将这些能力迁移到特定任务上。</p>
<h3 id="2-4-长文本处理能力"><a href="#2-4-长文本处理能力" class="headerlink" title="2.4 长文本处理能力"></a>2.4 长文本处理能力</h3><p>这是文档嵌入区别于词嵌入和句嵌入的关键。文档嵌入模型需要具备处理远超单一句子长度文本的能力，同时保持语义的连贯性和完整性。</p>
<h2 id="三、文档嵌入模型的分类与演进"><a href="#三、文档嵌入模型的分类与演进" class="headerlink" title="三、文档嵌入模型的分类与演进"></a>三、文档嵌入模型的分类与演进</h2><p>文档嵌入模型经历了从基于词袋到深度学习的演变。</p>
<h3 id="3-1-1-基于词袋-Bag-of-Words-及统计方法"><a href="#3-1-1-基于词袋-Bag-of-Words-及统计方法" class="headerlink" title="3.1 1. 基于词袋 (Bag-of-Words) 及统计方法"></a>3.1 1. 基于词袋 (Bag-of-Words) 及统计方法</h3><ul>
<li><p><strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>：</p>
<ul>
<li><strong>原理</strong>：计算词语在文档中的频率 (TF) 和在整个语料库中的逆文档频率 (IDF)，以评估词语的重要性。将文档表示为词语的 TF-IDF 权重向量。</li>
<li><strong>优点</strong>：简单、可解释。</li>
<li><strong>缺点</strong>：产生高维稀疏向量，无法捕捉词语的语义关系和顺序。</li>
</ul>
</li>
<li><p><strong>Word2Vec&#x2F;GloVe&#x2F;FastText 平均池化</strong>：</p>
<ul>
<li><strong>原理</strong>：先为文档中的每个词语生成预训练的词嵌入，然后通过平均 (Mean Pooling) 或加权平均 (例如，基于 TF-IDF 权重) 的方式聚合所有词嵌入得到文档嵌入。</li>
<li><strong>优点</strong>：引入了词语的语义信息。</li>
<li><strong>缺点</strong>：丢失了词语的顺序信息，对长文档的上下文捕捉能力有限。</li>
</ul>
</li>
</ul>
<h3 id="3-2-2-深度学习模型"><a href="#3-2-2-深度学习模型" class="headerlink" title="3.2 2. 深度学习模型"></a>3.2 2. 深度学习模型</h3><p>这是现代文档嵌入的主流。它们能够通过神经网络结构，尤其是 Transformer 架构，捕捉更深层次的语义和上下文。</p>
<h4 id="3-2-1-基于-Transformer-编码器-Encoder-based-Transformers"><a href="#3-2-1-基于-Transformer-编码器-Encoder-based-Transformers" class="headerlink" title="3.2.1 基于 Transformer 编码器 (Encoder-based Transformers)"></a>3.2.1 基于 Transformer 编码器 (Encoder-based Transformers)</h4><ul>
<li><p><strong>BERT 系列 (BERT, RoBERTa, Electra 等)</strong>：</p>
<ul>
<li><strong>原理</strong>：这些模型在预训练阶段学习了双向上下文理解。对于短文档（通常限制在 512 token 以内），可以将文档作为输入，并提取 <code>[CLS]</code> token 对应的输出向量作为文档嵌入，或者对所有 token 的输出向量进行平均池化。</li>
<li><strong>优点</strong>：强大的语义理解能力。</li>
<li><strong>缺点</strong>：<strong>严格的输入长度限制 (约 512 token)</strong>，对于长文档需要进行分块处理。</li>
</ul>
</li>
<li><p><strong>Sentence-BERT (SBERT) 系列</strong>：</p>
<ul>
<li><strong>原理</strong>：SBERT 通过修改 BERT 的结构并使用 Siamese (孪生) 或 Triplet (三元组) 网络进行微调，使其能够生成语义上有意义且可直接比较的句子&#x2F;段落嵌入。它将文档分割成句子&#x2F;段落，然后聚合这些句子的嵌入。</li>
<li><strong>优点</strong>：非常擅长生成用于语义相似性比较的嵌入，计算效率高（不需要像 BERT 那样复杂的交叉编码进行相似性计算）。</li>
<li><strong>缺点</strong>：大多数 SBERT 模型仍有输入长度限制，处理超长文档时仍需分块。</li>
</ul>
</li>
</ul>
<h4 id="3-2-2-专为长序列设计的-Transformer-模型"><a href="#3-2-2-专为长序列设计的-Transformer-模型" class="headerlink" title="3.2.2 专为长序列设计的 Transformer 模型"></a>3.2.2 专为长序列设计的 Transformer 模型</h4><p>由于标准 Transformer 的自注意力机制 (Self-Attention) 复杂度为 $O(N^2)$（其中 N 是序列长度），对长文档处理成本极高。因此，出现了一系列优化模型：</p>
<ul>
<li><strong>Longformer, BigBird, Reformer, Performer</strong>：<ul>
<li><strong>原理</strong>：这些模型通过引入稀疏注意力机制 (Sparse Attention) 或其他计算优化 (如局部注意力、全局注意力、随机注意力等)，将复杂度降低到接近 $O(N)$ 或 $O(N \log N)$，从而能够处理数千甚至数万 token 的长文档。</li>
<li><strong>优点</strong>：可以直接端到端地处理长文档，捕捉全局上下文。</li>
<li><strong>缺点</strong>：模型通常比标准 BERT 更大，训练和推理资源需求相对较高。</li>
</ul>
</li>
</ul>
<h4 id="3-2-3-商业-API-型嵌入模型"><a href="#3-2-3-商业-API-型嵌入模型" class="headerlink" title="3.2.3 商业 API 型嵌入模型"></a>3.2.3 商业 API 型嵌入模型</h4><ul>
<li><p><strong>OpenAI Embeddings (如 <code>text-embedding-ada-002</code>, <code>text-embedding-3-large</code>)</strong>：</p>
<ul>
<li><strong>原理</strong>：强大的专有模型，通过 API 提供文本嵌入服务。它们通常在海量数据和先进架构上训练，能够生成高质量的文档嵌入。</li>
<li><strong>优点</strong>：易于使用，无需管理模型，性能优异。</li>
<li><strong>缺点</strong>：对外不透明（黑盒），依赖第三方服务，可能存在数据隐私和成本考量。</li>
</ul>
</li>
<li><p><strong>Cohere Embeddings</strong>：</p>
<ul>
<li><strong>原理</strong>：由 Cohere 提供的文本嵌入服务，与 OpenAI 类似，注重性能和易用性。</li>
<li><strong>优点</strong>：性能出色，通常提供不同大小和性能的选项。</li>
<li><strong>缺点</strong>：同 OpenAI，为第三方服务。</li>
</ul>
</li>
</ul>
<h2 id="四、文档嵌入的生成过程"><a href="#四、文档嵌入的生成过程" class="headerlink" title="四、文档嵌入的生成过程"></a>四、文档嵌入的生成过程</h2><p>无论是基于哪种模型，文档嵌入的生成通常遵循以下通用流程：</p>
<div class="mermaid-wrap"><pre class="mermaid-src" data-config="{}" hidden>
    graph TD
    A[&quot;原始文档 (文章, 报告, 代码等)&quot;] --&gt; B{预处理}
    B --&gt; C{&quot;文本分块 (Text Splitting)&quot;}
    C -- (多个文本块) --&gt; D{&quot;嵌入模型 (如 SBERT, OpenAI API)&quot;}
    D -- 对每个文本块生成嵌入 --&gt; E[多个文本块嵌入向量]
    E -- (可选) 聚合策略 (Concat, Mean, Weighted Mean) --&gt; F[单一文档嵌入向量]
    F -- (可选) 向量归一化 --&gt; G[最终文档嵌入]
  </pre></div>

<p><strong>详细步骤</strong>：</p>
<ol>
<li><strong>原始文档输入</strong>：待处理的文档，可能长达数页甚至包含图片和表格的复杂格式。</li>
<li><strong>预处理</strong>：<ul>
<li><strong>清洗</strong>：去除HTML标签、特殊字符、噪音、重复内容。</li>
<li><strong>格式化</strong>：如果文档是 PDF、DOCX 等，需要先转换为纯文本。</li>
</ul>
</li>
<li><strong>文本分块 (Text Splitting)</strong>：对于超出模型上下文窗口的长文档，需要将其分割成多个大小适中的文本块 (chunks)。LangChain 的 <code>TextSplitters</code> 模块在此发挥关键作用。</li>
<li><strong>嵌入模型</strong>：将每个文本块输入到选定的嵌入模型中。<ul>
<li>模型内部进行分词 (tokenization)。</li>
<li>通过多层神经网络（例如 Transformer 编码器）捕获上下文语义。</li>
<li>通常通过池化层（如 <code>[CLS]</code> token 提取、平均池化 Mean Pooling）将 token 级别的嵌入聚合成块级别的稠密向量。</li>
</ul>
</li>
<li><strong>多个文本块嵌入向量</strong>：模型为每个文本块生成一个独立的嵌入向量。</li>
<li><strong>(可选) 聚合策略</strong>：如果目标是生成整个文档的单一嵌入向量，需要将所有文本块的嵌入向量进行聚合：<ul>
<li><strong>平均池化 (Mean Pooling)</strong>：最简单的方法，直接计算所有块嵌入的平均值。</li>
<li><strong>最大池化 (Max Pooling)</strong>：对每个维度取最大值。</li>
<li><strong>PCA&#x2F;LDA 等降维</strong>：对合并后的嵌入进行降维，减少噪声。</li>
<li><strong>专门的文档级编码器</strong>：有些模型（如 Doc2Vec）可以直接编码整个文档，或者长序列 Transformer 模型可以直接处理超长文档，避免分块和聚合的复杂性。</li>
</ul>
</li>
<li><strong>(可选) 向量归一化</strong>：将最终的文档嵌入向量的长度 (L2 范数) 归一化为 1。这对于后续的余弦相似度计算非常有益。</li>
<li><strong>最终文档嵌入</strong>：得到一个代表整个文档语义的稠密向量。</li>
</ol>
<h2 id="五、文档嵌入模型的应用场景"><a href="#五、文档嵌入模型的应用场景" class="headerlink" title="五、文档嵌入模型的应用场景"></a>五、文档嵌入模型的应用场景</h2><p>文档嵌入是现代 AI 系统的核心组成部分，尤其在以下领域有广泛应用：</p>
<ol>
<li><p><strong>RAG (Retrieval Augmented Generation) 知识检索</strong>：</p>
<ul>
<li><strong>原理</strong>：将知识库中的所有文档分块并嵌入。当用户提出问题时，将问题也嵌入为向量，然后通过向量相似性搜索从知识库中检索出最相关的文档块，作为 LLM 生成答案的上下文。</li>
<li><strong>示例</strong>：企业内部知识库、智能客服、专业领域问答系统。</li>
</ul>
</li>
<li><p><strong>语义搜索 (Semantic Search)</strong>：</p>
<ul>
<li><strong>原理</strong>：用户查询和文档都转换为嵌入向量，通过向量相似性匹配，实现超越关键词的语义理解搜索。</li>
<li><strong>示例</strong>：下一代搜索引擎、文档管理系统。</li>
</ul>
</li>
<li><p><strong>文档推荐系统 (Document Recommendation Systems)</strong>：</p>
<ul>
<li><strong>原理</strong>：根据用户阅读过的文档嵌入，推荐语义相似的其他文档。</li>
<li><strong>示例</strong>：新闻推荐、论文推荐、产品文档推荐。</li>
</ul>
</li>
<li><p><strong>文档聚类与分类 (Document Clustering &amp; Classification)</strong>：</p>
<ul>
<li><strong>原理</strong>：将文档转换为嵌入向量后，可以利用传统的机器学习算法（如 K-Means、SVM、Logistic Regression）进行无监督聚类或有监督分类。</li>
<li><strong>示例</strong>：主题发现、文档自动归档、垃圾邮件检测。</li>
</ul>
</li>
<li><p><strong>文本摘要 (Text Summarization)</strong>：</p>
<ul>
<li><strong>原理</strong>：寻找文档嵌入中最能代表整体主题的句子或段落进行抽取式摘要。</li>
<li><strong>示例</strong>：自动生成新闻摘要、会议纪要。</li>
</ul>
</li>
<li><p><strong>重复文档检测 (Duplicate Document Detection)</strong>：</p>
<ul>
<li><strong>原理</strong>：通过计算文档嵌入之间的相似度，识别语料库中的重复或高度相似文档。</li>
<li><strong>示例</strong>：知识库去重、爬虫数据清洗。</li>
</ul>
</li>
</ol>
<h2 id="六、Python-实践：使用-sentence-transformers-进行文档嵌入"><a href="#六、Python-实践：使用-sentence-transformers-进行文档嵌入" class="headerlink" title="六、Python 实践：使用 sentence-transformers 进行文档嵌入"></a>六、Python 实践：使用 <code>sentence-transformers</code> 进行文档嵌入</h2><p><code>sentence-transformers</code> 是一个流行的 Python 库，它提供了一系列预训练模型，专门用于计算句子、段落和短文档的语义嵌入，效率高且性能优秀。</p>
<p><strong>准备工作：</strong></p>
<ol>
<li><strong>安装库</strong>：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install sentence-transformers numpy scikit-learn</span><br></pre></td></tr></table></figure></li>
</ol>
<p><strong>Python 代码示例：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 加载预训练的文档嵌入模型 ---</span></span><br><span class="line"><span class="comment"># 这里选用一个通用且性能不错的模型</span></span><br><span class="line"><span class="comment"># &quot;all-MiniLM-L6-v2&quot; 是一个高效的小模型</span></span><br><span class="line"><span class="comment"># &quot;all-mpnet-base-v2&quot; 性能更强但稍慢</span></span><br><span class="line"><span class="comment"># 对于中文可以考虑 &quot;m3e-base&quot; 或 &quot;bge-base-zh-v1.5&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;BAAI/bge-small-zh-v1.5&quot;</span> <span class="comment"># 适用于中文文档</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Loading model: <span class="subst">&#123;model_name&#125;</span>...&quot;</span>)</span><br><span class="line">model = SentenceTransformer(model_name)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Model loaded.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 准备文档数据 ---</span></span><br><span class="line">documents = [</span><br><span class="line">    <span class="string">&quot;大语言模型正在改变我们与计算机交互的方式，它们在自然语言理解和生成方面表现卓越。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;人工智能是计算机科学的一个分支，旨在创建能够像人类一样思考和学习的机器。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;自动驾驶汽车的未来充满了挑战与机遇，技术需要不断进步以确保安全。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;自然语言处理 (NLP) 是人工智能的一个子领域，专注于人机语言交互。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;深度学习是机器学习的一个分支，使用多层神经网络来从数据中学习表示。&quot;</span>,</span><br><span class="line">    <span class="string">&quot;如何在RAG系统中优化文档分块和检索策略是提高LLM性能的关键。&quot;</span>,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于特别长的文档，这里需要先进行分块处理。</span></span><br><span class="line"><span class="comment"># 简单的示例中，我们假设这些文档是短到中等长度的。</span></span><br><span class="line"><span class="comment"># 如果是长文档，你需要先用 LangChain 的 Text Splitters 进行分块，</span></span><br><span class="line"><span class="comment"># 然后对每个块生成嵌入，再根据需求聚合。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 生成文档嵌入 ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\nGenerating embeddings for <span class="subst">&#123;<span class="built_in">len</span>(documents)&#125;</span> documents...&quot;</span>)</span><br><span class="line"><span class="comment"># model.encode 方法会自动将文本转化为向量</span></span><br><span class="line">doc_embeddings = model.encode(documents, convert_to_tensor=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># convert_to_tensor=False 返回 numpy 数组，方便后续处理</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Generated embeddings with dimension: <span class="subst">&#123;doc_embeddings.shape[<span class="number">1</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 进行语义搜索 (查找最相似的文档) ---</span></span><br><span class="line">query = <span class="string">&quot;我正在寻找关于AI在语言处理方面应用的信息。&quot;</span></span><br><span class="line">query_embedding = model.encode([query], convert_to_tensor=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算查询向量与所有文档向量的余弦相似度</span></span><br><span class="line">similarities = cosine_similarity(query_embedding, doc_embeddings)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n--- 语义搜索结果 --- (查询: &#x27;<span class="subst">&#123;query&#125;</span>&#x27;)&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i, (doc, sim) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(documents, similarities)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;文档 <span class="subst">&#123;i+<span class="number">1</span>&#125;</span>: &#x27;<span class="subst">&#123;doc&#125;</span>&#x27;&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  相似度: <span class="subst">&#123;sim:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到最相似的文档</span></span><br><span class="line">most_similar_index = np.argmax(similarities)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n最相似的文档是: &#x27;<span class="subst">&#123;documents[most_similar_index]&#125;</span>&#x27;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;相似度: <span class="subst">&#123;similarities[most_similar_index]:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 5. 查找重复或高度相关的文档 (两两相似度) ---</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;\n--- 文档两两相似度矩阵 (Top 3 最相似对) ---&quot;</span>)</span><br><span class="line"><span class="comment"># 计算所有文档两两之间的相似度矩阵</span></span><br><span class="line"><span class="comment"># 相似度矩阵是一个对称矩阵，对角线为1</span></span><br><span class="line">pairwise_similarities = cosine_similarity(doc_embeddings, doc_embeddings)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出除了自身以外最相似的文档对</span></span><br><span class="line">similar_pairs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(documents)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(documents)): <span class="comment"># 避免重复和自身</span></span><br><span class="line">        similar_pairs.append((similarities[i], i, j))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按相似度降序排序</span></span><br><span class="line">similar_pairs.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最相似的几对</span></span><br><span class="line"><span class="keyword">for</span> sim_val, idx1, idx2 <span class="keyword">in</span> similar_pairs[:<span class="number">3</span>]: <span class="comment"># 打印Top 3</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;&#x27;<span class="subst">&#123;documents[idx1]:<span class="number">.30</span>s&#125;</span>...&#x27; vs &#x27;<span class="subst">&#123;documents[idx2]:<span class="number">.30</span>s&#125;</span>...&#x27;: <span class="subst">&#123;sim_val:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 观察结果：</span></span><br><span class="line"><span class="comment"># &quot;大语言模型...&quot; 和 &quot;自然语言处理...&quot; 应该有很高的相似度。</span></span><br><span class="line"><span class="comment"># &quot;人工智能...&quot; 和 &quot;深度学习...&quot; 应该有较高的相似度。</span></span><br></pre></td></tr></table></figure>

<p><strong>代码解析要点:</strong></p>
<ol>
<li><strong><code>SentenceTransformer(model_name)</code></strong>: 加载一个预训练的 Sentence-BERT 模型。这里使用的是 <code>BAAI/bge-small-zh-v1.5</code>，这是一个在中文语料上表现优秀的小型模型，适合文档嵌入。</li>
<li><strong><code>model.encode(documents, convert_to_tensor=False)</code></strong>: 这是生成嵌入的核心函数。它接收一个字符串列表（文档列表），并返回一个 NumPy 数组，其中每行是一个文档对应的嵌入向量。</li>
<li><strong><code>cosine_similarity(query_embedding, doc_embeddings)</code></strong>: 使用 <code>sklearn</code> 库计算查询向量与所有文档向量的余弦相似度。结果是一个一维数组，对应于每个文档与查询的相似度。</li>
<li><strong>语义搜索演示</strong>: 通过比较相似度，可以找到与用户查询语义最相关的文档，这正是 RAG 和语义搜索的底层机制。</li>
<li><strong>两两相似度</strong>: 展示了如何计算语料库中任意两个文档之间的相似度，用于发现重复内容或进行文档聚类。</li>
</ol>
<p><strong>针对长文档的进一步考虑 (未在示例中完全实现，但概念重要):</strong></p>
<ul>
<li><strong>分块与聚合</strong>：对于非常长的文档，需要先使用 LangChain 的 <code>RecursiveCharacterTextSplitter</code> 等工具将其分割成多个文本块。然后对每个块生成嵌入。</li>
<li><strong>块嵌入的聚合策略</strong>：<ul>
<li><strong>平均池化</strong>：最简单且常用，将所有块的嵌入取平均值作为整个文档的嵌入。</li>
<li><strong>分层嵌入</strong>：训练一个模型，先嵌入句子，再嵌入段落，最后嵌入整个文档。</li>
<li><strong>权重聚合</strong>：根据每个块的重要性（例如，开头和结尾的块可能更重要）进行加权平均。</li>
<li><strong>专门文档模型</strong>：使用 Longformer、BigBird 等可以直接处理长序列的模型，但需要更多计算资源。</li>
</ul>
</li>
</ul>
<h2 id="七、文档嵌入的挑战与考量"><a href="#七、文档嵌入的挑战与考量" class="headerlink" title="七、文档嵌入的挑战与考量"></a>七、文档嵌入的挑战与考量</h2><ol>
<li><strong>长文档的处理</strong>：Transformer 模型普遍存在上下文窗口限制，如何有效捕捉超长文档的全局语义和局部细节是一个持续的挑战。分块、聚合策略的选择至关重要。</li>
<li><strong>计算资源</strong>：生成高质量嵌入和在海量向量数据库中进行高效检索都需要显著的计算资源。更大、更强的嵌入模型通常意味着更高的计算成本。</li>
<li><strong>模型选择</strong>：不同的模型在不同领域和任务上表现各异。选择一个适合你的数据领域、任务类型和资源预算的模型是关键。</li>
<li><strong>数据质量与预处理</strong>：原始文档的清洗、格式化、以及分块的质量直接影响嵌入的准确性。</li>
<li><strong>模型偏差</strong>：如同所有训练于大规模文本数据的模型，文档嵌入模型也可能继承和反映训练数据中的偏见，这需要在使用时特别注意。</li>
<li><strong>更新与维护</strong>：随着知识的更新，文档嵌入也可能需要定期重新生成和索引。</li>
</ol>
<h2 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h2><p>文档嵌入模型是连接非结构化文本数据与现代 AI 应用之间的桥梁。它们将复杂的语义信息压缩成紧密的数值向量，使得机器能够高效地理解、比较和操作文档。从传统的统计方法到强大的深度学习模型，文档嵌入技术持续演进，赋能了语义搜索、RAG、文档管理和推荐系统等一系列创新应用。</p>
<p>通过理解其基本原理、不同模型的特点以及实践中的应用流程，开发者可以有效地利用文档嵌入技术，构建出更智能、更高效的知识驱动型 AI 系统。在实际应用中，关键在于根据具体场景选择合适的模型、优化数据预处理和分块策略，并持续评估和迭代。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg">TeaTang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://blog.tbf1211.xx.kg/52b6fb1ec8d5/">https://blog.tbf1211.xx.kg/52b6fb1ec8d5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://blog.tbf1211.xx.kg" target="_blank">1024 维度</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2025/">2025</a><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/cover/default_cover-19.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/92d90a6caba1/" title="Python 项目管理工具 Poetry 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-17.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Python 项目管理工具 Poetry 详解</div></div><div class="info-2"><div class="info-item-1"> Poetry 是一款现代化的 Python 项目管理和打包工具。它将依赖管理、虚拟环境管理、打包和发布功能集成在一个直观的命令行界面中。Poetry 的核心理念是提供一个统一的、声明式的项目配置方式，以 pyproject.toml 文件 (遵循 PEP 518 和 PEP 621) 作为所有项目元数据和依赖的唯一真实来源。  核心思想：Poetry 旨在通过一个工具，简化 Python 项目从创建到发布的全生命周期管理，确保环境隔离、依赖可重现性和便捷的打包发布流程。   一、为什么需要 Poetry？传统的 Python 项目管理方式通常涉及多个工具和手动步骤，带来了诸多痛点：  pip 和 requirements.txt 的局限性： requirements.txt 仅记录直接依赖，不处理传递性依赖，容易导致环境不一致。 缺乏强大的依赖解析能力，解决包版本冲突困难。 没有统一的元数据管理，项目信息分散在 setup.py、README.md 等文件中。   虚拟环境管理不便： 需要手动创建 venv 或 virtualenv，并手动激活、切换。 项目与虚拟环境的关联不够...</div></div></div></a><a class="pagination-related" href="/84d87a2eca4d/" title="Caddy Web Server详解：现代Web服务器的优雅选择"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-05.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Caddy Web Server详解：现代Web服务器的优雅选择</div></div><div class="info-2"><div class="info-item-1"> Caddy 是一款用 Go 语言编写的开源 Web 服务器，以其自动 HTTPS 功能、简洁的配置以及强大的功能而闻名。它被设计成现代 Web 的瑞士军刀，能够胜任静态文件服务、反向代理、负载均衡、API 网关等多种任务，并且在安全性和易用性方面表现出色。  “Caddy 是未来 Web 服务器的样子：默认安全、易于管理、功能强大，并且能够自动处理 HTTPS 证书的申请和续期，让你的网站在几秒钟内上线并享受加密连接。”   一、Caddy 简介1.1 什么是 Caddy？Caddy 是一个高性能、可扩展的 Web 服务器，其核心特性包括：  自动 HTTPS：这是 Caddy 最吸引人的特性之一。对于绝大多数公共可访问的域名，Caddy 可以自动从 Let’s Encrypt 申请、配置和续期 SSL&#x2F;TLS 证书，无需手动干预。 配置简洁：Caddyfile 配置文件语法非常直观易懂，相比 Nginx 和 Apache 更加简洁。 HTTP&#x2F;2 和 HTTP&#x2F;3 支持：Caddy 默认启用 HTTP&#x2F;2，并且是首批支持 QUIC (H...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2ef7cb8bd831/" title="LangChain Text Splitters 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-02.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-03</div><div class="info-item-2">LangChain Text Splitters 详解</div></div><div class="info-2"><div class="info-item-1"> LangChain Text Splitters 是 LangChain 框架中的一个核心模块，用于将长文档或文本智能地分割成更小、更易于管理和处理的块 (chunks)。这个过程对于大语言模型 (LLM) 相关的应用至关重要，特别是当处理的文本长度超出 LLM 的上下文窗口限制时。  核心思想：将长文本分割成大小适中、语义连贯且包含一定重叠的块，以便 LLM 能够有效处理这些块，同时保持上下文完整性。LangChain 提供多种具有不同策略的 Text Splitters，以适应不同的文本结构和应用场景。   一、为什么需要 Text Splitters？在构建基于 LLM 的应用程序（尤其是问答 RAG (Retrieval Augmented Generation) 系统、文档摘要、聊天机器人等）时，我们经常遇到以下问题：  LLM 上下文窗口限制 (Context Window Limit)：大语言模型（如 GPT-3.5, GPT-4, Llama）通常有一个固定的最大输入长度。如果输入文本太长，会超出这个限制，导致模型无法处理。 性能和成本：即使模型支持很长的上下文...</div></div></div></a><a class="pagination-related" href="/1bd89b02cd88/" title="大型语言模型中的Token详解：数据、处理与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-21.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-20</div><div class="info-item-2">大型语言模型中的Token详解：数据、处理与意义</div></div><div class="info-2"><div class="info-item-1"> Token 是大型语言模型 (Large Language Models, LLMs) 处理文本的基本单位。它不是传统意义上的“词”，而是模型将人类可读的文字序列（如句子、段落）切分、编码并最终用于学习和生成文本的离散符号表示。理解 Token 的概念对于深入了解 LLMs 的工作原理、能力边界以及成本核算至关重要。  核心思想：LLMs 不直接处理原始文本，而是将其分解为一系列经过特殊编码的 Token。这些 Token 构成了模型输入和输出的最小单元，并直接影响模型的性能、效率和成本。   一、什么是 Token？在自然语言处理 (NLP) 领域，尤其是在 LLMs 中，Token 是指模型进行训练和推理时所使用的文本片段。它可能是：  一个完整的词 (Word)：例如 “cat”, “run”。 一个词的一部分 (Subword)：例如 “un”, “believe”, “able” 组合成 “unbelievable”。 一个标点符号 (Punctuation)：例如 “.”, “,”, “!”。 一个特殊符号或控制字符 (Special Token)：例如 [CLS]...</div></div></div></a><a class="pagination-related" href="/33aeea5bfccf/" title="大语言模型参数详解：规模、类型与意义"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-01-22</div><div class="info-item-2">大语言模型参数详解：规模、类型与意义</div></div><div class="info-2"><div class="info-item-1"> 参数 (Parameters) 是大型语言模型 (Large Language Models, LLMs) 的核心组成部分，它们是模型在训练过程中从海量数据中学习到的数值权重和偏置。这些参数共同构成了模型的“知识”和“理解”能力。参数的规模，尤其是数量，是衡量一个 LLM 大小的关键指标，并直接影响其性能、能力边界以及所需的计算资源。  核心思想：LLMs 的“智能”并非来自于明确的编程规则，而是通过在海量数据上优化数亿甚至数万亿个可学习参数而涌现。这些参数以分布式形式存储了语言的语法、语义、事实知识和世界常识。   一、什么是大语言模型参数？在神经网络的上下文中，参数是指模型在训练过程中需要学习和调整的所有权重 (weights) 和偏置 (biases)。它们是连接神经元之间强度的数值表示，决定了模型的输入如何被转换、处理并最终生成输出。  权重 (Weights)：定义了输入特征（或前一层神经元的输出）对当前神经元输出的贡献程度。一个较大的权重意味着该输入特征对结果有更强的影响。 偏置 (Biases)：是一种加性项，允许激活函数在不依赖任何输入的情况下被激活。它相当于调...</div></div></div></a><a class="pagination-related" href="/58479316819e/" title="多轮对话与上下文记忆详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-31.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-27</div><div class="info-item-2">多轮对话与上下文记忆详解</div></div><div class="info-2"><div class="info-item-1"> 在构建基于大型语言模型 (LLM) 的交互式应用时，仅仅能够进行单次问答是远远不够的。为了实现自然、流畅且富有意义的交流，我们需要让 LLM 能够进行多轮对话，并且记住并理解对话的先前内容，即拥有上下文记忆 (Context Memory)。这使得 LLM 能够在理解历史信息的基础上对新问题做出连贯且相关的响应。  核心思想：多轮对话要求 LLM “记住”之前的交流内容，并通过各种 “记忆策略” (例如拼接、总结、检索) 来将相关上下文传递给每次新的模型调用，从而实现连贯且智能的交互。    一、什么是多轮对话 (Multi-turn Conversation)多轮对话 指的是用户与 AI 之间的一系列相互关联、彼此依赖的交流轮次。与单轮对话（一次提问，一次回答，对话结束）不同，多轮对话中的每一次交互都会受到先前对话内容的影响，并且会为后续对话提供新的上下文。 特点：  连续性：多个请求和响应构成一个逻辑流，而非孤立的事件。 上下文依赖：用户后续的提问或指令常常省略先前已经提及的信息，需要 AI 自动关联。 共同状态维护：用户和 AI 在对话过程中逐渐建立起对某个主题或任务的共...</div></div></div></a><a class="pagination-related" href="/8be247b5e9ae/" title="微调大模型 (Finetuning LLMs) 详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-21.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-20</div><div class="info-item-2">微调大模型 (Finetuning LLMs) 详解</div></div><div class="info-2"><div class="info-item-1"> 微调 (Finetuning) 是人工智能领域，特别是大语言模型 (LLM) 领域中的一项关键技术。它指的是在预训练好的大型模型基础上，使用特定任务或领域的数据集进一步训练模型的过程。通过微调，我们可以将一个通用的、在海量数据上学习到基础语言理解和生成能力的大模型，高效地适配到具体的场景需求，从而显著提升模型在该特定任务上的性能、准确性和可靠性。  核心思想：微调的核心在于利用通用大模型强大的“基础能力”，并通过小规模、高质量的领域数据进行“二次开发”，使其专业化。对于LLM而言，参数高效微调 (PEFT) 极大降低了微调的资源门槛，使其在实践中变得可行且高效。   一、为什么需要微调大模型？通用大语言模型（如 GPT-系列、Llama、Mistral 等）在预训练阶段学习了海量的文本数据，拥有强大的泛化能力、语言理解能力和常识。然而，它们在直接应用于特定任务或领域时仍存在局限性：  知识截止日期 (Knowledge Cut-off)：预训练数据通常有截止日期，模型无法获取最新信息。 幻觉 (Hallucination)：模型可能会生成看似合理但实际上错误或捏造的信息。 领域...</div></div></div></a><a class="pagination-related" href="/30bb7c0cff3b/" title="Transformer 模型深度详解"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-30.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-14</div><div class="info-item-2">Transformer 模型深度详解</div></div><div class="info-2"><div class="info-item-1"> Transformer 模型由 Google Brain 团队在 2017 年的论文 “Attention Is All You Need” 中提出。它彻底改变了自然语言处理 (NLP) 领域，并成为了当前大语言模型 (LLM) 的基石。Transformer 模型以其强大的并行计算能力和卓越的长距离依赖建模能力，取代了传统的循环神经网络 (RNN) 和长短期记忆网络 (LSTM) 结构，成为了序列建模任务的主流架构。  核心思想：Transformer 放弃了传统的循环和卷积结构，完全依赖于注意力机制 (Attention Mechanism)来捕捉输入序列中的依赖关系。通过精心设计的自注意力 (Self-Attention) 机制，模型能够同时关注输入序列中的所有位置，从而实现高效的并行计算和对任意距离依赖的有效建模。   一、为什么需要 Transformer？在 Transformer 出现之前，RNN 及其变体 (如 LSTM 和 GRU) 是序列建模任务的主流。然而，它们存在一些固有的局限性：  顺序依赖：RNN 必须顺序地处理序列中的每个元素，后一个元素的计算依赖...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">TeaTang</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">443</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">225</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">80</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/teatang"><i class="fab fa-github"></i><span>GitHub主页</span></a><div class="card-info-social-icons"><a class="social-icon" href="mailto:tea.tang1211@gmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fas fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">网站更多功能即将上线，敬请期待！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-text">一、为什么需要文档嵌入模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%B8%8E%E6%96%B9%E6%B3%95"><span class="toc-text">二、文档嵌入的核心概念与方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E4%B8%8E%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-text">2.1 向量空间与相似性度量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%B8%8A%E4%B8%8B%E6%96%87%E5%85%B3%E8%81%94-Contextualization"><span class="toc-text">2.2 上下文关联 (Contextualization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0-Transfer-Learning"><span class="toc-text">2.3 迁移学习 (Transfer Learning)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E9%95%BF%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E8%83%BD%E5%8A%9B"><span class="toc-text">2.4 长文本处理能力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%86%E7%B1%BB%E4%B8%8E%E6%BC%94%E8%BF%9B"><span class="toc-text">三、文档嵌入模型的分类与演进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E5%9F%BA%E4%BA%8E%E8%AF%8D%E8%A2%8B-Bag-of-Words-%E5%8F%8A%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95"><span class="toc-text">3.1 1. 基于词袋 (Bag-of-Words) 及统计方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2 2. 深度学习模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E5%9F%BA%E4%BA%8E-Transformer-%E7%BC%96%E7%A0%81%E5%99%A8-Encoder-based-Transformers"><span class="toc-text">3.2.1 基于 Transformer 编码器 (Encoder-based Transformers)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-%E4%B8%93%E4%B8%BA%E9%95%BF%E5%BA%8F%E5%88%97%E8%AE%BE%E8%AE%A1%E7%9A%84-Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2.2 专为长序列设计的 Transformer 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-%E5%95%86%E4%B8%9A-API-%E5%9E%8B%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B"><span class="toc-text">3.2.3 商业 API 型嵌入模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E7%9A%84%E7%94%9F%E6%88%90%E8%BF%87%E7%A8%8B"><span class="toc-text">四、文档嵌入的生成过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">五、文档嵌入模型的应用场景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81Python-%E5%AE%9E%E8%B7%B5%EF%BC%9A%E4%BD%BF%E7%94%A8-sentence-transformers-%E8%BF%9B%E8%A1%8C%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5"><span class="toc-text">六、Python 实践：使用 sentence-transformers 进行文档嵌入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E6%96%87%E6%A1%A3%E5%B5%8C%E5%85%A5%E7%9A%84%E6%8C%91%E6%88%98%E4%B8%8E%E8%80%83%E9%87%8F"><span class="toc-text">七、文档嵌入的挑战与考量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%80%BB%E7%BB%93"><span class="toc-text">八、总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-06.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CFFI (C Foreign Function Interface for Python) 详解"/></a><div class="content"><a class="title" href="/0c48c5fa5942/" title="CFFI (C Foreign Function Interface for Python) 详解">CFFI (C Foreign Function Interface for Python) 详解</a><time datetime="2025-12-23T22:24:00.000Z" title="发表于 2025-12-24 06:24:00">2025-12-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-32.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="行为驱动开发 (BDD) 详解"/></a><div class="content"><a class="title" href="/03cebd3cc28a/" title="行为驱动开发 (BDD) 详解">行为驱动开发 (BDD) 详解</a><time datetime="2025-12-21T22:24:00.000Z" title="发表于 2025-12-22 06:24:00">2025-12-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-07.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="测试驱动开发 (TDD) 详解"/></a><div class="content"><a class="title" href="/7bb082434be0/" title="测试驱动开发 (TDD) 详解">测试驱动开发 (TDD) 详解</a><time datetime="2025-12-19T22:24:00.000Z" title="发表于 2025-12-20 06:24:00">2025-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-01.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="IPFS (InterPlanetary File System) 详解"/></a><div class="content"><a class="title" href="/24ffc0bedb41/" title="IPFS (InterPlanetary File System) 详解">IPFS (InterPlanetary File System) 详解</a><time datetime="2025-12-16T22:24:00.000Z" title="发表于 2025-12-17 06:24:00">2025-12-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"><img src="/img/loading.gif" data-lazy-src="/img/cover/default_cover-13.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="L7 负载均衡详解 (Layer 7 Load Balancing Explained)"/></a><div class="content"><a class="title" href="/974825c9c64f/" title="L7 负载均衡详解 (Layer 7 Load Balancing Explained)">L7 负载均衡详解 (Layer 7 Load Balancing Explained)</a><time datetime="2025-12-14T22:24:00.000Z" title="发表于 2025-12-15 06:24:00">2025-12-15</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/cover/default_cover-19.jpg);"><div class="footer-flex"><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">我的轨迹</div><div class="footer-flex-content"><a href="/archives/2023/" target="_blank" title="📌 2023">📌 2023</a><a href="/archives/2024/" target="_blank" title="❓ 2024">❓ 2024</a><a href="/archives/2025/" target="_blank" title="🚀 2025">🚀 2025</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">维度</div><div class="footer-flex-content"><a href="/categories/" target="_blank" title="分类">分类</a><a href="/tags/" target="_blank" title="标签">标签</a><a href="/categories/" target="_blank" title="时间线">时间线</a></div></div></div><div class="footer-flex-items"><div class="footer-flex-item"><div class="footer-flex-title">其他</div><div class="footer-flex-content"><a href="/shuoshuo" target="_blank" title="说说">说说</a></div></div></div></div><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2023 - 2025 By TeaTang</span><span class="framework-info"><span class="footer-separator">|</span><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.3"></script><script src="/js/main.js?v=5.5.3"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@6.1.7/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const runMermaid = ele => {
    window.loadMermaid = true
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

    ele.forEach((item, index) => {
      const mermaidSrc = item.firstElementChild
      const config = mermaidSrc.dataset.config ? JSON.parse(mermaidSrc.dataset.config) : {}
      if (!config.theme) {
        config.theme = theme
      }
      const mermaidThemeConfig = `%%{init: ${JSON.stringify(config)}}%%\n`
      const mermaidID = `mermaid-${index}`
      const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent

      const renderFn = mermaid.render(mermaidID, mermaidDefinition)
      const renderMermaid = svg => {
        mermaidSrc.insertAdjacentHTML('afterend', svg)
      }

      // mermaid v9 and v10 compatibility
      typeof renderFn === 'string' ? renderMermaid(renderFn) : renderFn.then(({ svg }) => renderMermaid(svg))
    })
  }

  const codeToMermaid = () => {
    const codeMermaidEle = document.querySelectorAll('pre > code.mermaid')
    if (codeMermaidEle.length === 0) return

    codeMermaidEle.forEach(ele => {
      const preEle = document.createElement('pre')
      preEle.className = 'mermaid-src'
      preEle.hidden = true
      preEle.textContent = ele.textContent
      const newEle = document.createElement('div')
      newEle.className = 'mermaid-wrap'
      newEle.appendChild(preEle)
      ele.parentNode.replaceWith(newEle)
    })
  }

  const loadMermaid = () => {
    if (true) codeToMermaid()
    const $mermaid = document.querySelectorAll('#article-container .mermaid-wrap')
    if ($mermaid.length === 0) return

    const runMermaidFn = () => runMermaid($mermaid)
    btf.addGlobalFn('themeChange', runMermaidFn, 'mermaid')
    window.loadMermaid ? runMermaidFn() : btf.getScript('https://cdn.jsdelivr.net/npm/mermaid@11.12.2/dist/mermaid.min.js').then(runMermaidFn)
  }

  btf.addGlobalFn('encrypt', loadMermaid, 'mermaid')
  window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
})()</script><script>(() => {
  const applyThemeDefaultsConfig = theme => {
    if (theme === 'dark-mode') {
      Chart.defaults.color = "rgba(255, 255, 255, 0.8)"
      Chart.defaults.borderColor = "rgba(255, 255, 255, 0.2)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    } else {
      Chart.defaults.color = "rgba(0, 0, 0, 0.8)"
      Chart.defaults.borderColor = "rgba(0, 0, 0, 0.1)"
      Chart.defaults.scale.ticks.backdropColor = "transparent"
    }
  }

  // Recursively traverse the config object and automatically apply theme-specific color schemes
  const applyThemeConfig = (obj, theme) => {
    if (typeof obj !== 'object' || obj === null) return

    Object.keys(obj).forEach(key => {
      const value = obj[key]
      // If the property is an object and has theme-specific options, apply them
      if (typeof value === 'object' && value !== null) {
        if (value[theme]) {
          obj[key] = value[theme] // Apply the value for the current theme
        } else {
          // Recursively process child objects
          applyThemeConfig(value, theme)
        }
      }
    })
  }

  const runChartJS = ele => {
    window.loadChartJS = true

    Array.from(ele).forEach((item, index) => {
      const chartSrc = item.firstElementChild
      const chartID = item.getAttribute('data-chartjs-id') || ('chartjs-' + index) // Use custom ID or default ID
      const width = item.getAttribute('data-width')
      const existingCanvas = document.getElementById(chartID)

      // If a canvas already exists, remove it to avoid rendering duplicates
      if (existingCanvas) {
          existingCanvas.parentNode.remove()
      }

      const chartDefinition = chartSrc.textContent
      const canvas = document.createElement('canvas')
      canvas.id = chartID

      const div = document.createElement('div')
      div.className = 'chartjs-wrap'

      if (width) {
        div.style.width = width
      }

      div.appendChild(canvas)
      chartSrc.insertAdjacentElement('afterend', div)

      const ctx = document.getElementById(chartID).getContext('2d')

      const config = JSON.parse(chartDefinition)

      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark-mode' : 'light-mode'

      // Set default styles (initial setup)
      applyThemeDefaultsConfig(theme)

      // Automatically traverse the config and apply dual-mode color schemes
      applyThemeConfig(config, theme)

      new Chart(ctx, config)
    })
  }

  const loadChartJS = () => {
    const chartJSEle = document.querySelectorAll('#article-container .chartjs-container')
    if (chartJSEle.length === 0) return

    window.loadChartJS ? runChartJS(chartJSEle) : btf.getScript('https://cdn.jsdelivr.net/npm/chart.js@4.5.1/dist/chart.umd.min.js').then(() => runChartJS(chartJSEle))
  }

  // Listen for theme change events
  btf.addGlobalFn('themeChange', loadChartJS, 'chartjs')
  btf.addGlobalFn('encrypt', loadChartJS, 'chartjs')

  window.pjax ? loadChartJS() : document.addEventListener('DOMContentLoaded', loadChartJS)
})()</script></div><script data-pjax src="/self/btf.js"></script><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="ture"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-fluttering-ribbon.min.js"></script><script id="canvas_nest" defer="defer" color="0,200,200" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js" defer="defer"></script><script>document.addEventListener('DOMContentLoaded', () => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => {
      try {
        fn()
      } catch (err) {
        console.debug('Pjax callback failed:', err)
      }
    })
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      true
        ? pjax.loadUrl('/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})</script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><i class="fas fa-spinner fa-pulse" id="loading-status" hidden="hidden"></i><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="local-search-input"><input placeholder="搜索文章" type="text"/></div><hr/><div id="local-search-results"></div><div class="ais-Pagination" id="local-search-pagination" style="display:none;"><ul class="ais-Pagination-list"></ul></div><div id="local-search-stats"></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=5.5.3"></script></div></div></body></html>